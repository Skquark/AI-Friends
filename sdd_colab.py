import os, subprocess, sys, shutil, re, argparse
import random as rnd
from typing import Optional
from pathlib import Path

parser = argparse.ArgumentParser()
parser.add_argument("--storage_type", type=str, default="Colab Google Drive")
parser.add_argument("--saved_settings_json", type=str, default="/content/drive/MyDrive/AI/Stable_Diffusion/sdd-settings.json")
parser.add_argument("--tunnel_type", type=str, default="localtunnel")
parser.add_argument("--auto_launch_website", default=False, action='store_true')
flags = parser.parse_args()
storage_type = flags.storage_type
saved_settings_json = flags.saved_settings_json
if not bool(saved_settings_json): saved_settings_json = "/content/drive/MyDrive/AI/Stable_Diffusion/sdd-settings.json"
tunnel_type = flags.tunnel_type
auto_launch_website = flags.auto_launch_website
save_to_GDrive = True
force_updates = True
newest_flet = True
upgrade_torch = True
SDD_version = "v1.9.0"
from IPython.display import clear_output
root_dir = '/content/'
dist_dir = root_dir
is_Colab = True
try:
  import google.colab
  from google.colab import output
  output.enable_custom_widget_manager()
  root_dir = '/content/'
except:
  root_dir = os.getcwd()
  dist_dir = os.path.join(root_dir, 'dist', 'Stable-Diffusion-Deluxe')
  if not os.path.isdir(dist_dir):
    dist_dir = root_dir
  print(f'Root: {root_dir} Dist:{dist_dir}')
  is_Colab = False
  pass
stable_dir = root_dir
env = os.environ.copy()
def run_sp(cmd_str, cwd=None, realtime=False, output_column=None):
  cmd_list = cmd_str if type(cmd_str) is list else cmd_str.split()
  cwd_arg = {} if cwd is None else {'cwd': cwd}
  if realtime or output_column != None:
    process = subprocess.Popen(cmd_str, shell=True, env=env, bufsize=1, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, encoding='utf-8', errors='replace', **cwd_arg) 
    while True:
      realtime_output = process.stdout.readline()
      if realtime_output == '' and process.poll() is not None:
        break
      if realtime_output:
        if not output_column:
            print(realtime_output.strip(), flush=False)
        else:
            from flet import Text
            output_column.controls.append(Text(realtime_output.strip()))
            output_column.update()
        sys.stdout.flush()
  else:
    returned = subprocess.run(cmd_list, stdout=subprocess.PIPE, env=env, **cwd_arg).stdout.decode('utf-8')
    if 'ERROR' in returned:
      print(f"Error Running {cmd_str} - {returned}")
    return returned

save_to_GDrive = storage_type == "Colab Google Drive"
if save_to_GDrive:
  if not os.path.isdir(f'{root_dir}drive'):
    from google.colab import drive
    drive.mount('/content/drive')
elif storage_type == "PyDrive Google Drive":
  "pip install PyDrive2"
stable_dir = os.path.join(root_dir, 'Stable_Diffusion')
if not os.path.exists(stable_dir):
  os.makedirs(stable_dir)
uploads_dir = os.path.join(root_dir, "uploads")
if not os.path.exists(uploads_dir):
  os.makedirs(uploads_dir)
sample_data = '/content/sample_data'
if os.path.exists(sample_data):
  for f in os.listdir(sample_data):
    os.remove(os.path.join(sample_data, f))
  os.rmdir(sample_data)
#os.chdir(stable_dir)
#loaded_Stability_api = False
#loaded_img2img = False
#use_Stability_api = False
def version_checker():
  response = requests.get("https://raw.githubusercontent.com/Skquark/AI-Friends/main/DSD_version.txt")
  current_v = response.text.strip()
  if current_v != SDD_version:
    print(f'A new update is available. You are running {SDD_version} and {current_v} is up. We recommended refreshing Stable Diffusion Deluxe for the latest cool features or fixes.\nhttps://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb\nChangelog if interested: https://github.com/Skquark/AI-Friends/commits/main/Stable_Diffusion_Deluxe.ipynb')
def ng():
  response = requests.get("https://raw.githubusercontent.com/Skquark/AI-Friends/main/_ng")
  ng_list = response.text.strip().split('\n')
  _ng = rnd.choice(ng_list).partition('_')
  return _ng[2]+_ng[1]+_ng[0]

from urllib.parse import urlparse, unquote
def download_file(url: str, to: Optional[str] = None, filename: Optional[str] = None, 
                  raw: bool = True, ext: str = "png", replace: bool = False) -> str:
    if filename is None:
        parsed_url = urlparse(url)
        filename = os.path.basename(unquote(parsed_url.path))
        if not filename:
            filename = "downloaded_file"
        if '?' in filename:
            filename = filename.split('?')[0]
    if '.' not in filename:
        filename = f"{filename}.{ext}"
    to = to or uploads_dir#os.getcwd()
    local_filename = os.path.join(to, filename)
    os.makedirs(to, exist_ok=True)
    if os.path.isfile(local_filename) and not replace:
        return local_filename
    with requests.get(url, stream=True) as r:
        r.raise_for_status()
        with open(local_filename, 'wb') as f:
            if raw:
                shutil.copyfileobj(r.raw, f)
            else:
                f.write(r.content)
    return local_filename

def wget(url, to):
    res = subprocess.run(['wget', '-q', url, '-O', to], stdout=subprocess.PIPE).stdout.decode('utf-8')

try:
  import flet
  from packaging import version
  if version.parse(flet.version.version) < version.parse("0.23.1"):
    raise ImportError("Must upgrade Flet")
except ImportError as e:
  if newest_flet:
    run_sp("pip install --upgrade --quiet flet==0.23.1", realtime=False)
    #run_sp("pip install --upgrade flet==0.22.1", realtime=False)
    #0.19.0, 0.21.2, 0.22.0, 0.22.1, 0.23.0.dev2664, 0.23.0.dev2665, 0.23.0.dev2666, 0.23.0.dev2679, 0.23.0.dev2697, 0.23.0.dev2734, 0.23.0.dev2744, 0.23.0.dev2766, 0.23.0.dev2768, 0.23.0.dev2868, 0.23.0.dev2870, 0.23.0.dev2872, 0.23.0.dev2880)
  else:
    run_sp("pip install --upgrade flet==0.3.2", realtime=False)
  #run_sp("pip install -i https://test.pypi.org/simple/ flet")
  #run_sp("pip install --upgrade git+https://github.com/flet-dev/flet.git@controls-s3#egg=flet-dev")
  #run_sp("pip install --upgrade flet_ivid")
  pass
if is_Colab:
  try:
    import nest_asyncio
  except ModuleNotFoundError:
    run_sp("pip install nest_asyncio", realtime=False)
  finally:
    import nest_asyncio
    nest_asyncio.apply()
    pass
'''try:
  from flet_ivid import VideoContainer
except ModuleNotFoundError:
  run_sp("pip install --upgrade flet_ivid")
  from flet_ivid import VideoContainer
  pass'''
try:
  import requests
except ModuleNotFoundError:
  run_sp("pip install -q requests", realtime=True)
  import requests
  pass
try:
  from emoji import emojize
except ImportError as e:
  run_sp("pip install emoji --quiet", realtime=False)
  from emoji import emojize
  pass
if 'url' not in locals():
  url=""
if tunnel_type == "ngrok":
  try:
    import pyngrok
  except ImportError as e:
    run_sp("pip install pyngrok --quiet", realtime=False)
    #run_sp(f"ngrok authtoken {ng()}", realtime=False)
    run_sp(f"ngrok config add-authtoken {ng()}", realtime=False)
    run_sp("ngrok config upgrade", realtime=False)
    import pyngrok
    pass
elif tunnel_type == "localtunnel":
  if not bool(url):
    import re
    run_sp("npm install -g -q localtunnel", realtime=False)
    #localtunnel = subprocess.Popen(['lt', '--port', '80', 'http'], stdout=subprocess.PIPE)
    #url = str(localtunnel.stdout.readline())
    #url = (re.search("(?P<url>https?:\/\/[^\s]+loca.lt)", url).group("url"))
    #print(url)

gdrive = None
if storage_type == "PyDrive Google Drive":
  if not os.path.isfile(Google_OAuth_client_secret_json):
    raise ValueError("Couldn't locate your client_secret.json file to authenticate. Follow instructions below then copy certificate to your root dir.")
  try:
    from pydrive2.auth import GoogleAuth, ServiceAccountCredentials
    from pydrive2.drive import GoogleDrive
    from oauth2client.contrib.gce import AppAssertionCredentials
  except ImportError as e:
    run_sp("pip install PyDrive2 -q")
    from pydrive2.auth import GoogleAuth, ServiceAccountCredentials
    from pydrive2.drive import GoogleDrive
    from oauth2client.contrib.gce import AppAssertionCredentials
    pass
  import httplib2

  old_local_webserver_auth = GoogleAuth.LocalWebserverAuth
  def LocalWebServerAuth(self, *args, **kwargs):
      if isinstance(self.credentials, AppAssertionCredentials):
          self.credentials.refresh(httplib2.Http())
          return
      return old_local_webserver_auth(self, *args, **kwargs)
  GoogleAuth.LocalWebserverAuth = LocalWebServerAuth

  #scope = 'https://www.googleapis.com/auth/drive'
  #credentials = ServiceAccountCredentials.from_json_keyfile_name(Google_OAuth_client_secret_json, scope)
  gauth = GoogleAuth()
  gauth.LoadCredentialsFile(Google_OAuth_client_secret_json)
  #gauth.LocalWebserverAuth()
  if is_Colab: gauth.CommandLineAuth()
  else:
    gauth.LocalWebserverAuth()
    gauth.SaveCredentialsFile(Google_OAuth_client_secret_json)
  gdrive = GoogleDrive(gauth)
slash = '/'
from pathlib import Path
if not is_Colab:
    image_output = os.path.join(Path.home(), "Pictures", "Stable_Diffusion")
    if "\\" in image_output:
        slash = '\\'
else:
    image_output = '/content/drive/MyDrive/AI/Stable_Diffusion/images_out'

favicon = os.path.join(root_dir, "favicon.png")
loading_animation = os.path.join(root_dir, "icons", "loading-animation.png")
assets = os.path.join(root_dir, "assets")
if not os.path.isfile(favicon):
    download_file("https://github.com/Skquark/AI-Friends/blob/main/assets/favicon.png?raw=true")
if not os.path.isfile(loading_animation):
    download_file("https://github.com/Skquark/AI-Friends/blob/main/assets/loading-animation.png?raw=true", to=os.path.join(root_dir, "icons"))
if not os.path.exists(assets):
    os.makedirs(assets)
    download_file("https://github.com/Skquark/AI-Friends/blob/main/assets/snd-alert.mp3?raw=true", to=assets)
    download_file("https://github.com/Skquark/AI-Friends/blob/main/assets/snd-delete.mp3?raw=true", to=assets)
    download_file("https://github.com/Skquark/AI-Friends/blob/main/assets/snd-error.mp3?raw=true", to=assets)
    download_file("https://github.com/Skquark/AI-Friends/blob/main/assets/snd-done.mp3?raw=true", to=assets)
    download_file("https://github.com/Skquark/AI-Friends/blob/main/assets/snd-drop.mp3?raw=true", to=assets)
#clear_output()

import json
prefs = {}
def load_settings_file():
  global prefs
  if os.path.isfile(saved_settings_json):
    with open(saved_settings_json) as settings:
      prefs = json.load(settings)
    print("Successfully loaded settings json...")
  else:
    print("Settings file not found, starting with defaults...")
    prefs = {
      'save_to_GDrive': True,
      'image_output': image_output,
      'file_prefix': 'sd-',
      'file_suffix_seed': False,
      'file_max_length': 220,
      'file_allowSpace': False,
      'file_datetime': False,
      'file_from_1': False,
      'save_image_metadata': True,
      'meta_ArtistName':'',
      'meta_Copyright': '',
      'save_config_in_metadata': True,
      'save_config_json': False,
      'theme_mode': 'Dark',
      'theme_color': 'Green',
      'theme_custom_color': '#69d9ab',
      'enable_sounds': True,
      'show_stats': False,
      'stats_used': True,
      'stats_update': 5,
      'slider_stack': False,
      'start_in_installation': False,
      'disable_nsfw_filter': True,
      'retry_attempts': 3,
      'HuggingFace_api_key': "",
      'Stability_api_key': "",
      'OpenAI_api_key': "",
      'PaLM_api_key': "",
      'Anthropic_api_key': "",
      'TextSynth_api_key': "",
      'Replicate_api_key': "",
      'AIHorde_api_key': "0000000000",
      'Perplexity_api_key': "",
      'luma_api_key': "",
      'HuggingFace_username': "",
      'scheduler_mode': "DDIM",
      'higher_vram_mode': False,
      'enable_xformers': False,
      'enable_attention_slicing': True,
      'enable_bitsandbytes': False,
      'memory_optimization': "None",
      'sequential_cpu_offload': False,
      'vae_slicing': True,
      'vae_tiling': False,
      'enable_torch_compile': False,
      'enable_stable_fast': False,
      'enable_tome': False,
      'tome_ratio': 0.5,
      'enable_freeu': False,
      'freeu_args': {'b1': 1.2, 'b2':1.4, 's1':0.9, 's2':0.2},
      'enable_hidiffusion': False,
      'enable_deepcache': False,
      'cache_dir': '',
      'install_diffusers': True,
      'install_interpolation': False,
      'install_text2img': False,
      'install_img2img': False,
      'install_SDXL': True,
      'install_SD3': False,
      'install_megapipe': True,
      'install_CLIP_guided': False,
      'install_OpenAI': False,
      'install_TextSynth': False,
      'install_dreamfusion': False,
      'install_repaint': False,
      'install_imagic': False,
      'install_composable': False,
      'install_safe': False,
      'install_versatile': False,
      'install_depth2img': False,
      'install_alt_diffusion': False,
      'install_attend_and_excite': False,
      'install_SAG': False,
      'install_panorama': False,
      'install_upscale': False,
      'upscale_method': 'Real-ESRGAN',
      'upscale_model': 'realesr-general-x4v3',
      'safety_config': 'Strong',
      'use_imagic': False,
      'SD_compel': False,
      'use_SDXL': False,
      'SDXL_high_noise_frac': 0.7,
      'SDXL_negative_conditions': False,
      'SDXL_compel': False,
      'SDXL_watermark': False,
      'SDXL_model': 'SDXL-Base v1',
      'SDXL_custom_model': '',
      'SDXL_custom_models': [],
      'use_SD3': True,
      'SD3_compel': False,
      'SD3_model': 'Stable Diffusion 3 Medium',
      'SD3_custom_model': '',
      'SD3_custom_models': [],
      'SD3_cpu_offload': True,
      'SD3_bitsandbytes_8bit': False,
      'use_composable': False,
      'use_safe': False,
      'use_versatile': False,
      'use_alt_diffusion': False,
      'use_attend_and_excite': False,
      'max_iter_to_alter': 25,
      'use_SAG': False,
      'sag_scale': 0.75,
      'use_panorama': False,
      'panorama_width': 2048,
      'panorama_circular_padding': False,
      'use_upscale': False,
      'upscale_noise_level': 20,
      'install_conceptualizer': False,
      'use_conceptualizer': False,
      'concepts_model': 'cat-toy',
      'use_ip_adapter': False,
      'ip_adapter_image': '',
      'ip_adapter_model': 'SD v1.5',
      'ip_adapter_SDXL_model': 'SDXL',
      'ip_adapter_strength': 0.8,
      'model_ckpt': 'Stable Diffusion v1.5',
      'finetuned_model': 'Midjourney v4 style',
      'dreambooth_model': 'disco-diffusion-style',
      'custom_model': '',
      'custom_models': [],
      'tortoise_custom_voices': [],
      'custom_dance_diffusion_models': [],
      'clip_model_id': "laion/CLIP-ViT-B-32-laion2B-s34B-b79K",
      'install_Stability_api': False,
      'use_Stability_api': False,
      'model_checkpoint': "Stable Diffusion 3",
      'generation_sampler': "K_EULER_ANCESTRAL",
      'clip_guidance_preset': "FAST_BLUE",
      'install_AIHorde_api': False,
      'use_AIHorde_api': False,
      'AIHorde_model': 'stable_diffusion',
      'AIHorde_sampler': 'k_euler_a',
      'AIHorde_post_processing': "None",
      'AIHorde_karras': False,
      'AIHorde_tiling': False,
      'AIHorde_transparent': False,
      'AIHorde_hires_fix': False,
      'AIHorde_strip_background': False,
      'AIHorde_lora_layer': 'Horde Aesthetics Improver',
      'AIHorde_lora_layer_alpha': 1.0,
      'AIHorde_custom_lora_layer': '',
      'custom_CivitAI_LoRA_models': [],
      'AIHorde_lora_map': [],
      'AIHorde_use_controlnet': False,
      'AIHorde_controlnet': "Canny",
      'install_ESRGAN': True,
      'batch_folder_name': "",
      'batch_size': 1,
      'n_iterations': 1,
      'steps': 50,
      'eta': 0.4,
      'seed': 0,
      'guidance_scale': 8,
      'width': 960,
      'height': 512,
      'init_image': "",
      'mask_image': "",
      'init_image_strength': 0.25,
      'alpha_mask': False,
      'invert_mask': False,
      'negative_prompt': "",
      'precision': 'autocast',
      'use_inpaint_model': False,
      'centipede_prompts_as_init_images': False,
      'use_depth2img': False,
      'use_LoRA_model': False,
      'LoRA_model': 'Von Platen LoRA',
      'active_LoRA_layers': [],
      'active_SDXL_LoRA_layers': [],
      'active_SD3_LoRA_layers': [],
      'custom_LoRA_models': [],
      'custom_LoRA_model': "",
      'SDXL_LoRA_model': 'Papercut SDXL',
      'custom_SDXL_LoRA_models': [],
      'custom_SDXL_LoRA_model': "",
      'SD3_LoRA_model': '',
      'custom_SD3_LoRA_models': [],
      'custom_SD3_LoRA_model': "",
      'use_interpolation': False,
      'num_interpolation_steps': 22,
      'use_clip_guided_model': False,
      'clip_guidance_scale': 571,
      'use_cutouts': True,
      'num_cutouts': 4,
      'unfreeze_unet': True,
      'unfreeze_vae': True,
      'apply_ESRGAN_upscale': True,
      'enlarge_scale': 1.5,
      'face_enhance':False,
      'display_upscaled_image': False,
      'negatives': ["Blurry"],
      'custom_negatives': "",
      'prompt_styler': '',
      'prompt_style': 'cinematic-default',
      'prompt_styles': ['cinematic-default'],
      'prompt_styler_multi': False,
      'prompt_list': [],
      'prompt_generator': {
          'phrase': '',
          'subject_detail': '',
          'phrase_as_subject': False,
          'amount': 10,
          'random_artists': 0,
          'random_styles': 0,
          'permutate_artists': False,
          'request_mode': 3,
          'AI_temperature': 0.8,
          'AI_engine': "ChatGPT-3.5 Turbo",
          'AIHorde_model': "LLaMA-13B-Psyfighter2",
          'Perplexity_model': "llama-3-sonar-small-32k-chat",
          'economy_mode': True,
      },
      'prompt_remixer': {
          'seed_prompt': '',
          'optional_about_influencer': '',
          'amount': 10,
          'random_artists': 0,
          'random_styles': 0,
          'permutate_artists': False,
          'request_mode': 3,
          'AI_temperature': 0.8,
          'AI_engine': "ChatGPT-3.5 Turbo",
          'AIHorde_model': "LLaMA-13B-Psyfighter2",
          'Perplexity_model': "llama-3-sonar-small-32k-chat",
      },
      'prompt_brainstormer': {
          'AI_engine': 'ChatGPT-3.5 Turbo',
          'about_prompt': '',
          'request_mode': 'Brainstorm',
          'AI_temperature': 0.8,
          'AIHorde_model': "LLaMA-13B-Psyfighter2",
          'Perplexity_model': "llama-3-sonar-small-32k-chat",
      },
      'prompt_writer': {
          'art_Subjects': '',
          'negative_prompt': '',
          'by_Artists': '',
          'art_Styles': '',
          'amount': 10,
          'random_artists': 2,
          'random_styles': 1,
          'permutate_artists': False,
      },
    }

load_settings_file()
#version_checker()




#@title ## **▶️ Run Stable Diffusion Deluxe** - Flet/Flutter WebUI App
import flet as ft
#from flet import *
import flet.canvas as cv
from flet import Page, View, Column, Row, ResponsiveRow, Container, Text, Stack, TextField, Checkbox, Switch, Image, ElevatedButton, FilledButton, IconButton, Markdown, Tab, Tabs, AppBar, Divider, VerticalDivider, GridView, Tooltip, SnackBar, AnimatedSwitcher, ButtonStyle, FloatingActionButton, Audio, Theme, Dropdown, Slider, ListTile, ListView, TextButton, PopupMenuButton, PopupMenuItem, AlertDialog, Banner, Icon, ProgressBar, ProgressRing, GestureDetector, KeyboardEvent, FilePicker, FilePickerResultEvent, FilePickerUploadFile, FilePickerUploadEvent, UserControl, Ref
from flet import icons, dropdown, colors, padding, margin, alignment, border_radius, theme, animation, KeyboardType, TextThemeStyle, AnimationCurve
from flet import TextAlign, FontWeight, ClipBehavior, MainAxisAlignment, CrossAxisAlignment, ScrollMode, ImageFit, ThemeMode, BlendMode
from flet import Image as Img
try:
    import PIL
except ModuleNotFoundError:
    run_sp("pip install Pillow", realtime=False)
    run_sp("pip install image", realtime=False)
    import PIL
    pass
from PIL import Image as PILImage # Avoids flet conflict
if not hasattr(PILImage, 'Resampling'):  # Allow Pillow<9.0
   PILImage.Resampling = PILImage
import random as rnd
import io, shutil, traceback, string, gc, datetime, time, threading
try:
    import packaging
except ModuleNotFoundError:
    run_sp("pip install packaging==23.2", realtime=False)
    pass
from packaging import version
from contextlib import redirect_stdout
from typing import Optional
try:
  import numpy as np
except ModuleNotFoundError:
  run_sp("pip install numpy", realtime=False)
  import numpy as np
  pass
try:
    import psutil
except ModuleNotFoundError:
    run_sp("pip install -U psutil", realtime=False)
    import psutil
    pass
try:
    import cv2
except ModuleNotFoundError:
    run_sp("pip install opencv-python", realtime=False)
    import cv2
    pass

if 'prefs' not in locals():
    raise ValueError("Setup not initialized. Run the previous code block first and authenticate your Drive storage.")
status = {
    'installed_diffusers': False,
    'installed_txt2img': False,
    'installed_img2img': False,
    'installed_SDXL': False,
    'installed_SD3': False,
    'installed_stability': False,
    'installed_AIHorde': False,
    'installed_megapipe': False,
    'installed_interpolation': False,
    'installed_clip': False,
    'installed_ESRGAN': False,
    'installed_OpenAI': False,
    'installed_TextSynth': False,
    'installed_conceptualizer': False,
    'installed_dreamfusion': False,
    'installed_repaint': False,
    'installed_imagic': False,
    'installed_composable': False,
    'installed_safe': False,
    'installed_versatile': False,
    'installed_depth2img': False,
    'installed_attend_and_excite': False,
    'installed_SAG': False,
    'installed_panorama': False,
    'installed_alt_diffusion': False,
    'installed_upscale': False,
    'installed_xformers': False,
    'finetuned_model': False,
    'loaded_scheduler': '',
    'loaded_model': '',
    'loaded_task': '',
    'loaded_controlnet': '',
    'loaded_controlnet_type': '',
    'loaded_SDXL': '',
    'loaded_SDXL_model': '',
    'loaded_SD3': '',
    'loaded_SD3_model': '',
    'loaded_ip_adapter_mode': '',
    'loaded_ip_adapter': '',
    'changed_prefs': False,
    'kandinsky_3': 'Kandinsky 3.0',
    'kandinsky_version': 'Kandinsky 3.0',
    'kandinsky_2_2': True,
    'kandinsky_fuse_2_2': True,
    'gpu_used': None,
    'gpu_memory': None,
    'cpu_used': None,
    'cpu_memory': None,
    'initialized': False,
    'updated': [],
}

if 'last_updated' in prefs and prefs['last_updated'] is not None:
    try:
        last_time = datetime.datetime.strptime(prefs['last_updated'], '%Y-%m-%dT%H:%M:%S.%fZ')
    except ValueError:
        force_updates = True
        pass
    #diff = datetime.datetime.now() - last_time
    #force_updates = force_updates or (last_time < datetime.datetime.now() - datetime.timedelta(days=4))
    if last_time <  datetime.datetime.now() - datetime.timedelta(days=4):
        force_updates = True
else:
    force_updates = True

sdd_utils_py = os.path.join(root_dir, "sdd_utils.py")
sdd_components_py = os.path.join(root_dir, "sdd_components.py")
if not os.path.exists(sdd_utils_py) or force_updates:
    download_file("https://raw.githubusercontent.com/Skquark/AI-Friends/main/sdd_utils.py", to=root_dir, raw=False, replace=True)
if not os.path.exists(sdd_components_py): #or force_updates
    download_file("https://raw.githubusercontent.com/Skquark/AI-Friends/main/sdd_components.py", to=root_dir, raw=False, replace=True)
try:
    from flet_contrib.color_picker import ColorPicker
except ModuleNotFoundError: #Also flexible_slider, vertical_splitter, shimmer
    run_sp("pip install --upgrade flet-contrib", realtime=False)
    from flet_contrib.color_picker import ColorPicker
    pass
#sys.path.append(sdd_utils_py)
import sdd_utils
from sdd_utils import LoRA_models, SDXL_models, SDXL_LoRA_models, finetuned_models, dreambooth_models, styles, artists, concepts, Real_ESRGAN_models, SwinIR_models, SD_XL_BASE_RATIOS, AIHorde_models, CivitAI_LoRAs, SD3_models
from sdd_components import PanZoom, VideoContainer
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning, module="pip._vendor.packaging.specifiers")
warnings.filterwarnings("ignore", category=DeprecationWarning)
def save_settings_file(page, change_icon=True):
    if change_icon:
        page.app_icon_save()
    #print(os.path.dirname(saved_settings_json))
    try:
        if not os.path.isfile(saved_settings_json):
            settings_path = os.path.dirname(saved_settings_json) #saved_settings_json.rpartition(slash)[0]
            os.makedirs(settings_path, exist_ok=True)
        with open(saved_settings_json, "w") as write_file:
            json.dump(prefs, write_file, indent=4)
    except Exception as e:
        alert_msg(page, f"ERROR: Something went wrong saving settings file...", content=Column([Text(f"Path: {settings_path} | Json: {saved_settings_json}"), Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        print(str(traceback.format_exc()))
        pass

current_tab = 1 if prefs['start_in_installation'] else 0
def tab_on_change(e):
    global current_tab, status
    t = e.control
    #print (f"tab changed from {current_tab} to: {t.selected_index}")
    #print(str(t.tabs[t.selected_index].text))
    if current_tab == 1:
      e.page.show_install_fab(False)
    if current_tab == 2:
      if status['changed_prefs']:
        update_args()
        e.page.update_prompts()
      e.page.show_apply_fab(False)
    if current_tab == 3:
      if status['changed_prefs']:
        e.page.save_prompts()
      e.page.show_run_diffusion_fab(False, p=e.page)
    if status['changed_prefs']:
      save_settings_file(e.page)
      status['changed_prefs'] = False

    current_tab = t.selected_index
    if current_tab == 1:
      refresh_installers(e.page.Installers.controls[0].content.controls)
      e.page.show_install_fab(True)
      #page.Installers.init_boxes()
    if current_tab == 2:
      update_parameters(e.page)
      #for p in e.page.Parameters.content.controls:
      e.page.Parameters.controls[0].content.update()
      e.page.Parameters.update()
      e.page.show_apply_fab(len(prompts) > 0 and status['changed_prefs'])
    if current_tab == 3:
      e.page.show_run_diffusion_fab(True, p=e.page) #len(prompts) > 0
    e.page.update()

def tab_changed(e):
    global status
    t = e.control
    if status['changed_prefs']:
      save_settings_file(e.page)
      status['changed_prefs'] = False

def buildTabs(page):
    page.Settings = buildSettings(page)
    page.Installers = buildInstallers(page)
    page.Parameters = buildParameters(page)
    page.PromptsList = buildPromptsList(page)
    page.PromptHelpers = buildPromptHelpers(page)
    page.Images = buildImages(page)
    page.ImageAIs = buildImageAIs(page)
    page.Trainers = buildTrainers(page)
    page.VideoAIs = buildVideoAIs(page)
    page.AudioAIs = buildAudioAIs(page)
    page.Text3DAIs = build3DAIs(page)
    page.Extras = buildExtras(page)
    page.PromptHelpers.on_change = tab_changed
    
    t = Tabs(selected_index=1 if prefs['start_in_installation'] else 0, animation_duration=300, expand=1,
        tabs=[
            Tab(text="Settings", content=page.Settings, icon=icons.SETTINGS_OUTLINED),
            Tab(text="Installation", content=page.Installers, icon=icons.INSTALL_DESKTOP),
            Tab(text="Image Parameters", content=page.Parameters, icon=icons.DISPLAY_SETTINGS),
            Tab(text="Prompts List", content=page.PromptsList, icon=icons.FORMAT_LIST_BULLETED),
            Tab(text="Generate Images", content=page.Images, icon=icons.IMAGE_OUTLINED),
            Tab(text="Prompt Helpers", content=page.PromptHelpers, icon=icons.BUBBLE_CHART_OUTLINED),
            Tab(text="Image AIs", content=page.ImageAIs, icon=icons.PALETTE),
            Tab(text="Video AIs", content=page.VideoAIs, icon=icons.VIDEO_CAMERA_BACK),
            Tab(text="3D AIs", content=page.Text3DAIs, icon=icons.VIEW_IN_AR),
            Tab(text="Audio AIs", content=page.AudioAIs, icon=icons.EQUALIZER),
            Tab(text="AI Trainers", content=page.Trainers, icon=icons.TSUNAMI),
            Tab(text="Extras", content=page.Extras, icon=icons.ALL_INBOX),
        ],
    )
    page.tabs = t
    return t

def buildPromptHelpers(page):
    page.generator = buildPromptGenerator(page)
    page.remixer = buildPromptRemixer(page)
    page.brainstormer = buildPromptBrainstormer(page)
    page.writer = buildPromptWriter(page)
    page.negatives = buildNegatives(page)
    page.styler = buildPromptStyler(page)
    page.Image2Text = buildImage2Text(page)
    page.MagicPrompt = buildMagicPrompt(page)
    page.DistilGPT2 = buildDistilGPT2(page)
    page.SuperPrompt = buildSuperPrompt(page)
    page.RetrievePrompts = buildRetrievePrompts(page)
    page.InitFolder = buildInitFolder(page)
    page.InitVideo = buildInitVideo(page)
    page.BLIP2Image2Text = buildBLIP2Image2Text(page)
    promptTabs = Tabs(selected_index=0, animation_duration=300, expand=1,
        tabs=[
            Tab(text="Prompt Writer", content=page.writer, icon=icons.CLOUD_CIRCLE),
            Tab(text="Prompt Generator", content=page.generator, icon=icons.CLOUD),
            Tab(text="Prompt Remixer", content=page.remixer, icon=icons.CLOUD_SYNC_ROUNDED),
            Tab(text="Prompt Brainstormer", content=page.brainstormer, icon=icons.CLOUDY_SNOWING),
            Tab(text="Prompt Styler", content=page.styler, icon=icons.FORMAT_COLOR_FILL),
            Tab(text="Negatives", content=page.negatives, icon=icons.REMOVE_CIRCLE),
            Tab(text="Image2Text", content=page.Image2Text, icon=icons.WRAP_TEXT),
            Tab(text="Magic Prompt", content=page.MagicPrompt, icon=icons.AUTO_FIX_HIGH),
            Tab(text="SuperPrompt", content=page.SuperPrompt, icon=icons.EV_STATION),
            Tab(text="Distil GPT-2", content=page.DistilGPT2, icon=icons.FILTER_ALT),
            Tab(text="Retrieve Prompt from Image", content=page.RetrievePrompts, icon=icons.PHOTO_LIBRARY_OUTLINED),
            Tab(text="Init Images from Folder", content=page.InitFolder, icon=icons.FOLDER_SPECIAL),
            Tab(text="Init Images from Video", content=page.InitVideo, icon=icons.SWITCH_VIDEO),
            Tab(text="BLIP2 Image2Text", content=page.BLIP2Image2Text, icon=icons.BATHTUB),
        ],
    )
    return promptTabs

def buildImageAIs(page):
    page.RePainter = buildRepainter(page)
    page.unCLIP = buildUnCLIP(page)
    page.unCLIP_Interpolation = buildUnCLIP_Interpolation(page)
    page.unCLIP_ImageInterpolation = buildUnCLIP_ImageInterpolation(page)
    page.UnCLIP_ImageVariation = buildUnCLIP_ImageVariation(page)
    page.BLIPDiffusion = buildBLIPDiffusion(page)
    page.AnyText = buildAnyText(page)
    page.IP_Adapter = buildIP_Adapter(page)
    page.HD_Painter = buildHD_Painter(page)
    page.Reference = buildReference(page)
    page.ControlNetQR = buildControlNetQR(page)
    page.ControlNetSegmentAnything = buildControlNetSegmentAnything(page)
    page.Null_Text = buildNull_Text(page)
    page.EDICT = buildEDICT(page)
    page.DiffEdit = buildDiffEdit(page)
    page.ImageVariation = buildImageVariation(page)
    page.CLIPstyler = buildCLIPstyler(page)
    page.MagicMix = buildMagicMix(page)
    page.SemanticGuidance = buildSemanticGuidance(page)
    page.DemoFusion = buildDemoFusion(page)
    page.PaintByExample = buildPaintByExample(page)
    page.InstructPix2Pix = buildInstructPix2Pix(page)
    page.LEdits = buildLEdits(page)
    page.ControlNet = buildControlNet(page)
    page.ControlNetXL = buildControlNetXL(page)
    page.ControlNetSD3 = buildControlNetSD3(page)
    page.ControlNetXS = buildControlNetXS(page)
    page.DeepFloyd = buildDeepFloyd(page)
    page.Amused = buildAmused(page)
    page.Wuerstchen = buildWuerstchen(page)
    page.StableCascade = buildStableCascade(page)
    page.PixArtAlpha = buildPixArtAlpha(page)
    page.PixArtSigma = buildPixArtSigma(page)
    page.Hunyuan = buildHunyuanDiT(page)
    page.Lumina = buildLuminaNext(page)
    page.Kolors = buildKolors(page)
    page.AuraFlow = buildAuraFlow(page)
    page.LayerDiffusion = buildLayerDiffusion(page)
    page.Differential_Diffusion = buildDifferential_Diffusion(page)
    page.LMD_Plus = buildLMD_Plus(page)
    page.LCM = buildLCM(page)
    page.LCMInterpolation = buildLCMInterpolation(page)
    page.InstaFlow = buildInstaFlow(page)
    page.PAG = buildPAG(page)
    page.MaterialDiffusion = buildMaterialDiffusion(page)
    page.DallE = buildDallE3(page)
    page.DallE2 = buildDallE2(page)
    page.DallE3 = buildDallE3(page)
    page.Kandinsky = buildKandinsky3(page) if status['kandinsky_version'] == "Kandinsky 3.0" else buildKandinsky(page)
    page.KandinskyFuse = buildKandinskyFuse(page) if status['kandinsky_fuse_2_2'] else buildKandinsky21Fuse(page)
    page.KandinskyControlNet = buildKandinskyControlNet(page)
    page.DiT = buildDiT(page)
    page.TaskMatrix = buildTaskMatrix(page)
    page.DreamFusion = buildDreamFusion(page)
    page.Point_E = buildPoint_E(page)
    page.Shap_E = buildShap_E(page)
    page.InstantNGP = buildInstantNGP(page)
    page.DeepDaze = buildDeepDaze(page)
    diffusersTabs = Tabs(selected_index=0, animation_duration=300, expand=1,
        tabs=[
            Tab(text="Instruct Pix2Pix", content=page.InstructPix2Pix, icon=icons.SOLAR_POWER),
            Tab(text="ControlNet", content=page.ControlNet, icon=icons.HUB),
            Tab(text="ControlNet SDXL", content=page.ControlNetXL, icon=icons.PEST_CONTROL),
            Tab(text="ControlNet SD3", content=page.ControlNetSD3, icon=icons.ARCHITECTURE),
            Tab(text="ControlNet-XS", content=page.ControlNetXS, icon=icons.WEBHOOK),
            Tab(text="Kandinsky", content=page.Kandinsky, icon=icons.TOLL),
            Tab(text="Kandinsky Fuse", content=page.KandinskyFuse, icon=icons.FIREPLACE),
            Tab(text="Kandinsky ControlNet", content=page.KandinskyControlNet, icon=icons.CAMERA_ENHANCE),
            Tab(text="QRCode", content=page.ControlNetQR, icon=icons.QR_CODE_2),
            Tab(text="DALL•E", content=page.DallE, icon=icons.BLUR_ON),
            Tab(text="Lumina-Next", content=page.Lumina, icon=icons.SYNAGOGUE),
            Tab(text="Hunyuan-DiT", content=page.Hunyuan, icon=icons.TEMPLE_BUDDHIST),
            Tab(text="Stable Cascade", content=page.StableCascade, icon=icons.SPA),
            Tab(text="Würstchen", content=page.Wuerstchen, icon=icons.SAVINGS),
            Tab(text="aMUSEd", content=page.Amused, icon=icons.ATTRACTIONS),
            Tab(text="PixArt-Σ", content=page.PixArtSigma, icon=icons.FUNCTIONS),
            Tab(text="PixArt-α", content=page.PixArtAlpha, icon=icons.PIX),
            Tab(text="AuraFlow", content=page.AuraFlow, icon=icons.MONOCHROME_PHOTOS),
            Tab(text="Kolors", content=page.Kolors, icon=icons.DIRTY_LENS),
            Tab(text="Layer Diffusion", content=page.LayerDiffusion, icon=icons.WINE_BAR),
            Tab(text="Differential Diffusion", content=page.Differential_Diffusion, icon=icons.SENTIMENT_NEUTRAL),
            Tab(text="DemoFusion", content=page.DemoFusion, icon=icons.COTTAGE),
            Tab(text="DeepFloyd-IF", content=page.DeepFloyd, icon=icons.LOOKS),
            Tab(text="LMD+", content=page.LMD_Plus, icon=icons.HIGHLIGHT_ALT),
            Tab(text="LCM", content=page.LCM, icon=icons.MEMORY),
            Tab(text="LCM Interpolation", content=page.LCMInterpolation, icon=icons.TRANSFER_WITHIN_A_STATION),
            Tab(text="InstaFlow", content=page.InstaFlow, icon=icons.ELECTRIC_BOLT),
            Tab(text="PAG", content=page.PAG, icon=icons.SYNC_PROBLEM),
            Tab(text="unCLIP", content=page.unCLIP, icon=icons.ATTACHMENT_SHARP),
            Tab(text="unCLIP Interpolation", content=page.unCLIP_Interpolation, icon=icons.TRANSFORM),
            Tab(text="unCLIP Image Interpolation", content=page.unCLIP_ImageInterpolation, icon=icons.ANIMATION),
            Tab(text="unCLIP Image Variation", content=page.UnCLIP_ImageVariation, icon=icons.AIRLINE_STOPS),
            Tab(text="Image Variation", content=page.ImageVariation, icon=icons.FORMAT_COLOR_FILL),
            Tab(text="BLIP-Diffusion", content=page.BLIPDiffusion, icon=icons.RADAR),
            Tab(text="HD-Painter", content=page.HD_Painter, icon=icons.BRUSH),
            Tab(text="IP-Adapter", content=page.IP_Adapter, icon=icons.ROOM_PREFERENCES),
            Tab(text="Reference-Only", content=page.Reference, icon=icons.CRISIS_ALERT),
            Tab(text="Material Diffusion", content=page.MaterialDiffusion, icon=icons.TEXTURE),
            Tab(text="Re-Segment-Anything", content=page.ControlNetSegmentAnything, icon=icons.SEND_TIME_EXTENSION),
            Tab(text="LEdits++", content=page.LEdits, icon=icons.PUBLISHED_WITH_CHANGES),
            Tab(text="Null-Text", content=page.Null_Text, icon=icons.FORMAT_OVERLINE),
            Tab(text="EDICT Edit", content=page.EDICT, icon=icons.AUTO_AWESOME),
            Tab(text="DiffEdit", content=page.DiffEdit, icon=icons.AUTO_GRAPH),
            Tab(text="AnyText", content=page.AnyText, icon=icons.TEXT_ROTATE_VERTICAL),
            Tab(text="TaskMatrix", content=page.TaskMatrix, icon=icons.ADD_TASK),
            Tab(text="RePainter", content=page.RePainter, icon=icons.FORMAT_PAINT),
            Tab(text="MagicMix", content=page.MagicMix, icon=icons.BLENDER),
            Tab(text="Paint-by-Example", content=page.PaintByExample, icon=icons.FORMAT_SHAPES),
            Tab(text="CLIP-Styler", content=page.CLIPstyler, icon=icons.STYLE),
            Tab(text="Semantic Guidance", content=page.SemanticGuidance, icon=icons.ROUTE),
            #Tab(text="DALL•E 2", content=page.DallE2, icon=icons.BLUR_CIRCULAR),
            #Tab(text="DALL•E 3", content=page.DallE3, icon=icons.BLUR_ON),
            Tab(text="DiT", content=page.DiT, icon=icons.ANALYTICS),
            Tab(text="DeepDaze", content=page.DeepDaze, icon=icons.FACE),
        ],
    )
    return diffusersTabs

def build3DAIs(page):
    page.DreamFusion = buildDreamFusion(page)
    page.Point_E = buildPoint_E(page)
    page.Shap_E = buildShap_E(page)
    page.ZoeDepth = buildZoeDepth(page)
    page.MarigoldDepth = buildMarigoldDepth(page)
    page.Tripo = buildTripo(page)
    page.InstantMesh = buildInstantMesh(page)
    page.SplatterImage = buildSplatterImage(page)
    page.CRM = buildCRM(page)
    page.LDM3D = buildLDM3D(page)
    page.InstantNGP = buildInstantNGP(page)
    page.Meshy = buildMeshy(page)
    page.Luma = buildLuma(page)
    diffusersTabs = Tabs(selected_index=0, animation_duration=300, expand=1,
        tabs=[
            Tab(text="DreamFusion 3D", content=page.DreamFusion, icon=icons.THREED_ROTATION),
            Tab(text="Point-E 3D", content=page.Point_E, icon=icons.SWIPE_UP),
            Tab(text="Shap-E 3D", content=page.Shap_E, icon=icons.PRECISION_MANUFACTURING),
            Tab(text="ZoeDepth 3D", content=page.ZoeDepth, icon=icons.GRADIENT),
            Tab(text="MarigoldDepth", content=page.MarigoldDepth, icon=icons.FILTER_VINTAGE),
            Tab(text="Tripo", content=page.Tripo, icon=icons.CONNECTING_AIRPORTS),
            Tab(text="InstantMesh", content=page.InstantMesh, icon=icons.ELECTRIC_BOLT),
            Tab(text="Splatter Image", content=page.SplatterImage, icon=icons.DIRTY_LENS),
            Tab(text="CRM-3D", content=page.CRM, icon=icons.ENGINEERING),
            Tab(text="LDM3D", content=page.LDM3D, icon=icons.ROTATE_90_DEGREES_CW),
            Tab(text="Instant-NGP", content=page.InstantNGP, icon=icons.STADIUM),
            Tab(text="Meshy.ai", content=page.Meshy, icon=icons.IRON),
            Tab(text="Luma Video-to-3D", content=page.Luma, icon=icons.NIGHTS_STAY),
        ],
    )
    return diffusersTabs

def buildTrainers(page):
    page.DreamBooth = buildDreamBooth(page)
    page.TexualInversion = buildTextualInversion(page)
    page.LoRA_Dreambooth = buildLoRA_Dreambooth(page)
    page.LoRA = buildLoRA(page)
    page.Converter = buildConverter(page)
    page.CheckpointMerger = buildCheckpointMerger(page)
    trainersTabs = Tabs(selected_index=0, animation_duration=300, expand=1,
        tabs=[
            Tab(text="LoRA DreamBooth", content=page.LoRA_Dreambooth, icon=icons.SETTINGS_BRIGHTNESS),
            Tab(text="LoRA", content=page.LoRA, icon=icons.SETTINGS_SYSTEM_DAYDREAM),
            Tab(text="DreamBooth", content=page.DreamBooth, icon=icons.PHOTO),
            Tab(text="Texual-Inversion", content=page.TexualInversion, icon=icons.PHOTO_ALBUM),
            Tab(text="Model Converter", content=page.Converter, icon=icons.PUBLISHED_WITH_CHANGES),
            Tab(text="Checkpoint Merger", content=page.CheckpointMerger, icon=icons.JOIN_FULL),
        ],
    )
    return trainersTabs

def buildVideoAIs(page):
    page.TextToVideo = buildTextToVideo(page)
    page.TextToVideoZero = buildTextToVideoZero(page)
    page.VideoToVideo = buildVideoToVideo(page)
    page.InfiniteZoom = buildInfiniteZoom(page)
    page.Potat1 = buildPotat1(page)
    page.Latte = buildLatte(page)
    page.StableAnimation = buildStableAnimation(page)
    page.SVD = buildSVD(page)
    page.AnimateDiffImage2Video = buildAnimateDiffImage2Video(page)
    page.AnimateDiffSDXL = buildAnimateDiffSDXL(page)
    page.DiffSynth = buildDiffSynth(page)
    page.PIA = buildPIA(page)
    page.EasyAnimate = buildEasyAnimate(page)
    page.I2VGenXL = buildI2VGenXL(page)
    page.ControlNet = buildControlNet(page)
    page.ControlNet_Video2Video = buildControlNet_Video2Video(page)
    page.TemporalNet_XL = buildTemporalNet_XL(page)
    page.Roop = buildROOP(page)
    page.Hallo = buildHallo(page)
    page.OpenSoraPlan = buildOpenSoraPlan(page)
    page.VideoInfinity = buildVideoInfinity(page)
    page.Video_ReTalking = buildVideoReTalking(page)
    page.LivePortrait = buildLivePortrait(page)
    page.StyleCrafter = buildStyleCrafter(page)
    page.RAVE = buildRAVE(page)
    page.Fresco = buildFrescoV2V(page)
    page.TokenFlow = buildTokenFlow(page)
    page.AnimateDiff = buildAnimateDiff(page)
    page.HotshotXL = buildHotshotXL(page)
    page.Rerender_a_video = buildRerender_a_video(page)

    videoAIsTabs = Tabs(selected_index=0, animation_duration=300, expand=1,
        tabs=[
            Tab(text="AnimateDiff", content=page.AnimateDiff, icon=icons.AUTO_MODE),
            Tab(text="Stable Animation", content=page.StableAnimation, icon=icons.SHUTTER_SPEED),
            Tab(text="SVD Image-to-Video", content=page.SVD, icon=icons.SLOW_MOTION_VIDEO),
            Tab(text="AnimateDiff to-Video", content=page.AnimateDiffImage2Video, icon=icons.CATCHING_POKEMON),
            Tab(text="AnimateDiff SDXL", content=page.AnimateDiffSDXL, icon=icons.TWO_WHEELER),
            Tab(text="DiffSynth", content=page.DiffSynth, icon=icons.FIREPLACE),
            Tab(text="EasyAnimate", content=page.EasyAnimate, icon=icons.PHOTO_CAMERA_BACK),
            Tab(text="Open-Sora", content=page.OpenSoraPlan, icon=icons.GRASS),
            Tab(text="I2VGen-XL", content=page.I2VGenXL, icon=icons.TIPS_AND_UPDATES),
            Tab(text="PIA Image Animator", content=page.PIA, icon=icons.EMERGENCY_RECORDING),
            Tab(text="Text-to-Video", content=page.TextToVideo, icon=icons.MISSED_VIDEO_CALL),
            Tab(text="Text-to-Video Zero", content=page.TextToVideoZero, icon=icons.ONDEMAND_VIDEO),
            Tab(text="Latte", content=page.Latte, icon=icons.COFFEE),
            Tab(text="Potat1", content=page.Potat1, icon=icons.FILTER_1),
            Tab(text="ROOP Face-Swap", content=page.Roop, icon=icons.FACE_RETOUCHING_NATURAL),
            Tab(text="Hallo", content=page.Hallo, icon=icons.WAVING_HAND),
            Tab(text="Video-ReTalking", content=page.Video_ReTalking, icon=icons.RECORD_VOICE_OVER),
            Tab(text="LivePortrait", content=page.LivePortrait, icon=icons.FACE_2),
            Tab(text="Infinite Zoom", content=page.InfiniteZoom, icon=icons.ZOOM_IN_MAP),
            Tab(text="Video-Infinity", content=page.VideoInfinity, icon=icons.ALL_INCLUSIVE),
            Tab(text="FRESCO", content=page.Fresco, icon=icons.PARK),
            Tab(text="StyleCrafter", content=page.StyleCrafter, icon=icons.HIGHLIGHT),
            Tab(text="RAVE", content=page.RAVE, icon=icons.FLUTTER_DASH),
            Tab(text="Rerender-a-Video", content=page.Rerender_a_video, icon=icons.MEMORY),
            Tab(text="TokenFlow", content=page.TokenFlow, icon=icons.TOKEN),
            Tab(text="Hotshot-XL", content=page.HotshotXL, icon=icons.HOT_TUB),
            Tab(text="ControlNet Video2Video", content=page.ControlNet_Video2Video, icon=icons.PSYCHOLOGY),
            Tab(text="Video-to-Video", content=page.VideoToVideo, icon=icons.CAMERA_ROLL),
            Tab(text="TemporalNet-XL", content=page.TemporalNet_XL, icon=icons.HOURGLASS_BOTTOM),
            Tab(text="ControlNet Init-Video", content=page.ControlNet, icon=icons.HUB),
        ],
    )
    return videoAIsTabs

def buildAudioAIs(page):
    page.TortoiseTTS = buildTortoiseTTS(page)
    page.MusicGen = buildMusicGen(page)
    page.DanceDiffusion = buildDanceDiffusion(page)
    page.AudioDiffusion = buildAudioDiffusion(page)
    page.AudioLDM = buildAudioLDM(page)
    page.AudioLDM2 = buildAudioLDM2(page)
    page.ZetaEditing = buildZetaEditing(page)
    page.MusicLDM = buildMusicLDM(page)
    page.StableAudio = buildStableAudio(page)
    page.Bark = buildBark(page)
    page.Riffusion = buildRiffusion(page)
    page.MusicLang = buildMusicLang(page)
    page.Mubert = buildMubert(page)
    page.Whisper = buildWhisper(page)
    page.OpenAI_TTS = buildOpenAI_TTS(page)
    page.VoiceFixer = buildVoiceFixer(page)
    audioAIsTabs = Tabs(selected_index=0, animation_duration=300, expand=1,
        tabs=[
            Tab(text="Tortoise-TTS", content=page.TortoiseTTS, icon=icons.RECORD_VOICE_OVER),
            Tab(text="MusicLDM", content=page.MusicLDM, icon=icons.EARBUDS),
            Tab(text="Stable Audio", content=page.StableAudio, icon=icons.MIC_EXTERNAL_ON),
            Tab(text="AudioLDM", content=page.AudioLDM, icon=icons.NOISE_AWARE),
            Tab(text="AudioLDM-2", content=page.AudioLDM2, icon=icons.NOISE_CONTROL_OFF),
            Tab(text="ZETA Editing", content=page.ZetaEditing, icon=icons.HEADSET),
            Tab(text="Bark", content=page.Bark, icon=icons.PETS),
            Tab(text="Riffusion", content=page.Riffusion, icon=icons.SPATIAL_AUDIO),
            Tab(text="Audio Diffusion", content=page.AudioDiffusion, icon=icons.GRAPHIC_EQ),
            Tab(text="MusicLang", content=page.MusicLang, icon=icons.PIANO),
            Tab(text="MusicGen", content=page.MusicGen, icon=icons.MUSIC_NOTE),
            Tab(text="Whisper-STT", content=page.Whisper, icon=icons.HEARING),
            Tab(text="OpenAI-TTS", content=page.OpenAI_TTS, icon=icons.PHONE_IN_TALK),
            Tab(text="Voice Fixer", content=page.VoiceFixer, icon=icons.VOICE_CHAT),
            Tab(text="HarmonAI Dance Diffusion", content=page.DanceDiffusion, icon=icons.QUEUE_MUSIC),
            Tab(text="Mubert Music", content=page.Mubert, icon=icons.MUSIC_VIDEO),
        ],
    )
    return audioAIsTabs

def buildExtras(page):
    page.ESRGAN_upscaler = buildESRGANupscaler(page)
    page.CachedModelManager = buildCachedModelManager(page)
    page.CustomModelManager = buildCustomModelManager(page)
    page.MaskMaker = buildDreamMask(page)
    page.BackgroundRemover = buildBackgroundRemover(page)
    page.reGen = buildHordeWorker(page)
    #page.Kandinsky21 = buildKandinsky21(page)
    #page.Kandinsky21Fuse = buildKandinsky21Fuse(page)
    extrasTabs = Tabs(selected_index=0, animation_duration=300, expand=1,
        tabs=[
            Tab(text="Real-ESRGAN Batch Upscaler", content=page.ESRGAN_upscaler, icon=icons.PHOTO_SIZE_SELECT_LARGE),
            Tab(text="Model Manager", content=page.CustomModelManager, icon=icons.DIFFERENCE),
            Tab(text="Cache Manager", content=page.CachedModelManager, icon=icons.CACHED),
            #Tab(text="Dream Mask Maker", content=page.MaskMaker, icon=icons.GRADIENT),
            Tab(text="Background Remover", content=page.BackgroundRemover, icon=icons.WALLPAPER),
            Tab(text="Horde Worker reGen", content=page.reGen, icon=icons.BACKUP),
            #Tab(text="Kandinsky 2.1", content=page.Kandinsky21, icon=icons.AC_UNIT),
            #Tab(text="Kandinsky Fuse", content=page.Kandinsky21Fuse, icon=icons.FIREPLACE),
        ],
    )
    return extrasTabs

def print_tabs(page, detailed=False):
    for t in page.tabs.tabs:
        print(f"{t.text}:")
        if isinstance(t.content, Tabs):
            tabs = []
            for s in t.content.tabs:
                try:
                    title = s.text
                    if detailed:
                        header = s.content.controls[0].content.controls[0]
                        if isinstance(header, Header):
                            description = header.title
                            if bool(header.subtitle):
                                description += f" - {header.subtitle}"
                            print(description)
                        #title += f":{s.content}"
                    tabs.append(title)
                except Exception as e:
                    print(e)
                    pass
            print(and_list(tabs))

def b_style():
    return ButtonStyle(elevation=8)
def dict_diff(dict1, dict2):
    return {k: v for k, v in dict1.items() if k in dict2 and v != dict2[k]}
def arg_diffs(dict1, dict2):
    diff = dict_diff(dict1, dict2)
    if len(diff) > 0:
      dif = []
      for k, v in diff.items():
        dif.append(f'{to_title(k)}: {v}')
      return ', '.join(dif)
    else: return None
def get_color(color):
    if color == "green": return colors.GREEN
    elif color == "blue": return colors.BLUE
    elif color == "indigo": return colors.INDIGO
    elif color == "red": return colors.RED
    elif color == "purple": return colors.DEEP_PURPLE
    elif color == "orange": return colors.ORANGE
    elif color == "amber": return colors.AMBER
    elif color == "brown": return colors.BROWN
    elif color == "teal": return colors.TEAL
    elif color == "yellow": return colors.YELLOW
    elif color == "custom": return prefs['theme_custom_color']


# Delete these after everyone's updated
prefs.setdefault('negative_prompt', '')
prefs.setdefault('file_datetime', False)
prefs.setdefault('file_from_1', False)
prefs.setdefault('theme_custom_color', '#69d9ab')
prefs.setdefault('install_conceptualizer', False)
prefs.setdefault('use_conceptualizer', False)
prefs.setdefault('concepts_model', 'cat-toy')
prefs.setdefault('memory_optimization', 'None')
prefs.setdefault('enable_xformers', False)
prefs.setdefault('enable_attention_slicing', True)
prefs.setdefault('enable_bitsandbytes', False)
prefs.setdefault('sequential_cpu_offload', False)
prefs.setdefault('vae_slicing', True)
prefs.setdefault('vae_tiling', False)
prefs.setdefault('show_stats', False)
prefs.setdefault('stats_used', True)
prefs.setdefault('stats_update', 5)
prefs.setdefault('upscale_method', 'Real-ESRGAN')
prefs.setdefault('upscale_model', 'realesr-general-x4v3')
prefs.setdefault('use_inpaint_model', False)
prefs.setdefault('cache_dir', '')
prefs.setdefault('Replicate_api_key', '')
prefs.setdefault('Perplexity_api_key', '')
prefs.setdefault('install_dreamfusion', False)
prefs.setdefault('install_repaint', False)
prefs.setdefault('finetuned_model', 'Midjourney v4 style')
prefs.setdefault('dreambooth_model', 'disco-diffusion-style')
prefs.setdefault('custom_model', '')
prefs.setdefault('custom_models', [])
prefs.setdefault('start_in_installation', False)
prefs.setdefault('slider_stack', False)
prefs.setdefault('install_imagic', False)
prefs.setdefault('use_imagic', False)
prefs.setdefault('install_composable', False)
prefs.setdefault('use_composable', False)
prefs.setdefault('install_safe', False)
prefs.setdefault('use_safe', False)
prefs.setdefault('safety_config', "Strong")
prefs.setdefault('install_versatile', False)
prefs.setdefault('use_versatile', False)
prefs.setdefault('install_depth2img', False)
prefs.setdefault('use_depth2img', False)
prefs.setdefault('install_alt_diffusion', False)
prefs.setdefault('use_alt_diffusion', False)
prefs.setdefault('install_upscale', False)
prefs.setdefault('use_upscale', False)
prefs.setdefault('upscale_noise_level', 20)
prefs.setdefault('alpha_mask', False)
prefs.setdefault('invert_mask', False)
prefs.setdefault('clip_guidance_preset', "FAST_BLUE")
prefs.setdefault('tortoise_custom_voices', [])
prefs.setdefault('use_LoRA_model', False)
prefs.setdefault('LoRA_model', "Von Platen LoRA")
prefs.setdefault('active_LoRA_layers', [])
prefs.setdefault('active_SDXL_LoRA_layers', [])
prefs.setdefault('active_SD3_LoRA_layers', [])
prefs.setdefault('custom_LoRA_models', [])
prefs.setdefault('custom_LoRA_model', '')
prefs.setdefault('SDXL_LoRA_model', "Papercut SDXL")
prefs.setdefault('custom_SDXL_LoRA_models', [])
prefs.setdefault('custom_SDXL_LoRA_model', '')
prefs.setdefault('SD3_LoRA_model', "")
prefs.setdefault('custom_SD3_LoRA_models', [])
prefs.setdefault('custom_SD3_LoRA_model', '')
prefs.setdefault('custom_dance_diffusion_models', [])
prefs['prompt_writer'].setdefault('negative_prompt', '')
prefs.setdefault('install_attend_and_excite', False)
prefs.setdefault('use_attend_and_excite', False)
prefs.setdefault('max_iter_to_alter', 25)
prefs.setdefault('install_SAG', False)
prefs.setdefault('use_SAG', False)
prefs.setdefault('sag_scale', 0.75)
prefs.setdefault('SD_compel', False)
prefs.setdefault('install_SDXL', True)
prefs.setdefault('use_SDXL', False)
prefs.setdefault('SDXL_high_noise_frac', 0.7)
prefs.setdefault('SDXL_compel', False)
prefs.setdefault('SDXL_negative_conditions', False)
prefs.setdefault('SDXL_watermark', False)
prefs.setdefault('SDXL_model', 'SDXL-Base v1')
prefs.setdefault('SDXL_custom_model', '')
prefs.setdefault('install_SD3', False)
prefs.setdefault('use_SD3', True)
prefs.setdefault('SD3_compel', False)
prefs.setdefault('SD3_model', 'Stable Diffusion 3 Medium')
prefs.setdefault('SD3_custom_model', '')
prefs.setdefault('SD3_custom_models', [])
prefs.setdefault('SD3_cpu_offload', True)
prefs.setdefault('SD3_bitsandbytes_8bit', False)
prefs.setdefault('install_panorama', False)
prefs.setdefault('use_panorama', False)
prefs.setdefault('panorama_circular_padding', False)
prefs.setdefault('panorama_width', 2048)
prefs['prompt_generator'].setdefault('AI_engine', 'OpenAI ChatGPT')
prefs['prompt_remixer'].setdefault('AI_engine', 'OpenAI ChatGPT')
prefs['prompt_brainstormer'].setdefault('AI_engine', 'OpenAI ChatGPT')
prefs.setdefault('meshy_api_key', '')
prefs.setdefault('luma_api_key', '')
prefs.setdefault('AIHorde_api_key', '0000000000')
prefs.setdefault('install_AIHorde_api', False)
prefs.setdefault('use_AIHorde_api', False)
prefs.setdefault('AIHorde_model', 'stable_diffusion')
prefs.setdefault('AIHorde_sampler', 'k_euler_a')
prefs.setdefault('AIHorde_post_processing', "None")
prefs.setdefault('AIHorde_lora_layer', 'Horde Aesthetics Improver')
prefs.setdefault('AIHorde_lora_layer_alpha', 1.0)
prefs.setdefault('AIHorde_karras', False)
prefs.setdefault('AIHorde_tiling', False)
prefs.setdefault('AIHorde_transparent', False)
prefs.setdefault('AIHorde_hires_fix', False)
prefs.setdefault('AIHorde_strip_background', False)
prefs.setdefault('custom_CivitAI_LoRA_models', [])
prefs.setdefault('AIHorde_custom_lora_layer', '')
prefs.setdefault('AIHorde_lora_map', [])
prefs.setdefault('AIHorde_use_controlnet', False)
prefs.setdefault('AIHorde_controlnet', 'Canny')
prefs['prompt_generator'].setdefault('AIHorde_model', 'LLaMA-13B-Psyfighter2')
prefs['prompt_remixer'].setdefault('AIHorde_model', 'LLaMA-13B-Psyfighter2')
prefs['prompt_brainstormer'].setdefault('AIHorde_model', 'LLaMA-13B-Psyfighter2')
prefs['prompt_generator'].setdefault('Perplexity_model', 'llama-3-sonar-small-32k-chat')
prefs['prompt_remixer'].setdefault('Perplexity_model', 'llama-3-sonar-small-32k-chat')
prefs['prompt_brainstormer'].setdefault('Perplexity_model', 'llama-3-sonar-small-32k-chat')
prefs['prompt_generator'].setdefault('TextSynth_model', 'Mixtral Instruct')
prefs['prompt_remixer'].setdefault('TextSynth_model', 'Mixtral Instruct')
prefs['prompt_brainstormer'].setdefault('TextSynth_model', 'Mixtral Instruct')
prefs['prompt_generator'].setdefault('OpenAI_model', 'GPT-4 Turbo')
prefs['prompt_remixer'].setdefault('OpenAI_model', 'GPT-4 Turbo')
prefs['prompt_brainstormer'].setdefault('OpenAI_model', 'GPT-4 Turbo')
prefs.setdefault('PaLM_api_key', '')
prefs.setdefault('Anthropic_api_key', '')
prefs.setdefault('enable_torch_compile', False)
prefs.setdefault('enable_stable_fast', False)
prefs.setdefault('enable_tome', False)
prefs.setdefault('tome_ratio', 0.5)
prefs.setdefault('enable_freeu', False)
prefs.setdefault('freeu_args', {'b1': 1.2, 'b2':1.4, 's1':0.9, 's2':0.2})
prefs.setdefault('enable_hidiffusion', False)
prefs.setdefault('enable_deepcache', False)
prefs.setdefault('negatives', ['Blurry'])
prefs.setdefault('custom_negatives', "")
prefs.setdefault('prompt_style', "cinematic-default")
prefs.setdefault('prompt_styles', ["cinematic-default"])
prefs.setdefault('prompt_styler', "")
prefs.setdefault('prompt_styler_multi', False)
prefs.setdefault('use_ip_adapter', False)
prefs.setdefault('ip_adapter_image', "")
prefs.setdefault('ip_adapter_model', "SD v1.5")
prefs.setdefault('ip_adapter_SDXL_model', "SDXL")
prefs.setdefault('ip_adapter_strength', 0.8)
try:
    int(prefs['seed'])
except ValueError:
    prefs['seed'] = 0

if bool(prefs['init_image']):
    if not os.path.isfile(prefs['init_image']): prefs['init_image'] = ""
if bool(prefs['mask_image']):
    if not os.path.isfile(prefs['mask_image']): prefs['mask_image'] = ""

from enum import Enum
class Snd(Enum):
    ALERT = 1
    DELETE = 2
    ERROR = 3
    DONE = 4
    DROP = 5

def play_snd(snd:Snd, page:Page):
    if prefs['enable_sounds']:
        if snd == Snd.ALERT: page.snd_alert.play()
        elif snd == Snd.DELETE: page.snd_delete.play()
        elif snd == Snd.ERROR: page.snd_error.play()
        elif snd == Snd.DONE: page.snd_done.play()
        elif snd == Snd.DROP: page.snd_drop.play()

def initState(page):
    global status, current_tab
    if os.path.isdir(os.path.join(root_dir, 'Real-ESRGAN')):
      status['installed_ESRGAN'] = True
      show_upscalers(page)
    page.load_prompts()
    # TODO: Try to load from assets folder
    page.snd_alert = Audio(src=asset_dir(os.path.join(assets, "snd-alert.mp3")), autoplay=False)
    page.snd_delete = Audio(src=asset_dir(os.path.join(assets, "snd-delete.mp3")), autoplay=False)
    page.snd_error = Audio(src=asset_dir(os.path.join(assets, "snd-error.mp3")), autoplay=False)
    page.snd_done = Audio(src=asset_dir(os.path.join(assets, "snd-done.mp3")), autoplay=False)
    page.snd_drop = Audio(src=asset_dir(os.path.join(assets, "snd-drop.mp3")), autoplay=False)
    #page.snd_notification = Audio(src="https://github.com/Skquark/AI-Friends/blob/main/assets/snd-notification.mp3?raw=true", autoplay=False)
    page.overlay.append(page.snd_alert)
    page.overlay.append(page.snd_delete)
    page.overlay.append(page.snd_error)
    page.overlay.append(page.snd_done)
    page.overlay.append(page.snd_drop)
    #page.overlay.append(page.snd_notification)
    #print("Running Init State")
    import importlib
    if '_PYIBoot_SPLASH' in os.environ and importlib.util.find_spec("pyi_splash"):
      import pyi_splash
      pyi_splash.update_text('Ready to get creative...')
      pyi_splash.close()
      #log.info('Splash screen closed.')
    if prefs['scheduler_mode'] != "DDIM":
      for eta in page.etas:
        if isinstance(eta, SliderRow):
          eta.show = False
        else:
          eta.visible = False
          eta.update()
    if prefs['start_in_installation'] and current_tab == 1:
      page.show_install_fab(True)
    '''  page.tabs.selected_index = 1
      page.tabs.update()
      page.update()
      current_tab = 1
    else:
      page.tabs.selected_index = 0
      page.tabs.update()
      page.update()'''
    if prefs['prompt_generator']['AI_engine']=="AI-Horde" or prefs['prompt_remixer']['AI_engine']=="AI-Horde" or prefs['prompt_brainstormer']['AI_engine']=="AI-Horde":
        load_horde_text(page, False)
    page.update()
    if prefs['show_stats']:
      start_thread(page)
      #start_polling(prefs['stats_update'], update_stats(page))
    #time.sleep(8)
    #page.update()
    #print_tabs(page, False)
    #show_upscalers(page)

def buildSettings(page):
  global prefs, status
  def open_url(e):
    page.launch_url(e.data)
  def save_settings(e):
    save_settings_file(e.page)
    page.snack_bar = SnackBar(content=Text(f"Saving all settings to {saved_settings_json.rpartition(slash)[2]}"))
    page.snack_bar.open = True
    page.tabs.selected_index = 1
    page.tabs.update()
    page.update()
  def changed(e, pref=None):
      if pref is not None:
        prefs[pref] = e.control.value
      page.update()
      status['changed_prefs'] = True
  def change_theme_mode(e):
    prefs['theme_mode'] = e.control.value
    if prefs['theme_mode'].lower() == "dark":
      page.dark_theme = Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))
    else:
      page.theme = theme.Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))
    page.theme_mode = prefs['theme_mode'].lower()
    page.update()
    status['changed_prefs'] = True
  def change_theme_color(e):
    prefs['theme_color'] = e.control.value
    color_icon.visible = prefs['theme_color'].lower() == "custom"
    color_icon.update()
    if prefs['theme_mode'].lower() == "dark":
      page.dark_theme = Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))
    else:
      page.theme = theme.Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))
    page.update()
    status['changed_prefs'] = True
  def open_color_picker(e):
      d.open = True
      page.update()
  color_picker = ColorPicker(color=prefs['theme_custom_color'], width=300)
  color_icon = Container(ft.IconButton(icon=ft.icons.BRUSH, icon_color=prefs['theme_custom_color'], on_click=open_color_picker), visible=prefs['theme_color'].lower() == "custom")
  def change_color(e):
      color_icon.content.icon_color = color_picker.color
      color_icon.content.update()
      prefs['theme_custom_color'] = color_picker.color
      if prefs['theme_mode'].lower() == "dark":
        page.dark_theme = Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))
      else:
        page.theme = theme.Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))
      page.update()
      d.open = False
      page.update()
  def close_dialog(e):
      d.open = False
      d.update()
  d = ft.AlertDialog(
      content=color_picker,
      actions=[
          ft.TextButton("OK", on_click=change_color),
          ft.TextButton("Cancel", on_click=close_dialog),
      ],
      actions_alignment=ft.MainAxisAlignment.END,
      on_dismiss=change_color,
  )
  page.overlay.append(d)
  
  def toggle_nsfw(e):
    #TODO: Add Popup alert with disclaimer, age verification and I Accept the Terms
    global safety
    retry_attempts.width = 0 if e.control.value else None
    retry_attempts.update()
    changed(e, 'disable_nsfw_filter')
    safety = {'safety_checker':None, 'requires_safety_checker':False, 'feature_extractor':None} if prefs['disable_nsfw_filter'] else {}
  def default_cache_dir(e):
    default_dir = prefs['image_output'].strip()
    if default_dir.endswith(slash):
      default_dir = default_dir[:-1]
    default_dir = default_dir.rpartition(slash)[0]
    default_dir = os.path.join(default_dir, 'models')
    prefs['cache_dir'] = default_dir
    optional_cache_dir.value = default_dir
    optional_cache_dir.update()
  def folder_picker_result(e):
    folder = folder_picker.result
    if folder != None and folder.path != None:
      image_output.value = folder.path
      image_output.update()
  folder_picker = FilePicker(on_result=folder_picker_result)
  page.overlay.append(folder_picker)
  def pick_output_dir(e):
    if not is_Colab:
      folder_picker.get_directory_path(dialog_title="Pick Directory to Save Outputs")
  image_output = TextField(label="Image Output Path", value=prefs['image_output'], on_change=lambda e:changed(e, 'image_output'), col={"md":12, "lg":6}, suffix=IconButton(icon=icons.FOLDER_OUTLINED, on_click=pick_output_dir))
  optional_cache_dir = TextField(label="Optional Cache Directory (saves large models to drive)", hint_text="(button on right inserts recommended folder)", value=prefs['cache_dir'], on_change=lambda e:changed(e, 'cache_dir'), suffix=IconButton(icon=icons.ARCHIVE, tooltip="Insert recommended models cache path", on_click=default_cache_dir), col={"md":12, "lg":6})
  file_prefix = TextField(label="Filename Prefix",  value=prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))
  file_suffix_seed = Checkbox(label="Filename Suffix Seed   ", tooltip="Appends -seed# to the end of the image name", value=prefs['file_suffix_seed'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'file_suffix_seed'))
  file_allowSpace = Checkbox(label="Filename Allow Space", tooltip="Otherwise will replace spaces with _ underscores", value=prefs['file_allowSpace'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'file_allowSpace'))
  file_max_length = TextField(label="Filename Max Length", tooltip="How long can the name taken from prompt text be? Max 250", value=prefs['file_max_length'], keyboard_type=KeyboardType.NUMBER, width=150, height=60, on_change=lambda e:changed(e, 'file_max_length'))
  file_datetime = Checkbox(label="Filename DateTime instead of Prompt Text", tooltip="Save File with Date-Time Stamp to protect your prompt.", value=prefs['file_datetime'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'file_datetime'))
  file_from_1 = Checkbox(label="Number from 1", tooltip="Otherwise image numbering starts at -0.", value=prefs['file_from_1'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'file_from_1'))
  save_image_metadata = Checkbox(label="Save Image Metadata in png", tooltip="Embeds your Artist Name & Copyright in the file's EXIF", value=prefs['save_image_metadata'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'save_image_metadata'))
  meta_ArtistName = TextField(label="Artist Name Metadata", value=prefs['meta_ArtistName'], keyboard_type=KeyboardType.NAME, on_change=lambda e:changed(e, 'meta_ArtistName'))
  meta_Copyright = TextField(label="Copyright Metadata", value=prefs['meta_Copyright'], keyboard_type=KeyboardType.NAME, on_change=lambda e:changed(e, 'meta_Copyright'))
  save_config_in_metadata = Checkbox(label="Save Config in Metadata    ", tooltip="Embeds all prompt parameters in the file's EXIF to recreate", value=prefs['save_config_in_metadata'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'save_config_in_metadata'))
  save_config_json = Checkbox(label="Save Config JSON files", tooltip="Creates a json text file with all prompt parameters with each image", value=prefs['save_config_json'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'save_config_json'))
  theme_mode = Dropdown(label="Theme Mode", width=200, options=[dropdown.Option("Dark"), dropdown.Option("Light")], value=prefs['theme_mode'], on_change=change_theme_mode)
  theme_color = Dropdown(label="Accent Color", width=200, options=[dropdown.Option("Green"), dropdown.Option("Blue"), dropdown.Option("Red"), dropdown.Option("Indigo"), dropdown.Option("Purple"), dropdown.Option("Orange"), dropdown.Option("Amber"), dropdown.Option("Brown"), dropdown.Option("Teal"), dropdown.Option("Yellow"), dropdown.Option("Custom")], value=prefs['theme_color'], on_change=change_theme_color)
  enable_sounds = Checkbox(label="Enable UI Sound Effects    ", tooltip="Turn on for audible errors, deletes and generation done notifications", value=prefs['enable_sounds'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_sounds'))
  start_in_installation = Checkbox(label="Start in Installation Page", tooltip="When launching app, switch to Installer tab. Saves time..", value=prefs['start_in_installation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'start_in_installation'))
  slider_stack = Checkbox(label="Slider Row Stack", tooltip="Customize UI of Sliders to Label above the Slider instead of one Row (applies on restart).", value=prefs['slider_stack'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'slider_stack'))
  disable_nsfw_filter = Checkbox(label="Disable NSFW Filters for Uncensored Images", value=prefs['disable_nsfw_filter'], tooltip="If you're over 18 & promise not to abuse, allow Not Safe For Work. Otherwise, will filter mature content...", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=toggle_nsfw)
  retry_attempts = Container(NumberPicker(label="Retry Attempts if Not Safe", min=0, max=8, value=prefs['retry_attempts'], on_change=lambda e:changed(e, 'retry_attempts')), padding=padding.only(left=20), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  retry_attempts.width = 0 if prefs['disable_nsfw_filter'] else None
  def toggle_stats(e):
      global stop_thread
      prefs['show_stats'] = e.control.value
      if prefs['show_stats']:
          start_thread(page)
      else:
          stop_thread = True
          page.stats.controls[0].value = ""
          page.stats.controls[1].value = ""
          page.stats.update()
      stats_settings.width = 0 if not prefs['show_stats'] else None
      stats_settings.update()
  def toggle_used(e):
      prefs['stats_used'] = e.control.value
      update_stats(page)
  show_stats = Checkbox(label="Show Memory Stats", tooltip="Gives an updating VRAM and RAM information on top appbar.", value=prefs['show_stats'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=toggle_stats)
  stats_used = Checkbox(label="Memory Used", tooltip="Otherwise show available free memory", value=prefs['stats_used'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=toggle_used)
  page.stats_used = stats_used
  stats_settings = Container(Row([stats_used, NumberPicker(label=" Update Interval (s):", min=1, max=30, value=prefs['stats_update'], on_change=lambda e:changed(e, 'stats_update'))]), padding=padding.only(left=0), animate_size=animation.Animation(700, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  stats_settings.width = 0 if not prefs['show_stats'] else None
  
  api_instructions = Container(height=170, content=Markdown("Get **HuggingFace API key** from https://huggingface.co/settings/tokens, preferably the WRITE access key.\n\nGet **Stability-API key** from https://beta.dreamstudio.ai/membership?tab=apiKeys then API key\n\nGet **OpenAI GPT-3 API key** from https://beta.openai.com, user menu, View API Keys\n\nGet **Google Gemini API Token** from https://developers.generativeai.google/tutorials/setup\n\nGet **Anthropic Claude API Key** from https://console.anthropic.com/settings/keys\n\nGet **TextSynth GPT-J Key** from https://TextSynth.com, login, Setup\n\nGet **AIHorde API Token** from https://aihorde.net/register, for Stable Horde cloud\n\nGet **Perplexity PPLX API Key** from https://www.perplexity.ai/settings/api, login and Generate API", extension_set="gitHubWeb", on_tap_link=open_url))
  HuggingFace_api = TextField(label="HuggingFace API Key", value=prefs['HuggingFace_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'HuggingFace_api_key'))
  Stability_api = TextField(label="Stability.ai API Key (optional)", value=prefs['Stability_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'Stability_api_key'))
  OpenAI_api = TextField(label="OpenAI API Key (optional)", value=prefs['OpenAI_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'OpenAI_api_key'))
  PaLM_api = TextField(label="Google Gemini API Key (optional)", value=prefs['PaLM_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'PaLM_api_key'))
  Anthropic_api = TextField(label="Anthropic Claude API Key (optional)", value=prefs['Anthropic_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'Anthropic_api_key'))
  TextSynth_api = TextField(label="TextSynth API Key (optional)", value=prefs['TextSynth_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'TextSynth_api_key'))
  Replicate_api = TextField(label="Replicate API Key (optional)", value=prefs['Replicate_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'Replicate_api_key'))
  AIHorde_api = TextField(label="AIHorde API Key (optional)", value=prefs['AIHorde_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'AIHorde_api_key'))
  Perplexity_api = TextField(label="Perplexity.ai API Key (optional)", value=prefs['Perplexity_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'Perplexity_api_key'))
  #save_button = ElevatedButton(content=Text(value="💾  Save Settings", size=20), on_click=save_settings, style=b_style())

  c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("⚙️   Stable Diffusion Deluxe Settings & Preferences"),
        ResponsiveRow([image_output, optional_cache_dir], run_spacing=2),
        Row([file_prefix, file_suffix_seed]) if (page.width if page.web else page.window.width) > 500 else Column([file_prefix, file_suffix_seed]),
        Row([file_max_length, file_allowSpace]),
        Row([file_datetime, file_from_1]),
        Row([disable_nsfw_filter, retry_attempts]),
        save_image_metadata,
        Row([meta_ArtistName, meta_Copyright]) if (page.width if page.web else page.window.width) > 712 else Column([meta_ArtistName, meta_Copyright]),
        Row([save_config_in_metadata, save_config_json]),
        Row([theme_mode, theme_color, color_icon]),
        Row([enable_sounds, start_in_installation, slider_stack, show_stats, stats_settings]),
        HuggingFace_api,
        Stability_api,
        OpenAI_api,
        PaLM_api,
        Anthropic_api,
        TextSynth_api,
        #Replicate_api,
        AIHorde_api,
        Perplexity_api,
        api_instructions,
        #save_button,
        Container(content=None, height=32),
      ],
  ))], scroll=ScrollMode.AUTO,)
  return c

def run_process(cmd_str, cwd=None, realtime=True, page=None, close_at_end=False, show=False, print=False): # show when debugging
  cmd_list = cmd_str if type(cmd_str) is list else cmd_str.split()
  if realtime:
    if cwd is None:
      process = subprocess.Popen(cmd_str, shell = True, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' )
    else:
      process = subprocess.Popen(cmd_str, shell = True, cwd=cwd, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' )
    while True:
      realtime_output = process.stdout.readline()
      if realtime_output == '' and process.poll() is not None:
        break
      if realtime_output and show and page != None:
        #print(realtime_output.strip(), flush=False)
        page.top_banner.content.controls.append(Text(realtime_output.strip()))
        page.update()
        sys.stdout.flush()
      if print:
        print(realtime_output.strip())
    if close_at_end:
      page.top_banner.open = False
      page.update()
  else:
    if cwd is None:
      output = subprocess.run(cmd_list, stdout=subprocess.PIPE, env=env).stdout.decode('utf-8')
    else:
      output = subprocess.run(cmd_list, stdout=subprocess.PIPE, env=env, cwd=cwd).stdout.decode('utf-8')
    if print:
      print(output)
    return output

def latest_version(package_name):
  try:
    output = subprocess.run(["pip", "show", package_name], capture_output=True, text=True, check=True)
    match = re.search(r"Version: (.*)", output.stdout)
    if match:
      return match.group(1)
    else:
      return None
  except subprocess.CalledProcessError:
    return None

def close_alert_dlg(e):
    e.page.alert_dlg.open = False
    e.page.update()
def alert_msg(page:Page, msg:str, content=None, okay="", sound=True, width=None, wide=False, debug_pref=None, buttons=[]):
    try:
        if page.alert_dlg.open == True: return
    except Exception: pass
    try:
        if prefs['enable_sounds'] and sound: page.snd_error.play()
    except Exception:
        msg += " May have to restart runtime."
        pass
    content_msg = content
    def send_debug(e):
        nonlocal content_msg
        debug_msg = msg + "\n\n"
        if isinstance(content_msg, list):
            content_msg = Column(controls=content_msg)
        if not isinstance(content_msg, Column):
            content_msg = Column(controls=[content_msg])
        for c in content_msg.controls:
            if isinstance(c, Text) or isinstance(c, Markdown):
                debug_msg += str(c.value) + "\n"
        import platform
        memory = f"GPU VRAM: {status['gpu_used']:.1f}/{status['gpu_memory']:.0f}GB - CPU RAM: {status['cpu_used']:.1f}/{status['cpu_memory']:.0f}GB{' - on Colab' if is_Colab else ''}"
        os_info = f" - OS: {platform.system()} {platform.version()} - Python {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro} - Torch {torch.__version__} - Transformers {transformers.__version__}"
        debug_msg += memory + os_info + "\n"
        if debug_pref != None:
            debug_msg += str(debug_pref)
        # TODO: Add another window to submit report with optional From name, email & notes before sending
        send_debug_email(debug_msg)
        toast_msg(page, f"📧  Sent Debug Crash Report Email to Skquark... Thanks for helping Beta Test.")
    show_debug = content != None and sound
    okay_button = ElevatedButton(content=Text("👌  OKAY " if okay == "" else okay, size=18), on_click=close_alert_dlg)
    debug_button = Container(content=None) if not show_debug else ft.OutlinedButton(content=Text("Submit Error", size=18), on_click=send_debug)
    if content == None: content = Container(content=None)
    if not isinstance(content, list):
        content = [content]
    content = [Text(c) if isinstance(c, str) else c for c in content]
    actions = buttons + [okay_button] if buttons else [debug_button, okay_button]
    page.alert_dlg = AlertDialog(title=Text(msg), content=Column(content, scroll=ScrollMode.AUTO), actions=actions, actions_alignment=MainAxisAlignment.END)#, width=None if not wide else (page.width if page.web else page.window.width) - 200)
    page.overlay.append(page.alert_dlg)
    page.alert_dlg.open = True
    try:
        page.update()
    except Exception: pass

def send_debug_email(message, subject="DiffusionDeluxe Error", from_name=None, from_email=None):
    try:
        import smtplib
        from email.mime.text import MIMEText
    except ModuleNotFoundError:
        run_sp("pip install secure-smtplib")
        import smtplib
        from email.mime.text import MIMEText
        pass
    email_from = "Skquark@diffusiondeluxe.com"
    email_password = "StableDiff"
    email_to = "Alan@Skquark.com"
    smtp_server = "mail.diffusiondeluxe.com"
    smtp_port = 587#465
    body = f"An error occurred:\n\n{message}" if 'Error' in subject else message
    msg = MIMEText(body, _subtype='plain', _charset='utf-8')
    msg["Subject"] = subject
    msg["From"] = email_from
    msg["To"] = email_to
    if bool(from_name):
        msg["From"] = f"{from_name} <{from_email if bool(from_email) else email_from}>"
    if bool(from_email):
        msg["Reply-To"] = from_email
    try:
        with smtplib.SMTP(smtp_server, smtp_port) as server:
            server.starttls()
            server.login(email_from, email_password)
            server.sendmail(email_from, [email_to], msg.as_string())
    except Exception as e:
        print(f"Failed to send email: {e}")

def toast_msg(page:Page, msg:str, duration=None):
    page.snack_bar = ft.SnackBar(content=Text(msg, color=colors.ON_TERTIARY), duration=duration, bgcolor=colors.TERTIARY)
    page.snack_bar.open = True
    page.update()

def save_installers(controls):
  for c in controls:
    if isinstance(c, Switch):
      #print(f"elif c.value == '{c.label}': prefs[''] = c.value")
      if c.value == 'Install HuggingFace Diffusers Pipeline': prefs['install_diffusers'] = c.value
      elif c.value == 'Install Stability-API DreamStudio Pipeline': prefs['install_Stability_api'] = c.value
      elif c.value == 'Install Real-ESRGAN AI Upscaler': prefs['install_ESRGAN'] = c.value
      elif c.value == 'Install OpenAI GPT Text Engine': prefs['install_OpenAI'] = c.value
      elif c.value == 'Install TextSynth GPT-J Text Engine': prefs['install_TextSynth'] = c.value
    '''elif isinstance(c, Container):
      try:
        for i in c.content.controls:
          if isinstance(i, Switch):
            print(f"elif i.value == '{c.label}': prefs[''] = i.value")
      except: continue'''
def refresh_installers(controls):
  for c in controls:
    if isinstance(c, Switch):
      c.update()

def buildInstallers(page):
  global prefs, status, model_path
  def changed(e, pref=None):
      if pref is not None:
        prefs[pref] = e.control.value
      #page.update()
      status['changed_prefs'] = True
  def changed_status(e, stat=None):
      if stat is not None:
        status[stat] = e.control.value
  def toggle_diffusers(e):
      prefs['install_diffusers'] = e.control.value
      diffusers_settings.height=None if prefs['install_diffusers'] else 0
      diffusers_settings.update()
      status['changed_prefs'] = True
  install_diffusers = Switcher(label="Install HuggingFace Diffusers Pipeline", value=prefs['install_diffusers'], disabled=status['installed_diffusers'], on_change=toggle_diffusers, tooltip="Required Libraries for most Image Generation functionality. Disable if your Video Card GPU can't handle CUDA.")
  def change_scheduler(e):
      show = e.control.value == "DDIM"
      update = prefs['scheduler_mode'] == "DDIM" or show
      changed(e, 'scheduler_mode')
      if update:
          for eta in page.etas:
            if isinstance(eta, SliderRow):
              eta.show = show
            else:
              eta.visible = show
              eta.update()
  scheduler_mode = Dropdown(label="Scheduler/Sampler Mode", hint_text="They're very similar, with minor differences in the generated noise", width=230,
            options=[
                dropdown.Option("DDIM"),
                dropdown.Option("LMS Discrete"),
                dropdown.Option("PNDM"),
                dropdown.Option("IPNDM"),
                dropdown.Option("DPM Solver"),
                dropdown.Option("DPM Solver++"),
                dropdown.Option("DPM Solver Inverse"),
                dropdown.Option("SDE-DPM Solver++"),
                #dropdown.Option("DPM Stochastic"),
                dropdown.Option("K-Euler Discrete"),
                dropdown.Option("K-Euler Ancestral"),
                #dropdown.Option("Flow Match Euler Discrete"),
                dropdown.Option("EDM Euler"),
                dropdown.Option("DEIS Multistep"),
                dropdown.Option("UniPC Multistep"),
                dropdown.Option("Heun Discrete"),
                dropdown.Option("Karras Heun Discrete"),
                dropdown.Option("K-DPM2 Ancestral"),
                dropdown.Option("K-DPM2 Discrete"),
                dropdown.Option("Karras-LMS"),
                dropdown.Option("TCD"),
                dropdown.Option("LCM"),
            ], value=prefs['scheduler_mode'], autofocus=False, on_change=change_scheduler,
        )
  def model_card_update():
      time.sleep(0.4)
      page.model_card.update()
  def changed_finetuned_model(e):
      changed(e, 'finetuned_model')
      model = get_finetuned_model(e.control.value)
      model_card.value = f"  [**Model Card**](https://huggingface.co/{model['path']})"
      model_card_update()
  def changed_dreambooth_library(e):
      changed(e, 'dreambooth_model')
      model = get_dreambooth_model(e.control.value)
      model_card.value = f"  [**Model Card**](https://huggingface.co/{model['path']})"
      model_card_update()
  def changed_custom_model(e):
      changed(e, 'custom_model')
      model = {'name': 'Custom Model', 'path': e.control.value, 'prefix': ''}
      model_card.value = f"  [**Model Card**](https://huggingface.co/{model['path']})"
      #model_card.update()
  def changed_SDXL_model(e):
      changed(e, 'SDXL_model')
      SDXL_custom_model.visible = prefs['SDXL_model'] == 'Custom Model'
      SDXL_custom_model.update()
      model_card_SDXL.visible = not prefs['SDXL_model'] == 'Custom Model'
      model_card_SDXL.update()
      #model = {'name': 'Custom Model', 'path': e.control.value, 'prefix': ''}
      #model_card.value = f"  [**Model Card**](https://huggingface.co/{model['path']})"
      #model_card.update()
      model_SDXL = get_SDXL_model(e.control.value)
      model_card_SDXL.value = f"  [**Model Card**](https://huggingface.co/{model_SDXL['path']})"
      try:
          model_card_SDXL.update()
      except Exception:
          pass
  def changed_SD3_model(e):
      changed(e, 'SD3_model')
      SD3_custom_model.visible = prefs['SD3_model'] == 'Custom Model'
      SD3_custom_model.update()
      model_card_SD3.visible = not prefs['SD3_model'] == 'Custom Model'
      model_card_SD3.update()
      model_SD3 = get_SD3_model(e.control.value)
      model_card_SD3.value = f"  [**Model Card**](https://huggingface.co/{model_SD3['path']})"
      try:
          model_card_SD3.update()
      except Exception:
          pass
  def scheduler_help(e):
      alert_msg(page, "🎰   Sampler/Scheduler Modes Info", content=[Text("It's difficult to explain the visible difference each scheduler will make on your diffusion, and we can't really say which is better for what. The variations are subtle, and there's a lot of complex random math going on, so it depends on your personal taste, style and prompts."),
        Text("All the samplers are different algorithms for numerically approximating solutions to differential equations (DEs). In SD's case this is a high-dimensional differential equation that determines how the initial noise must be diffused (spread around the image) to produce a result image that minimizes a loss function (essentially the distance to a hypothetical 'perfect' match to the initial noise, but with additional push applied by the prompt). This incredibly complex differential equation is basically what's encoded in the billion+ floating-point numbers that make up a Stable Diffusion model."),
        Text("A sampler essentially works by taking the given number of steps, sampling the latent space on each step to compute the local gradient (slope), to figure out which direction the next step should be taken in. Like a ball rolling down a hill, the sampler tries to get as low as possible in terms of minimizing the loss function. But what locally looks like the fastest route may not actually net you an optimal solution – you may get stuck in a local optimum (a valley) and sometimes you have to first go up to find a better route down."),
        Markdown("""* **DDIM -** Denoising Diffusion Implicit Models is one of the first samplers for solving diffusion models. The image direction is approximated by the noise estimated by the noise predictor. It adds an _ETA_ parameter to set the amount of starting noise.
* **LMS Discrete -** Linear Multistep Scheduler algorithm is a simple and widely used method based on Least Mean Square for estimating the parameters of a linear model. It iteratively updates the model's parameters in order to minimize the error between the model's predictions and the actual outputs of the system.
* **PNDM -** Pseudo Numerical methods for Diffusion Models proposes uses advanced Ordinary Differential Equations integration techniques, namely Runge-Kutta method and a linear multi-step method. Generates higher quality synthetic images with fewer steps.
* **IPNDM -** A fourth-order Improved Pseudo Linear Multistep scheduler with interesting results.
* **DPM Solver -** Diffusion Probabilistic Model solver specifically designed for solving diffusion differential equations.
* **DPM Solver++ -** Improved version of DPM that refines results at high guidance scale (CFG) values.
* **DPM Solver Inverse -** Inverted scheduler from DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps. For Null-text Inversion for Editing Real Images.
* **SDE-DPM Solver++ -** A fast Stochastic Differential Equation solver for the reverse diffusion SDE. Introduces some random drift to the process on each step to possibly find a route to a better solution than a fully deterministic solver.
* **K-Euler Discrete -** This is a fast scheduler which can often generate good outputs in 20-40 steps. From the Elucidating the Design Space of Diffusion-Based Generative Models paper.
* **K-Euler Ancestral -** Uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 30-40 steps. One of my personal favorites with the subtleties.
* **EDM Euler -** The Euler scheduler in EDM formulation as presented in Elucidating the Design Space of Diffusion-Based Generative Models. Solely intended for models that use EDM formulation, like SVD
* **DEIS Multistep -** Diffusion Exponential Integrator Sampler modifies the polynomial fitting formula in log-rho space instead of the original linear `t` space in the DEIS paper. The modification enjoys closed-form coefficients for exponential multistep update instead of replying on the numerical solver. aims to accelerate the sampling process while maintaining high sample quality. 
* **UniPC Multistep -** Unified Predictor-Corrector inspired by the predictor-corrector method in ODE solvers, it can achieve high-quality image generation in 5-10 steps. Combines UniC and UniP to create a powerful image improvement tool.
* **Heun Discrete -** A more accurate improvement to Euler's method, but needs to predict noise twice in each step, so it is twice as slow as Euler. Uses a correction step to reduce error and is thus an example of a predictor–corrector algorithm.
* **Karras Heun Discrete -** Uses a different noise schedule empirically found by Tero Karras et al. Takes large steps at first and small steps at the end. In the context of DPMs, it's employed to simulate the evolution of data over time. 
* **K-DPM2 Ancestral -** Karras Diffusion Probabilistic Model Solver is accurate up to the second order. Ancestral sampling traces the data's evolution backward in time, from its final form back to its initial noisy state.
* **K-DPM2 Discrete -** The solver discretizes the diffusion process into smaller time steps. This discretization allows for efficient and accurate sampling, balanced between speed and sample quality.
* **Karras-LMS -** Linear Multi-Step Method is a standard method for solving ordinary differential equations. It aims at improving accuracy by clever use of the values of the previous time steps.
* **TCD -** Trajectory Consistency Distillation scheduler capable of generating good samples in a small number of steps. Encompasses trajectory consistency function and strategic stochastic sampling.
* **LCM -** Multistep and onestep scheduler (Algorithm 3) used alongside Latent Consistency Model Pipeline in 1-8 steps. Use with LCM LoRA too.

[Diffusers Scheduler Overview](https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/schedulers/overview.md) | [Stable Diffusion Samplers: A Comprehensive Guide](https://stable-diffusion-art.com/samplers/) | [Sampler Differences Explained](https://www.reddit.com/r/StableDiffusion/comments/zgu6wd/comment/izkhkxc/)""", on_tap_link=lambda e: e.page.launch_url(e.data))
      ], okay="🥴  Hard to Pick...", sound=False, wide=True)
  scheduler_help_btn = IconButton(icons.HELP_OUTLINE, tooltip="Help with Sampler/Scheduler Modes", on_click=scheduler_help)
  def toggle_safe(e):
      changed(e, 'install_safe')
      safety_config.visible = e.control.value
      safety_config.update()
  def toggle_SD(e):
      changed(e, 'install_text2img')
      SD_params.height = None if e.control.value else 0
      SD_params.update()
  def toggle_SDXL(e):
      changed(e, 'install_SDXL')
      SDXL_params.height = None if e.control.value else 0
      SDXL_params.update()
  def toggle_SD3(e):
      changed(e, 'install_SD3')
      SD3_params.height = None if e.control.value else 0
      SD3_params.update()
  model = get_model(prefs['model_ckpt'])
  model_SDXL = get_SDXL_model(prefs['SDXL_model'])
  model_path = model['path']
  model_card = Markdown(f"  [**Model Card**](https://huggingface.co/{model['path']})", on_tap_link=lambda e: e.page.launch_url(e.data))
  def get_model_card(path):
      page.model_card = Markdown(f"  [**Model Card**](https://huggingface.co/{path})", on_tap_link=lambda e: e.page.launch_url(e.data))
      return page.model_card
  page.model_card = model_card
  custom_area = Container(model_card, col={'xs':12, 'lg':6})
  def changed_model_ckpt(e):
      nonlocal custom_area, model_card
      changed(e, 'model_ckpt')
      model = get_model(e.control.value)
      model_card = get_model_card(model['path'])
      #model_card.value = f"  [**Model Card**](https://huggingface.co/{model['path']})"
      try:
          if e.control.value.startswith("Stable"):
            custom_area.content = model_card
          elif e.control.value == "Community Finetuned Model":
            custom_area.content = Row([finetuned_model, model_card])
          elif e.control.value == "DreamBooth Library Model":
            custom_area.content = Row([dreambooth_library, model_card])
          elif e.control.value == "Custom Model Path":
            custom_area.content = Row([custom_model, model_card])
          custom_area.update()
          page.model_row.update()
          #model_card_update()
      except Exception as ex:
          print(f"{ex}: {e.control.value} {type(custom_area)}")
          pass
  model_ckpt = Container(Dropdown(label="SD Model Checkpoint", width=280, options=[
      dropdown.Option("Stable Diffusion v2.1 x768"), dropdown.Option("Stable Diffusion v2.1 x512"),
      dropdown.Option("Stable Diffusion v2.0 x768"), dropdown.Option("Stable Diffusion v2.0 x512"), dropdown.Option("Stable Diffusion v1.5"), dropdown.Option("Stable Diffusion v1.4"),
      dropdown.Option("Community Finetuned Model"), dropdown.Option("DreamBooth Library Model"), dropdown.Option("Custom Model Path")], value=prefs['model_ckpt'], tooltip="Make sure you accepted the HuggingFace Model Cards first", autofocus=False, on_change=changed_model_ckpt), col={'xs':9, 'lg':4}, width=280)
  finetuned_model = Dropdown(label="Finetuned Model", tooltip="Make sure you accepted the HuggingFace Model Cards first", width=370, options=[], value=prefs['finetuned_model'], autofocus=False, on_change=changed_finetuned_model, col={'xs':11, 'lg':6})
  model_card_SDXL = Markdown(f"  [**Model Card**](https://huggingface.co/{model_SDXL['path']})", on_tap_link=lambda e: e.page.launch_url(e.data))
  for mod in finetuned_models:
      finetuned_model.options.append(dropdown.Option(mod["name"]))
  page.finetuned_model = finetuned_model
  dreambooth_library = Dropdown(label="DreamBooth Library", hint_text="", width=370, options=[], value=prefs['dreambooth_model'], autofocus=False, on_change=changed_dreambooth_library, col={'xs':10, 'md':4})
  for db in dreambooth_models:
      dreambooth_library.options.append(dropdown.Option(db["name"]))
  custom_model = TextField(label="Custom Model Path", value=prefs['custom_model'], width=370, on_change=changed_custom_model)
  #custom_area = AnimatedSwitcher(model_card, transition="scale", duration=500, reverse_duration=200, switch_in_curve=AnimationCurve.EASE_OUT, switch_out_curve="easeIn")
  if prefs['model_ckpt'].startswith("Stable"):
      custom_area.content = model_card
  elif prefs['model_ckpt'] == "Community Finetuned Model":
      custom_area.content = Row([finetuned_model, model_card], col={'xs':9, 'lg':4})
  elif prefs['model_ckpt'] == "DreamBooth Library Model":
      custom_area.content = Row([dreambooth_library, model_card], col={'xs':9, 'lg':4})
  elif prefs['model_ckpt'] == "Custom Model Path":
      custom_area.content = Row([custom_model, model_card], col={'xs':9, 'lg':4})
  page.model_row = Row([model_ckpt, custom_area], run_spacing=8, vertical_alignment=CrossAxisAlignment.CENTER)
  SDXL_model = Dropdown(label="SDXL Model Checkpoint", hint_text="", width=280, options=[dropdown.Option("Custom Model")], value=prefs['SDXL_model'], autofocus=False, on_change=changed_SDXL_model, col={'xs':9, 'md':4})
  for xl in SDXL_models:
      SDXL_model.options.append(dropdown.Option(xl["name"]))
  SDXL_custom_model = TextField(label="Custom Model Path", value=prefs['SDXL_custom_model'], width=370, expand=True, visible=prefs['SDXL_model']=='Custom Model', on_change=lambda e:changed(e,'SDXL_custom_model'), col={'xs':3, 'md':8})
  SDXL_model_row = Row([SDXL_model, SDXL_custom_model, model_card_SDXL], run_spacing=8, vertical_alignment=CrossAxisAlignment.CENTER)

  enable_xformers = Checkbox(label="Enable Xformers Mem Efficient Attention", tooltip="If using Torch < 2, speeds up diffusion process.", value=prefs['enable_xformers'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_xformers'))
  enable_bitsandbytes = Checkbox(label="Enable BitsandBytes", tooltip="For 8-Bit Low-Mem Optimizers and Trainers.", value=prefs['enable_bitsandbytes'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_bitsandbytes'))
  memory_optimization = Dropdown(label="Enable Memory Optimization", width=290, options=[dropdown.Option("None"), dropdown.Option("Attention Slicing")], value=prefs['memory_optimization'], on_change=lambda e:changed(e, 'memory_optimization'))
  if version.parse(torch.__version__) < version.parse("2.0.0"):
      memory_optimization.options.append(dropdown.Option("Xformers Mem Efficient Attention"))
      enable_xformers.visible = True
  else:
      enable_xformers.visible = False
      prefs['enable_xformers'] = False
  higher_vram_mode = Checkbox(label="Higher VRAM Mode", tooltip="Adds a bit more precision & uses much more GPU memory. Not recommended unless you have >16GB VRAM.", value=prefs['higher_vram_mode'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'higher_vram_mode'))
  sequential_cpu_offload = Checkbox(label="Enable Sequential CPU Offload", tooltip="Offloads all models to CPU using accelerate, significantly reducing memory usage.", value=prefs['sequential_cpu_offload'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'sequential_cpu_offload'))
  enable_attention_slicing = Checkbox(label="Enable Attention Slicing", tooltip="Saves VRAM while creating images so you can go bigger without running out of mem.", value=prefs['enable_attention_slicing'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_attention_slicing'))
  enable_vae_tiling = Checkbox(label="Enable VAE Tiling", tooltip="The VAE will split the input tensor into tiles to compute decoding and encoding in several steps. This is useful to save a large amount of memory and to allow the processing of larger images.", value=prefs['vae_tiling'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'vae_tiling'))
  enable_vae_slicing = Checkbox(label="Enable VAE Slicing", tooltip="Sliced VAE decode latents for larger batches of images with limited VRAM. Splits the input tensor in slices to compute decoding in several steps", value=prefs['vae_slicing'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'vae_slicing'))
  enable_tome = Checkbox(label="Enable Token Merging", tooltip="ToMe optimizes the Pipelines to create images faster, at the expense of some quality. Works by merging the redundant tokens / patches progressively in the forward pass of a Transformer-based network.", value=prefs['enable_tome'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_tome'))
  enable_hidiffusion = Checkbox(label="Enable HiDiffusion", tooltip="Unlock Higher-Resolution Creativity and Efficiency in Pretrained Diffusion Models. Faster and better image details.", value=prefs['enable_hidiffusion'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_hidiffusion'))
  enable_deepcache = Checkbox(label="Enable DeepCache", tooltip="Accelerates pipeline by strategically caching and reusing high-level features while efficiently updating low-level features by taking advantage of the U-Net architecture. Slightly reduces quality for speed...", value=prefs['enable_deepcache'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_deepcache'))
  enable_torch_compile = Checkbox(label="Enable Torch Compiling", tooltip="Speeds up Torch 2.0 Processing, but takes a bit longer to initialize.", value=prefs['enable_torch_compile'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_torch_compile'))
  enable_torch_compile.visible = not sys.platform.startswith('win')
  enable_freeu = Checkbox(label="Enable FreeU: Free Lunch", tooltip="Technique to improve image quality by rebalancing the contributions from the UNet’s skip connections and backbone feature maps. Applied during inference, does not require any additional training or mem. Works on most pipeline tasks.", value=prefs['enable_freeu'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_freeu'))
  '''def toggle_freeu(e):
      changed(e,'enable_freeu')
      freeu_args.height = None if prefs['enable_freeu'] else 0
      freeu_args.update()
  b1 = SliderRow(label="b1", min=1.0, max=1.2, divisions=4, round=2, pref=prefs['freeu_args'], key='b1', expand=True, col={'md':6}, tooltip="Backbone factor of the first stage block of decoder.")
  b2 = SliderRow(label="b2", min=1.2, max=1.6, divisions=8, round=2, pref=prefs['freeu_args'], key='b2', expand=True, col={'md':6}, tooltip="Backbone factor of the second stage block of decoder.")
  s1 = SliderRow(label="s1", min=0, max=1, divisions=20, round=1, pref=prefs['freeu_args'], key='s1', expand=True, col={'md':6}, tooltip="Skip factor of the first stage block of decoder.")
  s2 = SliderRow(label="s2", min=0, max=1, divisions=20, round=1, pref=prefs['freeu_args'], key='s2', expand=True, col={'md':6}, tooltip="Skip factor of the second stage block of decoder.")
  freeu_args = Container(content=Column([ResponsiveRow([b1, b2, s1, s2])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(top=0, left=13), height = None if prefs['enable_freeu'] else 0)'''

  #install_megapipe = Switcher(label="Install Stable Diffusion txt2image, img2img & Inpaint Mega Pipeline", value=prefs['install_megapipe'], disabled=status['installed_megapipe'], on_change=lambda e:changed(e, 'install_megapipe'))
  #install_text2img = Switcher(label="Install Stable Diffusion text2image, image2image & Inpaint Pipeline (/w Long Prompt Weighting)", value=prefs['install_text2img'], disabled=status['installed_txt2img'], on_change=lambda e:changed(e, 'install_text2img'), tooltip="The best general purpose component. Create images with long prompts, weights & models")
  install_text2img = Switcher(label="Install Stable Diffusion 1.5-2.1 text2image, image2image & Inpaint Pipelines", value=prefs['install_text2img'], disabled=status['installed_txt2img'], on_change=toggle_SD, tooltip="Best general-purpose component for most SD models <= 2.1, but don't need if using SDXL.")
  SD_compel = Checkbox(label="Use Compel Long Prompt Weighting Embeds with SD", tooltip="Re-weight different parts of a prompt string like positive+++ AND (bad negative)-- or (subject)1.3 syntax.", value=prefs['SD_compel'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'SD_compel'))
  #SD_compel = Switcher(label="Use Compel Long Prompt Weighting Embeds with SD", tooltip="Re-weight different parts of a prompt string like positive+++ AND (bad negative)-- or (subject)1.3 syntax.", value=prefs['SD_compel'], on_change=lambda e:changed(e, 'SD_compel'))
  SD_params = Container(Column([SD_compel]), padding=padding.only(top=0, left=32), height=None if prefs['install_text2img'] else 0, animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)

  #SDXL_model_card = Markdown(f"  [**Accept Model Card**](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)", on_tap_link=lambda e: e.page.launch_url(e.data))
  install_SDXL = Switcher(label="Install Stable Diffusion XL 1.0 text2image, image2image & Inpaint Pipeline", value=prefs['install_SDXL'], disabled=status['installed_SDXL'], on_change=toggle_SDXL, tooltip="Latest SDXL v1.0 trained on 1080p images.")
  SDXL_compel = Checkbox(label="Use Compel Long Prompt Weighting Embeds with SDXL", tooltip="Re-weight different parts of a prompt string like positive+++ AND (bad negative)-- or (subject)1.3 syntax.", value=prefs['SDXL_compel'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'SDXL_compel'))
  #SDXL_compel = Switcher(label="Use Compel Long Prompt Weighting Embeds with SDXL", tooltip="Re-weight different parts of a prompt string like positive+++ AND (bad negative)-- or (subject)1.3 syntax.", value=prefs['SDXL_compel'], on_change=lambda e:changed(e,'SDXL_compel'))
  SDXL_params = Container(Column([SDXL_compel]), padding=padding.only(top=0, left=32), height=None if prefs['install_SDXL'] else 0, animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  #SDXL_params.visible = False
  install_SD3 = Switcher(label="Install Stable Diffusion 3 text2image, image2image & Inpaint Pipeline", value=prefs['install_SD3'], on_change=toggle_SD3, tooltip="Latest SD3 with three text encoders and special Flow-Match Euler Discrete Scheduler. Great prompt coherance & text writing, but currently has some limitations..")
  SD3_compel = Checkbox(label="Use Compel Long Prompt Weighting", tooltip="Re-weight different parts of a prompt string like positive+++ AND (bad negative)-- or (subject)1.3 syntax.", value=prefs['SD3_compel'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'SD3_compel'))
  SD3_cpu_offload = Checkbox(label="CPU Offload Model", tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.", value=prefs['SD3_cpu_offload'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'SD3_cpu_offload'))
  SD3_bitsandbytes_8bit = Checkbox(label="BitsAndBytes 8-bit", tooltip="Load and quantize the T5-XXL text encoder to 8-bit precision. This allows you to keep using all three text encoders while only slightly impacting performance.", value=prefs['SD3_bitsandbytes_8bit'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'SD3_bitsandbytes_8bit'))
  SD3_options = Row([SD3_cpu_offload, SD3_bitsandbytes_8bit])#SD3_compel, 
  SD3_model = Dropdown(label="SD3 Model Checkpoint", hint_text="", width=280, options=[dropdown.Option("Custom Model")], value=prefs['SD3_model'], autofocus=False, on_change=changed_SD3_model, col={'xs':9, 'md':4})
  for model in SD3_models:
      SD3_model.options.append(dropdown.Option(model["name"]))
  model_SD3 = get_SD3_model(prefs['SD3_model'])
  SD3_custom_model = TextField(label="Custom Model Path", value=prefs['SD3_custom_model'], width=370, expand=True, visible=prefs['SD3_model']=='Custom Model', on_change=lambda e:changed(e,'SD3_custom_model'), col={'xs':3, 'md':8})
  model_card_SD3 = Markdown(f"  [**Accept Model Card**](https://huggingface.co/{model_SD3['path']})", on_tap_link=lambda e: e.page.launch_url(e.data))
  SD3_model_row = Row([SD3_model, SD3_custom_model, model_card_SD3], run_spacing=8, vertical_alignment=CrossAxisAlignment.CENTER)
  #SDXL_compel = Switcher(label="Use Compel Long Prompt Weighting Embeds with SDXL", tooltip="Re-weight different parts of a prompt string like positive+++ AND (bad negative)-- or (subject)1.3 syntax.", value=prefs['SDXL_compel'], on_change=lambda e:changed(e,'SDXL_compel'))
  SD3_params = Container(Column([SD3_options]), padding=padding.only(top=0, left=32), height=None if prefs['install_SD3'] else 0, animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)

  install_img2img = Switcher(label="Install Stable Diffusion Specialized Inpainting Model for image2image & Inpaint Pipeline", value=prefs['install_img2img'], disabled=status['installed_img2img'], on_change=lambda e:changed(e, 'install_img2img'), tooltip="Gets more coherant results modifying Inpaint init & mask images")
  #install_repaint = Tooltip(message="Without using prompts, redraw masked areas to remove and repaint.", content=Switcher(label="Install Stable Diffusion RePaint Pipeline", value=prefs['install_repaint'], disabled=status['installed_repaint'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'install_repaint')))
  install_interpolation = Switcher(label="Install Stable Diffusion Prompt Walk Interpolation Pipeline", value=prefs['install_interpolation'], disabled=status['installed_interpolation'], on_change=lambda e:changed(e, 'install_interpolation'), tooltip="Create multiple tween images between prompts latent space. Almost animation.")
  #install_dreamfusion = Tooltip(message="Generate interesting mesh .obj, texture and preview video from a prompt.", content=Switcher(label="Install Stable Diffusion DreamFusion 3D Pipeline", value=prefs['install_dreamfusion'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, disabled=status['installed_dreamfusion'], on_change=lambda e:changed(e, 'install_dreamfusion')))
  install_alt_diffusion = Switcher(label="Install AltDiffusion text2image & image2image Multilingual Pipeline", value=prefs['install_alt_diffusion'], disabled=status['installed_alt_diffusion'], on_change=lambda e:changed(e, 'install_alt_diffusion'), tooltip="Multilingual Stable Diffusion supporting English, Chinese, Spanish, French, Russian, Japanese, Korean, Arabic and Italian.")
  install_attend_and_excite = Switcher(label="Install Attend and Excite text2image Pipeline", value=prefs['install_attend_and_excite'], disabled=status['installed_attend_and_excite'], on_change=lambda e:changed(e, 'install_attend_and_excite'), tooltip="Provides textual Attention-Based Semantic Guidance control over the image generation.")
  install_SAG = Switcher(label="Install Self-Attention Guidance (SAG) text2image Pipeline", value=prefs['install_SAG'], disabled=status['installed_SAG'], on_change=lambda e:changed(e, 'install_SAG'), tooltip="Intelligent guidance that can plugged into any diffusion model using their self-attention map, improving sample quality.")
  install_panorama = Switcher(label="Install MultiDiffusion Panorama text2image Pipeline", value=prefs['install_panorama'], disabled=status['installed_panorama'], on_change=lambda e:changed(e, 'install_panorama'), tooltip="Generate panorama-like wide images, Fusing Diffusion Paths for Controlled Image Generation")
  install_imagic = Switcher(label="Install Stable Diffusion iMagic image2image Pipeline", value=prefs['install_imagic'], disabled=status['installed_imagic'], on_change=lambda e:changed(e, 'install_imagic'), tooltip="Edit your image according to the prompted instructions like magic.")
  install_depth2img = Switcher(label="Install Stable Diffusion Depth2Image Pipeline", value=prefs['install_depth2img'], disabled=status['installed_depth2img'], on_change=lambda e:changed(e, 'install_depth2img'), tooltip="Uses Depth-map of init image for text-guided image to image generation.")
  install_composable = Switcher(label="Install Stable Diffusion Composable text2image Pipeline", value=prefs['install_composable'], disabled=status['installed_composable'], on_change=lambda e:changed(e, 'install_composable'), tooltip="Craft your prompts with | precise | weights AND composed together components | with AND NOT negatives.")
  install_safe = Switcher(label="Install Stable Diffusion Safe text2image Pipeline", value=prefs['install_safe'], disabled=status['installed_safe'], on_change=toggle_safe, tooltip="Use a content quality tuned safety model, providing levels of NSFW protection.")
  safety_config = Container(Dropdown(label="Model Safety Level", width=350, options=[dropdown.Option("Weak"), dropdown.Option("Medium"), dropdown.Option("Strong"), dropdown.Option("Max")], value=prefs['safety_config'], on_change=lambda e:changed(e, 'safety_config')), padding=padding.only(left=32))
  safety_config.visible = prefs['install_safe']
  install_versatile = Switcher(label="Install Versatile Diffusion text2image, Dual Guided & Image Variation Pipeline", value=prefs['install_versatile'], disabled=status['installed_versatile'], on_change=lambda e:changed(e, 'install_versatile'), tooltip="Multi-flow model that provides both image and text data streams and conditioned on both text and image.")

  def toggle_clip(e):
      prefs['install_CLIP_guided'] = e.control.value
      status['changed_prefs'] = True
      clip_settings.height=None if prefs['install_CLIP_guided'] else 0
      clip_settings.update()
  install_CLIP_guided = Switcher(label="Install Stable Diffusion CLIP-Guided Pipeline", value=prefs['install_CLIP_guided'], disabled=status['installed_clip'], on_change=toggle_clip, tooltip="Uses alternative LAION & OpenAI ViT diffusion. Takes more VRAM, so may need to make images smaller")
  clip_model_id = Dropdown(label="CLIP Model ID", width=350,
            options=[
                dropdown.Option("laion/CLIP-ViT-B-32-laion2B-s34B-b79K"),
                dropdown.Option("laion/CLIP-ViT-L-14-laion2B-s32B-b82K"),
                dropdown.Option("laion/CLIP-ViT-H-14-laion2B-s32B-b79K"),
                dropdown.Option("laion/CLIP-ViT-g-14-laion2B-s12B-b42K"),
                dropdown.Option("openai/clip-vit-base-patch32"),
                dropdown.Option("openai/clip-vit-base-patch16"),
                dropdown.Option("openai/clip-vit-large-patch14"),
            ], value=prefs['clip_model_id'], autofocus=False, on_change=lambda e:changed(e, 'clip_model_id'),
        )
  clip_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32, top=4), content=Column([clip_model_id]))

  def toggle_conceptualizer(e):
      changed(e, 'install_conceptualizer')
      conceptualizer_settings.height = None if e.control.value else 0
      conceptualizer_settings.update()
  def change_concepts_model(e):
      nonlocal concept
      changed(e, 'concepts_model')
      concept = get_concept(e.control.value)
      concepts_info.value = f"To use the concept, include keyword token **<{concept['token']}>** in your Prompts. Info at [https://huggingface.co/sd-concepts-library/{concept['name']}](https://huggingface.co/sd-concepts-library/{concept['name']})"
      concepts_info.update()
  def open_url(e):
      page.launch_url(e.data)
  def copy_token(e):
      nonlocal concept
      page.set_clipboard(f"<{concept['token']}>")
      toast_msg(page, f"📋  Token <{concept['token']}> copied to clipboard... Paste as word in your Prompt Text.")
  install_conceptualizer = Switcher(label="Install Stable Diffusion Textual-Inversion Conceptualizer Pipeline", value=prefs['install_conceptualizer'], on_change=toggle_conceptualizer, tooltip="Loads specially trained concept models to include in prompt with token")
  concept = get_concept(prefs['concepts_model'])
  concepts_model = Dropdown(label="SD-Concepts Library Model", hint_text="Specially trained community models made with Textual-Inversion", width=451, options=[], value=prefs['concepts_model'], on_change=change_concepts_model)
  copy_token_btn = IconButton(icon=icons.CONTENT_COPY, tooltip="Copy Token to Clipboard", on_click=copy_token)
  concepts_row = Row([concepts_model, copy_token_btn])
  concepts_info = Markdown(f"To use the concept, include keyword token **<{concept['token']}>** in your Prompts. Info at [https://huggingface.co/sd-concepts-library/{concept['name']}](https://huggingface.co/sd-concepts-library/{concept['name']})", selectable=True, on_tap_link=open_url)
  conceptualizer_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32, top=5), content=Column([concepts_row, concepts_info]))
  conceptualizer_settings.height = None if prefs['install_conceptualizer'] else 0
  for c in concepts: concepts_model.options.append(dropdown.Option(c['name']))
  install_upscale = Switcher(label="Install Stable Diffusion v2 Upscale 4X Pipeline", value=prefs['install_upscale'], disabled=status['installed_upscale'], on_change=lambda e:changed(e, 'install_upscale'), tooltip="Allows you to enlarge images with prompts. Note: Will run out of mem for images larger than 512px, start small.")

  diffusers_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, content=
                                 Column([Container(Column([Container(None, height=3), page.model_row, SDXL_model_row, SD3_model_row,
                                Container(content=None, height=4), Row([scheduler_mode, scheduler_help_btn]),
                                 Row([enable_attention_slicing, higher_vram_mode, enable_xformers,#memory_optimization,
                                      ]),
                                 Row([enable_vae_slicing, enable_vae_tiling, enable_torch_compile, enable_bitsandbytes]),
                                 #enable_attention_slicing,
                                 #Row([sequential_cpu_offload, enable_vae_tiling]),
                                 Row([enable_freeu, enable_tome, enable_hidiffusion, enable_deepcache]),
                                 ]), padding=padding.only(left=32, top=4)),
                                         install_text2img, SD_params, install_SDXL, SDXL_params, install_SD3, SD3_params, install_img2img, #install_repaint, #install_megapipe, install_alt_diffusion,
                                         install_interpolation, install_CLIP_guided, clip_settings, install_conceptualizer, conceptualizer_settings, install_safe, safety_config,
                                         install_versatile, install_SAG, install_attend_and_excite, install_panorama, install_imagic, install_depth2img, install_composable, install_upscale]))
  def toggle_stability(e):
      prefs['install_Stability_api'] = e.control.value
      stability_settings.height=None if prefs['install_Stability_api'] else 0
      stability_settings.update()
      page.update()
  install_Stability_api = Switcher(label="Install Stability-API DreamStudio Pipeline", value=prefs['install_Stability_api'], disabled=status['installed_stability'], on_change=toggle_stability, tooltip="Use DreamStudio.com servers without your GPU to create images on CPU.")
  use_Stability_api = Checkbox(label="Use Stability-ai API by default", tooltip="Instead of using Diffusers, generate images in their cloud. Can toggle to compare batches..", value=prefs['use_Stability_api'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'use_Stability_api'))
  model_checkpoint = Dropdown(label="Model Checkpoint", hint_text="", width=350, options=[dropdown.Option("Stable Image Core"), dropdown.Option("Stable Diffusion 3"), dropdown.Option("Stable Diffusion 3 Turbo"), dropdown.Option("stable-diffusion-xl-1024-v1-0"), dropdown.Option("stable-diffusion-xl-1024-v0-9"), dropdown.Option("stable-diffusion-xl-beta-v2-2-2"), dropdown.Option("stable-diffusion-768-v2-1"), dropdown.Option("stable-diffusion-512-v2-1"), dropdown.Option("stable-diffusion-768-v2-0"), dropdown.Option("stable-diffusion-512-v2-0"), dropdown.Option("stable-diffusion-v1-5"), dropdown.Option("stable-diffusion-v1"), dropdown.Option("stable-inpainting-512-v2-0"), dropdown.Option("stable-inpainting-v1-0")], value=prefs['model_checkpoint'], autofocus=False, on_change=lambda e:changed(e, 'model_checkpoint'))
  clip_guidance_preset = Dropdown(label="Clip Guidance Preset", width=350, options=[dropdown.Option("SIMPLE"), dropdown.Option("FAST_BLUE"), dropdown.Option("FAST_GREEN"), dropdown.Option("SLOW"), dropdown.Option("SLOWER"), dropdown.Option("SLOWEST"), dropdown.Option("NONE")], value=prefs['clip_guidance_preset'], autofocus=False, on_change=lambda e:changed(e, 'clip_guidance_preset'))
  #generation_sampler = Dropdown(label="Generation Sampler", hint_text="", width=350, options=[dropdown.Option("ddim"), dropdown.Option("plms"), dropdown.Option("k_euler"), dropdown.Option("k_euler_ancestral"), dropdown.Option("k_heun"), dropdown.Option("k_dpm_2"), dropdown.Option("k_dpm_2_ancestral"), dropdown.Option("k_lms")], value=prefs['generation_sampler'], autofocus=False, on_change=lambda e:changed(e, 'generation_sampler'))
  generation_sampler = Dropdown(label="Generation Sampler", hint_text="", width=350, options=[dropdown.Option("DDIM"), dropdown.Option("DDPM"), dropdown.Option("K_EULER"), dropdown.Option("K_EULER_ANCESTRAL"), dropdown.Option("K_HEUN"), dropdown.Option("K_DPMPP_2M"), dropdown.Option("K_DPM_2_ANCESTRAL"), dropdown.Option("K_LMS"), dropdown.Option("K_DPMPP_2S_ANCESTRAL"), dropdown.Option("K_DPM_2")], value=prefs['generation_sampler'], autofocus=False, on_change=lambda e:changed(e, 'generation_sampler'))
  #"K_EULER" "K_DPM_2" "K_LMS" "K_DPMPP_2S_ANCESTRAL" "K_DPMPP_2M" "DDIM" "DDPM" "K_EULER_ANCESTRAL" "K_HEUN" "K_DPM_2_ANCESTRAL"
  stability_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32), content=Column([use_Stability_api, model_checkpoint, generation_sampler, clip_guidance_preset]))
  # TODO: Move this to Parameters and show if prefs['install_AIHorde_api'] and status['installed_AIHorde_api']
  def toggle_AIHorde(e):
      prefs['install_AIHorde_api'] = e.control.value
      AIHorde_settings.height=None if prefs['install_AIHorde_api'] else 0
      AIHorde_settings.update()
      page.update()
  def toggle_AIHorde_controlnet(e):
      prefs['AIHorde_use_controlnet'] = e.control.value
      AIHorde_controlnet.visible = prefs['AIHorde_use_controlnet']
      AIHorde_controlnet.update()
      page.update()
  def models_AIHorde(e):
      model_request = "https://aihorde.net/api/v2/status/models"
      headers = {'apikey': prefs['AIHorde_api_key']}
      response = requests.get(model_request, headers=headers)
      if response != None:
          if response.status_code == 200:
            horde_models = json.loads(response.content)
            horde_models = sorted(horde_models, key=lambda x: (-x['count'], x['name']), reverse=False)
            model_info = [f"{model['name']} - Count: {model['count']}{f' Jobs: '+str(int(model['jobs'])) if model['jobs'] != 0.0 else ''}" for model in horde_models]
            alert_msg(e.page, "🏇  AI-Horde Current Model Stats", model_info, sound=False)
          else: print(response)
  install_AIHorde = Switcher(label="Install AI-Horde Crowdsorced Pipeline", value=prefs['install_AIHorde_api'], on_change=toggle_AIHorde, tooltip="Use AIHorde.net Crowdsourced cloud without your GPU to create images on CPU.")
  use_AIHorde = Checkbox(label="Use Stable Horde API by default", tooltip="Instead of using Diffusers, generate images in their cloud. Can toggle to compare batches..", value=prefs['use_AIHorde_api'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'use_AIHorde_api'))
  AIHorde_model = Dropdown(label="Model Checkpoint", hint_text="", width=375, options=[], value=prefs['AIHorde_model'], autofocus=False, on_change=lambda e:changed(e, 'AIHorde_model'))
  AIHorde_model.options = [dropdown.Option(m) for m in AIHorde_models]
  horde_models_info = IconButton(icons.HELP_OUTLINE, tooltip="Show AI-Horde Models Stat List", on_click=models_AIHorde)
  AIHorde_sampler = Dropdown(label="Generation Sampler", hint_text="", width=375, options=[dropdown.Option("k_lms"), dropdown.Option("k_heun"), dropdown.Option("k_euler"), dropdown.Option("k_euler_a"), dropdown.Option("k_dpm_2"), dropdown.Option("k_dpm_2_a"), dropdown.Option("k_dpm_fast"), dropdown.Option("k_dpm_adaptive"), dropdown.Option("k_dpmpp_2s_a"), dropdown.Option("k_dpmpp_2m"), dropdown.Option("dpmsolver"), dropdown.Option("k_dpmpp_sde"), dropdown.Option("DDIM")], value=prefs['AIHorde_sampler'], autofocus=False, on_change=lambda e:changed(e, 'AIHorde_sampler'))
  AIHorde_post_processing = Dropdown(label="Post-Processing", hint_text="", width=375, options=[dropdown.Option("None"), dropdown.Option("GFPGAN"), dropdown.Option("RealESRGAN_x4plus"), dropdown.Option("RealESRGAN_x2plus"), dropdown.Option("RealESRGAN_x4plus_anime_6B"), dropdown.Option("NMKD_Siax"), dropdown.Option("4x_AnimeSharp"), dropdown.Option("CodeFormers")], value=prefs['AIHorde_post_processing'], autofocus=False, on_change=lambda e:changed(e, 'AIHorde_post_processing'))
  AIHorde_karras = Checkbox(label="Karras", tooltip="Noise Scheduling tweaks prioritize fine-tuning the noise levels, emphasizing appearance customization & promoting diversity in generated content.", value=prefs['AIHorde_karras'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'AIHorde_karras'))
  AIHorde_tiling = Checkbox(label="Tiling", tooltip="Create tiled images that stitch together seamlessly.", value=prefs['AIHorde_tiling'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'AIHorde_tiling'))
  AIHorde_transparent = Checkbox(label="Transparent", tooltip="Uses Layer Diffusion to make the background with png Transparency.", value=prefs['AIHorde_transparent'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'AIHorde_transparent'))
  AIHorde_hires_fix = Checkbox(label="Hires Fix", tooltip="Process the image at base resolution before upscaling and re-processing with SD 1.5 models or to use Stable Cascade 2-pass.", value=prefs['AIHorde_hires_fix'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'AIHorde_hires_fix'))
  AIHorde_strip_background = Checkbox(label="Strip Background", tooltip="Try to isolate subject and remove the background.", value=prefs['AIHorde_strip_background'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'AIHorde_strip_background'))
  AIHorde_use_controlnet = Checkbox(label="Use ControlNet on Init-Image  ", tooltip="Applies ControlNet Processing to your initial image.", value=prefs['AIHorde_use_controlnet'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=toggle_AIHorde_controlnet)
  AIHorde_controlnet = Dropdown(label="ControlNet Type", hint_text="", width=180, options=[dropdown.Option("Canny"), dropdown.Option("HED"), dropdown.Option("Depth"), dropdown.Option("Normal"), dropdown.Option("OpenPose"), dropdown.Option("Seg"), dropdown.Option("Scribble"), dropdown.Option("FakeScribbles"), dropdown.Option("Hough")], value=prefs['AIHorde_controlnet'], autofocus=False, visible=prefs['AIHorde_use_controlnet'], on_change=lambda e:changed(e, 'AIHorde_controlnet'))
  def changed_AIHorde_lora_layer(e):
    prefs['AIHorde_lora_layer'] = e.control.value
    AIHorde_custom_lora_layer.visible = e.control.value == "Custom"
    AIHorde_custom_lora_layer.update()
  def add_AIHorde_lora(e, LoRA_map=None):
    if LoRA_map != None:
      lora = LoRA_map['name']
      lora_scale = LoRA_map['scale']
      lora_layer = LoRA_map
    else:
      lora = prefs['AIHorde_lora_layer']
      lora_scale = prefs['AIHorde_lora_layer_alpha']
      lora_layer = {}
      if lora == "Custom":
        lora_layer = {'name': 'Custom', 'model': prefs['AIHorde_custom_lora_layer'], 'scale': lora_scale}
      else:
        for l in CivitAI_LoRAs:
          if l['name'] == lora:
            lora_layer = l.copy()
            lora_layer['scale'] = lora_scale
        if not lora_layer:
          for l in prefs['custom_CivitAI_LoRA_models']:
            if l['name'] == lora:
              lora_layer = l.copy()
              lora_layer['scale'] = lora_scale
        for l in prefs['AIHorde_lora_map']:
          if l['name'] == lora:
            return
      prefs['AIHorde_lora_map'].append(lora_layer)
    title = Markdown(f"[**{lora_layer['name']}**](https://civitai.com/models/{lora_layer['model']}) - Scale: [{lora_layer['scale']}] - Model: {lora_layer['model']}")
    AIHorde_lora_layer_map.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
      items=[
          PopupMenuItem(icon=icons.DELETE, text="Delete LoRA Layer", on_click=delete_AIHorde_lora_layer, data=lora_layer),
          PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_AIHorde_lora_layers, data=lora_layer),
      ]), data=lora_layer))
    if LoRA_map == None:
      AIHorde_lora_layer_map.update()
  def delete_AIHorde_lora_layer(e):
      for l in prefs['AIHorde_lora_map']:
        if l['name'] == e.control.data['name']:
          prefs['AIHorde_lora_map'].remove(l)
        #del l #pia_prefs['lora_map'][]
      for c in AIHorde_lora_layer_map.controls:
        if c.data['name'] == e.control.data['name']:
            AIHorde_lora_layer_map.controls.remove(c)
            break
      AIHorde_lora_layer_map.update()
  def delete_all_AIHorde_lora_layers(e):
      prefs['AIHorde_lora_map'].clear()
      AIHorde_lora_layer_map.controls.clear()
      AIHorde_lora_layer_map.update()
  AIHorde_lora_layer = Dropdown(label="LoRA Layer Map", width=260, options=[dropdown.Option("Custom")], value=prefs['AIHorde_lora_layer'], on_change=changed_AIHorde_lora_layer)
  page.AIHorde_lora_layer = AIHorde_lora_layer
  if len(prefs['custom_CivitAI_LoRA_models']) > 0:
      for l in prefs['custom_CivitAI_LoRA_models']:
          AIHorde_lora_layer.options.append(dropdown.Option(l['name']))
  AIHorde_custom_lora_layer = TextField(label="Custom LoRA CivitAI (Model ID)", value=prefs['AIHorde_custom_lora_layer'], expand=True, visible=prefs['AIHorde_lora_layer']=="Custom", on_change=lambda e:changed(e,'AIHorde_custom_lora_layer'))
  for m in CivitAI_LoRAs:
      AIHorde_lora_layer.options.append(dropdown.Option(m['name']))
  AIHorde_lora_layer_alpha = SliderRow(label="LoRA Alpha", min=-5, max=5, divisions=20, round=1, expand=True, pref=prefs, key='AIHorde_lora_layer_alpha', tooltip="The Weight of the custom LoRA Model to influence diffusion.")
  AIHorde_add_lora_layer = ft.FilledButton("➕  Add LoRA", on_click=add_AIHorde_lora)
  AIHorde_lora_layer_map = Column([], spacing=0)
  for l in prefs['AIHorde_lora_map']:
      add_AIHorde_lora(None, l)
  AIHorde_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32), content=Column(
    [use_AIHorde, Row([AIHorde_model, horde_models_info]),
      Row([AIHorde_sampler, AIHorde_use_controlnet, AIHorde_controlnet]),
      Row([AIHorde_post_processing, AIHorde_tiling, AIHorde_karras, AIHorde_hires_fix, AIHorde_transparent, AIHorde_strip_background]),
      Row([AIHorde_lora_layer, AIHorde_custom_lora_layer, AIHorde_lora_layer_alpha, AIHorde_add_lora_layer]),
      AIHorde_lora_layer_map]))
  AIHorde_settings.height = None if prefs['install_AIHorde_api'] else 0
  
  def toggle_upscale(e):
      prefs['install_ESRGAN'] = e.control.value
      upscale_settings.height=None if prefs['install_ESRGAN'] else 0
      upscale_settings.update()
      page.update()
  def change_upscale_model(e):
      prefs['upscale_model'] = e.control.value
      for u in Real_ESRGAN_models:
        if u['name'] == prefs['upscale_model']:
          model_info.value = f"  [**Model Card**]({u['info']})"
          model_info.update()
  install_ESRGAN = Switcher(label="Install Real-ESRGAN AI Upscaler", value=prefs['install_ESRGAN'], disabled=status['installed_ESRGAN'], on_change=toggle_upscale, tooltip="Recommended to enlarge & sharpen all images as they're made.")
  upscale_model = Dropdown(label="ESRGAN Upscale Model", hint_text="", width=300, options=[], value=prefs['upscale_model'], autofocus=False, on_change=change_upscale_model)
  for u in Real_ESRGAN_models:
    upscale_model.options.append(dropdown.Option(u['name']))
    if u['name'] == prefs['upscale_model']:
      current_model = u
  model_info = Markdown(f"  [**Model Info**]({current_model['info']})", on_tap_link=lambda e: e.page.launch_url(e.data))
  upscale_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32, top=6), content=Row([upscale_model, model_info]))
  upscale_settings.height = None if prefs['install_ESRGAN'] else 0
  
  install_OpenAI = Switcher(label="Install OpenAI GPT Text & DALL-E Engine", value=prefs['install_OpenAI'], disabled=status['installed_OpenAI'], on_change=lambda e:changed(e, 'install_OpenAI'), tooltip="Use advanced AI to help make creative prompts. Also enables DALL-E 2 generation.")
  install_TextSynth = Switcher(label="Install TextSynth GPT-J Text Engine", value=prefs['install_TextSynth'], disabled=status['installed_TextSynth'], on_change=lambda e:changed(e, 'install_TextSynth'), tooltip="Alternative Text AI for brainstorming & rewriting your prompts. Pretty smart..")
  diffusers_settings.height = None if prefs['install_diffusers'] else 0
  stability_settings.height = None if prefs['install_Stability_api'] else 0
  clip_settings.height = None if prefs['install_CLIP_guided'] else 0

  def run_installers(e):
      global force_updates, prefs, status
      def console_clear():
        page.top_banner.content.controls = []
        page.update()
      def console_msg(msg, clear=True, show_progress=True):
        if not page.top_banner.open:
          page.top_banner.open = True
        if clear:
          page.top_banner.content.controls = []
        if show_progress:
          page.top_banner.content.controls.append(Row([Stack([Icon(icons.DOWNLOADING, color=colors.AMBER, size=48), Container(content=ProgressRing(), padding=padding.only(top=6, left=6), alignment=alignment.center)]), Container(content=Text("  " + msg.strip() , weight=FontWeight.BOLD, color=colors.ON_SECONDARY_CONTAINER, size=18), alignment=alignment.bottom_left, padding=padding.only(top=6)) ]))
          #page.top_banner.content.controls.append(Stack([Container(content=Text(msg.strip() + "  ", weight=FontWeight.BOLD, color=colors.ON_SECONDARY_CONTAINER, size=18), alignment=alignment.bottom_left, padding=padding.only(top=6)), Container(content=ProgressRing(), alignment=alignment.center if (page.width if page.web else page.window.width) > 768 else alignment.center_right)]))
          #page.top_banner.content.controls.append(Stack([Container(content=Text(msg.strip() + "  ", weight=FontWeight.BOLD, color=colors.ON_SECONDARY_CONTAINER, size=18), alignment=alignment.bottom_left, padding=padding.only(top=6)), Container(content=ProgressRing(), alignment=alignment.center)]))
          #page.top_banner.content.controls.append(Row([Text(msg.strip() + "  ", weight=FontWeight.BOLD, color=colors.GREEN_600), ProgressRing()]))
        else:
          page.top_banner.content.controls.append(Text(msg.strip(), weight=FontWeight.BOLD, color=colors.GREEN_600))
        page.update()
      page.console_msg = console_msg
      if status['changed_prefs']:
        save_settings_file(page, change_icon=False)
        status['changed_prefs'] = False
      # Temporary until I get Xformers to work
      #prefs['memory_optimization'] = 'Attention Slicing' if prefs['enable_attention_slicing'] else 'None'
      if prefs['install_diffusers'] and not bool(prefs['HuggingFace_api_key']):
        alert_msg(e.page, "You must provide your HuggingFace API Key to use Diffusers.")
        return
      if prefs['install_Stability_api'] and not bool(prefs['Stability_api_key']):
        alert_msg(e.page, "You must have your DreamStudio.ai Stability-API Key to use Stability.  Note that it will use your tokens.")
        return
      if prefs['install_OpenAI'] and not bool(prefs['OpenAI_api_key']):
        alert_msg(e.page, "You must have your OpenAI API Key to use GPT-3 Text AI.")
        return
      if prefs['install_TextSynth'] and not bool(prefs['TextSynth_api_key']):
        alert_msg(e.page, "You must have your TextSynth API Key to use GPT-J Text AI.")
        return
      if prefs['install_AIHorde_api'] and not bool(prefs['AIHorde_api_key']):
        alert_msg(e.page, "You must have your AIHorde.net API Key to use Stable Horde.  Note that it will use your Kudos.")
        return
      page.top_banner.content = Column([], scroll=ScrollMode.AUTO, auto_scroll=True, tight=True, spacing=0, alignment=MainAxisAlignment.END)
      page.top_banner.open = True
      page.update()
      if prefs['install_diffusers']:
        console_msg("Installing Hugging Face Diffusers Pipeline...")
        get_diffusers(page)
        status['installed_diffusers'] = True
      if prefs['install_text2img'] and prefs['install_diffusers']:
        console_msg("Downloading Stable Diffusion Text2Image, Image2Image & Inpaint Pipeline...")
        #with io.StringIO() as buf, redirect_stdout(buf):
        #print('redirected')
        page.status(f"...loading {get_model(prefs['model_ckpt'])['name']}")
        get_text2image(page)
        page.status()
        #output = buf.getvalue()
        #page.top_banner.content.controls.append(Text(output.strip()))
        status['installed_txt2img'] = True
        page.img_block.height = None
        page.img_block.update()
        page.update()
      if prefs['install_img2img'] and prefs['install_diffusers']:
        console_msg("Downloading Stable Diffusion Inpaint Model & Image2Image Pipeline...")
        get_image2image(page)
        status['installed_img2img'] = True
        page.img_block.height = None
        page.img_block.update()
        page.use_inpaint_model.visible = True
        page.use_inpaint_model.update()
        if not status['installed_txt2img']:
          prefs['use_inpaint_model'] = True
      '''if prefs['install_megapipe'] and prefs['install_diffusers']:
        console_msg("Downloading Stable Diffusion Unified Mega Pipeline...")
        get_text2image(page)
        status['installed_megapipe'] = True
        page.img_block.height = None
        page.img_block.update()'''
      if prefs['install_SDXL'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion XL Text2Image, Image2Image & Inpaint Pipeline...")
        page.status(f"...loading {get_SDXL_model(prefs['SDXL_model'])['name']}")
        if get_SDXL(page):
          status['installed_SDXL'] = True
          page.img_block.height = None
          page.img_block.update()
          page.use_SDXL.visible = True
          page.use_SDXL.update()
          page.SDXL_params.visible = True
          page.SDXL_params.update()
        page.status()
      if prefs['install_SD3'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion 3 Text2Image & Image2Image Pipeline...")
        page.status(f"...loading {get_SD3_model(prefs['SD3_model'])['name']}")
        if get_SD3(page):
          status['installed_SD3'] = True
          page.img_block.height = None
          page.img_block.update()
          page.use_SD3.visible = True
          page.use_SD3.update()
          #page.SD3_params.visible = True
          #page.SD3_params.update()
        page.status()
      if prefs['install_alt_diffusion'] and prefs['install_diffusers']:
        console_msg("Installing AltDiffusion Text2Image & Image2Image Pipeline...")
        get_alt_diffusion(page)
        status['installed_alt_diffusion'] = True
        page.use_alt_diffusion.visible = True
        page.use_alt_diffusion.update()
      if prefs['install_interpolation'] and prefs['install_diffusers']:
        console_msg("Downloading Stable Diffusion Walk Interpolation Pipeline...")
        get_interpolation(page)
        status['installed_interpolation'] = True
        page.interpolation_block.visible = True
        page.interpolation_block.update()
      if prefs['install_CLIP_guided'] and prefs['install_diffusers']:
        console_msg("Downloading Stable Diffusion CLIP-Guided Pipeline...")
        get_clip(page)
        status['installed_clip'] = True
        page.use_clip_guided_model.visible = True
        page.use_clip_guided_model.update()
        page.clip_block.height = None if prefs['use_clip_guided_model'] else 0
        page.clip_block.update()
        if prefs['use_clip_guided_model']:
          page.img_block.height = 0
          page.img_block.update()
      if prefs['install_conceptualizer'] and prefs['install_diffusers']:
        console_msg("Installing SD Concepts Library Textual Inversion Pipeline...")
        get_conceptualizer(page)
        page.use_conceptualizer_model.visible = True
        page.use_conceptualizer_model.update()
        if prefs['use_conceptualizer']:
          page.img_block.height = 0
          page.img_block.update()
        status['installed_conceptualizer'] = True
      if prefs['install_repaint'] and not status['installed_repaint'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion RePaint Pipeline...")
        get_repaint(page)
        status['installed_repaint'] = True
      if prefs['install_depth2img'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion 2 Depth2Image Pipeline...")
        get_depth2img(page)
        status['installed_depth2img'] = True
        if not status['installed_txt2img']:
          page.img_block.height = None
          page.img_block.update()
        page.use_depth2img.visible = True
        page.use_depth2img.update()
      if prefs['install_SAG'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion Self-Attention Guidance Text2Image Pipeline...")
        get_SAG(page)
        status['installed_SAG'] = True
        page.use_SAG.visible = True
        page.use_SAG.update()
      if prefs['install_attend_and_excite'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion Attend and Excite Text2Image Pipeline...")
        get_attend_and_excite(page)
        status['installed_attend_and_excite'] = True
        page.use_attend_and_excite.visible = True
        page.use_attend_and_excite.update()
      if prefs['install_imagic'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion iMagic Image2Image Pipeline...")
        get_imagic(page)
        status['installed_imagic'] = True
        if not status['installed_txt2img']:
          page.img_block.height = None
          page.img_block.update()
        page.use_imagic.visible = True
        page.use_imagic.update()
      if prefs['install_composable'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion Composable Text2Image Pipeline...")
        get_composable(page)
        status['installed_composable'] = True
        page.use_composable.visible = True
        page.use_composable.update()
      if prefs['install_versatile'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion Versatile Text2Image, Variation & Inpaint Pipeline...")
        get_versatile(page)
        status['installed_versatile'] = True
        if not status['installed_txt2img']:
          page.img_block.height = None
          page.img_block.update()
        page.use_versatile.visible = True
        page.use_versatile.update()
      if prefs['install_safe'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion Safe Text2Image Pipeline...")
        get_safe(page)
        status['installed_safe'] = True
        page.use_safe.visible = True
        page.use_safe.update()
      if prefs['install_panorama'] and prefs['install_diffusers']:
        console_msg("Installing MultiDiffusion Panorama Text2Image Pipeline...")
        get_panorama(page)
        status['installed_panorama'] = True
        page.use_panorama.visible = True
        page.use_panorama.update()
      if prefs['install_upscale'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion 4X Upscale Pipeline...")
        get_upscale(page)
        status['installed_upscale'] = True
        page.use_upscale.visible = True
        page.use_upscale.update()
      if prefs['install_dreamfusion'] and not status['installed_dreamfusion'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion DreamFusion 3D Pipeline...")
        get_dreamfusion(page) # No longer installing from here
        status['installed_dreamfusion'] = True
      if prefs['install_Stability_api']:
        console_msg("Installing Stability-API DreamStudio.ai Pipeline...")
        get_stability(page)
        status['installed_stability'] = True
      if prefs['install_AIHorde_api']:
        console_msg("Installing Stable Horde AIHorde.net Pipeline...")
        get_AIHorde(page)
      if prefs['install_ESRGAN'] and not status['installed_ESRGAN']:
        if not os.path.isdir(os.path.join(dist_dir, 'Real-ESRGAN')):
          get_ESRGAN(page)
          console_msg("Installing Real-ESRGAN Upscaler...")
        status['installed_ESRGAN'] = True
      if prefs['install_ESRGAN']:
        '''ESRGAN_blocks = [
          #page.ESRGAN_block,
          #page.ESRGAN_block_material,
          #page.ESRGAN_block_dalle,
          #page.ESRGAN_block_dalle3, --Fix later
          #page.ESRGAN_block_kandinsky,
          #page.ESRGAN_block_kandinsky_fuse,
          #page.ESRGAN_block_kandinsky_controlnet,
          #page.ESRGAN_block_kandinsky21,
          #page.ESRGAN_block_kandinsky21_fuse,
          #page.ESRGAN_block_deepfloyd,
          #page.ESRGAN_block_amused,
          #page.ESRGAN_block_stable_cascade,
          #page.ESRGAN_block_wuerstchen,
          #page.ESRGAN_block_pixart_alpha,
          #page.ESRGAN_block_pixart_sigma,
          #page.ESRGAN_block_hunyuan,
          #page.ESRGAN_block_lumina,
          #page.ESRGAN_block_kolors,
          #page.ESRGAN_block_auraflow,
          #page.ESRGAN_block_layer_diffusion,
          #page.ESRGAN_block_lcm,
          #page.ESRGAN_block_lmd_plus,
          #page.ESRGAN_block_ip_adapter,
          #page.ESRGAN_block_blip_diffusion,
          #page.ESRGAN_block_reference,
          #page.ESRGAN_block_instaflow,
          #page.ESRGAN_block_unCLIP,
          #page.ESRGAN_block_unCLIP_image_variation,
          #page.ESRGAN_block_unCLIP_interpolation,
          #page.ESRGAN_block_unCLIP_image_interpolation,
          #page.ESRGAN_block_semantic,
          #page.ESRGAN_block_EDICT,
          #page.ESRGAN_block_DiffEdit,
          #page.ESRGAN_block_pag,
          #page.ESRGAN_block_hd_painter,
          #page.ESRGAN_block_anytext,
          #page.ESRGAN_block_null_text,
          #page.ESRGAN_block_magic_mix,
          #page.ESRGAN_block_paint_by_example,
          #page.ESRGAN_block_instruct_pix2pix,
          #page.ESRGAN_block_controlnet,
          #page.ESRGAN_block_controlnet_xl,
          #page.ESRGAN_block_controlnet_sd3,
          #page.ESRGAN_block_controlnet_qr,
          #page.ESRGAN_block_controlnet_segment,
          #page.ESRGAN_block_styler,
          #page.ESRGAN_block_deep_daze,
          #page.ESRGAN_block_DiT,
          #page.ESRGAN_block_animate_diff, 
          #page.ESRGAN_block_text_to_video,
          #page.ESRGAN_block_text_to_video_zero,
          #page.ESRGAN_block_stable_animation,
        ]
        for b in ESRGAN_blocks:
          try:
            b.height = None
            b.update()
          except Exception:
            print(f"Failed ESRGAN block {b}")
            pass'''
        show_upscalers(page)
      if prefs['install_OpenAI'] and not status['installed_OpenAI']:
        try:
          import openai
        except ModuleNotFoundError as e:
          console_msg("Installing OpenAI GPT-3 Libraries...")
          run_process("pip install openai -qq", page=page)
          pass
        status['installed_OpenAI'] = True
      if prefs['install_TextSynth'] and not status['installed_TextSynth']:
        try:
          from textsynthpy import TextSynth, Complete
        except ModuleNotFoundError as e:
          console_msg("Installing TextSynth GPT-J Libraries...")
          run_process("pip install textsynthpy -qq", page=page)
          pass
        status['installed_TextSynth'] = True
      #print('Done Installing...')
      play_snd(Snd.DONE, page)
      console_clear()
      page.top_banner.open = False
      page.top_banner.update()
      page.update()
      install_diffusers.update()
      #install_text2img.update()
      #install_img2img.update()
      install_Stability_api.update()
      install_CLIP_guided.update()
      install_ESRGAN.update()
      #install_OpenAI.update()
      #install_TextSynth.update()
      update_parameters(page)
      page.Parameters.controls[0].content.update()
      #page.Parameters.updater()
      if force_updates:
        prefs['last_updated'] = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S.%fZ')
        #force_updates = False
      if current_tab==1:
        page.Installers.controls[0].content.update()
        page.Installers.update()
        page.show_install_fab(False)
        page.tabs.selected_index = 2
        page.tabs.update()
        page.update()
  def show_install_fab(show = True):
    if show:
      page.floating_action_button = FloatingActionButton(content=Row([Icon(icons.FILE_DOWNLOAD), Text("Run Installations", size=18)], alignment="center", spacing=5), width=205, shape=ft.RoundedRectangleBorder(radius=22), on_click=run_installers)
      #page.floating_action_button = FloatingActionButton(icon=icons.FILE_DOWNLOAD, text="Run Installations", on_click=run_installers)
      page.update()
    else:
      if page.floating_action_button is not None:
        page.floating_action_button = None
        page.update()
  page.show_install_fab = show_install_fab
  #install_button = ElevatedButton(content=Text(value="⏬   Run Installations ", size=20), on_click=run_installers)
  #image_output = TextField(label="Image Output Path", value=prefs['image_output'], on_change=changed)
  c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
                content=Column([
        Header("📥  Stable Diffusion Required & Optional Installers", subtitle="Run this every time you Start the App or make Changes. Only pick what you plan to use this session..."),
        install_diffusers,
        diffusers_settings,
        #install_text2img,
        #install_img2img,
        install_Stability_api,
        stability_settings,
        install_AIHorde,
        AIHorde_settings,
        #install_CLIP_guided,
        #clip_settings,
        install_ESRGAN, upscale_settings,
        #install_OpenAI,
        #install_TextSynth,
        #install_button,
        Container(content=None, height=32),
      ],
  ))], scroll=ScrollMode.AUTO)
  def init_boxes():
    diffusers_settings.height = None if prefs['install_diffusers'] else 0
    stability_settings.height = None if prefs['install_Stability_api'] else 0
    clip_settings.height = None if prefs['install_CLIP_guided'] else 0
    diffusers_settings.update()
    stability_settings.update()
    clip_settings.update()
    page.update()
  #init_boxes()
  return c

def update_parameters(page):
  #page.img_block.height = None if status['installed_img2img'] or status['installed_megapipe'] or status['installed_stability'] else 0
  #page.img_block.height = None if (status['installed_txt2img'] or status['installed_stability'] or status['installed_AIHorde'] or status['installed_SDXL']) and not (status['installed_clip'] and prefs['use_clip_guided_model']) else 0
  page.clip_block.height = None if status['installed_clip']  and prefs['use_clip_guided_model'] else 0
  #page.ESRGAN_block.height = None if status['installed_ESRGAN'] else 0
  page.img_block.update()
  page.clip_block.update()
  #page.ESRGAN_block.update()
  page.Parameters.update()
  #print("Updated Parameters")

if is_Colab:
    from google.colab import files

#LoRA_models = [{'name': 'Von Platen LoRA', 'path': 'patrickvonplaten/lora'}, {'name': 'Dog Example', 'path':'patrickvonplaten/lora_dreambooth_dog_example'}, {'name': 'Trauter LoRAs', 'path': 'YoungMasterFromSect/Trauter_LoRAs'}, {'name': 'Capitalize T5', 'path': 'ShengdingHu/Capitalize_T5-LoRA'}, {'name': 'SayakPaul LoRA-T4', 'path': 'sayakpaul/sd-model-finetuned-lora-t4'}]
#[{'name': 'sample-dog', 'path': 'lora-library/lora-dreambooth-sample-dog', 'prefix': 'sksdog'}, {'name': 'kdekuni', 'path': 'lora-library/kdekuni', 'prefix': 'a kdekuni golden funkopop'}, {'name': 'yarosnnv', 'path': 'lora-library/yarosnnv', 'prefix': 'yarosnnv'}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, ]
LoRA_models = [{'name': 'Dog Example', 'path':'patrickvonplaten/lora_dreambooth_dog_example'}, {'name': 'SayakPaul LoRA-T4', 'path': 'sayakpaul/sd-model-finetuned-lora-t4'}, {'name':'Openjourney LoRA', 'path':'prompthero/openjourney-lora', 'prefix': ''}, {'name':'Analog Diffusion', 'path':'https://replicate.delivery/pbxt/IzbeguwVsW3PcC1gbiLy5SeALwk4sGgWroHagcYIn9I960bQA/tmpjlodd7vazekezip.safetensors', 'prefix':'<1> '}, {'name': 'Analog.Redmond', 'path': 'artificialguybr/analogredmond', 'prefix':'AnalogRedmAF'}, {'name': 'LogoLoraForSDXL', 'path': 'artificialguybr/LogoRedmond-LogoLoraForSDXL', 'prefix':'LogoRedAF'}]

def buildParameters(page):
  global prefs, status, args
  def changed(e, pref=None, asInt=False, apply=True):
      if pref is not None:
        prefs[pref] = e.control.value if not asInt else int(e.control.value)
      if page.floating_action_button is None and apply:
        show_apply_fab(len(prompts) > 0)
      #if apply_changes_button.visible != (len(prompts) > 0): #status['changed_prefs']:
      #  apply_changes_button.visible = len(prompts) > 0
      #  apply_changes_button.update()
      status['changed_prefs'] = True
      #page.update()
  def change(e):
      if page.floating_action_button is None:
        show_apply_fab(len(prompts) > 0)
      status['changed_prefs'] = True
  def run_parameters(e):
      save_parameters()
      #page.tabs.current_tab = 3
      page.show_apply_fab(False)
      page.tabs.selected_index = 3
      page.tabs.update()
      page.update()
  def save_parameters():
      update_args()
      page.update_prompts(False)
      save_settings_file(page)
      status['changed_prefs'] = False
  def apply_to_prompts(e):
      update_args()
      page.apply_changes(e)
      save_settings_file(page)
      show_apply_fab(False)
      #apply_changes_button.visible = False
      #apply_changes_button.update()
  def pick_files_result(e: FilePickerResultEvent):
      # TODO: This is not working on Colab, maybe it can get_upload_url on other platform?
      if e.files:
        img = e.files
        uf = []
        fname = img[0]
        print(", ".join(map(lambda f: f.name, e.files)))
        #print(os.path.join(fname.path, fname.name))
        #src_path = os.path.join(fname.path, fname.name)
        #for f in pick_files_dialog.result.files:
        src_path = page.get_upload_url(fname.name, 600)
        uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))
        pick_files_dialog.upload(uf)
        print(str(src_path))
        #src_path = ''.join(src_path)
        print(str(uf[0]))
        dst_path = os.path.join(root_dir, fname.name)
        print(f'Copy {src_path} to {dst_path}')
        #shutil.copy(src_path, dst_path)
        # TODO: is init or mask?
        init_image.value = dst_path
      #selected_files.value = (", ".join(map(lambda f: f.name, e.files)) if e.files else "Cancelled!")
      #selected_files.update()

  pick_files_dialog = FilePicker(on_result=pick_files_result)
  page.overlay.append(pick_files_dialog)
  #selected_files = Text()

  def file_picker_result(e: FilePickerResultEvent):
      if e.files != None:
        upload_files(e)
  def on_upload_progress(e: FilePickerUploadEvent):
    nonlocal pick_type
    if e.progress == 1:
      save_file(e.file_name)
  def save_file(file_name):
      if not slash in file_name:
        fname = os.path.join(root_dir, file_name)
      else:
        fname = file_name
      if pick_type == "init":
        init_image.value = fname
        init_image.update()
        prefs['init_image'] = fname
      elif pick_type == "mask":
        mask_image.value = fname
        mask_image.update()
        prefs['mask_image'] = fname
      page.update()
  file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
  def upload_files(e):
      uf = []
      if file_picker.result != None and file_picker.result.files != None:
          for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
          file_picker.upload(uf)
  page.overlay.append(file_picker)
  pick_type = ""
  #page.overlay.append(pick_files_dialog)
  def pick_init(e):
      nonlocal pick_type
      pick_type = "init"
      file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "JPG"], dialog_title="Pick Init Image File")
  def pick_mask(e):
      nonlocal pick_type
      pick_type = "mask"
      file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "JPG"], dialog_title="Pick Black & White Mask Image")
  def toggle_clip(e):
      if e.control.value:
        page.img_block.height = 0
        page.clip_block.height = None if status['installed_clip'] else 0
      else:
        page.img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0
        page.clip_block.height = 0
      page.img_block.update()
      page.clip_block.update()
      changed(e, 'use_clip_guided_model')
  def change_use_cutouts(e):
      num_cutouts.visible = e.control.value
      num_cutouts.update()
      changed(e, 'use_cutouts')
  def toggle_interpolation(e):
      interpolation_steps_slider.height = None if e.control.value else 0
      interpolation_steps_slider.update()
      if e.control.value: page.img_block.height = 0
      else: page.img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0
      page.img_block.update()
      changed(e, 'use_interpolation', apply=False)
  def change_interpolation_steps(e):
      interpolation_steps_value.value = f" {int(e.control.value)} steps"
      interpolation_steps_value.update()
      changed(e, 'num_interpolation_steps', asInt=True, apply=False)
  def toggle_SAG(e):
      sag_scale_slider.height = None if e.control.value else 0
      sag_scale_slider.update()
      changed(e, 'use_SAG', apply=False)
  def change_sag_scale(e):
      sag_scale_value.value = f" {float(e.control.value)}"
      sag_scale_value.update()
      changed(e, 'sag_scale', apply=False)
  def toggle_attend_and_excite(e):
      max_iter_to_alter_slider.height = None if e.control.value else 0
      max_iter_to_alter_slider.update()
      changed(e, 'use_attend_and_excite', apply=False)
  def change_max_iter_to_alter(e):
      max_iter_to_alter_value.value = f" {int(e.control.value)} Iterations"
      max_iter_to_alter_value.update()
      changed(e, 'max_iter_to_alter', asInt=True, apply=False)
  def change_strength(e):
      strength_value.value = f" {int(e.control.value * 100)}%"
      strength_value.update()
      changed(e, 'init_image_strength')
  def toggle_SDXL(e):
      changed(e,'use_SDXL', apply=False)
      page.SDXL_params.height = None if e.control.value else 0
      page.SDXL_params.update()
  def toggle_SD3(e):
      changed(e,'use_SD3', apply=False)
      #page.SD3_params.height = None if e.control.value else 0
      #page.SD3_params.update()
  def toggle_conceptualizer(e):
      if e.control.value:
        page.img_block.height = 0
      else:
        page.img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0
      page.img_block.update()
      changed(e, 'use_conceptualizer')
  def toggle_centipede(e):
      changed(e,'centipede_prompts_as_init_images', apply=False)
      image_pickers.height = None if not e.control.value else 0
      image_pickers.update()
  def toggle_LoRA(e):
      changed(e,'use_LoRA_model', apply=False)
      LoRA_block.height = None if e.control.value else 0
      LoRA_block.update()
  def changed_LoRA(e):
      changed(e, 'LoRA_model', apply=False)
      custom_LoRA_model.visible = True if prefs['LoRA_model'] == "Custom LoRA Path" else False
      custom_LoRA_model.update()
  def changed_SDXL_LoRA(e):
      changed(e, 'SDXL_LoRA_model', apply=False)
      custom_SDXL_LoRA_model.visible = True if prefs['SDXL_LoRA_model'] == "Custom SDXL LoRA Path" else False
      custom_SDXL_LoRA_model.update()
  batch_folder_name = TextField(label="Batch Folder Name", value=prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name', apply=False))
  #batch_size = TextField(label="Batch Size", value=prefs['batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'batch_size'))
  #n_iterations = TextField(label="Number of Iterations", value=prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations'))
  batch_size = NumberPicker(label="Batch Size: ", min=1, max=10, value=prefs['batch_size'], tooltip="Generates multiple images at the same time. Uses more memory...", on_change=lambda e: changed(e, 'batch_size'))
  n_iterations = NumberPicker(label="Number of Iterations: ", min=1, max=30, value=prefs['n_iterations'], tooltip="Creates multiple images in batch seperately", on_change=lambda e: changed(e, 'n_iterations'))
  #steps = TextField(label="Steps", value=prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', asInt=True))
  steps = SliderRow(label="Steps", min=0, max=200, divisions=200, pref=prefs, key='steps', on_change=change)
  #eta = TextField(label="DDIM ETA", value=prefs['eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'eta'))
  eta = SliderRow(label="DDIM ETA", min=0, max=1, divisions=20, round=1, pref=prefs, key='eta', tooltip="", visible=False, on_change=change)
  page.etas.append(eta)
  seed = TextField(label="Seed", value=prefs['seed'], keyboard_type=KeyboardType.NUMBER, width = 160, on_change=lambda e:changed(e,'seed'))
  param_rows = Row([Column([batch_folder_name, seed, batch_size]), Column([steps, eta, n_iterations])])
  batch_row = Row([batch_folder_name, seed])
  number_row = Row([batch_size, n_iterations])
  guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=prefs, key='guidance_scale', on_change=change)
  width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=prefs, key='width', on_change=change)
  height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=prefs, key='height', on_change=change)

  init_image = TextField(label="Init Image (optional)", value=prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
  mask_image = TextField(label="Mask Image (optional)", value=prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))
  alpha_mask = Checkbox(label="Alpha Mask", value=prefs['alpha_mask'], tooltip="Use Transparent Alpha Channel of Init as Mask", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))
  invert_mask = Checkbox(label="Invert Mask", value=prefs['invert_mask'], tooltip="Reverse Black & White of Image Mask", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
  image_pickers = Container(content=ResponsiveRow([Row([init_image, alpha_mask], col={"lg":6}), Row([mask_image, invert_mask], col={"lg":6})]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
  image_pickers.height = None if not prefs['centipede_prompts_as_init_images'] else 0
  init_image_strength = Slider(min=0.0, max=1.0, divisions=20, label="{value}", round=2, value=prefs['init_image_strength'], on_change=change_strength, expand=True)
  strength_value = Text(f" {int(prefs['init_image_strength'] * 100)}%", weight=FontWeight.BOLD)
  strength_slider = Row([Text("Init Image Strength: "), strength_value, init_image_strength])
  page.use_SDXL = Switcher(label="Use Stable Diffusion XL Pipeline as Primary", tooltip="The latest SDXL base model, with img2img Refiner. It's tasty..", value=prefs['use_SDXL'], on_change=toggle_SDXL)
  page.use_SDXL.visible = status['installed_SDXL']
  page.use_SD3 = Switcher(label="Use Stable Diffusion 3 Pipeline as Primary", tooltip="The latest SD3 base model. It's good..", value=prefs['use_SD3'], on_change=toggle_SD3)
  page.use_SD3.visible = status['installed_SD3']
  #SDXL_compel = Switcher(label="Use Compel Long Prompt Weighting Embeds", tooltip="Re-weight different parts of a prompt string like positive+++ AND (bad negative)-- or (subject)1.3 syntax.", value=prefs['SDXL_compel'], on_change=lambda e:changed(e,'SDXL_compel'))
  SDXL_high_noise_frac = SliderRow(label="SDXL High Noise Fraction", min=0, max=1, divisions=20, round=2, pref=prefs, key='SDXL_high_noise_frac', tooltip="Percentage of Steps to use Base model, then Refiner model. Known as an Ensemble of Expert Denoisers. Value of 1 skips Refine steps.", on_change=lambda e:changed(e,'SDXL_high_noise_frac', apply=False))
  SDXL_negative_conditions = Switcher(label="Use Negative Conditions on Image Size", tooltip="Pass negative conditions about an image's size and position to avoid undesirable cropping behavior in the generated image, and improve image resolution.", value=prefs['SDXL_negative_conditions'], on_change=lambda e:changed(e,'SDXL_negative_conditions', apply=False))
  page.SDXL_params = Container(Column([SDXL_high_noise_frac, SDXL_negative_conditions]), padding=padding.only(top=5, left=20), height=None if prefs['use_SDXL'] else 0, visible=status['installed_SDXL'], animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  page.use_inpaint_model = Switcher(label="Use Specialized Inpaint Model Instead", tooltip="When using init_image and/or mask, use the newer pipeline for potentially better results", value=prefs['use_inpaint_model'], on_change=lambda e:changed(e,'use_inpaint_model', apply=False))
  page.use_inpaint_model.visible = status['installed_img2img']
  page.use_alt_diffusion = Switcher(label="Use AltDiffusion Pipeline Model Instead", value=prefs['use_versatile'], on_change=lambda e:changed(e,'use_versatile', apply=False), tooltip="Supports 9 different languages for text2image & image2image")
  page.use_alt_diffusion.visible = status['installed_alt_diffusion']
  page.use_versatile = Switcher(label="Use Versatile Pipeline Model Instead", value=prefs['use_versatile'], on_change=lambda e:changed(e,'use_versatile', apply=False), tooltip="Dual Guided between prompt & image, or create Image Variation.")
  page.use_versatile.visible = status['installed_versatile']
  use_LoRA_model = Switcher(label="Use LoRA Model Adapter Layers ", value=prefs['use_LoRA_model'], on_change=toggle_LoRA, tooltip="Applies custom trained weighted attention model on top of loaded model")
  def delete_lora_layer(e):
      for l in prefs['active_LoRA_layers']:
        if l['name'] == e.control.data['name']:
          prefs['active_LoRA_layers'].remove(l)
      for c in active_LoRA_layers.controls:
        if c.data['name'] == e.control.data['name']:
            active_LoRA_layers.controls.remove(c)
            break
      active_LoRA_layers.update()
  def delete_all_lora_layers(e):
      prefs['active_LoRA_layers'].clear()
      active_LoRA_layers.controls.clear()
      active_LoRA_layers.update()
  def change_LoRA(e):
      for l in prefs['active_LoRA_layers']:
        if l['name'] == e.control.data['name']:
          l['scale'] = e.control.value
  def add_LoRA(e, LoRA_map=None):
      if LoRA_map != None:
        lora = LoRA_map['name']
        lora_scale = LoRA_map['scale']
        lora_layer = LoRA_map
      else:
        lora = prefs['LoRA_model']
        lora_scale = 1.
        lora_layer = {}
        if lora.startswith("Custom"):
          num = 1
          for c in prefs['active_LoRA_layers']:
              if c['name'].startswith("Custom"):
                  if num == int(c['name'].rpartition('-')[2]):
                      num += 1
          lora_layer = {'name': f'Custom-{num}', 'file':'', 'path':prefs['custom_LoRA_model'], 'scale': lora_scale}
        else:
          for l in prefs['active_LoRA_layers']:
            if l['name'] == lora:
              return
          lora_layer = get_LoRA_model(lora)
          lora_layer['scale'] = lora_scale
        prefs['active_LoRA_layers'].append(lora_layer)
      lora_scaler = SliderRow(label="Scale", min=-3, max=3, divisions=12, round=1, pref=lora_layer, key='scale', tooltip="", expand=True, data=lora_layer, on_change=change_LoRA)
      title_link = Markdown(f"[**{lora_layer['name']}**](https://huggingface.co/{lora_layer['path']})", on_tap_link=lambda e: e.page.launch_url(e.data))
      title = Row([title_link, lora_scaler])#Text(lora_layer['name'])
      active_LoRA_layers.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
        items=[
            PopupMenuItem(icon=icons.DELETE, text="Delete LoRA Layer", on_click=delete_lora_layer, data=lora_layer),
            PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_lora_layers, data=lora_layer),
        ]), data=lora_layer))
      if LoRA_map == None:
        active_LoRA_layers.update()
  
  def delete_SDXL_lora_layer(e):
      for l in prefs['active_SDXL_LoRA_layers']:
        if l['name'] == e.control.data['name']:
          prefs['active_SDXL_LoRA_layers'].remove(l)
      for c in active_SDXL_LoRA_layers.controls:
        if c.data['name'] == e.control.data['name']:
            active_SDXL_LoRA_layers.controls.remove(c)
            break
      active_SDXL_LoRA_layers.update()
  def delete_all_SDXL_lora_layers(e):
      prefs['active_SDXL_LoRA_layers'].clear()
      active_SDXL_LoRA_layers.controls.clear()
      active_SDXL_LoRA_layers.update()
  def change_SDXL_LoRA(e):
      for l in prefs['active_SDXL_LoRA_layers']:
        if l['name'] == e.control.data['name']:
          l['scale'] = e.control.value
  def add_SDXL_LoRA(e, LoRA_map=None):
      if LoRA_map != None:
        lora = LoRA_map['name']
        lora_scale = LoRA_map['scale']
        lora_layer = LoRA_map
      else:
        lora = prefs['SDXL_LoRA_model']
        lora_scale = 1.
        lora_layer = {}
        if lora.startswith("Custom"):
          num = 1
          for c in prefs['active_SDXL_LoRA_layers']:
              if c['name'].startswith("Custom"):
                  if num == int(c['name'].rpartition('-')[2]):
                      num += 1
          lora_layer = {'name': f'Custom-{num}', 'file':'', 'path':prefs['custom_SDXL_LoRA_model'], 'scale': lora_scale}
        else:
          for l in prefs['active_SDXL_LoRA_layers']:
            if l['name'] == lora:
              return
          lora_layer = get_SDXL_LoRA_model(lora)
          lora_layer['scale'] = lora_scale
        prefs['active_SDXL_LoRA_layers'].append(lora_layer)
      lora_scaler = SliderRow(label="Scale", min=-3, max=3, divisions=12, round=1, pref=lora_layer, key='scale', tooltip="", expand=True, data=lora_layer, on_change=change_SDXL_LoRA)
      title_link = Markdown(f"[**{lora_layer['name']}**](https://huggingface.co/{lora_layer['path']})", on_tap_link=lambda e: e.page.launch_url(e.data))
      title = Row([title_link, lora_scaler])
      active_SDXL_LoRA_layers.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
        items=[
            PopupMenuItem(icon=icons.DELETE, text="Delete LoRA Layer", on_click=delete_SDXL_lora_layer, data=lora_layer),
            PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_SDXL_lora_layers, data=lora_layer),
        ]), data=lora_layer))
      if LoRA_map == None:
        active_SDXL_LoRA_layers.update()
  page.LoRA_model = Dropdown(label="LoRA Model Weights", width=235, options=[], value=prefs['LoRA_model'], on_change=changed_LoRA)
  if len(prefs['custom_LoRA_models']) > 0:
    for l in prefs['custom_LoRA_models']:
      page.LoRA_model.options.append(dropdown.Option(l['name']))
  for m in LoRA_models:
      page.LoRA_model.options.append(dropdown.Option(m['name']))
  page.LoRA_model.options.append(dropdown.Option("Custom LoRA Path"))
  custom_LoRA_model = TextField(label="Custom LoRA Model Path", value=prefs['custom_LoRA_model'], expand=True, on_change=lambda e:changed(e, 'custom_LoRA_model', apply=False))
  custom_LoRA_model.visible = True if prefs['LoRA_model'] == "Custom LoRA Path" else False
  
  page.SDXL_LoRA_model = Dropdown(label="SDXL LoRA Model Weights", width=235, options=[], value=prefs['SDXL_LoRA_model'], on_change=changed_SDXL_LoRA)
  if len(prefs['custom_SDXL_LoRA_models']) > 0:
    for l in prefs['custom_SDXL_LoRA_models']:
      page.SDXL_LoRA_model.options.append(dropdown.Option(l['name']))
  for m in SDXL_LoRA_models:
      page.SDXL_LoRA_model.options.append(dropdown.Option(m['name']))
  page.SDXL_LoRA_model.options.append(dropdown.Option("Custom SDXL LoRA Path"))
  custom_SDXL_LoRA_model = TextField(label="Custom SDXL LoRA Model Path", value=prefs['custom_SDXL_LoRA_model'], expand=True, on_change=lambda e:changed(e, 'custom_SDXL_LoRA_model', apply=False))
  custom_SDXL_LoRA_model.visible = True if prefs['SDXL_LoRA_model'] == "Custom SDXL LoRA Path" else False
  active_LoRA_layers = Column([], spacing=0, tight=True)
  active_SDXL_LoRA_layers = Column([], spacing=0, tight=True)
  add_LoRA_layer = ft.FilledButton("➕ Add LoRA", on_click=add_LoRA)
  add_SDXL_LoRA_layer = ft.FilledButton("➕ Add SDXL LoRA", on_click=add_SDXL_LoRA)
  for l in prefs['active_LoRA_layers']:
      add_LoRA(None, l)
  for l in prefs['active_SDXL_LoRA_layers']:
      add_SDXL_LoRA(None, l)
  LoRA_block = Container(ResponsiveRow([Column([Row([page.LoRA_model, custom_LoRA_model, add_LoRA_layer]), active_LoRA_layers], col={'lg': 6}), 
                                        Column([Row([page.SDXL_LoRA_model, custom_SDXL_LoRA_model, add_SDXL_LoRA_layer]), active_SDXL_LoRA_layers], col={'lg': 6})]), padding=padding.only(top=6, left=10), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  LoRA_block.height = None if prefs['use_LoRA_model'] else 0
  def toggle_ip_adapter(e):
      prefs['use_ip_adapter'] = e.control.value
      ip_adapter_container.height = None if e.control.value else 0
      ip_adapter_container.update()
      ip_adapter_model.visible = e.control.value
      ip_adapter_model.update()
      ip_adapter_SDXL_model.visible = e.control.value
      ip_adapter_SDXL_model.update()
  use_ip_adapter = Switcher(label="Use IP-Adapter Reference Image  ", value=prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip="Image Prompt applies to Pipelines for SD 1.5, SDXL, Inpainting, Self-Attention Guidance, Attend & Excite, Safe and Panorama")
  ip_adapter_model = Dropdown(label="IP-Adapter SD Model", width=220, options=[], value=prefs['ip_adapter_model'], visible=prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))
  for m in ip_adapter_models:
      ip_adapter_model.options.append(dropdown.Option(m['name']))
  ip_adapter_SDXL_model = Dropdown(label="IP-Adapter SDXL Model", width=220, options=[], value=prefs['ip_adapter_SDXL_model'], visible=prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_SDXL_model'))
  for m in ip_adapter_SDXL_models:
      ip_adapter_SDXL_model.options.append(dropdown.Option(m['name']))
  ip_adapter_image = FileInput(label="IP-Adapter Image", pref=prefs, key='ip_adapter_image', col={'lg':6}, page=page)
  ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=prefs, key='ip_adapter_strength', col={'lg':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
  ip_adapter_container = Container(ResponsiveRow([ip_adapter_image, ip_adapter_strength]), height = None if prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)

  centipede_prompts_as_init_images = Switcher(label="Centipede Prompts as Init Images", tooltip="Feeds each image to the next prompt sequentially down the line. Gives interesting results..", value=prefs['centipede_prompts_as_init_images'], on_change=toggle_centipede)
  use_interpolation = Switcher(label="Use Interpolation to Walk Latent Space between Prompts", tooltip="Creates animation frames transitioning, but it's not always perfect.", value=prefs['use_interpolation'], on_change=toggle_interpolation)
  interpolation_steps = Slider(min=1, max=100, divisions=99, label="{value}", value=prefs['num_interpolation_steps'], on_change=change_interpolation_steps, expand=True)
  interpolation_steps_value = Text(f" {int(prefs['num_interpolation_steps'])} steps", weight=FontWeight.BOLD)
  interpolation_steps_slider = Container(Row([Text(f"Number of Interpolation Steps between Prompts: "), interpolation_steps_value, interpolation_steps]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  Row([Text(f"Number of Interpolation Steps between Prompts: "), interpolation_steps_value, interpolation_steps])
  if not bool(prefs['use_interpolation']):
    interpolation_steps_slider.height = 0
  page.interpolation_block = Column([use_interpolation, interpolation_steps_slider])
  page.img_block = Container(Column([image_pickers, strength_slider, page.use_inpaint_model, centipede_prompts_as_init_images, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  if not status['installed_interpolation']:
    page.interpolation_block.visible = False
  elif bool(prefs['use_interpolation']):
    page.img_block.height = 0
  use_SAG = Switcher(label="Use Self-Attention Guidance (SAG) Text-to-Image", value=prefs['use_SAG'], on_change=toggle_SAG, tooltip="Can drastically boost the performance and quality. Extracts the intermediate attention map from a diffusion model at every iteration and selects tokens above a certain attention score for masking and blurring to obtain a partially blurred input.")
  sag_scale = Slider(min=0, max=1, divisions=20, label="{value}", value=prefs['sag_scale'], tooltip="How much Self-Attention Guidance to apply.", on_change=change_sag_scale, expand=True)
  sag_scale_value = Text(f" {float(prefs['sag_scale'])}", weight=FontWeight.BOLD)
  sag_scale_slider = Container(Row([Text(f"SAG Scale: "),sag_scale_value, sag_scale]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  page.use_SAG = Column([use_SAG, sag_scale_slider])
  if not bool(prefs['use_SAG']):
    sag_scale_slider.height = 0
  if not status['installed_SAG']:
    page.use_SAG.visible = False
  use_attend_and_excite = Switcher(label="Use Attend and Excite", value=prefs['use_attend_and_excite'], on_change=toggle_attend_and_excite, tooltip="To use, include plus signs before subject words in prompt to indicate token indices, like 'a +cat and a +frog'.")
  max_iter_to_alter = Slider(min=1, max=100, divisions=99, label="{value}", value=prefs['max_iter_to_alter'], tooltip="The first denoising steps are where the attend-and-excite is applied. I.e. if `max_iter_to_alter` is 25 and there are a total of `30` denoising steps, the first 25 denoising steps will apply attend-and-excite and the last 5 will not apply attend-and-excite.", on_change=change_max_iter_to_alter, expand=True)
  max_iter_to_alter_value = Text(f" {int(prefs['max_iter_to_alter'])} iterations", weight=FontWeight.BOLD)
  max_iter_to_alter_slider = Container(Row([Text(f"Max Iterations to Alter: "), max_iter_to_alter_value, max_iter_to_alter]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  page.use_attend_and_excite = Column([use_attend_and_excite, max_iter_to_alter_slider])
  if not bool(prefs['use_attend_and_excite']):
    max_iter_to_alter_slider.height = 0
  if not status['installed_attend_and_excite']:
    page.use_attend_and_excite.visible = False
  page.use_clip_guided_model = Switcher(label="Use CLIP-Guided Model", tooltip="Uses more VRAM, so you'll probably need to make image size smaller", value=prefs['use_clip_guided_model'], on_change=toggle_clip)
  clip_guidance_scale = Slider(min=1, max=5000, divisions=4999, label="{value}", value=prefs['clip_guidance_scale'], on_change=lambda e:changed(e,'clip_guidance_scale'), expand=True)
  clip_guidance_scale_slider = Row([Text("CLIP Guidance Scale: "), clip_guidance_scale])
  use_cutouts = Checkbox(label="Use Cutouts", value=bool(prefs['use_cutouts']), fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=change_use_cutouts)
  num_cutouts = NumberPicker(label="    Number of Cutouts: ", min=1, max=10, value=prefs['num_cutouts'], on_change=lambda e: changed(e, 'num_cutouts', asInt=True))
  num_cutouts.visible = bool(prefs['use_cutouts'])
  #num_cutouts = TextField(label="Number of Cutouts", value=prefs['num_cutouts'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_cutouts', asInt=True))
  unfreeze_unet = Checkbox(label="Unfreeze UNET", value=prefs['unfreeze_unet'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'unfreeze_unet', apply=False))
  unfreeze_vae = Checkbox(label="Unfreeze VAE", value=prefs['unfreeze_vae'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'unfreeze_vae', apply=False))
  page.clip_block = Container(Column([clip_guidance_scale_slider, Row([use_cutouts, num_cutouts], expand=False), unfreeze_unet, unfreeze_vae, Divider(height=9, thickness=2)]), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  page.use_conceptualizer_model = Switcher(label="Use Custom Conceptualizer Model", tooltip="Use Textual-Inversion Community Model Concepts", value=prefs['use_conceptualizer'], on_change=toggle_conceptualizer)
  page.use_conceptualizer_model.visible = bool(status['installed_conceptualizer'])
  page.use_depth2img = Switcher(label="Use Depth2Image Pipeline for img2img init image generation", value=prefs['use_depth2img'], on_change=lambda e:changed(e,'use_depth2img', apply=False), tooltip="To use, provide init_image with a good composition and prompts to approximate same depth.")
  page.use_depth2img.visible = bool(status['installed_depth2img'])
  page.use_imagic = Switcher(label="Use iMagic for img2img init image editing", value=prefs['use_imagic'], on_change=lambda e:changed(e,'use_imagic', apply=False), tooltip="Allows you to edit an image with prompt text.")
  page.use_imagic.visible = bool(status['installed_imagic'])
  page.use_composable = Switcher(label="Use Composable Prompts for txt2img Weight | Segments", value=prefs['use_composable'], on_change=lambda e:changed(e,'use_composable', apply=False), tooltip="Allows conjunction and negation operators for compositional generation with conditional diffusion models")
  page.use_composable.visible = bool(status['installed_composable'])
  page.use_panorama = Column([Switcher(label="Use Panorama text2image Pipeline as Primary", value=prefs['use_panorama'], on_change=lambda e:changed(e,'use_panorama', apply=False), tooltip="Fuses together images to make extra-wide 2048x512"),
                              Checkbox(label="Use Circular Padding to remove stitching artifacts", value=prefs['panorama_circular_padding'], tooltip="To seamlessly generate a transition from the right to left, maintaining consistency in a 360-degree sense.", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'panorama_circular_padding', apply=False)),
                              Row([Text("Panoramic Width x 512:"), Slider(min=1024, max=4608, divisions=28, label="{value}px", expand=True, value=prefs['panorama_width'], on_change=lambda e:changed(e, 'panorama_width', asInt=True, apply=False))])])
  page.use_panorama.visible = status['installed_panorama']
  page.use_safe = Switcher(label="Use Safe Diffusion Pipeline as Primary", value=prefs['use_safe'], on_change=lambda e:changed(e,'use_safe', apply=False), tooltip="Models trained only on Safe images")
  page.use_safe.visible = bool(status['installed_safe'])
  page.use_upscale = Switcher(label="Upscale 4X with Stable Diffusion 2", value=prefs['use_upscale'], on_change=lambda e:changed(e,'use_upscale', apply=False), tooltip="Enlarges your Image Generations guided by the same Prompt.")
  page.use_upscale.visible = bool(status['installed_upscale'])
  upscaler = UpscaleBlock(prefs)
  page.upscalers.append(upscaler)
  #page.img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0
  page.use_clip_guided_model.visible = status['installed_clip']
  page.clip_block.height = None if status['installed_clip'] and prefs['use_clip_guided_model'] else 0
  parameters_button = ElevatedButton(content=Text(value="📜   Continue to Image Prompts", size=20), on_click=run_parameters)
  parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)
  #apply_changes_button = ElevatedButton(content=Text(value="🔀   Apply Changes to Current Prompts", size=20), on_click=apply_to_prompts)
  #apply_changes_button.visible = len(prompts) > 0 and status['changed_prefs']
  def show_apply_fab(show = True):
    if show:
      page.floating_action_button = FloatingActionButton(content=Row([Icon(icons.TRANSFORM), Text("Apply Changes to Current Prompts", size=18)], alignment="center", spacing=5), width=333, shape=ft.RoundedRectangleBorder(radius=22), on_click=apply_to_prompts)
      #page.floating_action_button = FloatingActionButton(icon=icons.TRANSFORM, text="Apply Changes to Current Prompts", on_click=apply_to_prompts)
      page.update()
    else:
      if page.floating_action_button is not None:
        page.floating_action_button = None
        page.update()
  show_apply_fab(len(prompts) > 0 and status['changed_prefs'])
  page.show_apply_fab = show_apply_fab
  def updater():
      #parameters.update()
      c.update()
      page.update()
      #print("Updated Parameters Page")

  c = Column([Container(
      padding=padding.only(18, 14, 20, 10), content=Column([
        Header("📝  Stable Diffusion Image Parameters"),
        #param_rows,
        batch_row,
        number_row,
        steps,
        guidance,
        eta,
        width_slider, height_slider, #Divider(height=9, thickness=2),
        page.interpolation_block, page.img_block, page.use_safe, page.use_SDXL, page.SDXL_params, page.use_SD3, page.use_alt_diffusion, page.use_clip_guided_model, page.clip_block, page.use_versatile, page.use_SAG, page.use_attend_and_excite, page.use_panorama, page.use_conceptualizer_model,
        use_LoRA_model, LoRA_block,
        Row([use_ip_adapter, ip_adapter_model, ip_adapter_SDXL_model], vertical_alignment=CrossAxisAlignment.START),
        ip_adapter_container,
        page.use_imagic, page.use_depth2img, page.use_composable, page.use_upscale, upscaler,
        #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
        #parameters_row,
      ],
  ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, eta, seed,
  return c

prompts = []
args = {}

def update_args():
    global args
    args = {
        "batch_size":int(prefs['batch_size']),
        "n_iterations":int(prefs['n_iterations']),
        "steps":int(prefs['steps']),
        "eta":float(prefs['eta']),
        "width":int(prefs['width']),
        "height":int(prefs['height']),
        "guidance_scale":float(prefs['guidance_scale']),
        "seed":int(prefs['seed']),
        "precision":prefs['precision'],
        "init_image": prefs['init_image'],
        "init_image_strength": prefs['init_image_strength'],
        "mask_image": prefs['mask_image'],
        "alpha_mask": prefs['alpha_mask'],
        "invert_mask": prefs['invert_mask'],
        "prompt2": None, "tweens": 10,
        "negative_prompt": "",
        "use_clip_guided_model": prefs['use_clip_guided_model'],
        "clip_prompt": "",
        "clip_guidance_scale": float(prefs['clip_guidance_scale']),
        "use_cutouts": bool(prefs['use_cutouts']),
        "num_cutouts": int(prefs['num_cutouts']),
        "unfreeze_unet": prefs['unfreeze_unet'],
        "unfreeze_vae": prefs['unfreeze_vae'],
        "use_Stability": False,
        "use_conceptualizer": False}

update_args()

class Dream:
    def __init__(self, prompt, **kwargs):
        self.prompt = prompt
        self.arg = args.copy()
        for key, value in kwargs.items():
          if key=='arg': self.arg = value
          elif key=="batch_size": self.arg[key] = int(value)
          elif key=="n_iterations": self.arg[key] = int(value)
          elif key=="steps": self.arg[key] = int(value)
          elif key=="eta": self.arg[key] = float(value)
          elif key=="width": self.arg[key] = int(value)
          elif key=="height": self.arg[key] = int(value)
          elif key=="guidance_scale": self.arg[key] = float(value)
          elif key=="seed": self.arg[key] = int(value)
          elif key=="precision": self.arg[key] = value
          elif key=="init_image": self.arg[key] = value
          elif key=="init_image_strength": self.arg[key] = value
          elif key=="mask_image": self.arg[key] = value
          elif key=="alpha_mask": self.arg[key] = value
          elif key=="invert_mask": self.arg[key] = value
          elif key=="prompt2": self.arg[key] = value
          elif key=="tweens": self.arg[key] = int(value)
          elif key=="negative_prompt": self.arg[key] = value
          elif key=="clip_prompt": self.arg[key] = value
          elif key=="use_clip_guided_model": self.arg[key] = value
          elif key=="clip_guidance_scale": self.arg[key] = float(value)
          elif key=="use_cutouts": self.arg[key] = value
          elif key=="num_cutouts": self.arg[key] = int(value)
          elif key=="unfreeze_unet": self.arg[key] = value
          elif key=="unfreeze_vae": self.arg[key] = value
          elif key=="use_Stability": self.arg[key] = value
          elif key=="use_conceptualizer": self.arg[key] = value
          elif key=="prompt": self.prompt = value
          else: print(f"Unknown argument: {key} = {value}")
    def __getitem__(self, key):
        if key in self.arg:
          return self.arg[key]
        else:
          print(f"Error getting arg {key} from Dream")
          return ""
    def __setitem__(self, key, value):
        self.arg[key] = value


def format_filename(s, force_underscore=False, use_dash=False, max_length=None):
    if prefs['file_datetime'] and not use_dash and not force_underscore:
        return datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    file_max_length = int(prefs['file_max_length']) if max_length == None else int(max_length)
    valid_chars = "-_.() %s%s" % (string.ascii_letters, string.digits)
    filename = ''.join(c for c in s if c in valid_chars)
    if use_dash:
        filename = filename.replace(' ','-')
    else:
        if not prefs['file_allowSpace'] or force_underscore: filename = filename.replace(' ','_')
    filename = filename[:-1] if filename[-1] == '.' else filename
    return filename[:file_max_length]

def to_title(s, sentence=False, clean=True):
    if clean:
        s = s.replace('_',' ')
        s = s.replace('-',' ')
    if sentence:
        sentences = s.split(". ")
        sentences2 = [sentence[0].capitalize() + sentence[1:] for sentence in sentences]
        s2 = '. '.join(sentences2)
        return s2
    else:
        return string.capwords(s)

'''from collections import ChainMap
def merge_dict(*dicts):
    all_keys  = set(k for d in dicts for k in d.keys())
    chain_map = ChainMap(*reversed(dicts))
    return {k: chain_map[k] for k in all_keys}
def merge_dict(dict1, dict2):
    merged_dict = {**dict1, **dict2}
    return merged_dict
'''
def merge_dict(dict1, dict2):
    new_dict = {}
    for key in dict1:
        new_dict[key] = dict1[key]
    for key in dict2:
        new_dict[key] = dict2[key]
    return new_dict

import copy

def editPrompt(e):
    global prompts, prefs, status
    open_dream = e.control.data
    idx = prompts.index(open_dream)
    def changed_tweening(e):
        status['changed_prefs'] = True
        tweening_params.height = None if e.control.value else 0
        tweening_params.update()
        #prompt2.visible = e.control.value
        #tweens.visible = e.control.value
        prompt_tweening = e.control.value
        e.page.update()
    def changed_tweens(e):
        prefs['tweens'] = int(e.control.value)
    def close_dlg(e):
        nonlocal edit_dlg
        edit_dlg.open = False
        #e.page.dialog.open = False
        e.page.update()
        #del edit_dlg
        #e.page.dialog = None
    def open_dlg(e):
        nonlocal edit_dlg
        e.page.overlay.append(edit_dlg)
        edit_dlg.open = True
        e.page.update()
    def save_dlg(e):
        nonlocal arg, open_dream, edit_dlg
        dream = open_dream #e.control.data
        dream.prompt = edit_text.value
        arg['batch_size'] = int(batch_size.value)
        arg['n_iterations'] = int(n_iterations.value)
        arg['steps'] = int(steps.value)
        arg['eta'] = float(eta.value)
        arg['seed'] = int(seed.value)
        arg['guidance_scale'] = float(guidance_scale.value)
        arg['width'] = int(width_slider.value)
        arg['height'] = int(height_slider.value)
        arg['init_image'] = init_image.value
        arg['mask_image'] = mask_image.value
        arg['init_image_strength'] = float(init_image_strength.value)
        arg['alpha_mask'] = alpha_mask.value
        arg['invert_mask'] = invert_mask.value
        arg['prompt2'] = prompt2.value if bool(use_prompt_tweening.value) else None
        arg['tweens'] = int(tweens.value)
        arg['negative_prompt'] = negative_prompt.value if bool(negative_prompt.value) else None
        arg['use_clip_guided_model'] = use_clip_guided_model.value
        arg['clip_guidance_scale'] = float(clip_guidance_scale.value)
        arg['use_cutouts'] = use_cutouts.value
        arg['num_cutouts'] = int(num_cutouts.value)
        arg['unfreeze_unet'] = unfreeze_unet.value
        arg['unfreeze_vae'] = unfreeze_vae.value
        dream.arg = arg
        diffs = arg_diffs(arg, args)
        if bool(diffs):
          e.page.prompts_list.controls[idx].subtitle = Text("    " + diffs)
        else:
          e.page.prompts_list.controls[idx].subtitle = None
        e.page.prompts_list.controls[idx].title.value = dream.prompt # = Text(edit_text.value)
        status['changed_prefs'] = True
        edit_dlg.open = False
        e.page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
        nonlocal pick_type
        if e.progress == 1:
          save_file(e.file_name)
          e.page.update()
    def save_file(file_name):
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
        else:
          fname = file_name
        if pick_type == "init":
          init_image.value = fname
          init_image.update()
          prefs['init_image'] = fname
        elif pick_type == "mask":
          mask_image.value = fname
          mask_image.update()
          prefs['mask_image'] = fname
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if e.page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=e.page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    e.page.overlay.append(file_picker)
    pick_type = ""
    #page.overlay.append(pick_files_dialog)
    def pick_init(e):
        nonlocal pick_type
        pick_type = "init"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Init Image File")
    def pick_mask(e):
        nonlocal pick_type
        pick_type = "mask"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Black & White Mask Image")
    def toggle_clip(e):
        if e.control.value:
          img_block.height = 0
          clip_block.height = None if status['installed_clip'] else 0
        else:
          img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0
          clip_block.height = 0
        img_block.update()
        clip_block.update()
        #changed(e)
    arg = open_dream.arg #e.control.data.arg
    edit_text = TextField(label="Composable | Prompt | Text" if prefs['use_composable'] and status['installed_composable'] else "Prompt Text", col={'lg':9}, value=open_dream.prompt, multiline=True)
    negative_prompt = TextField(label="Segmented Weights 1 | -0.7 | 1.2" if prefs['use_composable'] and status['installed_composable'] else "Negative Prompt Text", col={'lg':3}, value=str((arg['negative_prompt'] or '') if 'negative_prompt' in arg else ''))
    #batch_folder_name = TextField(label="Batch Folder Name", value=arg['batch_folder_name'], on_change=changed)
    #print(str(arg))
    prompt_tweening = bool(arg['prompt2']) if 'prompt2' in arg else False
    use_prompt_tweening = Switcher(label="Prompt Tweening", value=prompt_tweening, on_change=changed_tweening)
    use_prompt_tweening.visible = True if status['installed_txt2img'] and prefs['higher_vram_mode'] else False
#TODO: Fix tweening code for float16 lpw pipeline to reactivate tweening
    prompt2 = TextField(label="Prompt 2 Transition Text", expand=True, value=arg['prompt2'] if 'prompt2' in arg else '')
    tweens = TextField(label="# of Tweens", value=str(arg['tweens'] if 'tweens' in arg else 8), keyboard_type=KeyboardType.NUMBER, width = 90)
    #tweens =  NumberPicker(label="# of Tweens: ", min=2, max=300, value=int(arg['tweens'] if 'tweens' in arg else 8), on_change=changed_tweens),
    #prompt2.visible = prompt_tweening
    #tweens.visible = prompt_tweening
    tweening_params = Container(Row([Container(content=None, width=8), prompt2, tweens]), padding=padding.only(top=4, bottom=3), animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    tweening_params.height = None if prompt_tweening else 0
    #tweening_row = Row([use_prompt_tweening, ])#tweening_params
    batch_size = NumberPicker(label="Batch Size: ", min=1, max=10, value=arg['batch_size'])
    n_iterations = NumberPicker(label="Number of Iterations: ", min=1, max=30, value=arg['n_iterations'])
    #batch_size = TextField(label="Batch Size", value=str(arg['batch_size']), keyboard_type=KeyboardType.NUMBER)
    #n_iterations = TextField(label="Number of Iterations", value=str(arg['n_iterations']), keyboard_type=KeyboardType.NUMBER)
    steps = TextField(label="Steps", value=str(arg['steps']), keyboard_type=KeyboardType.NUMBER)
    eta = TextField(label="DDIM ETA", value=str(arg['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise (only with DDIM sampler)")
    seed = TextField(label="Seed", value=str(arg['seed']), keyboard_type=KeyboardType.NUMBER, hint_text="0 or -1 picks a Random seed")
    guidance_scale = TextField(label="Guidance Scale", value=str(arg['guidance_scale']), keyboard_type=KeyboardType.NUMBER)
    param_columns = Row([Column([steps, seed, batch_size]), Column([guidance_scale, eta, n_iterations])])
    #guidance_scale = Slider(min=0, max=50, divisions=100, label="{value}", value=arg['guidance_scale'], expand=True)
    #guidance = Row([Text("Guidance Scale: "), guidance_scale])
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=arg, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=arg, key='height')
    init_image = TextField(label="Init Image", value=arg['init_image'], expand=1, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    mask_image = TextField(label="Mask Image", value=arg['mask_image'], expand=1, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))
    alpha_mask = Checkbox(label="Alpha Mask", value=arg['alpha_mask'], tooltip="Use Transparent Alpha Channel of Init as Mask", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)
    invert_mask = Checkbox(label="Invert Mask", value=arg['invert_mask'], tooltip="Reverse Black & White of Image Mask", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)
    image_row = ResponsiveRow([Row([init_image, alpha_mask], col={"lg":6}), Row([mask_image, invert_mask], col={"lg":6})])
    init_image_strength = Slider(min=0.1, max=0.9, divisions=16, label="{value}", round=2, value=float(arg['init_image_strength']), expand=True)
    strength_slider = Row([Text("Init Image Strength: "), init_image_strength])
    img_block = Container(content=Column([image_row, strength_slider]), padding=padding.only(top=4, bottom=3), animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    #img_block.height = None if (status['installed_txt2img'] or status['installed_stability'] or status['installed_SDXL']) else 0
    use_clip_guided_model = Switcher(label="Use CLIP-Guided Model", tooltip="Uses more VRAM, so you'll probably need to make image size smaller", value=arg['use_clip_guided_model'], on_change=toggle_clip)
    clip_guidance_scale = Slider(min=1, max=5000, divisions=4999, label="{value}", value=arg['clip_guidance_scale'], expand=True)
    clip_guidance_scale_slider = Row([Text("CLIP Guidance Scale: "), clip_guidance_scale])
    use_cutouts = Checkbox(label="Use Cutouts", value=bool(arg['use_cutouts']), fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)
    num_cutouts = NumberPicker(label="    Number of Cutouts: ", min=1, max=10, value=arg['num_cutouts'])
    #num_cutouts.visible = bool(prefs['use_cutouts'])
    #num_cutouts = TextField(label="Number of Cutouts", value=prefs['num_cutouts'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_cutouts', asInt=True))
    unfreeze_unet = Checkbox(label="Unfreeze UNET", value=arg['unfreeze_unet'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)
    unfreeze_vae = Checkbox(label="Unfreeze VAE", value=arg['unfreeze_vae'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)
    clip_block = Container(Column([clip_guidance_scale_slider, Row([use_cutouts, num_cutouts], expand=False), unfreeze_unet, unfreeze_vae, Divider(height=9, thickness=2)]), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    if not status['installed_clip']:
      use_clip_guided_model.visible = False
      clip_block.height = 0
    elif not arg['use_clip_guided_model']:
      clip_block.height = 0
    edit_dlg = AlertDialog(title=Text("📝  Edit Prompt Dream Parameters"), content=Container(Column([
          Container(content=None, height=7),
          ResponsiveRow([
            edit_text,
            negative_prompt,
          ]),
          #Text("Override any Default Parameters"),
          #use_prompt_tweening,
          #tweening_params,
          #batch_size, n_iterations, steps, eta, seed, guidance,
          param_columns,
          width_slider, height_slider, img_block,
          use_clip_guided_model, clip_block,
          #Row([Column([batch_size, n_iterations, steps, eta, seed,]), Column([guidance, width_slider, height_slider, Divider(height=9, thickness=2), (img_block if prefs['install_img2img'] else Container(content=None))])],),
        ], alignment=MainAxisAlignment.START, width=(e.page.width if e.page.web else e.page.window.width) - 200, height=(e.page.height if e.page.web else e.page.window.height) - 100, scroll=ScrollMode.AUTO), width=(e.page.width if e.page.web else e.page.window.width) - 200, height=(e.page.height if e.page.web else e.page.window.height) - 100),
        actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Prompt ", size=19, weight=FontWeight.BOLD), on_click=save_dlg)], actions_alignment=MainAxisAlignment.END)
    #open_dlg(e)
    e.page.overlay.append(edit_dlg)
    edit_dlg.open = True
    e.page.update()

def buildPromptsList(page):
  parameter = Ref[ListTile]()
  global prompts, args, prefs
  def changed(e, pref=None):
      if pref is not None:
          prefs[pref] = e.control.value
  def prompt_help(e):
      def close_help_dlg(e):
        nonlocal prompt_help_dlg
        prompt_help_dlg.open = False
        page.update()
      prompt_help_dlg = AlertDialog(title=Text("💁   Help with Prompt Creations"), content=Column([
          Text("You can keep your text prompts simple, or get really complex with it. Just describe the image you want it to dream up with as many details as you can think of. Add artists, styles, colors, adjectives and get creative..."),
          Text('Now you can add prompt weighting with Compel, so you can emphasize the strength of certain words between parentheses, and de-emphasize words between brackets. For example: "A (hyper realistic) painting of (magical:1.8) owl with the (((face of a cat))), without [[tail]], in a [twisted:0.6] tree, by Thomas Kinkade"'),
          Text('After adding your prompts, click on a prompt line to edit all the parameters of it. There you can add negative prompts like "lowres, bad_anatomy, error_body, bad_fingers, missing_fingers, error_lighting, jpeg_artifacts, signature, watermark, username, blurry" or anything else you don\'t want. Use Prompt Helpers -> Negatives to get a good list of negative prompts.'),
          Text('Then you can override all the parameters for each individual prompt, playing with variations of sizes, steps, guidance scale, init & mask image, seeds, etc.  In the prompts list, you can press the ... options button to duplicate, delete and move prompts in the batch queue.  When ready, Run Diffusion on Prompts...')
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😀  Very nice... ", on_click=close_help_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(prompt_help_dlg)
      prompt_help_dlg.open = True
      page.update()
  def paste_prompts(e):
      def save_prompts_list(e):
          close_dlg(e)
          plist = enter_text.value.strip()
          prompts_list = plist.split('\n')
          negative_prompt = negative_prompt_text.value
          negative = None
          if bool(negative_prompt):
              if '_' in negative_prompt:
                  negative_prompt = nsp_parse(negative_prompt)
              negative_prompt = prompt_parse(negative_prompt)
              negative = {'negative_prompt': negative_prompt}
          for pr in prompts_list:
              if bool(pr.strip()):
                  if '_' in pr:
                      pr = nsp_parse(pr)
                  pr = prompt_parse(pr)
                  add_to_prompts(pr.strip(), negative, update=False)
          page.update()
          status['changed_prefs'] = True
      def close_dlg(e):
          dlg_paste.open = False
          page.update()
      enter_text = TextField(label="Enter Prompts List with multiple lines", expand=True, filled=True, min_lines=30, multiline=True, autofocus=True)
      dlg_paste = AlertDialog(modal=False, title=Text("📝  Paste or Write Prompts List from Simple Text"), content=Container(Column([enter_text], alignment=MainAxisAlignment.START, tight=True, width=(page.width if page.web else page.window.width) - 180, height=(page.height if page.web else page.window.height) - 100, scroll="none"), width=(page.width if page.web else page.window.width) - 180, height=(page.height if page.web else page.window.height) - 100), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Add to Prompts List ", size=19, weight=FontWeight.BOLD), on_click=save_prompts_list)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(dlg_paste)
      dlg_paste.open = True
      page.update()
  def copy_prompts(e):
      def copy_prompts_list(pl):
          nonlocal text_list, enter_text
          page.set_clipboard(enter_text.value)
          close_dlg(e)
          toast_msg(page, f"📋   Prompt Text copied to clipboard...")
      def close_dlg(e):
          dlg_copy.open = False
          page.update()
      text_list = ""
      for d in prompts:
          p = d.prompt[0] if type(d.prompt) == list else d.prompt if bool(d.prompt) else d['prompt'] if 'prompt' in d else d.arg['prompt'] if 'prompt' in d.arg else ''
          text_list += f"{p}\n"
      enter_text = TextField(label="Prompts on multiple lines", value=text_list.strip(), expand=True, filled=True, multiline=True, autofocus=True)
      dlg_copy = AlertDialog(modal=False, title=Text("📝  Prompts List as Plain Text"), content=Container(Column([enter_text], alignment=MainAxisAlignment.START, tight=True, width=(page.width if page.web else page.window.width) - 180, height=(page.height if page.web else page.window.height) - 100, scroll="none"), width=(page.width if page.web else page.window.width) - 180, height=(page.height if page.web else page.window.height) - 100), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Copy Prompts List to Clipboard", size=19, weight=FontWeight.BOLD), data=text_list, on_click=lambda ev: copy_prompts_list(text_list))], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(dlg_copy)
      dlg_copy.open = True
      page.update()
  def delete_prompt(e):
      idx = prompts.index(e.control.data)
      prompts.pop(idx)
      prompts_list.controls.pop(idx)
      prompts_list.update()
      status['changed_prefs'] = True
      play_snd(Snd.DELETE, page)
  def copy_prompt(e):
      open_dream = e.control.data
      page.set_clipboard(open_dream.prompt)
      toast_msg(page, f"📋   Prompt Text copied to clipboard...")
  def duplicate_prompt(e):
      open_dream = e.control.data
      add_to_prompts(open_dream.prompt, open_dream.arg)
  def duplicate_multiple(e):
      open_dream = e.control.data
      num_times = 2
      def close_dlg(e):
          duplicate_modal.open = False
          page.update()
      def save_dlg(e):
          params = [
              {
                  'guidance_scale': round(guidance_scale * 100) / 100 if transition_guidance_scale.switch.value else open_dream.arg['guidance_scale'],
                  'steps': int(steps) if transition_steps.switch.value else open_dream.arg['steps'],
                  'init_image_strength': round(init_image_strength * 100) / 100 if transition_init_image_strength.switch.value else open_dream.arg['init_image_strength']
              }
              for guidance_scale, steps, init_image_strength in zip(
                  tween_value(start_guidance_scale.value, end_guidance_scale.value, num_times) if transition_guidance_scale.switch.value else [open_dream.arg['guidance_scale']] * num_times,
                  tween_value(start_steps.value, end_steps.value, num_times) if transition_steps.switch.value else [open_dream.arg['steps']] * num_times,
                  tween_value(start_init_image_strength.value, end_init_image_strength.value, num_times) if transition_init_image_strength.switch.value else [open_dream.arg['init_image_strength']] * num_times
              )
          ]
          '''params = [{} for _ in range(num_times)]
          if transition_guidance_scale.value:
              for i, p in enumerate(tween_value(start_guidance_scale.value, end_guidance_scale.value, num_times)):
                params[i]['guidance_scale'] = p
          if transition_steps.value:
              for i, p in enumerate(tween_value(start_steps.value, end_steps.value, num_times)):
                params[i]['steps'] = p
          if transition_init_image_strength.value:
              for i, p in enumerate(tween_value(start_init_image_strength.value, end_init_image_strength.value, num_times)):
                params[i]['steps'] = p'''
          for i in params:#range(num_times):
            #print(i)
            add_to_prompts(open_dream.prompt, merge_dict(open_dream.arg, i))
          duplicate_modal.open = False
          page.update()
      def change_num(e):
          nonlocal num_times
          num_times = int(e.control.value)
      def change_transition(e, prop):
          if prop == "guidance_scale":
            guidance_scale_row.visible = e.control.value
            guidance_scale_row.update()
          elif prop == "steps":
            steps_row.visible = e.control.value
            steps_row.update()
          elif prop == "init_image_strength":
            init_image_strength_row.visible = e.control.value
            init_image_strength_row.update()
      transition_guidance_scale = Switcher(label="Transition Guidance Scale", value=False, active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:change_transition(e,'guidance_scale'), tooltip="Create Prompts Tweening the Starting and Ending Values Between.")
      transition_steps = Switcher(label="Transition Interpolation Steps", value=False, active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:change_transition(e,'steps'), tooltip="Create Prompts Tweening the Starting and Ending Values Between.")
      transition_init_image_strength = Switcher(label="Transition Init Image Strength", value=False, active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:change_transition(e,'init_image_strength'), tooltip="Create Prompts Tweening the Starting and Ending Values Between.")
      start_guidance_scale = SliderRow(label="Start Guidance", min=0, max=50, divisions=100, round=1, value=float(open_dream['guidance_scale']), col={'md':6})
      end_guidance_scale = SliderRow(label="End Guidance", min=0, max=50, divisions=100, round=1, value=float(open_dream['guidance_scale']), col={'md':6})
      start_steps = SliderRow(label="Start Steps", min=0, max=100, divisions=100, value=int(open_dream['steps']), col={'md':6})
      end_steps = SliderRow(label="End Steps", min=0, max=100, divisions=100, value=int(open_dream['steps']), col={'md':6})
      #start_init_image_strength = Slider(min=0.1, max=0.9, divisions=16, label="{value}", round=2, value=float(open_dream['init_image_strength']), col={'md':6}, tooltip="The Starting init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
      #end_init_image_strength = Slider(min=0.1, max=0.9, divisions=16, label="{value}", round=2, value=float(open_dream['init_image_strength']), col={'md':6}, tooltip="The Ending init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
      start_init_image_strength = SliderRow(label="Start Init Strength", min=0.0, max=1.0, divisions=20, round=2, value=float(open_dream['init_image_strength']), col={'md':6}, tooltip="Starting init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
      end_init_image_strength = SliderRow(label="End Init Strength", min=0.0, max=1.0, divisions=20, round=2, value=float(open_dream['init_image_strength']), col={'md':6}, tooltip="Ending init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
      guidance_scale_row = ResponsiveRow([start_guidance_scale, end_guidance_scale], visible=False, width=(e.page.width if e.page.web else e.page.window.width) - 200)
      steps_row = ResponsiveRow([start_steps, end_steps], visible=False, width=(e.page.width if e.page.web else e.page.window.width) - 200)
      init_image_strength_row = ResponsiveRow([start_init_image_strength, end_init_image_strength], visible=False, width=(e.page.width if e.page.web else e.page.window.width) - 200)
      duplicate_modal = AlertDialog(modal=False, title=Text("🌀  Duplicate Prompt Multiple Times"), content=Container(Column([
            Container(content=None, height=7),
            NumberPicker(label="Number of Copies: ", min=1, max=99, value=num_times, on_change=change_num),
            transition_guidance_scale, guidance_scale_row,
            transition_steps, steps_row,
            transition_init_image_strength, init_image_strength_row
          ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO)), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":bowling:") + "  Duplicate Prompt ", size=19, weight=FontWeight.BOLD), on_click=save_dlg)], actions_alignment=MainAxisAlignment.END)
      e.page.overlay.append(duplicate_modal)
      duplicate_modal.open = True
      e.page.update()
  def move_down(e):
      idx = prompts.index(e.control.data)
      if idx < (len(prompts) - 1):
        d = prompts.pop(idx)
        prompts.insert(idx+1, d)
        dr = prompts_list.controls.pop(idx)
        prompts_list.controls.insert(idx+1, dr)
        prompts_list.update()
  def move_up(e):
      idx = prompts.index(e.control.data)
      if idx > 0:
        d = prompts.pop(idx)
        prompts.insert(idx-1, d)
        dr = prompts_list.controls.pop(idx)
        prompts_list.controls.insert(idx-1, dr)
        prompts_list.update()
  def add_prompt(e):
      positive_prompt = prompt_text.value
      negative_prompt = negative_prompt_text.value
      if '_' in positive_prompt:
        positive_prompt = nsp_parse(positive_prompt)
      if bool(negative_prompt):
        if '_' in negative_prompt:
          negative_prompt = nsp_parse(negative_prompt)
        add_to_prompts(positive_prompt, {'negative_prompt': negative_prompt})
      else:
        add_to_prompts(positive_prompt)
  def add_to_prompts(p, arg=None, update=True):
      global prompts
      update_args()
      p = prompt_parse(p)
      dream = Dream(p)
      if arg is not None:
        #if 'prompt' in arg: del arg['prompt']
        arg = merge_dict(args, arg)
        dream.arg = arg
      #if prefs['']
      prompts.append(dream)
      prompts_list.controls.append(ListTile(title=Text(p, max_lines=6, theme_style=TextThemeStyle.BODY_LARGE), dense=True, data=dream, on_click=editPrompt, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.EDIT, text="Edit Prompt", on_click=editPrompt, data=dream),
              PopupMenuItem(icon=icons.DELETE, text="Delete Prompt", on_click=delete_prompt, data=dream),
              PopupMenuItem(icon=icons.CONTROL_POINT_DUPLICATE, text="Duplicate Prompt", on_click=duplicate_prompt, data=dream),
              PopupMenuItem(icon=icons.CONTROL_POINT_DUPLICATE_SHARP, text="Duplicate Multiple", on_click=duplicate_multiple, data=dream),
              PopupMenuItem(icon=icons.CONTENT_COPY, text="Copy to Clipboard", on_click=copy_prompt, data=dream),
              PopupMenuItem(icon=icons.ARROW_UPWARD, text="Move Up", on_click=move_up, data=dream),
              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text="Move Down", on_click=move_down, data=dream),
          ],
      )))
      page.prompts_list = prompts_list
      #prompts_list.controls.append(Text("Prompt 1 added to the list of prompts"))
      if update:
          prompts_list.update()
      if prompts_buttons.visible==False:
          prompts_buttons.visible=True
          prompts_buttons.update()
          if current_tab == 3:
            show_run_diffusion_fab(True)
      if arg is not None:
          update_prompts(update=False)
      else:
          prompt_text.focus()
      if update:
          page.update()
      status['changed_prefs'] = True
  page.add_to_prompts = add_to_prompts

  def save_prompts():
      if len(prompts) > 0:
          #print("Saving your Prompts List")
          prompts_prefs = []
          for d in prompts:
            a = d.arg.copy()
            #a['prompt'] = d.prompt if bool(d.prompt) else d['prompt']
            a['prompt'] = d.prompt[0] if type(d.prompt) == list else d.prompt if bool(d.prompt) or type(d) == Dream else d.arg['prompt'] if 'prompt' in d.arg else ''
            if not bool(a['prompt']): print(f"No Prompt for {d} - {d.prompt} {type(d) == Dream}")
            if 'batch_size' in a: del a['batch_size']
            if 'n_iterations' in a: del a['n_iterations']
            if 'precision' in a: del a['precision']
            #a['sampler'] = prefs['generation_sampler'] if prefs['use_Stability_api'] else prefs['scheduler_mode']
            if prefs['use_Stability_api']: del a['eta']
            if 'use_Stability' in a: del a['use_Stability']
            if 'negative_prompt' in a:
              if not bool(a['negative_prompt']): del a['negative_prompt']
            if 'prompt2' in a:
              if not bool(a['prompt2']):
                del a['prompt2']
                del a['tweens']
            if 'init_image' in a:
              if not bool(a['init_image']):
                del a['init_image']
                del a['init_image_strength']
                del a['invert_mask']
                if 'alpha_image' in a:
                  del a['alpha_mask']
              elif bool(a['mask_image']) and 'alpha_image' in a:
                del a['alpha_mask']
            if 'mask_image' in a:
              if not bool(a['mask_image']):
                del a['mask_image']
                if 'alpha_image' in a:
                  del a['alpha_mask']
            if 'use_clip_guided_model' in a:
              if not bool(a['use_clip_guided_model']):
                del a["use_clip_guided_model"]
                del a["clip_prompt"]
                del a["clip_guidance_scale"]
                del a["num_cutouts"]
                del a["use_cutouts"]
                del a["unfreeze_unet"]
                del a["unfreeze_vae"]
              else:
                a["clip_model_id"] = prefs['clip_model_id']
            if 'use_conceptualizer' in a:
              if not bool(a['use_conceptualizer']):
                del a['use_conceptualizer']
            prompts_prefs.append(a)
            #j = json.dumps(a)
          prefs['prompt_list'] = prompts_prefs
          #print(f"Save Prompts {prompts_prefs}")
  page.save_prompts = save_prompts
  
  def load_prompts():
      saved_prompts = prefs['prompt_list']
      #print(f"Load Prompts {saved_prompts}")
      if len(saved_prompts) > 0:
          for d in saved_prompts:
            #print(f'Loading {d}')
            if 'prompt' not in d: continue
            p = d['prompt']
            page.add_to_prompts(p, d, update=False)
          page.prompts_list.update()
          page.update()
  page.load_prompts = load_prompts

  def update_prompts(update=True):
      if len(prompts_list.controls) > 0:
        for p in prompts_list.controls:
          diffs = arg_diffs(p.data.arg, args)
          if bool(diffs):
            subtitle = Text("    " + diffs)
          else: subtitle = None
          p.subtitle = subtitle
          if update:
            p.update()
        prompts_list.update()
  page.update_prompts = update_prompts

  def apply_changes(e):
      global prompts
      if len(prompts_list.controls) > 0:
        i = 0
        for p in prompts_list.controls:
          negative = prompts[i].arg['negative_prompt']
          init = prompts[i].arg['init_image']
          mask = prompts[i].arg['mask_image']
          prompts[i].arg = merge_dict(prompts[i].arg, args)
          prompts[i].arg['negative_prompt'] = negative
          if not bool(args['init_image']):
            prompts[i].arg['init_image'] = init
          if not bool(args['mask_image']):
            prompts[i].arg['mask_image'] = mask
          p.data = prompts[i]
          i += 1
        update_prompts()

  page.apply_changes = apply_changes
  def clear_prompt(e):
      prompt_text.value = ""
      prompt_text.update()
  def clear_negative_prompt(e):
      negative_prompt_text.value = ""
      negative_prompt_text.update()
  def clear_list(e):
      global prompts
      prompts_list.controls = []
      prompts_list.update()
      prompts = []
      prefs['prompt_list'] = []
      prompts_buttons.visible=False
      prompts_buttons.update()
      show_run_diffusion_fab(False)
      e.page.save_prompts()
      save_settings_file(e.page)
      #status['changed_prefs'] = True
      play_snd(Snd.DELETE, page)
  page.clear_prompts_list = clear_list
  def on_keyboard (e: KeyboardEvent):
      if e.key == "Escape":
        if current_tab == 3:
          clear_prompt(None)
  page.on_keyboard_event = on_keyboard
  def run_diffusion(e):
      if not status['installed_diffusers'] and not prefs['install_Stability_api'] and not prefs['install_AIHorde_api']:
        alert_msg(e.page, "You must Install the required Diffusers first or use Stability.ai API or AI-Horde to use without GPU...")
        return
      if prefs['use_interpolation'] and prefs['install_interpolation'] and not status['installed_interpolation']:
        alert_msg(e.page, "You must Install Walk Interpolation Pipeline first...")
        return
      if len(prompts) < 1:
        if not bool(prompt_text.value):
          alert_msg(e.page, "Add some Prompts to the Batch List before running, or at least fill in Prompt Text first.")
          return
        add_prompt(e)
      page.tabs.selected_index = 4
      page.tabs.update()
      show_run_diffusion_fab(False)
      if status['changed_prefs']:
        page.save_prompts()
        save_settings_file(page)
        status['changed_prefs'] = False
      page.update()
      start_diffusion(page)
  has_changed = False
  prompts_list = Column([],spacing=1)
  page.prompts_list = prompts_list
  prompt_text = TextField(label="Prompt Text", suffix=IconButton(icons.CLEAR, on_click=clear_prompt), autofocus=True, filled=True, multiline=True, max_lines=6, on_submit=add_prompt, col={'lg':9})
  negative_prompt_text = TextField(label="Segmented Weights 1 | -0.7 | 1.2" if prefs['use_composable'] and status['installed_composable'] else "Negative Prompt Text", filled=True, multiline=True, max_lines=4, value=prefs['negative_prompt'], on_change=lambda e:changed(e,'negative_prompt'), suffix=IconButton(icons.CLEAR, on_click=clear_negative_prompt), col={'lg':3})
  add_prompt_button = ElevatedButton(content=Text(value="➕  Add" + (" Prompt" if (page.width if page.web else page.window.width) > 720 else ""), size=17, weight=FontWeight.BOLD), height=52, on_click=add_prompt)
  prompt_help_button = IconButton(icons.HELP_OUTLINE, tooltip="Help with Prompt Creation", on_click=prompt_help)
  copy_prompts_button = IconButton(icons.COPY_ALL, tooltip="Save Prompts as Plain-Text List", on_click=copy_prompts)
  paste_prompts_button = IconButton(icons.CONTENT_PASTE, tooltip="Load Prompts from Plain-Text List", on_click=paste_prompts)
  prompt_row = Row([ResponsiveRow([prompt_text, negative_prompt_text], expand=True, vertical_alignment=CrossAxisAlignment.START), add_prompt_button])
  #diffuse_prompts_button = ElevatedButton(content=Text(value="▶️    Run Diffusion on Prompts ", size=20), on_click=run_diffusion)
  clear_prompts_button = ElevatedButton(content=Text("❌   Clear Prompts List", size=18), on_click=clear_list)
  prompts_buttons = Row([clear_prompts_button], alignment=MainAxisAlignment.SPACE_BETWEEN)
  def show_run_diffusion_fab(show = True, p = None):
    if p is None:
      p = page
    if show:
      p.floating_action_button = FloatingActionButton(content=Row([Icon(icons.PLAY_ARROW), Text("Run Diffusion on Prompts", size=18)], alignment="center", spacing=5), width=270, shape=ft.RoundedRectangleBorder(radius=22), on_click=run_diffusion)
      #page.floating_action_button = FloatingActionButton(icon=icons.PLAY_ARROW, text="Run Diffusion on Prompts", on_click=run_diffusion)
      p.update()
    else:
      if p.floating_action_button is not None:
        p.floating_action_button = None
        try:
          p.update()
        except Exception:
          print("Problem updating page while showing Run Diffusion FAB")
          pass

  page.show_run_diffusion_fab = show_run_diffusion_fab
  show_run_diffusion_fab(False)#(len(prompts_list.controls) > 0)
  #page.load_prompts()
  if len(prompts_list.controls) < 1:
    prompts_buttons.visible=False
  c = Column([Container(
      padding=padding.only(18, 14, 20, 10), content=Column([
        Header("🗒️   List of Prompts to Diffuse", actions=[prompt_help_button, paste_prompts_button, copy_prompts_button]),
        #add_prompt_button,
        prompt_row,
        prompts_list,
        prompts_buttons,
      ],
  ))], scroll=ScrollMode.AUTO)
  return c

def buildImages(page):
    def auto_scrolling(auto):
      page.imageColumn.auto_scroll = auto
      page.imageColumn.update()
      c.update()
    page.auto_scrolling = auto_scrolling
    page.imageColumn = Column([Text("▶️   Start Run from Prompts List.  Get ready...", theme_style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Divider(thickness=3, height=5, color=colors.SURFACE_VARIANT)], scroll=ScrollMode.AUTO, auto_scroll=True)
    c = Container(padding=padding.only(18, 12, 0, 0), content=page.imageColumn)
    return c

horde_text_models = []
OpenAI_models = {"GPT-4": "gpt-4", "GPT-4 Turbo": "gpt-4-turbo", "GPT-4o": "gpt-4o", "GPT-4o-mini": "gpt-4o-mini", "GPT-3.5 Turbo": "gpt-3.5-turbo", "GPT-3.5 Turbo Instruct": "gpt-3.5-turbo-instruct", "GPT-3": "text-davinci-003"}

def buildPromptGenerator(page):
    global horde_text_models
    def changed(e, pref=None):
      if pref is not None:
        prefs['prompt_generator'][pref] = e.control.value
      status['changed_prefs'] = True
    page.prompt_generator_list = Column([], spacing=0)
    def add_to_prompt_list(p):
      page.add_to_prompts(p)
      play_snd(Snd.DROP, page)
    def add_to_prompt_generator(p):
      page.prompt_generator_list.controls.append(ListTile(title=Text(p, max_lines=8, theme_style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))
      page.prompt_generator_list.update()
      generator_list_buttons.visible = True
      generator_list_buttons.update()
    page.add_to_prompt_generator = add_to_prompt_generator
    def click_prompt_generator(e):
      if status['installed_OpenAI']:
        run_prompt_generator(page)
      else:
        alert_msg(page, "You must Install OpenAI GPT Library first before using...")
    def add_to_list(e):
      for p in page.prompt_generator_list.controls:
        if hasattr(p, "title"):
            page.add_to_prompts(p.title.value)
      play_snd(Snd.DROP, page)
    def clear_prompts(e):
      page.prompt_generator_list.controls = []
      page.prompt_generator_list.update()
      #prompts = []
      generator_list_buttons.visible = False
      generator_list_buttons.update()
      play_snd(Snd.DELETE, page)
    def changed_request(e):
      request_slider.label = generator_request_modes[int(request_slider.value)]
      request_slider.update()
      changed(e, 'request_mode')
    def changed_engine(e):
      changed(e, 'AI_engine')
      if prefs['prompt_generator']['AI_engine']=="AI-Horde":
        load_horde_text(page)
      OpenAI_model_container.visible = prefs['prompt_generator']['AI_engine']=="OpenAI ChatGPT"
      OpenAI_model_container.update()
      AIHorde_model_container.visible = prefs['prompt_generator']['AI_engine']=="AI-Horde"
      AIHorde_model_container.update()
      Perplexity_model_container.visible = prefs['prompt_generator']['AI_engine']=="Perplexity"
      Perplexity_model_container.update()
    horde_models_info = IconButton(icons.HELP_OUTLINE, tooltip="Show AI-Horde Models Stat List", on_click=models_AIHorde)
    request_slider = Slider(label="{value}", min=0, max=7, divisions=7, expand=True, value=prefs['prompt_generator']['request_mode'], on_change=changed_request, tooltip="The way it asks for the visual description.")
    request_slider.label = generator_request_modes[int(prefs['prompt_generator']['request_mode'])]
    AI_engine = Dropdown(label="AI Engine", width=250, options=[dropdown.Option("OpenAI ChatGPT"), dropdown.Option("Google Gemini"), dropdown.Option("Google Gemini 1.5 Pro"), dropdown.Option("Google Gemini 1.5 Flash"), dropdown.Option("Anthropic Claude 3"), dropdown.Option("Anthropic Claude 3.5"), dropdown.Option("AI-Horde"), dropdown.Option("Perplexity")], value=prefs['prompt_generator']['AI_engine'], on_change=lambda e: changed_engine(e))
    AIHorde_model = Dropdown(label="Horde AI Engine", width=400, options=[], value=prefs['prompt_generator']['AIHorde_model'], on_change=lambda e: changed(e, 'AIHorde_model'))
    AIHorde_model_container = Container(Row([AIHorde_model, horde_models_info]), visible=prefs['prompt_generator']['AI_engine']=="AI-Horde")
    page.AIHorde_model_generator = AIHorde_model
    Perplexity_model = Dropdown(label="Perplexity AI Engine", width=400, options=[dropdown.Option(m) for m in ["llama-3-sonar-small-32k-chat", "llama-3-sonar-small-32k-online", "llama-3-sonar-large-32k-chat", "llama-3-sonar-large-32k-online", "llama-3-8b-instruct", "llama-3-70b-instruct", "mixtral-8x7b-instruct"]], value=prefs['prompt_generator']['Perplexity_model'], on_change=lambda e: changed(e, 'Perplexity_model'))
    Perplexity_model_container = Container(Row([Perplexity_model]), visible=prefs['prompt_generator']['AI_engine']=="OpenAI")
    OpenAI_model = Dropdown(label="OpenAI Engine", width=400, options=[dropdown.Option(m) for m in OpenAI_models.keys()], value=prefs['prompt_generator']['OpenAI_model'], on_change=lambda e: changed(e, 'OpenAI_model'))
    OpenAI_model_container = Container(Row([OpenAI_model]), visible=prefs['prompt_generator']['AI_engine']=="OpenAI ChatGPT")
    generator_list_buttons = Row([
        ElevatedButton(content=Text("❌   Clear Prompts", size=18), on_click=clear_prompts),
        FilledButton(content=Text("➕  Add All Prompts to List", size=20), on_click=add_to_list)
    ], alignment=MainAxisAlignment.SPACE_BETWEEN)
    if len(page.prompt_generator_list.controls) < 1:
      generator_list_buttons.visible = False
      #generator_list_buttons.update()
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🧠  OpenAI GPT-3/4/Gemini Prompt Generator", "Enter a phrase each prompt should start with and the amount of prompts to generate. Just experiment, AI will continue to surprise."),
        Row([TextField(label="Subject Phrase", expand=True, value=prefs['prompt_generator']['phrase'], multiline=True, on_change=lambda e: changed(e, 'phrase')),
             TextField(label="Subject Detail (optional)", expand=True, hint_text="About Details (optional)", value=prefs['prompt_generator']['subject_detail'], multiline=True, on_change=lambda e: changed(e, 'subject_detail')), 
             Checkbox(label="Phrase as Subject", value=prefs['prompt_generator']['phrase_as_subject'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, tooltip="Makes it about phrase and subject detail.", on_change=lambda e: changed(e, 'phrase_as_subject'))]),
        ResponsiveRow([
          Row([NumberPicker(label="Amount: ", min=1, max=20, value=prefs['prompt_generator']['amount'], on_change=lambda e: changed(e, 'amount')),
              NumberPicker(label="Random Artists: ", min=0, max=10, value=prefs['prompt_generator']['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
          Row([NumberPicker(label="Random Styles: ", min=0, max=10, value=prefs['prompt_generator']['random_styles'], on_change=lambda e: changed(e, 'random_styles')),
              Checkbox(label="Permutate Artists", value=prefs['prompt_generator']['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'), tooltip="Shuffles the list of Artists and Styles to make Combo Variations.")], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
        ]),
        Row([AI_engine, OpenAI_model_container, AIHorde_model_container, Perplexity_model_container]),
        ResponsiveRow([
          Row([Text("Request Mode:"), request_slider,], col={'lg':6}),
          Row([Text(" AI Temperature:"), Slider(label="{value}", min=0, max=1, divisions=10, round=1, expand=True, value=prefs['prompt_generator']['AI_temperature'], on_change=lambda e: changed(e, 'AI_temperature'))], col={'lg':6}),
        ]),
        ElevatedButton(content=Text("💭   Generate Prompts", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda e: run_prompt_generator(page)),
        page.prompt_generator_list,
        generator_list_buttons,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

def buildPromptRemixer(page):
    def changed(e, pref=None):
      if pref is not None:
        prefs['prompt_remixer'][pref] = e.control.value
      status['changed_prefs'] = True
    page.prompt_remixer_list = Column([], spacing=0)
    def click_prompt_remixer(e):
      if status['installed_OpenAI']:
        run_prompt_remixer(page)
      else:
        alert_msg(page, "You must Install OpenAI GPT Library first before using...")
    def add_to_prompt_list(p):
      play_snd(Snd.DROP, page)
      page.add_to_prompts(p)
    def add_to_prompt_remixer(p):
      page.prompt_remixer_list.controls.append(ListTile(title=Text(p, max_lines=4, theme_style=TextThemeStyle.BODY_LARGE), dense=True, data=p, on_click=lambda _: add_to_prompt_list(p)))
      page.prompt_remixer_list.update()
      remixer_list_buttons.visible = True
      remixer_list_buttons.update()
    page.add_to_prompt_remixer = add_to_prompt_remixer
    def add_to_list(e):
      play_snd(Snd.DROP, page)
      for p in page.prompt_remixer_list.controls:
        page.add_to_prompts(p.data)#(p.title.value)
    def clear_prompts(e):
      play_snd(Snd.DELETE, page)
      page.prompt_remixer_list.controls = []
      page.prompt_remixer_list.update()
      remixer_list_buttons.visible = False
      remixer_list_buttons.update()
    def changed_request(e):
      request_slider.label = remixer_request_modes[int(request_slider.value)]
      request_slider.update()
      changed(e, 'request_mode')
    def changed_engine(e):
      changed(e, 'AI_engine')
      if prefs['prompt_remixer']['AI_engine']=="AI-Horde":
        load_horde_text(page)
      OpenAI_model_container.visible = prefs['prompt_remixer']['AI_engine']=="OpenAI ChatGPT"
      OpenAI_model_container.update()
      AIHorde_model_container.visible = prefs['prompt_remixer']['AI_engine']=="AI-Horde"
      AIHorde_model_container.update()
      Perplexity_model_container.visible = prefs['prompt_remixer']['AI_engine']=="Perplexity"
      Perplexity_model_container.update()
      TextSynth_model_container.visible = prefs['prompt_remixer']['AI_engine']=="TextSynth"
      TextSynth_model_container.update()
    horde_models_info = IconButton(icons.HELP_OUTLINE, tooltip="Show AI-Horde Models Stat List", on_click=models_AIHorde)
    request_slider = Slider(label="{value}", min=0, max=8, divisions=8, expand=True, value=prefs['prompt_remixer']['request_mode'], on_change=changed_request)
    request_slider.label = remixer_request_modes[int(prefs['prompt_remixer']['request_mode'])]
    AI_engine = Dropdown(label="AI Engine", width=250, options=[dropdown.Option(c) for c in ["TextSynth", "OpenAI ChatGPT", "Google Gemini", "Google Gemini 1.5 Pro", "Google Gemini 1.5 Flash", "Anthropic Claude 3", "Anthropic Claude 3.5", "AI-Horde", "Perplexity"]], value=prefs['prompt_remixer']['AI_engine'], on_change=lambda e: changed_engine(e))
    OpenAI_model = Dropdown(label="OpenAI Engine", width=400, options=[dropdown.Option(m) for m in OpenAI_models.keys()], value=prefs['prompt_remixer']['OpenAI_model'], on_change=lambda e: changed(e, 'OpenAI_model'))
    OpenAI_model_container = Container(Row([OpenAI_model]), visible=prefs['prompt_remixer']['AI_engine']=="OpenAI ChatGPT")
    AIHorde_model = Dropdown(label="Horde AI Engine", width=400, options=[], value=prefs['prompt_remixer']['AIHorde_model'], on_change=lambda e: changed(e, 'AIHorde_model'))
    AIHorde_model_container = Container(Row([AIHorde_model, horde_models_info]), visible=prefs['prompt_remixer']['AI_engine']=="AI-Horde")
    page.AIHorde_model_remixer = AIHorde_model
    Perplexity_model = Dropdown(label="Perplexity AI Engine", width=400, options=[dropdown.Option(m) for m in ["llama-3-sonar-small-32k-chat", "llama-3-sonar-small-32k-online", "llama-3-sonar-large-32k-chat", "llama-3-sonar-large-32k-online", "llama-3-8b-instruct", "llama-3-70b-instruct", "mixtral-8x7b-instruct"]], value=prefs['prompt_remixer']['Perplexity_model'], on_change=lambda e: changed(e, 'Perplexity_model'))
    Perplexity_model_container = Container(Row([Perplexity_model]), visible=prefs['prompt_remixer']['AI_engine']=="Perplexity")
    TextSynth_model = Dropdown(label="TextSynth Engine", width=400, options=[dropdown.Option(m) for m in ["GPT-J", "Mistral", "Mistral Instruct", "Mixtral Instruct", "Llama2 7B", "Llama2 70B"]], value=prefs['prompt_remixer']['TextSynth_model'], on_change=lambda e: changed(e, 'TextSynth_model'))
    TextSynth_model_container = Container(Row([TextSynth_model]), visible=prefs['prompt_remixer']['AI_engine']=="TextSynth")

    remixer_list_buttons = Row([
        ElevatedButton(content=Text("❌   Clear Prompts", size=18), on_click=clear_prompts),
        FilledButton(content=Text("Add All Prompts to List", size=20), height=45, on_click=add_to_list),
    ], alignment=MainAxisAlignment.SPACE_BETWEEN)
    if len(page.prompt_remixer_list.controls) < 1:
      remixer_list_buttons.visible = False

    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🔄  Prompt Remixer - GPT-3/4/Gemini AI Helper", "Enter a complete prompt you've written that is well worded and descriptive, and get variations of it with our AI Friend. Experiment.", actions=[ElevatedButton(content=Text("🍜  NSP Instructions", size=18), on_click=lambda _: NSP_instructions(page))]),
        Row([TextField(label="Seed Prompt", expand=True, value=prefs['prompt_remixer']['seed_prompt'], multiline=True, on_change=lambda e: changed(e, 'seed_prompt')), 
             TextField(label="About Detail (optional)", expand=True, hint_text="Subject Details (optional)", value=prefs['prompt_remixer']['optional_about_influencer'], multiline=True, on_change=lambda e: changed(e, 'optional_about_influencer'))]),
        ResponsiveRow([
          Row([NumberPicker(label="Amount: ", min=1, max=20, value=prefs['prompt_remixer']['amount'], on_change=lambda e: changed(e, 'amount')),
              NumberPicker(label="Random Artists: ", min=0, max=10, value=prefs['prompt_remixer']['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
          Row([NumberPicker(label="Random Styles: ", min=0, max=10, value=prefs['prompt_remixer']['random_styles'], on_change=lambda e: changed(e, 'random_styles')),
              Checkbox(label="Permutate Artists", value=prefs['prompt_remixer']['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'), tooltip="Shuffles the list of Artists and Styles to make Combo Variations.")], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
        ]),
        Row([AI_engine, OpenAI_model_container, AIHorde_model_container, Perplexity_model_container, TextSynth_model_container]),
        ResponsiveRow([
          Row([Text("Request Mode:"), request_slider,], col={'lg':6}),
          Row([Text(" AI Temperature:"), Slider(label="{value}", min=0, max=1, divisions=10, round=1, expand=True, value=prefs['prompt_remixer']['AI_temperature'], on_change=lambda e: changed(e, 'AI_temperature'))], col={'lg':6}),
        ]),
        ElevatedButton(content=Text("🍹   Remix Prompts", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda e: run_prompt_remixer(page)),
        page.prompt_remixer_list,
        remixer_list_buttons,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

def buildPromptBrainstormer(page):
    def changed(e, pref=None):
      if pref is not None:
        prefs['prompt_brainstormer'][pref] = e.control.value
      status['changed_prefs'] = True
    def click_prompt_brainstormer(e):
      if prefs['prompt_brainstormer']['AI_engine'] == "OpenAI GPT-3":
        if status['installed_OpenAI']:
          run_prompt_brainstormer(page)
        else: alert_msg(page, "You must Install OpenAI GPT Library first before using this Request Mode...")
      elif "TextSynth" in prefs['prompt_brainstormer']['AI_engine']:
        if status['installed_TextSynth']:
          run_prompt_brainstormer(page)
        else: alert_msg(page, "You must Install TextSynth GPT-J Library first before using this Request Mode...")
      elif prefs['prompt_brainstormer']['AI_engine'] == "HuggingFace Bloom 176B":
        if bool(prefs['HuggingFace_api_key']):
          run_prompt_brainstormer(page)
        else: alert_msg(page, "You must provide your HuggingFace API Key in settings first before using this Request Mode...")
      elif prefs['prompt_brainstormer']['AI_engine'] == "HuggingFace Flan-T5 XXL":
        if bool(prefs['HuggingFace_api_key']):
          run_prompt_brainstormer(page)
        else: alert_msg(page, "You must provide your HuggingFace API Key in settings first before using this Request Mode...")
    page.prompt_brainstormer_list = Column([], spacing=0)
    def add_to_prompt_brainstormer(p):
      page.prompt_brainstormer_list.controls.append(Text(p, theme_style=TextThemeStyle.BODY_LARGE, selectable=True))
      page.prompt_brainstormer_list.update()
      brainstormer_list_buttons.visible = True
      brainstormer_list_buttons.update()
    page.add_to_prompt_brainstormer = add_to_prompt_brainstormer
    def add_to_prompts(e):
      page.add_to_prompts(new_prompt_text.value)
    def clear_prompts(e):
      page.prompt_brainstormer_list.controls = []
      page.prompt_brainstormer_list.update()
      brainstormer_list_buttons.visible = False
      brainstormer_list_buttons.update()
    def clear_prompt_text(e):
      new_prompt_text.value = ""
      new_prompt_text.update()
    def changed_engine(e):
      changed(e, 'AI_engine')
      if prefs['prompt_brainstormer']['AI_engine']=="AI-Horde":
        load_horde_text(page)
      OpenAI_model_container.visible = prefs['prompt_brainstormer']['AI_engine']=="OpenAI ChatGPT"
      OpenAI_model_container.update()
      AIHorde_model_container.visible = prefs['prompt_brainstormer']['AI_engine']=="AI-Horde"
      AIHorde_model_container.update()
      Perplexity_model_container.visible = prefs['prompt_brainstormer']['AI_engine']=="Perplexity"
      Perplexity_model_container.update()
      TextSynth_model_container.visible = prefs['prompt_brainstormer']['AI_engine']=="TextSynth"
      TextSynth_model_container.update()
    horde_models_info = IconButton(icons.HELP_OUTLINE, tooltip="Show AI-Horde Models Stat List", on_click=models_AIHorde)
    OpenAI_model = Dropdown(label="OpenAI Engine", width=400, options=[dropdown.Option(m) for m in OpenAI_models.keys()], value=prefs['prompt_brainstormer']['OpenAI_model'], on_change=lambda e: changed(e, 'OpenAI_model'))
    OpenAI_model_container = Container(Row([OpenAI_model]), visible=prefs['prompt_brainstormer']['AI_engine']=="OpenAI ChatGPT")
    AIHorde_model = Dropdown(label="Horde AI Engine", width=400, options=[], value=prefs['prompt_brainstormer']['AIHorde_model'], on_change=lambda e: changed(e, 'AIHorde_model'))
    AIHorde_model_container = Container(Row([AIHorde_model, horde_models_info]), visible=prefs['prompt_brainstormer']['AI_engine']=="AI-Horde")
    page.AIHorde_model_brainstormer = AIHorde_model
    Perplexity_model = Dropdown(label="Perplexity AI Engine", width=400, options=[dropdown.Option(m) for m in ["llama-3-sonar-small-32k-chat", "llama-3-sonar-small-32k-online", "llama-3-sonar-large-32k-chat", "llama-3-sonar-large-32k-online", "llama-3-8b-instruct", "llama-3-70b-instruct", "mixtral-8x7b-instruct"]], value=prefs['prompt_brainstormer']['Perplexity_model'], on_change=lambda e: changed(e, 'Perplexity_model'))
    Perplexity_model_container = Container(Row([Perplexity_model]), visible=prefs['prompt_brainstormer']['AI_engine']=="Perplexity")
    TextSynth_model = Dropdown(label="TextSynth Engine", width=400, options=[dropdown.Option(m) for m in ["GPT-J", "Mistral", "Mistral Instruct", "Mixtral Instruct", "Llama2 7B", "Llama2 70B"]], value=prefs['prompt_brainstormer']['TextSynth_model'], on_change=lambda e: changed(e, 'TextSynth_model'))
    TextSynth_model_container = Container(Row([TextSynth_model]), visible=prefs['prompt_brainstormer']['AI_engine']=="TextSynth")
    new_prompt_text = TextField(label="New Prompt Text", expand=True, filled=True, suffix=IconButton(icons.CLEAR, on_click=clear_prompt_text), autofocus=True, on_submit=add_to_prompts)
    add_to_prompts_button = ElevatedButton("➕  Add to Prompts", on_click=add_to_prompts)#, icon=icons.ADD_ROUNDED
    brainstormer_list_buttons = Row([
        new_prompt_text, add_to_prompts_button,
        ElevatedButton(content=Text("❌   Clear Brainstorms"), on_click=clear_prompts),
    ], alignment=MainAxisAlignment.END)

    if len(page.prompt_brainstormer_list.controls) < 1:
      brainstormer_list_buttons.visible = False
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🤔  Prompt Brainstormer - TextSynth GPT-J-6B, OpenAI GPT, Gemini & HuggingFace Bloom AI",
               "Get Inspiration on Prompt Engineering with Rewrite, Edit, Story, Description, Details, etc.", actions=[ElevatedButton(content=Text("🍜  NSP Instructions", size=18), on_click=lambda _: NSP_instructions(page))]),
        Row([Dropdown(label="AI Engine", width=250, options=[dropdown.Option(c) for c in ["TextSynth", "OpenAI GPT-3", "ChatGPT-3.5 Turbo", "OpenAI GPT-4", "GPT-4 Turbo", "GPT-4o", "HuggingFace Bloom 176B", "HuggingFace Flan-T5 XXL", "StableLM 7b", "StableLM 3b", "Google Gemini", "Google Gemini 1.5 Pro", "Google Gemini 1.5 Flash", "Anthropic Claude 3", "Anthropic Claude 3.5", "AI-Horde", "Perplexity"]], value=prefs['prompt_brainstormer']['AI_engine'], on_change=lambda e: changed_engine(e)),
          OpenAI_model_container, AIHorde_model_container, Perplexity_model_container, TextSynth_model_container,
          Dropdown(label="Request Mode", width=250, options=[dropdown.Option("Brainstorm"), dropdown.Option("Write"), dropdown.Option("Rewrite"), dropdown.Option("Edit"), dropdown.Option("Story"), dropdown.Option("Description"), dropdown.Option("Picture"), dropdown.Option("Raw Request")], value=prefs['prompt_brainstormer']['request_mode'], on_change=lambda e: changed(e, 'request_mode')),
        ], alignment=MainAxisAlignment.START),
        Row([TextField(label="About Prompt", expand=True, value=prefs['prompt_brainstormer']['about_prompt'], multiline=True, on_change=lambda e: changed(e, 'about_prompt')),]),
        ElevatedButton(content=Text("⛈️    Brainstorm Prompt", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_prompt_brainstormer(page)),
        page.prompt_brainstormer_list,
        brainstormer_list_buttons,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

def buildPromptWriter(page):
    def changed(e, pref=None):
      if pref is not None:
        prefs['prompt_writer'][pref] = e.control.value
      status['changed_prefs'] = True
    page.prompt_writer_list = Column([], spacing=0)
    def add_to_prompt_list(p):
      negative_prompt = prefs['prompt_writer']['negative_prompt']
      if bool(negative_prompt):
        if '_' in negative_prompt:
          negative_prompt = nsp_parse(negative_prompt)
        page.add_to_prompts(p, {'negative_prompt':negative_prompt})
      else:
        page.add_to_prompts(p)
      play_snd(Snd.DROP, page)
    def add_to_prompt_writer(p):
      page.prompt_writer_list.controls.append(ListTile(title=Text(p, max_lines=3, theme_style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))
      page.prompt_writer_list.update()
      writer_list_buttons.visible = True
      writer_list_buttons.update()
    page.add_to_prompt_writer = add_to_prompt_writer

    def add_to_list(e):
      play_snd(Snd.DROP, page)
      negative_prompt = prefs['prompt_writer']['negative_prompt']
      if bool(negative_prompt):
        if '_' in negative_prompt:
          negative_prompt = nsp_parse(negative_prompt)
      for p in page.prompt_writer_list.controls:
        if bool(negative_prompt):
          page.add_to_prompts(p.title.value, {'negative_prompt':negative_prompt})
        else:
          page.add_to_prompts(p.title.value)
    def clear_prompts(e):
      play_snd(Snd.DELETE, page)
      page.prompt_writer_list.controls = []
      page.prompt_writer_list.update()
      writer_list_buttons.visible = False
      writer_list_buttons.update()
    writer_list_buttons = Row([
        ElevatedButton(content=Text("❌   Clear Prompts", size=18), on_click=clear_prompts),
        FilledButton(content=Text("➕  Add All Prompts to List", size=20), on_click=add_to_list),
    ], alignment=MainAxisAlignment.SPACE_BETWEEN)
    if len(page.prompt_writer_list.controls) < 1:
      writer_list_buttons.visible = False

    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("📜 Advanced Prompt Writer with Noodle Soup Prompt random variables ", "Construct your Art descriptions easier, with all the extras you need to engineer perfect prompts faster. Note, you don't have to use any randoms if you rather do all custom.", actions=[ElevatedButton(content=Text("🍜  NSP Instructions", size=18), on_click=lambda _: NSP_instructions(page))]),
        ResponsiveRow([
          TextField(label="Prompt Art Subjects", value=prefs['prompt_writer']['art_Subjects'], on_change=lambda e: changed(e, 'art_Subjects'), multiline=True, max_lines=4, col={'lg':9}),
          TextField(label="Negative Prompt (optional)", value=prefs['prompt_writer']['negative_prompt'], on_change=lambda e: changed(e, 'negative_prompt'), multiline=True, max_lines=4, col={'lg':3}),
        ]),
        Row([TextField(label="by Artists", value=prefs['prompt_writer']['by_Artists'], on_change=lambda e: changed(e, 'by_Artists')),
             TextField(label="Art Styles", value=prefs['prompt_writer']['art_Styles'], on_change=lambda e: changed(e, 'art_Styles')),]),
        ResponsiveRow([
          Row([NumberPicker(label="Amount: ", min=1, max=20, value=prefs['prompt_writer']['amount'], on_change=lambda e: changed(e, 'amount')),
              NumberPicker(label="Random Artists: ", min=0, max=10, value=prefs['prompt_writer']['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
          Row([NumberPicker(label="Random Styles: ", min=0, max=10, value=prefs['prompt_writer']['random_styles'], on_change=lambda e: changed(e, 'random_styles')),
              Checkbox(label="Permutate Artists", value=prefs['prompt_writer']['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'), tooltip="Shuffles the list of Artists and Styles to make Combo Variations.")], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
        ]),
        ElevatedButton(content=Text("✍️   Write Prompts", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_prompt_writer(page)),
        page.prompt_writer_list,
        writer_list_buttons,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

magic_prompt_prefs = {
    'seed_prompt': '',
    'seed': 0,
    'amount': 8,
    'random_artists': 0,
    'random_styles': 0,
    'permutate_artists': True,
}

def buildMagicPrompt(page):
    global magic_prompt_prefs, prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          magic_prompt_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    page.magic_prompt_list = Column([], spacing=0)
    page.magic_prompt_output = Column([])
    def add_to_prompt_list(p):
      page.add_to_prompts(p)
      play_snd(Snd.DROP, page)
    def add_to_magic_prompt(p):
      page.magic_prompt_list.controls.append(ListTile(title=Text(p, max_lines=3, theme_style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))
      page.magic_prompt_list.update()
      magic_list_buttons.visible = True
      magic_list_buttons.update()
    page.add_to_magic_prompt = add_to_magic_prompt
    def add_to_list(e):
      play_snd(Snd.DROP, page)
      for p in page.magic_prompt_list.controls:
        page.add_to_prompts(p.title.value)
    def clear_prompts(e):
      play_snd(Snd.DELETE, page)
      page.magic_prompt_list.controls = []
      page.magic_prompt_list.update()
      magic_list_buttons.visible = False
      magic_list_buttons.update()
    seed = TextField(label="Seed", value=magic_prompt_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width = 90, on_change=lambda e:changed(e,'seed', ptype="int"))
    magic_list_buttons = Row([
        ElevatedButton(content=Text("❌   Clear Prompts", size=18), on_click=clear_prompts),
        FilledButton(content=Text("Add All Prompts to List", size=20), height=45, on_click=add_to_list),
    ], alignment=MainAxisAlignment.SPACE_BETWEEN)
    if len(page.magic_prompt_list.controls) < 1:
      magic_list_buttons.visible = False

    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎩  Magic Prompt Generator - GPT-2 AI Helper", "Generates new Image Prompts made for Stable Diffusion with a specially trained GPT-2 Text AI by Gustavosta...", actions=[ElevatedButton(content=Text("🍜  NSP Instructions", size=18), on_click=lambda _: NSP_instructions(page))]),
        Row([TextField(label="Starter Prompt Text", expand=True, value=magic_prompt_prefs['seed_prompt'], multiline=True, on_change=lambda e: changed(e, 'seed_prompt'))]),
        ResponsiveRow([
          Row([NumberPicker(label="Amount: ", min=1, max=40, value=magic_prompt_prefs['amount'], on_change=lambda e: changed(e, 'amount')), seed,
              NumberPicker(label="Random Artists: ", min=0, max=10, value=magic_prompt_prefs['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
          Row([NumberPicker(label="Random Styles: ", min=0, max=10, value=magic_prompt_prefs['random_styles'], on_change=lambda e: changed(e, 'random_styles')),
              Checkbox(label="Permutate Artists", value=magic_prompt_prefs['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'), tooltip="Shuffles the list of Artists and Styles to make Combo Variations.")], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
        ]),
        ElevatedButton(content=Text("🧙   Make Magic Prompts", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_magic_prompt(page)),
        page.magic_prompt_output,
        page.magic_prompt_list,
        magic_list_buttons,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

distil_gpt2_prefs = {
    'seed_prompt': '',
    'seed': 0,
    'amount': 8,
    'AI_temperature': 0.9,
    'top_k': 8,
    'max_length': 80,
    'repetition_penalty': 1.2,
    'penalty_alpha': 0.6,
    'no_repeat_ngram_size': 1,
    'random_artists': 0,
    'random_styles': 0,
    'permutate_artists': True,
}

def buildDistilGPT2(page):
    global distil_gpt2_prefs, prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          distil_gpt2_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    page.distil_gpt2_list = Column([], spacing=0)
    page.distil_gpt2_output = Column([])
    def add_to_prompt_list(p):
      page.add_to_prompts(p)
      play_snd(Snd.DROP, page)
    def add_to_distil_gpt2(p):
      page.distil_gpt2_list.controls.append(ListTile(title=Text(p, max_lines=3, theme_style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))
      page.distil_gpt2_list.update()
      distil_list_buttons.visible = True
      distil_list_buttons.update()
    page.add_to_distil_gpt2 = add_to_distil_gpt2
    def add_to_list(e):
      play_snd(Snd.DROP, page)
      for p in page.distil_gpt2_list.controls:
        page.add_to_prompts(p.title.value)
    def clear_prompts(e):
      play_snd(Snd.DELETE, page)
      page.distil_gpt2_list.controls = []
      page.distil_gpt2_list.update()
      distil_list_buttons.visible = False
      distil_list_buttons.update()
    AI_temperature = Row([Text("AI Temperature:"), Slider(label="{value}", min=0, max=1, divisions=10, round=1, expand=True, tooltip="The value used to module the next token probabilities", value=distil_gpt2_prefs['AI_temperature'], on_change=lambda e: changed(e, 'AI_temperature'))], col={'lg':6})
    top_k = Row([Text("Top-K Samples:"), Slider(label="{value}", min=0, max=50, divisions=50, expand=True, tooltip="Number of highest probability vocabulary tokens to keep for top-k-filtering", value=distil_gpt2_prefs['top_k'], on_change=lambda e: changed(e, 'top_k'))], col={'lg':6})
    #max_length = Row([Text("Max Length:"), Slider(label="{value}", min=0, max=1024, divisions=1024, expand=True, tooltip="The maximum length the generated tokens can have. Corresponds to the length of the input prompt + max_new_tokens.", value=distil_gpt2_prefs['max_length'], on_change=lambda e: changed(e, 'max_length', ptype="int"))], col={'lg':6})
    max_length = SliderRow(label="Max Length", min=0, max=1024, divisions=1024, pref=distil_gpt2_prefs, key='max_length')
    repetition_penalty = Row([Text("Repetition Penalty:"), Slider(label="{value}", min=1.0, max=3.0, divisions=20, round=2, expand=True, tooltip="Penalizes repetition by discounting the scores of previously generated tokens", value=distil_gpt2_prefs['repetition_penalty'], on_change=lambda e: changed(e, 'repetition_penalty'))], col={'lg':6})
    penalty_alpha = Row([Text("Penalty Alpha:"), Slider(label="{value}", min=0, max=1, divisions=10, round=1, expand=True, tooltip="The degeneration penalty for contrastive search; activate when it is larger than 0", value=distil_gpt2_prefs['penalty_alpha'], on_change=lambda e: changed(e, 'penalty_alpha', ptype="float"))], col={'lg':6})
    no_repeat_ngram_size = Row([Text("No Repeat NGRAM Size:"), Slider(label="{value}", min=0, max=50, expand=True, divisions=50, tooltip="If set > 0, all ngrams of that size can only occur once. 0 adds more commas.", value=distil_gpt2_prefs['no_repeat_ngram_size'], on_change=lambda e: changed(e, 'no_repeat_ngram_size', ptype="int"))], col={'lg':6})
    seed = TextField(label="Seed", value=distil_gpt2_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width = 90, on_change=lambda e:changed(e,'seed', ptype="int"))
    distil_list_buttons = Row([
        ElevatedButton(content=Text("❌   Clear Prompts", size=18), on_click=clear_prompts),
        FilledButton(content=Text("Add All Prompts to List", size=20), height=45, on_click=add_to_list),
    ], alignment=MainAxisAlignment.SPACE_BETWEEN)
    if len(page.distil_gpt2_list.controls) < 1:
      distil_list_buttons.visible = False

    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("⚗️  Distilled GPT-2 Generator - GPT-2 AI Helper", "Generates new Image Prompts with a model trained on 2,470,000 descriptive Stable Diffusion prompts...", actions=[ElevatedButton(content=Text("🍜  NSP Instructions", size=18), on_click=lambda _: NSP_instructions(page))]),
        Row([TextField(label="Starter Prompt Text", expand=True, value=distil_gpt2_prefs['seed_prompt'], multiline=True, on_change=lambda e: changed(e, 'seed_prompt'))]),
        AI_temperature,
        ResponsiveRow([top_k, no_repeat_ngram_size]),
        ResponsiveRow([repetition_penalty, penalty_alpha]),
        max_length,
        ResponsiveRow([
          Row([NumberPicker(label="Amount: ", min=1, max=40, value=distil_gpt2_prefs['amount'], on_change=lambda e: changed(e, 'amount')), seed,
              NumberPicker(label="Random Artists: ", min=0, max=10, value=distil_gpt2_prefs['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN, vertical_alignment=CrossAxisAlignment.START),
          Row([NumberPicker(label="Random Styles: ", min=0, max=10, value=distil_gpt2_prefs['random_styles'], on_change=lambda e: changed(e, 'random_styles')),
              Checkbox(label="Permutate Artists", value=distil_gpt2_prefs['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'), tooltip="Shuffles the list of Artists and Styles to make Combo Variations.")], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
        ]),
        ElevatedButton(content=Text("📝   Make Distil GPT-2 Prompts", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_distil_gpt2(page)),
        page.distil_gpt2_output,
        page.distil_gpt2_list,
        distil_list_buttons,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

superprompt_prefs = {
    'seed_prompt': '',
    'seed': 0,
    'amount': 1,
    'AI_temperature': 0.5,
    'top_p': 1,
    'top_k': 50,
    'max_new_tokens': 512,
    'repetition_penalty': 1.2,
    'random_artists': 0,
    'random_styles': 0,
    'permutate_artists': True,
}

def buildSuperPrompt(page):
    global superprompt_prefs, prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          superprompt_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def superprompt_help(e):
        alert_msg(page, "💁   Help with SuperPrompt", [
            "Brian Fitzgerald trained a 77M T5 model to expand prompts, and it meets or exceeds existing 1B+ parameter LLMs in quality and prompt alignment. The idea here is that the existing model (in my experiments, SDXL 1.0) is conditioned on CLIP text embeddings; and within a CLIP embedding space, the existing prompt, and a better prettier prompt, both exist - so I'd train a ~20M parameter MLP to convert the CLIP embeddings (unpooled, as is used in the diffusion model) from an worse looking prompt to a nicer one. For the dataset, I'd use the DiffusionDB prompt dataset, and use Llama2 to rewrite the prompts to remove any descriptors that might improve the fidelity of the image.",
            "Thinking about why the CLIP augmenter didn't work, I wondered if it might be easier to instead augment the latent during certain steps of the denoising process. Taking a page from SD Ultimate Upscale - which uses a secondary U-net to upscale a latent - I tried training a latent augmenter, while performing inference on prompts from the same DiffusionDB dataset. ",
            Markdown("[Project Page](https://brianfitzgerald.xyz/prompt-augmentation/) | [GitHub Repo](https://github.com/sammcj/superprompter) | [HuggingFace Model](https://huggingface.co/roborovski/superprompt-v1)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], "🧛  Super Cool...", False)
    page.superprompt_list = Column([], spacing=0)
    page.superprompt_output = Column([])
    def add_to_prompt_list(p):
      page.add_to_prompts(p)
      play_snd(Snd.DROP, page)
    def add_to_superprompt(p):
      page.superprompt_list.controls.append(ListTile(title=Text(p, max_lines=3, theme_style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))
      page.superprompt_list.update()
      superprompt_list_buttons.visible = True
      superprompt_list_buttons.update()
    page.add_to_superprompt = add_to_superprompt
    def add_to_list(e):
      play_snd(Snd.DROP, page)
      for p in page.superprompt_list.controls:
        page.add_to_prompts(p.title.value)
    def clear_prompts(e):
      play_snd(Snd.DELETE, page)
      page.superprompt_list.controls = []
      page.superprompt_list.update()
      superprompt_list_buttons.visible = False
      superprompt_list_buttons.update()
    AI_temperature = Row([Text("AI Temperature:"), Slider(label="{value}", min=0, max=1, divisions=10, round=1, expand=True, tooltip="The value used to module the next token probabilities", value=superprompt_prefs['AI_temperature'], on_change=lambda e: changed(e, 'AI_temperature'))], col={'lg':6})
    top_k = Row([Text("Top-K Samples:"), Slider(label="{value}", min=1, max=100, divisions=99, expand=True, tooltip="Higher k means more diverse outputs by considering a range of tokens. Number of highest probability vocabulary tokens to keep for top-k-filtering", value=superprompt_prefs['top_k'], on_change=lambda e: changed(e, 'top_k'))], col={'lg':6})
    top_p = Row([Text("Top-P Samples:"), Slider(label="{value}", min=0, max=2, divisions=40, round=2, expand=True, tooltip="Higher values sample more low-probability tokens.", value=superprompt_prefs['top_p'], on_change=lambda e: changed(e, 'top_p'))], col={'lg':6})
    max_new_tokens = SliderRow(label="Max New Tokens", min=250, max=512, divisions=262, pref=superprompt_prefs, key='max_new_tokens', col={'lg':6})
    repetition_penalty = SliderRow(label="Repetition Penalty", min=0, max=2, divisions=8, round=2, pref=superprompt_prefs, key='repetition_penalty', col={'lg':6})
    seed = TextField(label="Seed", value=superprompt_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width = 90, on_change=lambda e:changed(e,'seed', ptype="int"))
    superprompt_list_buttons = Row([
        ElevatedButton(content=Text("❌   Clear Prompts", size=18), on_click=clear_prompts),
        FilledButton(content=Text("Add All Prompts to List", size=20), height=45, on_click=add_to_list),
    ], alignment=MainAxisAlignment.SPACE_BETWEEN)
    if len(page.superprompt_list.controls) < 1:
      superprompt_list_buttons.visible = False

    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🦸  SuperPrompt v1 Detailer", "Generates more detailed prompts in a 77M Parameter custom trained Google Flan-T5...", actions=[ElevatedButton(content=Text("🍜  NSP Instructions", size=18), on_click=lambda _: NSP_instructions(page)), IconButton(icon=icons.HELP, tooltip="Help with SuperPrompt Settings", on_click=superprompt_help)]),
        Row([TextField(label="Starter Prompt Text", expand=True, value=superprompt_prefs['seed_prompt'], multiline=True, on_change=lambda e: changed(e, 'seed_prompt'))]),
        AI_temperature,
        ResponsiveRow([top_k, top_p]),
        ResponsiveRow([max_new_tokens, repetition_penalty]),
        ResponsiveRow([
          Row([NumberPicker(label="Amount: ", min=1, max=40, value=superprompt_prefs['amount'], on_change=lambda e: changed(e, 'amount')), seed,
              NumberPicker(label="Random Artists: ", min=0, max=10, value=superprompt_prefs['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN, vertical_alignment=CrossAxisAlignment.START),
          Row([NumberPicker(label="Random Styles: ", min=0, max=10, value=superprompt_prefs['random_styles'], on_change=lambda e: changed(e, 'random_styles')),
              Checkbox(label="Permutate Artists", value=superprompt_prefs['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'), tooltip="Shuffles the list of Artists and Styles to make Combo Variations.")], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
        ]),
        ElevatedButton(content=Text("🍄   Make SuperPrompts", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_superprompt(page)),
        page.superprompt_output,
        page.superprompt_list,
        superprompt_list_buttons,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c


def NSP_instructions(page):
    def open_url(e):
        if e.data.startswith('http'):
          page.launch_url(e.data)
        else:
          page.set_clipboard(e.data)
          toast_msg(page, f"📋   NSP variable {e.data} copied to clipboard...")
    NSP_markdown = r'''To use a term database, simply use any of the keys below in sentence. Copy to Clipboard with click.

For example if you wanted beauty adjective, you would write `_adj-beauty_` in your prompt.

## Terminology Keys (by [@WAS](https://rebrand.ly/easy-diffusion))

### Adjective Types
   - [\_adj-architecture\_](_adj-architecture_) - A list of architectural adjectives and styles
   - [\_adj-beauty\_](_adj-beauty_) - A list of beauty adjectives for people (maybe things?)
   - [\_adj-general\_](_adj-general_) - A list of general adjectives for people/things.
   - [\_adj-horror\_](_adj-horror_) - A list of horror adjectives
### Art Types
   - [\_artist\_](_artist_) - A comprehensive list of artists by [**MisterRuffian**](https://docs.google.com/spreadsheets/d/1_jgQ9SyvUaBNP1mHHEzZ6HhL_Es1KwBKQtnpnmWW82I/edit) (Discord _Misterruffian#2891_)
   - [\_color\_](_color_) - A comprehensive list of colors
   - [\_portrait-type\_](_portrait-type_) - A list of common portrait types/poses
   - [\_style\_](_style_) - A list of art styles and mediums
### Computer Graphics Types
   - [\_3d-terms\_](_3d-terms_) - A list of 3D graphics terminology
   - [\_color-palette\_](_color-palette_) - A list of computer and video game console color palettes
   - [\_hd\_](_hd_) - A list of high definition resolution terms
### Miscellaneous Types
   - [\_details\_](_details_) - A list of detail descriptors
   - [\_site\_](_site_) - A list of websites to query
   - [\_gen-modififer\_](_gen-modififer_) - A list of general modifiers adopted from [Weird Wonderful AI Art](https://weirdwonderfulai.art/)
   - [\_neg-weight\_](_neg-weight_) - A lsit of negative weight ideas
   - [\_punk\_](_punk_) - A list of punk modifier (eg. cyberpunk)
   - [ _pop-culture\_](_pop-culture_) - A list of popular culture movies, shows, etc
   - [\_pop-location\_](_pop-location_) - A list of popular tourist locations
   - [\_fantasy-setting\_](_fantasy-setting_) - A list of fantasy location settings
   - [\_fantasy-creature\_](_fantasy-creature_) - A list of fantasy creatures
   - [\_animals\_](_animals_) - A list of modern animals
### Noun Types
   - [\_noun-beauty\_](_noun-beauty_) - A list of beauty related nouns
   - [\_noun-emote\_](_noun-emote_) - A list of emotions and expressions
   - [\_noun-fantasy\_](_noun-fantasy_) - A list of fantasy nouns
   - [\_noun-general\_](_noun-general_) - A list of general nouns
   - [\_noun-horror\_](_noun-horror_) - A list of horror nouns
### People Types
   - [\_bodyshape\_](_bodyshape_) - A list of body shapes
   - [\_celeb\_](_celeb_) - A list of celebrities
   - [\_eyecolor\_](_eyecolor_) - A list of eye colors
   - [\_hair\_](_hair_) - A list of hair types
   - [\_nationality\_](_nationality_) - A list of nationalities
   - [\_occputation\_](_occputation_) A list of occupation types
   - [\_skin-color\_](_skin-color_) - A list of skin tones
   - [\_identity-young\_](_identity-young_) A list of young identifiers
   - [\_identity-adult\_](_identity-adult_) A list of adult identifiers
   - [\_identity\_](_identity_) A list of general identifiers
### Photography / Image / Film Types
   - [\_aspect-ratio\_](_aspect-ratio_) - A list of common aspect ratios
   - [\_cameras\_](_cameras_) - A list of camera models *(including manufactuerer)*
   - [\_camera-manu\_](_camera-manu_) - A list of camera manufacturers
   - [\_f-stop\_](_f-stop_) - A list of camera aperture f-stop
   - [\_focal-length\_](_focal-length_) - A list of focal length ranges
   - [\_photo-term\_](_photo-term_) - A list of photography terms relating to photos

So in Subject try something like: `A _color_ _noun-general_ that is _adj-beauty_ and _adj-general_ with a _noun-emote_ _noun-fantasy_`
'''
    def close_NSP_dlg(e):
      instruction_alert.open = False
      page.update()
    instruction_alert = AlertDialog(title=Text("🍜  Noodle Soup Prompt Variables Instructions"), content=Column([Markdown(NSP_markdown, extension_set="gitHubWeb", on_tap_link=open_url)], scroll=ScrollMode.AUTO), actions=[TextButton("🍲  Good Soup! ", on_click=close_NSP_dlg)], actions_alignment=MainAxisAlignment.END,)
    page.overlay.append(instruction_alert)
    instruction_alert.open = True
    page.update()

negatives = {
    'Blurry': 'blurry, blur, out of focus, jpeg artifacts, pixelated',
    'Text': 'text, words, signature, watermark',
    'Bad Quality': 'bad quality, low-res, worst quality, grainy',
    'Ugly': 'ugly, deformed, distorted, poorly drawn',
    'Black & White': 'black and white, monochrome, monotone, colorless',
    'Bad Hands': 'bad hands, deformed fingers, fewer digits, fused fingers',
    'Bad Body': 'bad body, disfigured, amputation, wrong anatomy, missing limbs, extra limbs',
    'Bad Eyes': 'bad eyes, deformed iris, deformed pupils, unclear eyes, cross-eyed',
    'Bad Legs': 'bad legs, missing legs, extra legs, missing feet, imperfect feet, bad knee, fused calf',
    'Mutilated': 'mutilated, mutilation, mutated, morbid, bad anatomy',
    'Proportion': 'bad proportions, gross proportions, long neck, long body, malformed',
    'Saturation': 'over saturated, unsaturated, washed out, bad saturation',
    'Contrast': 'high contrast, low contrast, High pass filter',
    'Boring': 'boring, unappealing, tasteless, tacky, lackluster',
    'Simple': 'simple, simplistic, sketch, amateur, plain background',
    'Unrealistic': 'unrealistic, vector, cartoon',
    'Cropped': 'cropped, crop, out of frame, cut off',
    'NSFW': 'NSFW, nude, naked, censored, censor_bar, nipples',
}

def buildNegatives(page):
    global prefs, negatives, status
    def change_neg(e):
        if e.control.data in prefs['negatives']:
            prefs['negatives'].remove(e.control.data)
        else:
            prefs['negatives'].append(e.control.data)
        update_negs()
        status['changed_prefs'] = True
    negs = ""
    def changed_custom(e):
        nonlocal negs
        prefs['custom_negatives'] = e.control.value
        cust = f", {prefs['custom_negatives']}"
        neg_text.value = negs + (cust if bool(prefs['custom_negatives']) else '')
        neg_text.update()
    def update_negs(update=True):
        nonlocal negs
        negs_list = []
        for n in prefs['negatives']:
            negs_list.append(negatives[n])
        negs = ', '.join(negs_list)
        cust = f", {prefs['custom_negatives']}"
        neg_text.value = negs + (cust if bool(prefs['custom_negatives']) else '')
        if update: neg_text.update()
    def copy_clip(e):
        page.set_clipboard(neg_text.value)
        toast_msg(page, f"📋  Copied to clipboard... Paste into your Negative Prompt Text.")
    neg_list = ResponsiveRow(controls=[])
    for k, v in negatives.items():
        neg_list.controls.append(Checkbox(label=k, tooltip=v, data=k, value=k in prefs['negatives'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=change_neg, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
    custom_negatives = TextField(label="Custom Negative Text", value=prefs['custom_negatives'], on_change=changed_custom)
    neg_text = Text(negs, size=18, color=colors.ON_SECONDARY_CONTAINER, selectable=True)
    update_negs(False)
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🚫   Negative Prompt Builder", "Generate your Negatives with ease to subtract what you don't want in your images."),
        neg_list,
        custom_negatives,
        ElevatedButton(content=Text("➖  Copy Negs to Clipboard", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=copy_clip),
        Container(neg_text, bgcolor=colors.SECONDARY_CONTAINER, padding=10, border_radius=border_radius.all(12), alignment = alignment.center, margin=margin.only(top=10)),#], alignment=CrossAxisAlignment.CENTER),
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

def buildPromptStyler(page):
    global prefs, status
    def styler_help(e):
        def close_styler_dlg(e):
          nonlocal styler_help_dlg
          styler_help_dlg.open = False
          page.update()
        styler_help_dlg = AlertDialog(title=Text("💁   Help with Prompt Styler"), content=Column([
            Text("This allows you to take a simple base prompt and apply a preset style to the the positive and negative prompt variables.  You can then add that stylized prompt to your Prompts List, or copy/paste it for other Image Generators."),
            Markdown("Credit goes to [Prompt Styler](https://github.com/twri/sdxl_prompt_styler) by twri and [Fooocus UI](https://github.com/lllyasviel/Fooocus) by Illyasviel for the Styler presets, which is a pretty good GUI alternative for easy SDXL generation. Launch [Fooocus Colab](https://colab.research.google.com/github/camenduru/Fooocus-colab/blob/main/Fooocus_colab.ipynb) and read this [Fooocus Style Reference Doc](https://docs.google.com/spreadsheets/d/1AF5bd-fALxlu0lguZQiQVn1yZwxUiBJGyh2eyJJWl74/edit#gid=0)...", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("💅  So Stylish... ", on_click=close_styler_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(styler_help_dlg)
        styler_help_dlg.open = True
        page.update()
    def toggle_multi(e):
        prefs['prompt_styler_multi'] = e.control.value
        styler_radio_container.visible=not prefs['prompt_styler_multi']
        styler_checkbox_container.visible=prefs['prompt_styler_multi']
        styler_checkbox_container.update()
        styler_radio_container.update()
        status['changed_prefs'] = True
    def change_style(e):
        prefs['prompt_style'] = e.control.value
        update_style()
        status['changed_prefs'] = True
    negative = ""
    prompt = ""
    def changed_custom(e):
        nonlocal prompt
        prefs['prompt_styler'] = e.control.value
        styler = sdd_utils.prompt_styles[prefs['prompt_style']]
        prompt = styler[0].replace("{prompt}", prefs['prompt_styler'])
        prompt = to_title(prompt, sentence=True, clean=False)
        prompt_text.value = prompt
        prompt_text.update()
        status['changed_prefs'] = True
    def changed_checkbox(e):
        on = e.control.value
        if e.control.data in prefs['prompt_styles']:
            prefs['prompt_styles'].remove(e.control.data)
        else:
            prefs['prompt_styles'].append(e.control.data)
        status['changed_prefs'] = True
    def update_style(update=True):
        nonlocal negative, prompt
        styler = sdd_utils.prompt_styles[prefs['prompt_style']]
        prompt = styler[0].replace("{prompt}", prefs['prompt_styler'])
        prompt = to_title(prompt, sentence=True, clean=False)
        negative = styler[1]
        prompt_text.value = prompt
        neg_text.value = negative
        if update:
          prompt_text.update()
          neg_text.update()
    def add_to_prompts(e):
        nonlocal negative, prompt
        page.add_to_prompts(prompt, {'negative_prompt': negative})
        play_snd(Snd.DROP, page)
    def copy_clip(e):
        page.set_clipboard(neg_text.value)
        toast_msg(page, f"📋  Copied to clipboard... Paste into your Negative Prompt Text.")
    def add_to_prompt_list(e):
        styler = sdd_utils.prompt_styles[e.control.data]
        pr = styler[0].replace("{prompt}", prefs['prompt_styler'])
        pr = to_title(pr, sentence=True, clean=False)
        negative = styler[1]
        arg = {'negative_prompt': negative}
        page.add_to_prompts(pr, arg)
        play_snd(Snd.DROP, page)
    def add_to_list(e):
        play_snd(Snd.DROP, page)
        for s in prefs['prompt_styles']:
            styler = sdd_utils.prompt_styles[s]
            pr = styler[0].replace("{prompt}", prefs['prompt_styler'])
            pr = to_title(pr, sentence=True, clean=False)
            negative = styler[1]
            arg = {'negative_prompt': negative}
            page.add_to_prompts(pr, arg)
    def clear_prompts(e):
        play_snd(Snd.DELETE, page)
        styler_results.controls.clear()
        styler_results.update()
    def generate_styles(e):
        page.styler.auto_scroll = True
        styler_results.controls.clear()
        for s in prefs['prompt_styles']:
            styler = sdd_utils.prompt_styles[s]
            pr = styler[0].replace("{prompt}", prefs['prompt_styler'])
            pr = to_title(pr, sentence=True, clean=False)
            negative = styler[1]
            styler_results.controls.append(ListTile(title=Text(pr, max_lines=3, theme_style=TextThemeStyle.BODY_LARGE), subtitle=Text(f"Negative: {negative}", max_lines=3), data=s, dense=True, on_click=add_to_prompt_list))
        styler_list_buttons = Row([
            ElevatedButton(content=Text("❌   Clear Prompts", size=18), on_click=clear_prompts),
            add_all_to_prompts_btn,
        ], alignment=MainAxisAlignment.SPACE_BETWEEN)
        styler_results.controls.append(styler_list_buttons)
        styler_results.update()
        nudge(page.styler, page)
        page.styler.auto_scroll = False
    style_list = ResponsiveRow(controls=[], run_spacing=0)
    style_checkboxes = ResponsiveRow(controls=[], run_spacing=0)
    for k in sdd_utils.style_keys:
        style_list.controls.append(ft.Radio(label=k, value=k, fill_color=colors.PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
        style_checkboxes.controls.append(Checkbox(label=k, data=k, value=k in prefs['prompt_styles'], on_change=changed_checkbox, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
    prompt_styler_multi = Switcher(label="Multi-Select ", value=prefs['prompt_styler_multi'], tooltip="Toggle between Single Style Selection to Multiple Prompt Generator.", label_position=ft.LabelPosition.LEFT, on_change=toggle_multi)
    prompt_styler = TextField(label="Subject Prompt Text", value=prefs['prompt_styler'], filled=True, on_change=changed_custom)
    prompt_text = Text(prompt, size=18, color=colors.ON_SECONDARY_CONTAINER, selectable=True)
    neg_text = Text(negative, size=18, color=colors.ON_SECONDARY_CONTAINER, selectable=True)
    style_radio = ft.RadioGroup(content=style_list, value=prefs['prompt_style'], on_change=change_style)
    styler_results = Column([], tight=True, spacing=0)
    generate_styles_btn = ElevatedButton(content=Text("👗  Generate Styles", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=generate_styles)
    add_all_to_prompts_btn = FilledButton(content=Text("➕  Add to Prompts List", size=20), height=45, on_click=add_to_list)
    add_to_prompts_btn = ElevatedButton(content=Text("➕  Add to Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=add_to_prompts)
    styler_result = ResponsiveRow([
          Container(prompt_text, bgcolor=colors.SECONDARY_CONTAINER, padding=10, border_radius=border_radius.all(12), margin=margin.only(top=10), col={'md': 8}),
          Container(neg_text, bgcolor=colors.SECONDARY_CONTAINER, padding=10, border_radius=border_radius.all(12), margin=margin.only(top=10), col={'md': 4}),
        ], vertical_alignment=CrossAxisAlignment.START)
    styler_checkbox_container = Container(Column([
      style_checkboxes,
      prompt_styler,
      generate_styles_btn,
      styler_results,
    ]), visible=prefs['prompt_styler_multi'])
    styler_radio_container = Container(Column([
      style_radio,
      prompt_styler,
      add_to_prompts_btn,
      styler_result,
    ]), visible=not prefs['prompt_styler_multi'])
    update_style(False)
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("👓   Prompt Styler", "Generate your Prompts with Premade Style Templates.", actions=[prompt_styler_multi, IconButton(icon=icons.HELP, tooltip="Help with Prompt Styler", on_click=styler_help)]),
        #prompt_styler_multi,
        styler_checkbox_container,
        styler_radio_container,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c


ESRGAN_prefs = {
    'enlarge_scale': 1.5,
    'face_enhance': False,
    'image_path': '',
    'save_to_GDrive': True,
    'upload_file': False,
    'download_locally': False,
    'display_image': False,
    'dst_image_path': '',
    'upscale_model': prefs['upscale_model'],
    'filename_suffix': '',
    'split_image_grid': False,
    'rows': 3,
    'cols': 3,
}
def buildESRGANupscaler(page):
    def changed(e, pref=None):
      if pref is not None:
        ESRGAN_prefs[pref] = e.control.value
    def add_to_ESRGAN_output(o):
      ESRGAN_output.controls.append(o)
      ESRGAN_output.update()
      if clear_button.visible == False:
        clear_button.visible = True
        clear_button.update()
      #generator_list_buttons.visible = True
      #generator_list_buttons.update()
    page.add_to_ESRGAN_output = add_to_ESRGAN_output
    def toggle_split(e):
      split_container.height = None if e.control.value else 0
      changed(e, 'split_image_grid')
      split_container.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      ESRGAN_output.controls = []
      ESRGAN_output.update()
      clear_button.visible = False
      clear_button.update()
    page.clear_ESRGAN_output = clear_output
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        save_file(e.file_name)
    def save_file(file_name):
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
        else:
          fname = file_name
        image_path.value = fname
        image_path.update()
        ESRGAN_prefs['image_path'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Image File to Enlarge")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    def pick_destination(e):
        alert_msg(page, "Switch to Colab tab and press Files button on the Left & Find the Path you want to Save Images into, Right Click and Copy Path, then Paste here")
    page.overlay.append(file_picker)
    def change_upscale_model(e):
        ESRGAN_prefs['upscale_model'] = e.control.value
        for u in Real_ESRGAN_models:
          if u['name'] == ESRGAN_prefs['upscale_model']:
            model_info.value = f"  [**Model Card**]({u['info']})"
            model_info.update()
    upscale_model = Dropdown(label="ESRGAN Upscale Model", hint_text="", width=300, options=[], value=ESRGAN_prefs['upscale_model'], autofocus=False, on_change=change_upscale_model)
    for u in Real_ESRGAN_models:
        upscale_model.options.append(dropdown.Option(u['name']))
        if u['name'] == ESRGAN_prefs['upscale_model']:
          current_model = u
    model_info = Markdown(f"  [**Model Info**]({current_model['info']})", on_tap_link=lambda e: e.page.launch_url(e.data))
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=ESRGAN_prefs, key='enlarge_scale')
    face_enhance = Checkbox(label="Use Face Enhance GPFGAN", value=ESRGAN_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))
    image_path = TextField(label="Image File or Folder Path", value=ESRGAN_prefs['image_path'], col={'md':6}, on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path))
    dst_image_path = FileInput(label="Destination Image Path", pref=ESRGAN_prefs, key='dst_image_path', ftype="folder", col={'md':6}, page=page)
    #dst_image_path = TextField(label="Destination Image Path", value=ESRGAN_prefs['dst_image_path'], col={'md':6}, on_change=lambda e:changed(e,'dst_image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_destination))
    filename_suffix = TextField(label="Optional Filename Suffix", hint_text="-big", value=ESRGAN_prefs['filename_suffix'], on_change=lambda e:changed(e,'filename_suffix'), width=260)
    download_locally = Checkbox(label="Download Images Locally", value=ESRGAN_prefs['download_locally'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'download_locally'))
    display_image = Checkbox(label="Display Upscaled Image", value=ESRGAN_prefs['display_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_image'))
    split_image_grid = Switcher(label="Split Image Grid", value=ESRGAN_prefs['split_image_grid'], on_change=toggle_split, tooltip="Handy when you have images saved in 2x2 combined together.")
    rows = NumberPicker(label="Rows: ", min=1, max=8, value=ESRGAN_prefs['rows'], on_change=lambda e: changed(e, 'rows'))
    cols = NumberPicker(label="Columns: ", min=1, max=8, value=ESRGAN_prefs['cols'], on_change=lambda e: changed(e, 'cols'))
    split_container = Container(Row([rows, Container(content=None, width=25), cols]), animate_size=animation.Animation(800, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=28), height=0)
    ESRGAN_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(ESRGAN_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("↕️   Real-ESRGAN AI Upscale Enlarging", "Select one or more files, or give path to image or folder. Save to your Google Drive and/or Download."),
        enlarge_scale_slider,
        face_enhance,
        ResponsiveRow([image_path, dst_image_path]),
        Row([upscale_model, model_info]),
        Row([filename_suffix, download_locally, display_image]),
        #Divider(thickness=2, height=4),
        split_image_grid,
        split_container,
        ElevatedButton(content=Text("🐘  Run AI Upscaling", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_upscaling(page)),
        ESRGAN_output,
        clear_button,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

#TODO: https://colab.research.google.com/github/AhabbscienceStudioPak/ESRGAN/blob/master/ESRGAN_Colab.ipynb
upscale_video_prefs = {
    'enlarge_scale': 1.5,
    'face_enhance': False,
    'image_path': '',
    'save_to_GDrive': True,
    'upload_file': False,
    'download_locally': False,
    'display_image': False,
    'dst_image_path': '',
    'filename_suffix': '',
}
def buildUpscaleVideo(page):
    global upscale_video_prefs
    def changed(e, pref=None):
      if pref is not None:
        upscale_video_prefs[pref] = e.control.value
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=upscale_video_prefs, key='enlarge_scale')
    face_enhance = Checkbox(label="Use Face Enhance GPFGAN", value=upscale_video_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))
    #image_path = TextField(label="Image File or Folder Path", value=upscale_video_prefs['image_path'], col={'md':6}, on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path))
    #dst_image_path = TextField(label="Destination Image Path", value=upscale_video_prefs['dst_image_path'], col={'md':6}, on_change=lambda e:changed(e,'dst_image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_destination))
    filename_suffix = TextField(label="Optional Filename Suffix", hint_text="-big", value=upscale_video_prefs['filename_suffix'], on_change=lambda e:changed(e,'filename_suffix'), width=260)
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("↕️   Upscale Videos with AI Enlarging", "Select one or more files, or give path to image or folder. Save to your Google Drive and/or Download."),
        enlarge_scale_slider,
        face_enhance,
        #ResponsiveRow([image_path, dst_image_path]),
        filename_suffix,
        #Row([download_locally, display_image]),
        #Divider(thickness=2, height=4),
        ElevatedButton(content=Text("🐘  Run Video Upscaling", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_upscaling(page)),
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

retrieve_prefs = {
    'image_path': '',
    'add_to_prompts': True,
    'display_full_metadata': False,
    'display_image': False,
    'upload_file': False,
}
def buildRetrievePrompts(page):
    def changed(e, pref=None):
        if pref is not None:
          retrieve_prefs[pref] = e.control.value
    def add_to_retrieve_output(o):
      retrieve_output.controls.append(o)
      retrieve_output.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      retrieve_output.controls = []
      retrieve_output.update()
      clear_button.visible = False
      clear_button.update()
    def pick_image(e):
        alert_msg(page, "Switch to Colab tab and press Files button on the Left & Find the Path you want to Retrieve, Right Click and Copy Path, then Paste here")
    page.add_to_retrieve_output = add_to_retrieve_output
    image_path = TextField(label="Image File or Folder Path", value=retrieve_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_image))
    add_to_prompts = Checkbox(label="Add to Prompts", value=retrieve_prefs['add_to_prompts'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'add_to_prompts'))
    display_full_metadata = Checkbox(label="Display Full Metadata", value=retrieve_prefs['display_full_metadata'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_full_metadata'))
    display_image = Checkbox(label="Display Image", value=retrieve_prefs['display_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_image'))
    retrieve_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(retrieve_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("📰  Retrieve Dream Prompts from Image Metadata", "Give it images made here and gives you all parameters used to recreate it. Either upload png file(s) or paste path to image or folder or config.json to revive your dreams.."),
        image_path,
        add_to_prompts,
        display_full_metadata,
        display_image,
        ElevatedButton(content=Text("😴  Retrieve Dream", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_retrieve(page)),
        retrieve_output,
        clear_button,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

initfolder_prefs = {
    'prompt_string': '',
    'negative_prompt': '',
    'init_folder': '',
    'include_strength': True,
    'image_strength': 0.5,
}
def buildInitFolder(page):
    def changed(e, pref=None):
        if pref is not None:
          initfolder_prefs[pref] = e.control.value
    def add_to_initfolder_output(o):
      initfolder_output.controls.append(o)
      initfolder_output.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      initfolder_output.controls = []
      initfolder_output.update()
      clear_button.visible = False
      clear_button.update()
    def get_directory_result(e):
        path = e.path
        if path:
            init_folder.value = path
            init_folder.update()
            initfolder_prefs['init_folder'] = path
    def pick_folder(e):
        if is_Colab:
            alert_msg(page, "Switch to Colab tab and press Files button on the Left & Find the Path you want to use as Init Folder, Right Click and Copy Path, then Paste here")
        else:
            get_directory_dialog = ft.FilePicker(on_result=lambda e: get_directory_result(e))
            page.overlay.append(get_directory_dialog)
            page.update()
            get_directory_dialog.get_directory_path(dialog_title="Select the folder containing Images to process")
    def toggle_strength(e):
      changed(e,'include_strength')
      strength_row.visible = e.control.value
      strength_row.update()
    page.add_to_initfolder_output = add_to_initfolder_output
    prompt_string = TextField(label="Prompt Text", value=initfolder_prefs['prompt_string'], col={'md': 9}, on_change=lambda e:changed(e,'prompt_string'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=initfolder_prefs['negative_prompt'], col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    init_folder = FileInput(label="Init Image Folder Path", pref=initfolder_prefs, key='init_folder', ftype="folder", page=page)
    #init_folder = TextField(label="Init Image Folder Path", value=initfolder_prefs['init_folder'], on_change=lambda e:changed(e,'init_folder'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_folder))
    include_strength = Checkbox(label="Include Strength", value=initfolder_prefs['include_strength'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=toggle_strength)
    image_strength = Slider(min=0.1, max=0.9, divisions=16, label="{value}", round=2, value=float(initfolder_prefs['image_strength']), expand=True)
    strength_row = Row([Text("Image Strength:"), image_strength])
    strength_row.visible = initfolder_prefs['include_strength']
    initfolder_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(initfolder_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("📂 Generate Prompts from Folder as Init Images", "Provide a Folder with a collection of images that you want to automatically add to prompts list with init_image overrides..."),
        init_folder,
        ResponsiveRow([prompt_string, negative_prompt]),
        include_strength,
        strength_row,
        ElevatedButton(content=Text("➕  Add to Prompts", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_initfolder(page)),
        initfolder_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

init_video_prefs = {
    'prompt': '',
    'negative_prompt': '',
    'init_folder': '',
    'include_strength': False,
    'image_strength': 0.5,
    'max_size': 1024,
    'file_prefix': 'frame-',
    'video_file': '',
    'fps': 15,
    'start_time': 0.0,
    'end_time': 0.0,
    'batch_folder_name': '',
    'show_images': False,
}
def buildInitVideo(page):
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          init_video_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_init_video_output(o):
      init_video_output.controls.append(o)
      init_video_output.update()
      if clear_button.visible == False:
          clear_button.visible = True
          clear_button.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      init_video_output.controls = []
      init_video_output.update()
      clear_button.visible = False
      clear_button.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        save_file(e.file_name)
    def save_file(file_name):
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
          init_video_prefs['file_name'] = file_name.rpartition('.')[0]
        else:
          fname = file_name
          init_video_prefs['file_name'] = file_name.rpartition(slash)[2].rpartition('.')[0]
        video_file.value = fname
        video_file.update()
        init_video_prefs['video_file'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    #page.overlay.append(pick_files_dialog)
    def pick_video(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["avi", "mp4", "mov"], dialog_title="Pick Video File")
    def toggle_strength(e):
      changed(e,'include_strength')
      strength_row.visible = e.control.value
      strength_row.update()
    page.add_to_init_video_output = add_to_init_video_output
    prompt = TextField(label="Prompt Text", value=init_video_prefs['prompt'], col={'md': 9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=init_video_prefs['negative_prompt'], col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=init_video_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Image Filename Prefix", value=init_video_prefs['file_prefix'], width=150, on_change=lambda e:changed(e,'file_prefix'))
    video_file = TextField(label="Video File", value=init_video_prefs['video_file'], on_change=lambda e:changed(e,'video_file'), height=60, suffix=IconButton(icon=icons.VIDEO_CALL, on_click=pick_video))
    #init_folder = TextField(label="Init Image Folder Path", value=init_video_prefs['init_folder'], on_change=lambda e:changed(e,'init_folder'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    fps = TextField(label="Frames Per Seccond", value=init_video_prefs['fps'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'fps', ptype="int"))
    start_time = TextField(label="Start Time (s)", value=init_video_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype="float"))
    end_time = TextField(label="End Time (0 for all)", value=init_video_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype="float"))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=init_video_prefs, key='max_size')
    show_images = Checkbox(label="Show Extracted Images", value=init_video_prefs['show_images'], tooltip="Fills up screen with all frames, you probably don't need to.", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'show_images'))
    include_strength = Checkbox(label="Include Strength   ", value=init_video_prefs['include_strength'], tooltip="Otherwise defaults to setting in Image Parameters", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=toggle_strength)
    image_strength = Slider(min=0.1, max=0.9, divisions=16, label="{value}", round=2, value=float(init_video_prefs['image_strength']), expand=True)
    strength_row = Row([Text("Image Strength:"), image_strength])
    strength_row.visible = init_video_prefs['include_strength']
    init_video_output = Column([])
    page.init_video_output = init_video_output
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Prompts List", size=18), on_click=page.clear_prompts_list), ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(init_video_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎥 Generate Prompts from Video File Frames", "Provide a short video clip to automatically add sequence to prompts list with init_image overrides..."),
        video_file, #init_folder,
        Row([fps, start_time, end_time]),
        Row([batch_folder_name, file_prefix]),
        ResponsiveRow([prompt, negative_prompt]),
        Row([include_strength, show_images]),
        strength_row,
        max_row,
        ElevatedButton(content=Text("⏩  Frames to Prompts", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_init_video(page)),
        init_video_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

image2text_prefs = {
    'method': 'Fuyu-8B',
    'mode': 'Best',
    'fuyu_mode': 'Detailed Caption',
    'gemini_mode': 'Detailed Caption',
    'openai_mode': 'Detailed Caption',
    'request_mode': 'Caption',
    'slow_workers': True,
    'trusted_workers': False,
    'question': '',
    'folder_path': '',
    'image_path': '',
    'max_size': 768,
    'save_csv': False,
    'images': [],
    'use_AIHorde': False,
}

def buildImage2Text(page):
    global prefs, image2text_prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          image2text_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_image2text_output(o):
      page.image2text_output.controls.append(o)
      page.image2text_output.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.image2text_output.controls = []
      page.image2text_output.update()
      save_dir = os.path.join(root_dir, 'image2text')
      if os.path.exists(save_dir):
        for f in os.listdir(save_dir):
            os.remove(os.path.join(save_dir, f))
        os.rmdir(save_dir)
      page.image2text_file_list.controls = []
      page.image2text_file_list.update()
      image2text_list_buttons.visible = False
      image2text_list_buttons.update()
    def i2t_help(e):
      def close_i2t_dlg(e):
        nonlocal i2t_help_dlg
        i2t_help_dlg.open = False
        page.update()
      i2t_help_dlg = AlertDialog(title=Text("💁   Help with Image2Text CLIP Interrogator"), content=Column([
          Text("You have 4 methods to get a caption prompt from you input image. Using BLIP Interrogation Mode with Best, Classic or Fast will use the older BLIP models that work great but takes longer to run. The latest is Google Gemini Pro Vision API is very impressive and fast, using your Bard API key for free. The Fuyu-8B is a much faster captioning technique using Transformers, and can give Detailed Coco-style prompts or simpler captioning. You can also use AIHorde to process the caption interrogation in the cloud using Stable Horde services without using your GPU."),
          Text("Fuyu-8B by AdeptAI Labs is a small version of the multimodal model. It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy. It’s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images. It’s fast - we can get responses for large images in less than 100 milliseconds."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😪  Okay then... ", on_click=close_i2t_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(i2t_help_dlg)
      i2t_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        save_file(e.file_name)
    def save_file(file_name):
        save_dir = os.path.join(root_dir, 'image2text')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        image2text_prefs['folder_path'] = save_dir
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
          fpath = os.path.join(save_dir, file_name)
        else:
          fname = file_name
          fpath = os.path.join(save_dir, file_name.rpartition(slash)[2])
        original_img = PILImage.open(fname)
        width, height = original_img.size
        width, height = scale_dimensions(width, height, image2text_prefs['max_size'])
        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
        original_img.save(fpath)
        #shutil.move(fname, fpath)
        page.image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.INFO, text="Image Details", on_click=image_details, data=fpath),
              PopupMenuItem(icon=icons.DELETE, text="Delete Image", on_click=delete_image, data=fpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All", on_click=delete_all_images, data=fpath),
          ]), data=fpath, on_click=image_details))
        page.image2text_file_list.update()
        image2text_prefs['images'].append(fpath)
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Image File to Enlarge")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
                #uf.append(FilePickerUploadFile(f.name, upload_url=f.path))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def delete_image(e):
        f = e.control.data
        if os.path.isfile(f):
          os.remove(f)
          for i, fl in enumerate(page.image2text_file_list.controls):
            if fl.title.value == f:
              del page.image2text_file_list.controls[i]
              page.image2text_file_list.update()
              continue
        if f in image2text_prefs['images']:
          image2text_prefs['images'].remove(f)
    def delete_all_images(e):
        for fl in page.image2text_file_list.controls:
          f = fl.title.value
          if os.path.isfile(f):
            os.remove(f)
        page.image2text_file_list.controls.clear()
        page.image2text_file_list.update()
        image2text_prefs['images'].clear()
    def image_details(e):
        img = e.control.data
        alert_msg(e.page, "Image Details", content=Image(src=img), sound=False)
    def add_image(e):
        save_dir = os.path.join(root_dir, 'image2text')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        image2text_prefs['folder_path'] = save_dir
        if image_path.value.startswith('http'):
          from io import BytesIO
          response = requests.get(image_path.value)
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
          width, height = original_img.size
          width, height = scale_dimensions(width, height, image2text_prefs['max_size'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
        elif os.path.isfile(image_path.value):
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(image_path.value)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, image2text_prefs['max_size'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          #shutil.copy(image_path.value, fpath)
        elif os.path.isdir(image_path.value):
          for f in os.listdir(image_path.value):
            file_path = os.path.join(image_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              fpath = os.path.join(save_dir, f)
              original_img = PILImage.open(file_path)
              width, height = original_img.size
              width, height = scale_dimensions(width, height, image2text_prefs['max_size'])
              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              original_img.save(fpath)
              #shutil.copy(file_path, fpath)
        else:
          if bool(image_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        page.image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.INFO, text="Image Details", on_click=image_details, data=fpath),
              PopupMenuItem(icon=icons.DELETE, text="Delete Image", on_click=delete_image, data=fpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All", on_click=delete_all_images, data=fpath),
          ]), data=fpath, on_click=image_details))
        page.image2text_file_list.update()
        image2text_prefs['images'].append(fpath)
        image_path.value = ""
        image_path.update()
    page.image2text_list = Column([], spacing=0)
    def add_to_prompt_list(p):
      page.add_to_prompts(p)
      play_snd(Snd.DROP, page)
    def add_to_image2text(p):
      page.image2text_list.controls.append(ListTile(title=Text(p, max_lines=18, theme_style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))
      page.image2text_list.update()
      image2text_list_buttons.visible = True
      image2text_list_buttons.update()
    page.add_to_image2text = add_to_image2text
    def add_to_list(e):
      play_snd(Snd.DROP, page)
      for p in page.image2text_list.controls:
        page.add_to_prompts(p.title.value)
    def toggle_AIHorde(e):
      use = e.control.value
      changed(e,'use_AIHorde')
      AIHorde_row.height=None if use else 0
      AIHorde_row.update()
      mode.visible = not use
      mode.update()
      request_mode.visible = use
      request_mode.update()
    def change_method(e):
      method = e.control.value
      changed(e,'method')
      AIHorde_row.visible = method=="AIHorde Crowdsourced"
      AIHorde_row.update()
      mode.visible = method=="BLIP-Interrogation"
      mode.update()
      request_mode.visible = method=="AIHorde Crowdsourced"
      request_mode.update()
      fuyu_mode.visible = method=="Fuyu-8B"
      fuyu_mode.update()
      gemini_mode.visible = method=="Google Gemini Pro"
      gemini_mode.update()
      openai_mode.visible = "GPT-4" in method or "Anthropic Claude 3" in method or method=="Moondream 2"
      openai_mode.update()
      question_prompt.visible = (method=="Fuyu-8B" and image2text_prefs['fuyu_mode']=="Question") or (method=="Google Gemini Pro" and image2text_prefs['gemini_mode']=="Question") or (("GPT-4" in method or "Anthropic Claude 3" in method or method=="Moondream 2") and image2text_prefs['openai_mode']=="Question")
      question_prompt.update()
    def change_fuyu(e):
      fuyu = e.control.value
      changed(e,'fuyu_mode')
      question_prompt.visible = fuyu=="Question"
      question_prompt.update()
    def change_gemini(e):
      gemini = e.control.value
      changed(e,'gemini_mode')
      question_prompt.visible = gemini=="Question"
      question_prompt.update()
    def change_openai(e):
      gemini = e.control.value
      changed(e,'openai_mode')
      question_prompt.visible = gemini=="Question"
      question_prompt.update()
    def clear_prompts(e):
      play_snd(Snd.DELETE, page)
      page.image2text_list.controls = []
      page.image2text_list.update()
      image2text_list_buttons.visible = False
      image2text_list_buttons.update()
    image2text_list_buttons = Row([
        ElevatedButton(content=Text("❌   Clear Prompts"), on_click=clear_prompts),
        FilledButton(content=Text("➕  Add All Prompts to List", size=20), height=45, on_click=add_to_list),
    ], alignment=MainAxisAlignment.SPACE_BETWEEN)
    if len(page.image2text_list.controls) < 1:
      image2text_list_buttons.visible = False

    method = Dropdown(label="Captioning Method", width=250, options=[dropdown.Option("Fuyu-8B"), dropdown.Option("Google Gemini Pro"), dropdown.Option("OpenAI GPT-4 Vision"), dropdown.Option("OpenAI GPT-4o"), dropdown.Option("Anthropic Claude 3 Vision"), dropdown.Option("Anthropic Claude 3.5 Vision"), dropdown.Option("Moondream 2"), dropdown.Option("BLIP-Interrogation"), dropdown.Option("AIHorde Crowdsourced")], value=image2text_prefs['method'], on_change=change_method)
    #use_AIHorde = Switcher(label="Use AIHorde Crowdsourced Interrogator", value=image2text_prefs['use_AIHorde'], on_change=toggle_AIHorde)
    mode = Dropdown(label="Interrogation Mode", width=200, options=[dropdown.Option("Best"), dropdown.Option("Classic"), dropdown.Option("Fast")], value=image2text_prefs['mode'], visible=image2text_prefs['method']=="BLIP-Interrogation", on_change=lambda e: changed(e, 'mode'))
    request_mode = Dropdown(label="Request Mode", width=200, options=[dropdown.Option("Caption"), dropdown.Option("Interrogation"), dropdown.Option("Full Prompt")], value=image2text_prefs['request_mode'], visible=image2text_prefs['method']=="AIHorde Crowdsourced", on_change=lambda e: changed(e, 'request_mode'))
    fuyu_mode = Dropdown(label="Fuyu Request Mode", width=200, options=[dropdown.Option("Detailed Caption"), dropdown.Option("Simple Caption"), dropdown.Option("Question")], value=image2text_prefs['fuyu_mode'], visible=image2text_prefs['method']=="Fuyu-8B", on_change=change_fuyu)
    gemini_mode = Dropdown(label="Request Mode", width=200, options=[dropdown.Option("Detailed Caption"), dropdown.Option("Poetic Caption"), dropdown.Option("Artistic Caption"), dropdown.Option("Technical Caption"), dropdown.Option("Simple Caption"), dropdown.Option("Question")], value=image2text_prefs['gemini_mode'], visible=image2text_prefs['method']=="Google Gemini Pro", on_change=change_gemini)
    openai_mode = Dropdown(label="Request Mode", width=200, options=[dropdown.Option("Detailed Caption"), dropdown.Option("Poetic Caption"), dropdown.Option("Artistic Caption"), dropdown.Option("Technical Caption"), dropdown.Option("Simple Caption"), dropdown.Option("Question")], value=image2text_prefs['openai_mode'], visible="GPT-4" in image2text_prefs['method'] or "Anthropic Claude 3" in image2text_prefs['method'] or image2text_prefs['method']=="Moondream 2", on_change=change_openai)
    slow_workers = Checkbox(label="Allow Slow Workers", tooltip="", value=image2text_prefs['slow_workers'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'slow_workers'))
    trusted_workers = Checkbox(label="Only Trusted Workers", tooltip="", value=image2text_prefs['trusted_workers'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'trusted_workers'))
    AIHorde_row = Container(content=Row([slow_workers, trusted_workers]), visible=image2text_prefs['method']=="AIHorde Crowdsourced", animate_size=animation.Animation(800, AnimationCurve.EASE_OUT_CIRC), clip_behavior=ClipBehavior.HARD_EDGE)
    question_prompt = TextField(label="Ask Question about Image", value=image2text_prefs['question'], visible=image2text_prefs['method']=="Fuyu-8B" and image2text_prefs['fuyu_mode']=="Question", expand=True, on_change=lambda e:changed(e,'question'))
    save_csv = Checkbox(label="Save CSV file of Prompts", tooltip="", value=image2text_prefs['save_csv'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_csv'))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=32, multiple=16, suffix="px", pref=image2text_prefs, key='max_size')
    image_path = TextField(label="Image File or Folder Path or URL to Train", value=image2text_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    add_image_button = ElevatedButton(content=Text("Add File or Folder"), on_click=add_image)
    page.image2text_file_list = Column([], tight=True, spacing=0)
    page.image2text_output = Column([])
    #clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    #clear_button.visible = len(page.image2text_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("😶‍🌫️  Image2Text CLIP-Interrogator", subtitle="Create text prompts by describing input images...", actions=[save_default(image2text_prefs, exclude=['images', 'image_path']), IconButton(icon=icons.HELP, tooltip="Help with Image2Text Interrogator", on_click=i2t_help)]),
        #mode,
        Row([method, fuyu_mode, mode, request_mode, gemini_mode, openai_mode, question_prompt, AIHorde_row]),
        max_row,
        Row([image_path, add_image_button]),
        page.image2text_file_list,
        ElevatedButton(content=Text("👨‍🎨️  Get Prompts from Images", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_image2text(page)),
        page.image2text_list,
        image2text_list_buttons,
        page.image2text_output,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

BLIP2_image2text_prefs = {
    'folder_path': '',
    'image_path': '',
    'max_size': 768,
    'num_captions': 1,
    'question_prompt': '',
    #'model_name': 'blip2_t5',
    'model_type': 'pretrain_flant5xxl', #pretrain_opt2.7b, pretrain_opt6.7b, caption_coco_opt2.7b, caption_coco_opt6.7b, pretrain_flant5xl, caption_coco_flant5xl
    'images': [],
}

def buildBLIP2Image2Text(page):
    global prefs, BLIP2_image2text_prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          BLIP2_image2text_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_BLIP2_image2text_output(o):
      page.BLIP2_image2text_output.controls.append(o)
      page.BLIP2_image2text_output.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.BLIP2_image2text_output.controls = []
      page.BLIP2_image2text_output.update()
      save_dir = os.path.join(root_dir, 'BLIP2_image2text')
      if os.path.exists(save_dir):
        for f in os.listdir(save_dir):
            os.remove(os.path.join(save_dir, f))
        os.rmdir(save_dir)
      page.BLIP2_image2text_file_list.controls = []
      page.BLIP2_image2text_file_list.update()
      BLIP2_image2text_list_buttons.visible = False
      BLIP2_image2text_list_buttons.update()
    def BLIP2_i2t_help(e):
      def close_BLIP2_i2t_dlg(e):
        nonlocal BLIP2_i2t_help_dlg
        BLIP2_i2t_help_dlg.open = False
        page.update()
      BLIP2_i2t_help_dlg = AlertDialog(title=Text("💁   Help with BLIP2 Image2Text Interrogator"), content=Column([
          Text("Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Model", weight=FontWeight.BOLD),
          Text("BLIP-2 is a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😑  Alright, got it... ", on_click=close_BLIP2_i2t_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(BLIP2_i2t_help_dlg)
      BLIP2_i2t_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        save_file(e.file_name)
    def save_file(file_name):
        save_dir = os.path.join(root_dir, 'BLIP2_image2text')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        BLIP2_image2text_prefs['folder_path'] = save_dir
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
          fpath = os.path.join(save_dir, file_name)
        else:
          fname = file_name
          fpath = os.path.join(save_dir, file_name.rpartition(slash)[2])
        original_img = PILImage.open(fname)
        width, height = original_img.size
        width, height = scale_dimensions(width, height, BLIP2_image2text_prefs['max_size'])
        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
        original_img.save(fpath)
        #shutil.move(fname, fpath)
        page.BLIP2_image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))
        page.BLIP2_image2text_file_list.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Image File to Enlarge")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def add_image(e):
        save_dir = os.path.join(root_dir, 'BLIP2_image2text')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        BLIP2_image2text_prefs['folder_path'] = save_dir
        if image_path.value.startswith('http'):
          import requests
          from io import BytesIO
          response = requests.get(image_path.value)
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
          width, height = original_img.size
          width, height = scale_dimensions(width, height, BLIP2_image2text_prefs['max_size'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          page.BLIP2_image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))
          page.BLIP2_image2text_file_list.update()
        elif os.path.isfile(image_path.value):
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(image_path.value)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, BLIP2_image2text_prefs['max_size'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          #shutil.copy(image_path.value, fpath)
          page.BLIP2_image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))
          page.BLIP2_image2text_file_list.update()
        elif os.path.isdir(image_path.value):
          for f in os.listdir(image_path.value):
            file_path = os.path.join(image_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              fpath = os.path.join(save_dir, f)
              original_img = PILImage.open(file_path)
              width, height = original_img.size
              width, height = scale_dimensions(width, height, BLIP2_image2text_prefs['max_size'])
              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              original_img.save(fpath)
              #shutil.copy(file_path, fpath)
              page.BLIP2_image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))
              page.BLIP2_image2text_file_list.update()
        else:
          if bool(image_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        image_path.value = ""
        image_path.update()
    page.BLIP2_image2text_list = Column([], spacing=0)
    def add_to_prompt_list(p):
      page.add_to_prompts(p)
      play_snd(Snd.DROP, page)
    def add_to_BLIP2_image2text(p):
      page.BLIP2_image2text_list.controls.append(ListTile(title=Text(p, max_lines=3, theme_style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))
      page.BLIP2_image2text_list.update()
      BLIP2_image2text_list_buttons.visible = True
      BLIP2_image2text_list_buttons.update()
    page.add_to_BLIP2_image2text = add_to_BLIP2_image2text
    def add_to_list(e):
      play_snd(Snd.DROP, page)
      for p in page.BLIP2_image2text_list.controls:
        page.add_to_prompts(p.title.value)
    def clear_prompts(e):
      play_snd(Snd.DELETE, page)
      page.BLIP2_image2text_list.controls = []
      page.BLIP2_image2text_list.update()
      prompts = []
      BLIP2_image2text_list_buttons.visible = False
      BLIP2_image2text_list_buttons.update()
    BLIP2_image2text_list_buttons = Row([
        ElevatedButton(content=Text("❌   Clear Prompts"), on_click=clear_prompts),
        FilledButton(content=Text("➕  Add All Prompts to List", size=20), height=45, on_click=add_to_list),
    ], alignment=MainAxisAlignment.SPACE_BETWEEN)
    if len(page.BLIP2_image2text_list.controls) < 1:
      BLIP2_image2text_list_buttons.visible = False
    #'pretrain_flant5xxl', pretrain_opt2.7b, pretrain_opt6.7b, caption_coco_opt2.7b, caption_coco_opt6.7b, pretrain_flant5xl, caption_coco_flant5xl
    #model_name = Dropdown(label="Model Nane", width=250, options=[dropdown.Option("blip2_t5"), dropdown.Option("blip2_opt"), dropdown.Option("img2prompt_vqa")], value=BLIP2_image2text_prefs['model_name'], on_change=lambda e: changed(e, 'model_name'))
    model_type = Dropdown(label="Model Type", width=250, options=[dropdown.Option("pretrain_flant5xxl"), dropdown.Option("pretrain_opt2.7b"), dropdown.Option("pretrain_opt6.7b"), dropdown.Option("caption_coco_opt6.7b"), dropdown.Option("caption_coco_flant5xl"), dropdown.Option("base")], value=BLIP2_image2text_prefs['model_type'], on_change=lambda e: changed(e, 'model_type'))
    num_captions = Container(NumberPicker(label="Number of Captions: ", min=1, max=10, value=BLIP2_image2text_prefs['num_captions'], on_change=lambda e: changed(e, 'num_captions')))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=BLIP2_image2text_prefs, key='max_size')
    question_prompt = TextField(label="Ask Question about Image (optional)", value=BLIP2_image2text_prefs['question_prompt'], on_change=lambda e:changed(e,'question_prompt'))
    image_path = TextField(label="Image File or Folder Path or URL to Examine", value=BLIP2_image2text_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=True)
    add_image_button = ElevatedButton(content=Text("Add File or Folder"), on_click=add_image)
    page.BLIP2_image2text_file_list = Column([], tight=True, spacing=0)
    page.BLIP2_image2text_output = Column([])
    #clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    #clear_button.visible = len(page.BLIP2_image2text_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🤳  BLIP2 Image2Text Examiner", subtitle="Create prompts by describing input images... Warning: Uses A LOT of VRAM, may crash session.", actions=[save_default(BLIP2_image2text_prefs, exclude=['images', 'image_path']), IconButton(icon=icons.HELP, tooltip="Help with Image2Text Interrogator", on_click=BLIP2_i2t_help)]),
        Row([model_type, num_captions]),
        max_row,
        question_prompt,
        Row([image_path, add_image_button]),
        page.BLIP2_image2text_file_list,
        page.BLIP2_image2text_list,
        BLIP2_image2text_list_buttons,
        ElevatedButton(content=Text("📸  Get Prompts from Images", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_BLIP2_image2text(page)),
        page.BLIP2_image2text_output,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c


dance_prefs = {
    'dance_model': 'maestro-150k',
    'installed_model': None,
    'inference_steps': 50,
    'batch_size': 1,
    'seed': 0,
    'audio_length_in_s': 4.5,
    'community_model': 'Daft Punk',
    'custom_model': '',
    'train_custom': False,#https://colab.research.google.com/github/Harmonai-org/sample-generator/blob/main/Finetune_Dance_Diffusion.ipynb
    'custom_name': '',
    'wav_path': '',
    'demo_every': 250,#Number of training steps between demos
    'checkpoint_every': 500,#Number of training steps between saving model checkpoints
    'sample_rate': 48000,#Sample rate to train at
    'sample_size': 65536,#Number of audio samples per training sample
    'random_crop': True,#If true, the audio samples provided will be randomly cropped to SAMPLE_SIZE samples. Turn off if you want to ensure the training data always starts at the beginning of the audio files (good for things like drum one-shots)
    'finetune_batch_size': 2,#Batch size to fine-tune (make it as high as it can go for your GPU)
    'accumulate_batches': 4,#Accumulate gradients over n batches, useful for training on one GPU. Effective batch size is BATCH_SIZE * ACCUM_BATCHES. Also increases the time between demos and saved checkpoints
    'save_model': False,
    'where_to_save_model': "Public Library",
    'readme_description': '',
}
community_dance_diffusion_models = [
    #{'name': 'LCD Soundsystem', 'download': 'https://drive.google.com/uc?id=1WX8nL4_x49h0OJE5iGrjXJnIJ0yvsTxI', 'ckpt':'lcd-soundsystem-200k.ckpt'}, #https://huggingface.co/Gecktendo/lcd-soundsystem/
    {'name': 'Daft Punk', 'download': 'https://drive.google.com/uc?id=1CZjWIcL528zbZa6GrS_triob0hUy6KEs', 'ckpt':'daft-punk-241.5k.ckpt'}, #https://huggingface.co/Gecktendo/daft-punk
    {'name': 'Vague Phrases', 'download': 'https://drive.google.com/uc?id=1nUn2qydqU7hlDUT-Skq_Ionte_8-Vdjr', 'ckpt': 'SingingInFepoch=1028-step=195500-pruned.ckpt'},
    {'name': 'Gesaffelstein', 'download': 'https://drive.google.com/uc?id=1-BuDzz4ajX-ufVByEX_fCkOtB00DVygB', 'ckpt':'Gesaffelstein_epoch=2537-step=445000.ckpt'},
    {'name': 'Smash Mouth Vocals', 'download': 'https://drive.google.com/uc?id=1h3fkJnByw3mKpXUiNPWKoYtzmpeg1QEt', 'ckpt':'epoch=773-step=191500.ckpt'},
    {'name': 'Dubstep Bass Growls', 'download': 'https://drive.google.com/uc?id=104Ni-suQ0-tt2Xe9SbWjTnWFOSkT6O47', 'ckpt':'epoch=1266-step=195000.ckpt'},
    {'name': 'Jumango Ambient', 'download': 'https://drive.google.com/uc?id=1-gpOee-v7ZGFJtzr76sYTKuIKTQKTCmN', 'ckpt':'jumango-ambient-v1.ckpt'},
    {'name': 'Serum Wavetables', 'download': 'https://drive.google.com/uc?id=1l0JhA2qTaXtt5pdyv7rW1Ls4wmAFWVD1', 'ckpt':'serumwavetables-49k.ckpt'},
    {'name': 'Textured Grooves Club', 'download': 'https://drive.google.com/uc?id=13VAGMSPaIGo7FGDAtedaykhcasrFk2Z_', 'ckpt':'TexturedGroovesLargeClub_step_592200.ckpt'},
    {'name': 'Abstract Vocal', 'download': 'https://drive.google.com/uc?id=1izVPIYgPhpIT8lZtaWIO8gbTnfEL9g_J', 'ckpt':'Singing-step277000-pruned.ckpt'},
    {'name': 'Gesaffelstein', 'download': 'https://drive.google.com/uc?id=1-BuDzz4ajX-ufVByEX_fCkOtB00DVygB', 'ckpt':'Gesaffelstein_epoch=2537-step=445000.ckpt'},
    {'name': 'Paul McCartney Vocals', 'download': 'https://drive.google.com/uc?id=1-_FtUwLMnMUGLMpvtnE0EDAcUjDlXSeS', 'ckpt':'epoch=1148-step=193000.ckpt'},
    {'name': 'Techno Kicks', 'download': 'https://drive.google.com/uc?id=1-gR9QFq7ZYHn2ep0gw5IyQ6WzRZJW_tG', 'ckpt':'epoch=1296-step=441500.ckpt'},
    {'name': 'Electronic Snare Drums', 'download': 'https://drive.google.com/uc?id=1-T-PFtfyc_JUan71Px_FWsxiiuBuNN_1', 'ckpt':'epoch=1078-step=195000.ckpt'},
    {'name': 'Electronic Snare Drums+', 'download': 'https://drive.google.com/uc?id=1-50R5wwyhNrQSlvaxEqQ8CiRJPYgzGsr', 'ckpt':'epoch=1110-step=195500.ckpt'},
    {'name': 'Electronic Kick Drums', 'download': 'https://drive.google.com/uc?id=1-46jYgYfz_Jbnu-dNvepWqa7rcSP__zo', 'ckpt':'epoch=1234-step=197500.ckpt'},
]
dance_pipe = None
def buildDanceDiffusion(page):
    global dance_pipe, dance_prefs
    def changed(e, pref=None, isInt=False):
        if pref is not None:
          if isInt:
            dance_prefs[pref] = int(e.control.value)
          else:
            dance_prefs[pref] = e.control.value
    def dance_help(e):
        def close_dance_dlg(e):
          nonlocal dance_help_dlg
          dance_help_dlg.open = False
          page.update()
        dance_help_dlg = AlertDialog(title=Text("💁   Help with Dance Diffusion"), content=Column([
            Text("HarmonAI Dance Diffusion"),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("👄  Sounds Good...I hope ", on_click=close_dance_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(dance_help_dlg)
        dance_help_dlg.open = True
        page.update()
    def delete_audio(e):
        f = e.control.data
        if os.path.isfile(f):
          os.remove(f)
          for i, fl in enumerate(page.dance_file_list.controls):
            if fl.title.value == f:
              del page.dance_file_list.controls[i]
              page.dance_file_list.update()
              if f in dance_prefs['custom_wavs']:
                dance_prefs['custom_wavs'].remove(f)
              continue
    def delete_all_audios(e):
        for fl in page.dance_file_list.controls:
          f = fl.title.value
          if os.path.isfile(f):
            os.remove(f)
        page.dance_file_list.controls.clear()
        page.dance_file_list.update()
        dance_prefs['custom_wavs'].clear()
    def add_file(fpath, update=True):
        page.dance_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[#TODO: View Image
              PopupMenuItem(icon=icons.DELETE, text="Delete Audio", on_click=delete_audio, data=fpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All", on_click=delete_all_audios, data=fpath),
          ])))
        dance_prefs['custom_wavs'].append(fpath)
        if update: page.dance_file_list.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    save_dir = os.path.join(root_dir, 'dance-audio')
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
          save_file(e.file_name)
    def save_file(file_name):
          if not os.path.exists(save_dir):
            os.mkdir(save_dir)
          if not slash in file_name:
            fname = os.path.join(root_dir, file_name)
            fpath = os.path.join(save_dir, file_name)
            shutil.move(fname, fpath)
          else:
            fname = file_name
            fpath = os.path.join(save_dir, file_name.rpartition(slash)[2])
          add_file(fpath)
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["wav", "WAV", "mp3", "MP3"], dialog_title="Pick Audio WAV or MP3 Files to Train")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def add_wav(e):
        if not os.path.exists(save_dir):
            os.mkdir(save_dir)
        if wav_path.value.startswith('http'):
            #import requests
            from io import BytesIO
            #response = requests.get(wav_path.value)
            fpath = download_file(wav_path.value)
            #fpath = os.path.join(save_dir, wav_path.value.rpartition(slash)[2])
            add_file(fpath)
        elif os.path.isfile(wav_path.value):
          fpath = os.path.join(save_dir, wav_path.value.rpartition(slash)[2])
          shutil.copy(wav_path.value, fpath)
          add_file(fpath)
        elif os.path.isdir(wav_path.value):
          for f in os.listdir(wav_path.value):
            file_path = os.path.join(wav_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.wav', '.WAV', '.mp3', '.MP3')):
              fpath = os.path.join(save_dir, f)
              shutil.copy(file_path, fpath)
              add_file(fpath)
        else:
          if bool(wav_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        wav_path.value = ""
        wav_path.update()
    def load_wavs():
        if os.path.exists(save_dir):
          for f in os.listdir(save_dir):
            existing = os.path.join(save_dir, f)
            if os.path.isdir(existing): continue
            if f.lower().endswith(('.wav', '.WAV', '.mp3', '.MP3')):
              add_file(existing, update=False)
    def toggle_custom(e):
        changed(e, 'train_custom')
        custom_box.height = None if dance_prefs['train_custom'] else 0
        custom_box.update()
        custom_audio_name.visible = dance_prefs['train_custom']
        custom_audio_name.update()
    def toggle_save(e):
        changed(e, 'save_model')
        where_to_save_model.visible = dance_prefs['save_model']
        where_to_save_model.update()
        readme_description.visible = dance_prefs['save_model']
        readme_description.update()
    def changed_model(e):
      dance_prefs['dance_model'] = e.control.value
      if e.control.value == 'Community':
        community_dance_diffusion_model.visible = True
        community_dance_diffusion_model.update()
      else:
        if community_dance_diffusion_model.visible:
          community_dance_diffusion_model.visible = False
          community_dance_diffusion_model.update()
      if e.control.value == 'Custom':
        custom_model.visible = True
        custom_model.update()
      else:
        if custom_model.visible:
          custom_model.visible = False
          custom_model.update()
    dance_model = Dropdown(label="Dance Diffusion Model", width=250, options=[dropdown.Option("maestro-150k"), dropdown.Option("glitch-440k"), dropdown.Option("jmann-small-190k"), dropdown.Option("jmann-large-580k"), dropdown.Option("unlocked-250k"), dropdown.Option("honk-140k"), dropdown.Option("gwf-440k"), dropdown.Option("Community"), dropdown.Option("Custom")], value=dance_prefs['dance_model'], on_change=changed_model)
    community_dance_diffusion_model = Dropdown(label="Community Model", width=250, options=[], value=dance_prefs['community_model'], on_change=lambda e: changed(e, 'community_model'))
    page.community_dance_diffusion_model = community_dance_diffusion_model
    custom_model = TextField(label="Custom Model Path", value=dance_prefs['custom_model'], on_change=lambda e:changed(e,'custom_model'))
    for c in prefs['custom_dance_diffusion_models']:
      community_dance_diffusion_model.options.append(dropdown.Option(c['name']))
    for c in community_dance_diffusion_models:
      community_dance_diffusion_model.options.append(dropdown.Option(c['name']))
    if not dance_prefs['dance_model'] == 'Community':
      community_dance_diffusion_model.visible = False
    if not dance_prefs['dance_model'] == 'Custom':
      custom_model.visible = False
    inference_row = SliderRow(label="Number of Inference Steps", min=10, max=200, divisions=190, pref=dance_prefs, key='inference_steps')
    batch_size = TextField(label="Batch Size", value=dance_prefs['batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'batch_size', isInt=True), width = 90)
    seed = TextField(label="Random Seed", value=dance_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', isInt=True), width = 110)
    audio_length_in_s = TextField(label="Audio Length in Seconds", value=dance_prefs['audio_length_in_s'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'audio_length_in_s'), width = 190)
    number_row = Row([batch_size, seed, audio_length_in_s])
    train_custom = Switcher(label="Train Custom Audio ", value=dance_prefs['train_custom'], on_change=toggle_custom)
    custom_audio_name = TextField(label="Custom Audio Name", value=dance_prefs['custom_name'], on_change=lambda e:changed(e,'custom_name'))
    wav_path = TextField(label="Audio Files or Folder Path or URL to Train", value=dance_prefs['wav_path'], on_change=lambda e:changed(e,'wav_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    add_wav_button = ElevatedButton(content=Text("Add Audio Files"), on_click=add_wav)
    page.dance_file_list = Column([], tight=True, spacing=0)
    sample_rate = Tooltip(message="Sample rate to train at", content=TextField(label="Sample Rate", value=dance_prefs['sample_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_rate', ptype='int'), width = 160))
    sample_size = Tooltip(message="Number of audio samples per training sample", content=TextField(label="Sample Size", value=dance_prefs['sample_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_size', ptype='int'), width = 160))
    finetune_batch_size = Tooltip(message="Batch size to fine-tune (make it as high as it can go for your GPU)", content=TextField(label="Finetune Batch Size", value=dance_prefs['finetune_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'finetune_batch_size', ptype='int'), width = 160))
    accumulate_batches = Tooltip(message="Accumulate gradients over n batches, useful for training on one GPU. Effective batch size is BATCH_SIZE * ACCUM_BATCHES. Also increases the time between demos and saved checkpoints", content=TextField(label="Accumulate Batches", value=dance_prefs['accumulate_batches'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'accumulate_batches', ptype='int'), width = 160))
    demo_every = Tooltip(message="Number of training steps between demos", content=TextField(label="Demo Every", value=dance_prefs['demo_every'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'demo_every', ptype='int'), width = 160))
    checkpoint_every = Tooltip(message="Number of training steps between saving model checkpoints", content=TextField(label="Checkpoint Every", value=dance_prefs['checkpoint_every'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'checkpoint_every', ptype='int'), width = 160))
    save_model = Switcher(label="Save Model to HuggingFace ", value=dance_prefs['save_model'], on_change=toggle_save, tooltip="Requires WRITE access on API Key to Upload Checkpoint")
    where_to_save_model = Dropdown(label="Where to Save Model", width=250, options=[dropdown.Option("Public Library"), dropdown.Option("Privately to my Profile")], value=dance_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))
    readme_description = TextField(label="Extra README Description", value=dance_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))
    where_to_save_model.visible = dance_prefs['save_model']
    readme_description.visible = dance_prefs['save_model']
    custom_box = Container(Column([
        Container(content=None, height=3),
        Row([sample_rate, sample_size]),
        Row([finetune_batch_size, accumulate_batches]),
        Row([demo_every, checkpoint_every]),
        Row([save_model, where_to_save_model]),
        readme_description,
        Text("Provide 3 or more ~10 second clips of Music as mp3 or wav files:", weight=FontWeight.BOLD),
        Row([wav_path, add_wav_button]),
        page.dance_file_list,]), padding=padding.only(left=11), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT_CIRC), clip_behavior=ClipBehavior.HARD_EDGE)
    custom_box.height = None if dance_prefs['train_custom'] else 0
    custom_audio_name.visible = dance_prefs['train_custom']
    load_wavs()
    #seed = TextField(label="Seed", value=dance_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    #lambda_entropy = TextField(label="Lambda Entropy", value=dreamfusdance_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    #max_steps = TextField(label="Max Steps", value=dance_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    page.dance_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.dance_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("👯   Dance Diffusion", "Create experimental music or sounds with HarmonAI trained audio models. Tools to train a generative model on arbitrary audio samples...", actions=[save_default(dance_prefs, exclude=['wav_path']), IconButton(icon=icons.HELP, tooltip="Help with DanceDiffusion Settings", on_click=dance_help)]),
        Row([dance_model, community_dance_diffusion_model, custom_model]),
        inference_row,
        number_row,
        Row([train_custom, custom_audio_name], vertical_alignment=CrossAxisAlignment.START),
        custom_box,
        ElevatedButton(content=Text("🎵  Run Dance Diffusion", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dance_diffusion(page)),
        page.dance_output,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

audio_diffusion_prefs = {
    'audio_file': '',
    'file_name': '',
    'audio_model': 'teticio/audio-diffusion-ddim-256',
    'scheduler': 'DDIM', #DDPM
    'steps': 50, #number of de-noising steps (defaults to 50 for DDIM, 1000 for DDPM)
    'start_step': 0, #step to start from
    'slice': 0, #slice number of audio to convert
    'eta': 0.2, #parameter between 0 and 1 used with DDIM scheduler
    'mask_start_secs': 0.0, #number of seconds of audio to mask (not generate) at start
    'mask_end_secs': 0.0, #number of seconds of audio to mask (not generate) at end
    'seed': 0,
    'audio_name': '',
    'wav_path': '',
    'batch_size': 1,
    'batch_folder_name': '',
    'file_prefix': 'ad-',
    'loaded_model': '',
}

def buildAudioDiffusion(page):
    global prefs, audio_diffusion_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            audio_diffusion_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_audio_diffusion_output(o):
        page.audio_diffusion_output.controls.append(o)
        page.audio_diffusion_output.update()
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.audio_diffusion_output.controls = []
        page.audio_diffusion_output.update()
        clear_button.visible = False
        clear_button.update()
    def audio_diffusion_help(e):
        def close_audio_diffusion_dlg(e):
          nonlocal audio_diffusion_help_dlg
          audio_diffusion_help_dlg.open = False
          page.update()
        audio_diffusion_help_dlg = AlertDialog(title=Text("💁   Help with Audio-Diffusion"), content=Column([
            Text("Audio Diffusion leverages the recent advances in image generation using diffusion models by converting audio samples to and from mel spectrogram images."),
            Markdown("The original codebase of this implementation can be found [here](https://github.com/teticio/audio-diffusion), including training scripts and example notebooks.\n[Audio Diffusion](https://github.com/teticio/audio-diffusion) by Robert Dargavel Smith.", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🔊  Sounds Groovie... ", on_click=close_audio_diffusion_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(audio_diffusion_help_dlg)
        audio_diffusion_help_dlg.open = True
        page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
            save_file(e.file_name)
    def save_file(file_name):
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
          audio_diffusion_prefs['file_name'] = file_name.rpartition('.')[0]
        else:
          fname = file_name
          fpath = os.path.join(root_dir, file_name.rpartition(slash)[2])
          audio_diffusion_prefs['file_name'] = file_name.rparition(slash)[2].rpartition('.')[0]
        audio_file.value = fname
        audio_file.update()
        audio_diffusion_prefs['audio_file'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def pick_audio(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["mp3", "wav"], dialog_title="Pick Init Audio File")
    audio_file = TextField(label="Input Audio File (optional)", value=audio_diffusion_prefs['audio_file'], on_change=lambda e:changed(e,'audio_file'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_audio))
    def change_eta(e):
        changed(e, 'eta', ptype="float")
        eta_value.value = f" {audio_diffusion_prefs['eta']}"
        eta_value.update()
        eta_row.update()
    steps_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=audio_diffusion_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", round=2, value=float(audio_diffusion_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=change_eta)
    eta_value = Text(f" {audio_diffusion_prefs['eta']}", weight=FontWeight.BOLD)
    eta_row = Row([Text("DDIM ETA:"), eta_value, eta,])
    page.etas.append(eta_row)
    audio_model = Dropdown(label="Audio Model", width=400, options=[dropdown.Option("teticio/audio-diffusion-ddim-256"), dropdown.Option("teticio/audio-diffusion-breaks-256"), dropdown.Option("teticio/audio-diffusion-instrumental-hiphop-256"), dropdown.Option("teticio/latent-audio-diffusion-256"), dropdown.Option("teticio/latent-audio-diffusion-ddim-256"), dropdown.Option("teticio/conditional-latent-audio-diffusion-512")], value=audio_diffusion_prefs['audio_model'], on_change=lambda e: changed(e, 'audio_model'))
    scheduler = Dropdown(label="De-noise Scheduler", width=250, options=[dropdown.Option("DDIM"), dropdown.Option("DDPM")], value=audio_diffusion_prefs['scheduler'], on_change=lambda e: changed(e, 'scheduler'))
    slice_audio = TextField(label="Slice of Audio", value=audio_diffusion_prefs['slice'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'slice', ptype='int'), width = 130)
    start_step = TextField(label="Starting Step", value=audio_diffusion_prefs['start_step'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'start_step', ptype='int'), width = 130)
    mask_start_secs = TextField(label="Mask Start (s)", value=audio_diffusion_prefs['mask_start_secs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'mask_start_secs', ptype='float'), width = 130)
    mask_end_secs = TextField(label="Mask End (s)", value=audio_diffusion_prefs['mask_end_secs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'mask_end_secs', ptype='float'), width = 130)
    audio_name = TextField(label="Audio File Name", value=audio_diffusion_prefs['audio_name'], on_change=lambda e:changed(e,'audio_name'))
    batch_folder_name = TextField(label="Batch Folder Name", value=audio_diffusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=audio_diffusion_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    batch_size = NumberPicker(label="Batch Size:  ", min=1, max=5, value=audio_diffusion_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    seed = TextField(label="Seed", value=audio_diffusion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)
    page.audio_diffusion_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.audio_diffusion_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎶  Audio Diffusion Modeling", "Converts Audio Samples to and from Mel Spectrogram Images...", actions=[save_default(audio_diffusion_prefs, exclude=['loaded_model', 'wav_path']), IconButton(icon=icons.HELP, tooltip="Help with Audio Diffusion-TTS Settings", on_click=audio_diffusion_help)]),
        audio_file,
        audio_model,
        #scheduler,
        steps_row,
        eta_row,
        Row([slice_audio, start_step, mask_start_secs, mask_end_secs]),
        Row([batch_size, seed, file_prefix]),
        Row([audio_name, batch_folder_name]),
        ElevatedButton(content=Text("🪗  Run Audio Diffusion", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_audio_diffusion(page)),
        page.audio_diffusion_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

music_gen_prefs = {
    'prompt': '',
    'audio_file': '',
    'file_name': '',
    'audio_model': 'medium',
    'duration': 30,
    'top_k': 280,
    'top_p': 1150,
    'temperature': 0.7,
    'guidance': 5,
    'overlap': 2,
    'dimension': 1,
    #'recondition': False,
    'harmony_only': False,
    'use_sampling': True,
    'two_step_cfg': False, #If True, performs 2 forward for Classifier Free Guidance, instead of batching together the two. This has some impact on how things are padded but seems to have little impact in practice.
    'seed': 0,
    'audio_name': '',
    'wav_path': '',
    'num_samples': 1,
    'batch_size': 1,
    'batch_folder_name': '',
    'file_prefix': 'musicgen-',
    'loaded_model': '',
}

def buildMusicGen(page):
    global prefs, music_gen_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            music_gen_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_music_gen_output(o):
        page.music_gen_output.controls.append(o)
        page.music_gen_output.update()
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.music_gen_output.controls = []
        page.music_gen_output.update()
        clear_button.visible = False
        clear_button.update()
    def music_gen_help(e):
        def close_music_gen_dlg(e):
          nonlocal music_gen_help_dlg
          music_gen_help_dlg.open = False
          page.update()
        music_gen_help_dlg = AlertDialog(title=Text("💁   Help with Audiocraft MusicGen"), content=Column([
            Text("Meta releases new SOTA text to music model MusicGen. Demonstrated samples are better than existing models including Google's MusicLM. We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen."),
            Text("Small: This model has 300M parameters and can only generate music from text. It is the quickest model, however it may not yield the best results.\nMedium: This model has 1.5B parameters and can generate music from text as well. It is slower than the little model, but it produces better results.\nMelody: This 1.5B parameter model can generate music from both text and melody. It is the slowest model, but it produces the best results.\nLarge: This model has 3.3B parameters and can only generate music from text. It is the slowest model, but it produces the best results."),
            Markdown("[GitHub Code](https://github.com/facebookresearch/audiocraft) | [Paper](https://arxiv.org/abs/2306.05284)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🥁  Let's hear it... ", on_click=close_music_gen_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(music_gen_help_dlg)
        music_gen_help_dlg.open = True
        page.update()
    #audio_file = TextField(label="Melody Conditioning Audio File (optional)", value=music_gen_prefs['audio_file'], on_change=lambda e:changed(e,'audio_file'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_audio))
    audio_file = FileInput(label="Melody Conditioning Audio File (optional)", pref=music_gen_prefs, key='audio_file', ftype="audio", page=page)
    prompt = TextField(label="Prompt to generate a track (genre, theme, etc.)", value=music_gen_prefs['prompt'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'prompt'))
    duration_row = SliderRow(label="Duration", min=1, max=720, divisions=718, suffix="s", expand=True, pref=music_gen_prefs, key='duration')
    #steps_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=music_gen_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    audio_model = Dropdown(label="Audio Model", width=150, options=[dropdown.Option("small"), dropdown.Option("medium"), dropdown.Option("large")], value=music_gen_prefs['audio_model'], on_change=lambda e: changed(e, 'audio_model'))
    guidance = SliderRow(label="Classifier Free Guidance", min=0, max=10, divisions=20, round=1, pref=music_gen_prefs, key='guidance', tooltip="Large => better quality and relavancy to text; Small => better diversity", col={'lg':6})
    temperature = SliderRow(label="AI Temperature", min=0, max=1, divisions=10, round=1, pref=music_gen_prefs, key='temperature', tooltip="Softmax value used to module the next token probabilities", col={'lg':6})
    top_k = SliderRow(label="Top-K Samples", min=0, max=300, divisions=299, round=0, pref=music_gen_prefs, key='top_k', tooltip="Number of highest probability vocabulary tokens to keep for top-k-filtering", col={'lg':6})
    top_p = SliderRow(label="Top-P Samples", min=0, max=1500, divisions=1499, round=0, pref=music_gen_prefs, key='top_p', tooltip="Highest probability vocabulary tokens to keep, when set to 0 top_k is used", col={'lg':6})
    #recondition = Checkbox(label="Recondition Chunks over 30s", tooltip="Condition next chunks with the first chunk.", value=music_gen_prefs['recondition'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'recondition'), col={'lg':6})
    harmony_only = Checkbox(label="Harmony Only", tooltip="Remove Drums?", value=music_gen_prefs['harmony_only'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'harmony_only'), col={'lg':6})
    #harmony_only = gr.Radio(label="Harmony Only",choices=["No", "Yes"], value="No", interactive=True, info="Remove Drums?")
    overlap = SliderRow(label="Overlap", min=0, max=29, divisions=29, round=0, pref=music_gen_prefs, key='overlap', tooltip="Time to resample chunks longer than 30s.", col={'lg':6})
    dimension = SliderRow(label="Dimension", min=-2, max=2, divisions=3, round=0, pref=music_gen_prefs, key='dimension', tooltip="Which direction to add new segements of audio. (1 = stack tracks, 2 = lengthen, -2..0 = ?)", col={'lg':6})
    #dimension = gr.Slider(minimum=-2, maximum=2, value=2, step=1, label="Dimension", info="determines which direction to add new segements of audio. (1 = stack tracks, 2 = lengthen, -2..0 = ?)", interactive=True)
    audio_name = TextField(label="Audio File Name", value=music_gen_prefs['audio_name'], on_change=lambda e:changed(e,'audio_name'))
    batch_folder_name = TextField(label="Batch Folder Name", value=music_gen_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=music_gen_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    num_samples = NumberPicker(label="Number of Samples:  ", min=1, max=10, value=music_gen_prefs['num_samples'], on_change=lambda e: changed(e, 'num_samples'))
    batch_size = NumberPicker(label="Batch Size:  ", min=1, max=4, value=music_gen_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    seed = TextField(label="Seed", value=music_gen_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)
    page.music_gen_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.music_gen_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🪗   Meta Audiocraft MusicGen (might be broken)", "Simple and Controllable Music Generation with Audio tokenization model...", actions=[save_default(music_gen_prefs, exclude=['audio_file']), IconButton(icon=icons.HELP, tooltip="Help with MusicGen Settings", on_click=music_gen_help)]),
        prompt,
        audio_file,
        Row([audio_model, duration_row]),
        ResponsiveRow([guidance, temperature]),
        ResponsiveRow([top_k, top_p]),
        ResponsiveRow([dimension, overlap]),
        Row([num_samples, seed, file_prefix]),
        Row([audio_name, batch_folder_name]),
        ElevatedButton(content=Text("🎷  Run MusicGen", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_music_gen(page)),
        page.music_gen_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

dreamfusion_prefs = {
    'prompt_text': '',
    'training_iters': 5000,
    'learning_rate': 0.001,
    'training_nerf_resolution': 64,
    'seed': 0,
    'lambda_entropy': 0.0001,
    'max_steps': 512,
    'checkpoint': 'latest',
    'workspace': 'trial',
}

def buildDreamFusion(page):
    global prefs, dreamfusion_prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          dreamfusion_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_dreamfusion_output(o):
      page.dreamfusion_output.controls.append(o)
      page.dreamfusion_output.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.dreamfusion_output.controls = []
      page.dreamfusion_output.update()
      clear_button.visible = False
      clear_button.update()
    def df_help(e):
      def close_df_dlg(e):
        nonlocal df_help_dlg
        df_help_dlg.open = False
        page.update()
      df_help_dlg = AlertDialog(title=Text("💁   Help with DreamFusion"), content=Column([
          Text("It's difficult to explain exactly what all these parameters do, but keep it close to defaults, keep prompt simple, or experiment to see what's what, we don't know."),
          Text('It takes about 0.7s per training step, so the default 5000 training steps take around 1 hour to finish. A larger Training_iters usually leads to better results.'),
          Text('If CUDA OOM, try to decrease Max_steps and Training_nerf_resolution.'),
          Text('If the NeRF fails to learn anything (empty scene, only background), try to decrease Lambda_entropy which regularizes the learned opacity.')
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😊  So Exciting... ", on_click=close_df_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(df_help_dlg)
      df_help_dlg.open = True
      page.update()
    prompt_text = TextField(label="Prompt Text", value=dreamfusion_prefs['prompt_text'], filled=True, on_change=lambda e:changed(e,'prompt_text'))
    training_iters = TextField(label="Training Iterations", value=dreamfusion_prefs['training_iters'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'training_iters', ptype='int'), width = 160)
    learning_rate = TextField(label="Learning Rate", value=dreamfusion_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160)
    training_nerf_resolution = TextField(label="Training NERF Res", value=dreamfusion_prefs['training_nerf_resolution'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'training_nerf_resolution', ptype='int'), width = 160)
    seed = TextField(label="Seed", value=dreamfusion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    lambda_entropy = TextField(label="Lambda Entropy", value=dreamfusion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    max_steps = TextField(label="Max Steps", value=dreamfusion_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    workspace = TextField(label="Workspace Folder", value=dreamfusion_prefs['workspace'], on_change=lambda e:changed(e,'workspace'))
    page.dreamfusion_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.dreamfusion_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🗿  Create experimental DreamFusion 3D Model and Video", "Provide a prompt to render a model. Warning: May take over an hour to run the training...", actions=[save_default(dreamfusion_prefs), IconButton(icon=icons.HELP, tooltip="Help with DreamFusion Settings", on_click=df_help)]),
        prompt_text,
        Row([training_iters,learning_rate, lambda_entropy]),
        Row([seed, training_nerf_resolution, max_steps]),
        Row([workspace]),
        ElevatedButton(content=Text("🔨  Run DreamFusion", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dreamfusion(page)),
        page.dreamfusion_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

point_e_prefs = {
    'prompt_text': '',
    'init_image': '',
    'guidance_scale': 3.0,
    'base_model': 'base40M-textvec', #'base40M', 'base300M' or 'base1B'
    'upsample': False,
    'batch_size': 1,
    'batch_folder_name': '',
    'seed': 0,
    'max_steps': 512,
}

def buildPoint_E(page):
    global prefs, point_e_prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          point_e_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_point_e_output(o):
      page.point_e_output.controls.append(o)
      page.point_e_output.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.point_e_output.controls = []
      page.point_e_output.update()
      clear_button.visible = False
      clear_button.update()
    def df_help(e):
      def close_df_dlg(e):
        nonlocal df_help_dlg
        df_help_dlg.open = False
        page.update()
      df_help_dlg = AlertDialog(title=Text("💁   Help with OpenAI Point-E"), content=Column([
          Markdown("This is an interface for running the [official codebase](https://github.com/openai/point-e) for point cloud diffusion models and SDF regression models described in [Point-E: A System for Generating 3D Point Clouds from Complex Prompts](https://arxiv.org/abs/2212.08751). These models were trained and released by OpenAI. Following [Model Cards for Model Reporting (Mitchell et al.)](https://arxiv.org/abs/1810.03993), we're providing some information about how the models were trained and evaluated.", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("The Point-E models are trained for use as point cloud diffusion models and SDF regression models. Our image-conditional models are often capable of producing coherent 3D point clouds, given a single rendering of a 3D object. However, the models sometimes fail to do so, either producing incorrect geometry where the rendering is occluded, or producing geometry that is inconsistent with visible parts of the rendering. The resulting point clouds are relatively low-resolution, and are often noisy and contain defects such as outliers or cracks. Our text-conditional model is sometimes capable of producing 3D point clouds which can be recognized as the provided text description, especially when the text description is simple. However, we find that this model fails to generalize to complex prompts or unusual objects."),
          Text("While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models for you to use."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("☝️  Good Points... ", on_click=close_df_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(df_help_dlg)
      df_help_dlg.open = True
      page.update()
    prompt_text = TextField(label="Prompt Text", value=point_e_prefs['prompt_text'], filled=True, on_change=lambda e:changed(e,'prompt_text'))
    #init_image = TextField(label="Sample Image (instead of prompt)", value=point_e_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))
    init_image = FileInput(label="Sample Image (instead of prompt)", pref=point_e_prefs, key='init_image', page=page)
    base_model = Dropdown(label="Base Model", width=250, options=[dropdown.Option("base40M-imagevec"), dropdown.Option("base40M-textvec"), dropdown.Option("base40M"), dropdown.Option("base300M"), dropdown.Option("base1B")], value=point_e_prefs['base_model'], on_change=lambda e: changed(e, 'base_model'))
    batch_folder_name = TextField(label="3D Model Folder Name", value=point_e_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    #batch_size = TextField(label="Batch Size", value=point_e_prefs['batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'batch_size', isInt=True), width = 90)
    batch_size = NumberPicker(label="Batch Size: ", min=1, max=5, value=point_e_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    guidance = SliderRow(label="Guidance Scale", min=0, max=10, divisions=20, round=1, pref=point_e_prefs, key='guidance_scale')
    #seed = TextField(label="Seed", value=point_e_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    page.point_e_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.point_e_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("👆  Point-E 3D Point Clouds", "Provide a Prompt or Image to render from a CLIP ViT-L/14 diffusion model...", actions=[save_default(point_e_prefs, exclude=['init_image']), IconButton(icon=icons.HELP, tooltip="Help with Point-E Settings", on_click=df_help)]),
        prompt_text,
        init_image,
        base_model,
        Row([batch_folder_name, batch_size]),
        guidance,
        ElevatedButton(content=Text("🐞  Run Point-E", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_point_e(page)),
        page.point_e_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

shap_e_prefs = {
    'prompt_text': ' with solid white background',
    'init_image': '',
    'init_images': [],
    'guidance_scale': 5.0,
    'base_model': 'base40M-textvec', #'base40M', 'base300M' or 'base1B'
    'render_mode': 'NeRF', #STF
    'use_karras': True,
    'karras_steps': 50,
    'size': 256,
    'save_frames': False,
    'use_original': False,
    'batch_size': 1,
    'batch_folder_name': '',
    #'seed': 0,
    #'max_steps': 512,
}

def buildShap_E(page):
    global prefs, shap_e_prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          shap_e_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.shap_e_output.controls = []
      page.shap_e_output.update()
      clear_button.visible = False
      clear_button.update()
    def df_help(e):
      def close_df_dlg(e):
        nonlocal df_help_dlg
        df_help_dlg.open = False
        page.update()
      df_help_dlg = AlertDialog(title=Text("💁   Help with OpenAI Shap-E"), content=Column([
          Text("We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space."),
          Markdown("[GitHub Page](https://github.com/openai/shap-e) - [Read the Paper](https://arxiv.org/pdf/2305.02463.pdf)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🚴  Shaping up... ", on_click=close_df_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(df_help_dlg)
      df_help_dlg.open = True
      page.update()
    def toggle_original(e):
      shap_e_prefs['use_original'] = e.control.value
      original_row.height=None if shap_e_prefs['use_original'] else 0
      original_row.update()
    def run_shap(e):
      if shap_e_prefs['use_original']:
        run_shap_e(page)
      else:
        run_shap_e2(page)
    prompt_text = TextField(label="Prompt Text", value=shap_e_prefs['prompt_text'], filled=True, on_change=lambda e:changed(e,'prompt_text'))
    init_image = FileInput(label="Sample Image (optional, instead of prompt)", pref=shap_e_prefs, key='init_image', page=page)
    #init_image = TextField(label="Sample Image (optional, instead of prompt)", value=shap_e_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))
    #TODO: Add Multi-Image List
    #base_model = Dropdown(label="Base Model", width=250, options=[dropdown.Option("base40M-imagevec"), dropdown.Option("base40M-textvec"), dropdown.Option("base40M"), dropdown.Option("base300M"), dropdown.Option("base1B")], value=shap_e_prefs['base_model'], on_change=lambda e: changed(e, 'base_model'))
    render_mode = Dropdown(label="Render Mode", width=150, options=[dropdown.Option("NeRF"), dropdown.Option("STF")], value=shap_e_prefs['render_mode'], on_change=lambda e: changed(e, 'render_mode'))
    size = SliderRow(label="Size of Render", min=32, max=512, divisions=15, multiple=32, tooltip="Higher values take longer to render.", suffix="px", pref=shap_e_prefs, key='size')
    karras_steps = SliderRow(label="Karras Steps", min=1, max=100, divisions=99, round=0, pref=shap_e_prefs, key='karras_steps')
    batch_folder_name = TextField(label="3D Model Folder Name", value=shap_e_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    #batch_size = TextField(label="Batch Size", value=shap_e_prefs['batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'batch_size', isInt=True), width = 90)
    batch_size = NumberPicker(label="Batch Size: ", min=1, max=5, value=shap_e_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    guidance = SliderRow(label="Guidance Scale", min=0, max=10, divisions=20, round=1, pref=shap_e_prefs, key='guidance_scale')
    use_original = Switcher(label="Use Original Shap-E Method", value=shap_e_prefs['use_original'], tooltip="By default uses latest HuggingFace Diffusers Pipeline.", on_change=toggle_original)
    save_frames = Checkbox(label="Save Preview Frames", tooltip="Saves PNG Sequence of camera rotation, same as animated gif preview.", value=shap_e_prefs['save_frames'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_frames'))
    original_row = Container(animate_size=animation.Animation(700, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height = None if shap_e_prefs['use_original'] else 0, padding=padding.only(top=4), content=Row([render_mode, save_frames]))
    #seed = TextField(label="Seed", value=shap_e_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    page.shap_e_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.shap_e_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🧊  Shap-E 3D Mesh", "Provide a Prompt or Image to Generate Conditional 3D PLY Models...", actions=[save_default(shap_e_prefs, exclude=['init_image', 'init_images']), IconButton(icon=icons.HELP, tooltip="Help with Shap-E Settings", on_click=df_help)]),
        prompt_text,
        init_image,
        Row([use_original, original_row]),
        guidance,
        karras_steps,
        size,
        Row([batch_folder_name, batch_size]),
        ElevatedButton(content=Text("🪀  Run Shap-E", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=run_shap),
        page.shap_e_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

zoe_depth_prefs = {
    'init_image': '',
    'keep_edges': False,
    'colorize': False,
    'pano_360': False,
    'zoe_model': 'ZoeD_N', #ZoeD_K, ZoeD_NK
    'loaded_model': '',
    'max_size': 1024,
    'batch_folder_name': '',
}
def buildZoeDepth(page):
    global zoe_depth_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          zoe_depth_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def zoe_depth_help(e):
      def close_zoe_depth_dlg(e):
        nonlocal zoe_depth_help_dlg
        zoe_depth_help_dlg.open = False
        page.update()
      zoe_depth_help_dlg = AlertDialog(title=Text("🙅   Help with Zoe Depth"), content=Column([
          Text("ZoeDepth is a deep learning model for metric depth estimation from a single image."),
          Text("Give it any of your favorite images and create a 3D glb file from the depth map to import into your 3D Modeling program with texture.... Simple as that, no prompt needed."),
          Markdown("[Paper](https://arxiv.org/abs/2302.12288) | [GitHub](https://github.com/isl-org/ZoeDepth) | [HuggingFace Space](https://huggingface.co/spaces/shariqfarooq/ZoeDepth)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🧊  The depths we go... ", on_click=close_zoe_depth_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(zoe_depth_help_dlg)
      zoe_depth_help_dlg.open = True
      page.update()
    init_image = FileInput(label="Initial Image", pref=zoe_depth_prefs, key='init_image', page=page)
    zoe_model = Dropdown(label="ZoeDepth Model", width=150, options=[dropdown.Option("ZoeD_N"), dropdown.Option("ZoeD_K"), dropdown.Option("ZoeD_NK")], value=zoe_depth_prefs['zoe_model'], on_change=lambda e:changed(e,'zoe_model'))
    keep_edges = Switcher(label="Keep Occlusion Edges", value=zoe_depth_prefs['keep_edges'], on_change=lambda e:changed(e,'keep_edges'))
    pano_360 = Switcher(label="Input 360 Panoramic", value=zoe_depth_prefs['pano_360'], on_change=lambda e:changed(e,'pano_360'))
    colorize = Switcher(label="Show Colorized Depth", value=zoe_depth_prefs['colorize'], on_change=lambda e:changed(e,'colorize'))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, expand=True, multiple=16, suffix="px", pref=zoe_depth_prefs, key='max_size')
    page.zoe_depth_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.zoe_depth_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🥐  ZoeDepth 3D Depth Model from Init Image", "Zero-shot Transfer by Combining Relative and Metric Depth...", actions=[save_default(zoe_depth_prefs, exclude=['init_image', 'loaded_model']), IconButton(icon=icons.HELP, tooltip="Help with Zoe Depth Settings", on_click=zoe_depth_help)]),
        init_image,
        Row([keep_edges, pano_360, colorize]),
        Row([zoe_model, max_row]),
        ElevatedButton(content=Text("✊  Get Zoe Depth", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_zoe_depth(page)),
        page.zoe_depth_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

marigold_depth_prefs = {
    'init_image': '',
    'denoising_steps': 10,
    'ensemble_size': 10,
    'match_input_res': True,
    'color_map': 'Spectral', #
    'processing_res': 768,
    'batch_folder_name': '',
    'use_LCM': False,
    'create_normals': False,
    'seed': 0,
}
def buildMarigoldDepth(page):
    global marigold_depth_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          marigold_depth_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def marigold_depth_help(e):
      def close_marigold_depth_dlg(e):
        nonlocal marigold_depth_help_dlg
        marigold_depth_help_dlg.open = False
        page.update()
      marigold_depth_help_dlg = AlertDialog(title=Text("🙅   Help with Marigold Depth"), content=Column([
          Text("Marigold is a universal monocular depth estimator that delivers accurate and sharp predictions in the wild. Based on Stable Diffusion, it is trained exclusively with synthetic depth data and excels in zero-shot adaptation to real-world imagery. This pipeline is an official implementation of the inference process. This depth estimation pipeline processes a single input image through multiple diffusion denoising stages to estimate depth maps. These maps are subsequently merged to produce the final output."),
          Text("Monocular depth estimation is a fundamental computer vision task. Recovering 3D depth from a single image is geometrically ill-posed and requires scene understanding, so it is not surprising that the rise of deep learning has led to a breakthrough. The impressive progress of monocular depth estimators has mirrored the growth in model capacity, from relatively modest CNNs to large Transformer architectures. Still, monocular depth estimators tend to struggle when presented with images with unfamiliar content and layout, since their knowledge of the visual world is restricted by the data seen during training, and challenged by zero-shot generalization to new domains. This motivates us to explore whether the extensive priors captured in recent generative diffusion models can enable better, more generalizable depth estimation. We introduce Marigold, a method for affine-invariant monocular depth estimation that is derived from Stable Diffusion and retains its rich prior knowledge. The estimator can be fine-tuned in a couple of days on a single GPU using only synthetic training data. It delivers state-of-the-art performance across a wide range of datasets, including over 20% performance gains in specific cases."),
          Markdown("[Project Page](https://marigoldmonodepth.github.io) | [Paper](https://arxiv.org/abs/2312.02145) | [GitHub](https://github.com/prs-eth/marigold) | [HuggingFace Space](https://huggingface.co/spaces/toshas/marigold)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Markdown("Credits go to [Bingxin Ke](http://www.kebingxin.com/), [Anton Obukhov](https://www.obukhov.ai/), [Shengyu Huang](https://shengyuh.github.io/), [Nando Metzger](https://nandometzger.github.io/), [Rodrigo Caye Daudt](https://rcdaudt.github.io/), [Konrad Schindler](https://scholar.google.com/citations?user=FZuNgqIAAAAJ&hl=en), [Tianfu Wang](https://tianfwang.github.io/), [Kevin Qu](https://www.linkedin.com/in/kevin-qu-b3417621b/?locale=en_US), [YiYi Xu](https://yiyixuxu.github.io/) and [Sayak Paul](https://sayak.dev/).", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🧊  The depths we go... ", on_click=close_marigold_depth_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(marigold_depth_help_dlg)
      marigold_depth_help_dlg.open = True
      page.update()
    init_image = FileInput(label="Initial Image", pref=marigold_depth_prefs, key='init_image', page=page)
    color_map = Dropdown(label="Colormap", width=150, options=[dropdown.Option(c) for c in ['Spectral', 'binary', 'PiYG', 'PRGn', 'BrBG', 'PuOr', 'RdGy', 'RdBu', 'RdYlBu', 'RdYlGn', 'coolwarm', 'bwr', 'seismic', 'None']], value=marigold_depth_prefs['color_map'], on_change=lambda e:changed(e,'color_map'))
    match_input_res = Switcher(label="Match Input Resolution", value=marigold_depth_prefs['match_input_res'], on_change=lambda e:changed(e,'match_input_res'), tooltip="Resize depth prediction to match input resolution.")
    denoising_steps = SliderRow(label="Number of Denoising Steps", min=1, max=50, divisions=49, pref=marigold_depth_prefs, key='denoising_steps', tooltip="Number of denoising steps of each inference pass.")
    ensemble_size = SliderRow(label="Ensemble Size", min=1, max=50, divisions=49, pref=marigold_depth_prefs, key='ensemble_size', tooltip="Number of inference passes in the ensemble.")
    #pano_360 = Switcher(label="Input 360 Panoramic", value=marigold_depth_prefs['pano_360'], on_change=lambda e:changed(e,'pano_360'))
    #colorize = Switcher(label="Show Colorized Depth", value=marigold_depth_prefs['colorize'], on_change=lambda e:changed(e,'colorize'))
    use_LCM = Switcher(label="Use LCM Model", value=marigold_depth_prefs['use_LCM'], on_change=lambda e:changed(e,'use_LCM'), tooltip="Runs much faster, at the expense of some quality. Set Denoising Steps to 2-8.")
    create_normals = Switcher(label="Create Normals Map", value=marigold_depth_prefs['create_normals'], on_change=lambda e:changed(e,'create_normals'), tooltip="Instead of Depth Map. The surface normals predictions are unit-length 3D vectors with values in the range from -1 to 1.")
    processing_res = SliderRow(label="Processing Resolution", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=marigold_depth_prefs, key='processing_res')
    seed = TextField(label="Seed", value=marigold_depth_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 90)
    page.marigold_depth_output = Column([])
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🪷   Marigold Depth Estimation", "Monocular depth estimator that delivers accurate & sharp predictions in the wild... Based on SD.", actions=[save_default(marigold_depth_prefs, exclude=['init_image']), IconButton(icon=icons.HELP, tooltip="Help with Marigold Depth Settings", on_click=marigold_depth_help)]),
        init_image,
        processing_res,
        denoising_steps,
        ensemble_size,
        #Row([match_input_res, pano_360, colorize]),
        Row([seed, color_map, match_input_res, use_LCM, create_normals]),
        ElevatedButton(content=Text("🌺  Get Marigold Depth", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_marigold_depth(page)),
        page.marigold_depth_output,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

tripo_prefs = {
    'init_image': '',
    'foreground_ratio': 0.85,
    'remove_background': True,
    'max_size': 512,
    'mesh_resolution': 256,
    'mesh_threshold': 25.0,
    'chunk_size': 8192,
    'batch_size': 1,
    'batch_folder_name': '',
    'title': '',
}

def buildTripo(page):
    global prefs, tripo_prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          tripo_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def tripo_help(e):
      def close_tripo_dlg(e):
        nonlocal tripo_help_dlg
        tripo_help_dlg.open = False
        page.update()
      tripo_help_dlg = AlertDialog(title=Text("💁   Help with Tripo"), content=Column([
          Text("TripoSR is a state-of-the-art open-source model for fast feedforward 3D reconstruction from a single image, developed in collaboration between Tripo AI and Stability AI. We closely follow LRM network architecture for the model design, where TripoSR incorporates a series of technical advancements over the LRM model in terms of both data curation as well as model and training improvements."),
          Markdown("[HuggingFace Space](https://huggingface.co/spaces/stabilityai/TripoSR) | [Model Card](https://huggingface.co/stabilityai/TripoSR) | [GitHub](https://github.com/VAST-AI-Research/TripoSR)| [Tripo3D.ai](https://www.tripo3d.ai/)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Credits go to Tochilkin, Dmitry and Pankratz, David and Liu, Zexiang and Huang, Zixuan and and Letts, Adam and Li, Yangguang and Liang, Ding and Laforte, Christian and Jampani, Varun and Cao, Yan-Pei and Stability.ai"),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("💺  What a Trip...Ohh.", on_click=close_tripo_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(tripo_help_dlg)
      tripo_help_dlg.open = True
      page.update()
    init_image = FileInput(label="Initial Image", pref=tripo_prefs, key='init_image', filled=True, page=page)
    batch_folder_name = TextField(label="3D Model Folder Name", value=tripo_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    title = TextField(label="Project Title", value=tripo_prefs['title'], expand=True, on_change=lambda e:changed(e,'title'))
    #batch_size = NumberPicker(label="Batch Size: ", min=1, max=5, value=tripo_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    remove_background = Switcher(label="Remove Background", value=tripo_prefs['remove_background'], on_change=lambda e:changed(e,'remove_background'), tooltip="You can clear the background yourself cleaner and save transparent png.")
    foreground_ratio = SliderRow(label="Foreground Ratio", min=0, max=1, divisions=20, round=2, expand=True, pref=tripo_prefs, key='foreground_ratio')
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=32, multiple=32, suffix="px", pref=tripo_prefs, key='max_size')
    mesh_resolution = SliderRow(label="Mesh Resolution", min=128, max=1024, divisions=14, multiple=64, suffix="px", pref=tripo_prefs, key='mesh_resolution')
    mesh_threshold = SliderRow(label="Mesh Threshold", min=0, max=50, divisions=100, round=1, pref=tripo_prefs, key='mesh_threshold')
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🏖️   Tripo.ai Image-to-3D", "State-of-the-art open-source model for fast feedforward 3D reconstruction from a single image...", actions=[save_default(tripo_prefs, exclude=['init_image', 'init_images']), IconButton(icon=icons.HELP, tooltip="Help with Tripo Settings", on_click=tripo_help)]),
        init_image,
        Row([remove_background, foreground_ratio]),
        max_row,
        mesh_resolution,
        mesh_threshold,
        Row([batch_folder_name, title]),
        ElevatedButton(content=Text("✈️  Run Tripo", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_tripo(page)),
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

instantmesh_prefs = {
    'init_image': '',
    'foreground_ratio': 0.85,
    'remove_background': True,
    'save_video': False,
    'max_size': 720,
    #'guidance_scale': 5.5,
    'steps': 75,
    'seed': 0,
    'batch_size': 1,
    'batch_folder_name': '',
    'title': '',
}

def buildInstantMesh(page):
    global prefs, instantmesh_prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          instantmesh_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def instantmesh_help(e):
      def close_instantmesh_dlg(e):
        nonlocal instantmesh_help_dlg
        instantmesh_help_dlg.open = False
        page.update()
      instantmesh_help_dlg = AlertDialog(title=Text("💁   Help with InstantMesh"), content=Column([
          Text("We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability. By synergizing the strengths of an off-the-shelf multiview diffusion model and a sparse-view reconstruction model based on the LRM architecture, InstantMesh is able to create diverse 3D assets within 10 seconds. To enhance the training efficiency and exploit more geometric supervisions, e.g, depths and normals, we integrate a differentiable iso-surface extraction module into our framework and directly optimize on the mesh representation. Experimental results on public datasets demonstrate that InstantMesh significantly outperforms other latest image-to-3D baselines, both qualitatively and quantitatively. We release all the code, weights, and demo of InstantMesh, with the intention that it can make substantial contributions to the community of 3D generative AI and empower both researchers and content creators."),
          Markdown("[HuggingFace Space](https://huggingface.co/spaces/TencentARC/InstantMesh) | [Model Card](https://huggingface.co/TencentARC/InstantMesh) | [GitHub](https://github.com/TencentARC/InstantMesh)| [InstantMesh Paper](https://arxiv.org/abs/2404.07191)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Credits go to Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, Ying Shan"),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("💙  Insta-Cool", on_click=close_instantmesh_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(instantmesh_help_dlg)
      instantmesh_help_dlg.open = True
      page.update()
    init_image = FileInput(label="Initial Image (clear background = better results)", pref=instantmesh_prefs, key='init_image', filled=True, page=page)
    batch_folder_name = TextField(label="3D Model Folder Name", value=instantmesh_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    title = TextField(label="Project Title", value=instantmesh_prefs['title'], expand=True, on_change=lambda e:changed(e,'title'))
    #batch_size = NumberPicker(label="Batch Size: ", min=1, max=5, value=instantmesh_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    remove_background = Switcher(label="Remove Background", value=instantmesh_prefs['remove_background'], on_change=lambda e:changed(e,'remove_background'), tooltip="You can clear the background yourself cleaner and save transparent png.")
    foreground_ratio = SliderRow(label="Foreground Ratio", min=0, max=1, divisions=20, round=2, expand=True, pref=instantmesh_prefs, key='foreground_ratio')
    save_video = Switcher(label="Save Video", value=instantmesh_prefs['save_video'], on_change=lambda e:changed(e,'save_video'))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=32, multiple=32, suffix="px", pref=instantmesh_prefs, key='max_size')
    steps = SliderRow(label="Inference Steps", min=5, max=75, divisions=14, pref=instantmesh_prefs, key='steps')
    #guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=instantmesh_prefs, key='guidance_scale')
    seed = TextField(label="Seed", width=90, value=str(instantmesh_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("⚡️  InstantMesh Image-to-3D (under construction)", "Single Image to 3D Textured Mesh with LRM/Instant3D architecture...", actions=[save_default(instantmesh_prefs, exclude=['init_image']), IconButton(icon=icons.HELP, tooltip="Help with InstantMesh Settings", on_click=instantmesh_help)]),
        init_image,
        Row([remove_background, foreground_ratio]),
        steps,
        #guidance,
        max_row,
        Row([seed, batch_folder_name, title, save_video]),
        ElevatedButton(content=Text("🦔  Run InstantMesh 3D", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_instantmesh(page)),
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

splatter_image_prefs = {
    'init_image': '',
    'foreground_ratio': 0.70,
    'remove_background': True,
    'save_video': True,
    'max_size': 720,
    #'guidance_scale': 5.5,
    'steps': 30,
    'seed': 0,
    'batch_size': 1,
    'batch_folder_name': '',
    'title': '',
}

def buildSplatterImage(page):
    global prefs, splatter_image_prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          splatter_image_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def splatter_image_help(e):
      def close_splatter_image_dlg(e):
        nonlocal splatter_image_help_dlg
        splatter_image_help_dlg.open = False
        page.update()
      splatter_image_help_dlg = AlertDialog(title=Text("💁   Help with Splatter Image"), content=Column([
          Text("Splatter Image formulates 3D reconstruction as an image-to-image translation task. It maps the input image to another image, in which every pixel represents one 3D Gaussian and the channels of the output represent parameters of these Gaussians, including their shapes, colours and locations. The resulting image thus represents a set of Gaussians (almost like a point cloud) which reconstruct the shape and colour of the object. The method is very cheap: the reconstruction amounts to a single forward pass of a neural network with only 2D operators (2D convolutions and attention). The rendering is also very fast, due to using Gaussian Splatting. Combined, this results in very cheap training and high-quality results."),
          Text("The Splatter Image is based on Gaussian Splatting, which has recently brought real-time rendering, fast training, and excellent scaling to multi-view reconstruction. For the first time, we apply Gaussian Splatting in a monocular reconstruction setting. Our approach is learning-based, and, at test time, reconstruction only requires the feed-forward evaluation of a neural network. The main innovation of the Splatter Image is its surprisingly straightforward design: it uses a 2D image-to-image network to map the input image to one 3D Gaussian per pixel. The resulting Gaussians thus have the form of an image, the Splatter Image.  We further extend the method to incorporate more than one image as input, which we do by adding cross-attention views. Owning to the speed of the renderer (588 FPS), furthermore, we can easily generate entire images during training, to optimize perceptual metrics like LPIPS. Furthermore, we use a single GPU for training. On standard benchmarks, we demonstrate not only fast reconstruction but also better results than recent and much more expensive baselines in terms of PSNR, LPIPS, and other metrics."),
          Markdown("[Project Page](https://szymanowiczs.github.io/splatter-image) | [HuggingFace Space](https://huggingface.co/spaces/szymanowiczs/splatter_image) | [Model Card](https://huggingface.co/szymanowiczs/splatter-image-v1) | [GitHub](https://github.com/szymanowiczs/splatter-image) | [SplatterImage Paper](https://arxiv.org/abs/2312.13150) | [Video](https://youtu.be/pcKTf9SVh4g)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Credits go to Stanislaw Szymanowicz, Christian Rupprecht, Andrea Vedaldi and Visual Geometry Group - University of Oxford"),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("💙  Splattastic!", on_click=close_splatter_image_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(splatter_image_help_dlg)
      splatter_image_help_dlg.open = True
      page.update()
    init_image = FileInput(label="Initial Image (clear background = better results)", pref=splatter_image_prefs, key='init_image', filled=True, page=page)
    batch_folder_name = TextField(label="3D Model Folder Name", value=splatter_image_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    title = TextField(label="Project Title", value=splatter_image_prefs['title'], expand=True, on_change=lambda e:changed(e,'title'))
    #batch_size = NumberPicker(label="Batch Size: ", min=1, max=5, value=splatter_image_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    remove_background = Switcher(label="Remove Background", value=splatter_image_prefs['remove_background'], on_change=lambda e:changed(e,'remove_background'), tooltip="You can clear the background yourself cleaner and save transparent png.")
    foreground_ratio = SliderRow(label="Foreground Ratio", min=0, max=1, divisions=20, round=2, expand=True, pref=splatter_image_prefs, key='foreground_ratio')
    #save_video = Switcher(label="Save Video", value=splatter_image_prefs['save_video'], on_change=lambda e:changed(e,'save_video'))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=32, multiple=32, suffix="px", pref=splatter_image_prefs, key='max_size')
    #steps = SliderRow(label="Inference Steps", min=0, max=100, divisions=100, pref=splatter_image_prefs, key='steps')
    #guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=splatter_image_prefs, key='guidance_scale')
    #seed = TextField(label="Seed", width=90, value=str(splatter_image_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🗿  Splatter Image 3D (under construction)", "Ultra-Fast Single-View 3D Reconstruction...", actions=[save_default(splatter_image_prefs, exclude=['init_image']), IconButton(icon=icons.HELP, tooltip="Help with SplatterImage Settings", on_click=splatter_image_help)]),
        init_image,
        Row([remove_background, foreground_ratio]),
        #steps,
        #guidance,
        max_row,
        Row([batch_folder_name, title]),
        ElevatedButton(content=Text("⌛  Run SplatterImage 3D", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_splatter_image(page)),
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

crm_prefs = {
    'init_image': '',
    'foreground_ratio': 0.85,
    'remove_background': True,
    'background_color': '#7F7F7F',
    'max_size': 720,
    'guidance_scale': 5.5,
    'steps': 30,
    'seed': 0,
    'batch_size': 1,
    'batch_folder_name': '',
    'title': '',
}

def buildCRM(page):
    global prefs, crm_prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          crm_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def crm_help(e):
      def close_crm_dlg(e):
        nonlocal crm_help_dlg
        crm_help_dlg.open = False
        page.update()
      crm_help_dlg = AlertDialog(title=Text("💁   Help with CRM"), content=Column([
          Text("Feed-forward 3D generative models like the Large Reconstruction Model (LRM) have demonstrated exceptional generation speed. However, the transformer-based methods do not leverage the geometric priors of the triplane component in their architecture, often leading to sub-optimal quality given the limited size of 3D data and slow training. In this work, we present the Convolutional Reconstruction Model (CRM), a high-fidelity feed-forward single image-to-3D generative model. Recognizing the limitations posed by sparse 3D data, we highlight the necessity of integrating geometric priors into network design. CRM builds on the key observation that the visualization of triplane exhibits spatial correspondence of six orthographic images. First, it generates six orthographic view images from a single input image, then feeds these images into a convolutional U-Net, leveraging its strong pixel-level alignment capabilities and significant bandwidth to create a high-resolution triplane. CRM further employs Flexicubes as geometric representation, facilitating direct end-to-end optimization on textured meshes. Overall, our model delivers a high-fidelity textured mesh from an image in just 10 seconds, without any test-time optimization."),
          Markdown("[HuggingFace Space](https://huggingface.co/spaces/Zhengyi/CRM) | [Model Card](https://huggingface.co/Zhengyi/CRM) | [GitHub](https://github.com/thu-ml/CRM)| [CRM Project Page](https://ml.cs.tsinghua.edu.cn/~zhengyi/CRM/)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Credits go to Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, Jun Zhu, Tsinghua University, Renmin University of China, ShengShu"),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("👷  Cool Re-Model", on_click=close_crm_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(crm_help_dlg)
      crm_help_dlg.open = True
      page.update()
    init_image = FileInput(label="Initial Image (clear background = better results)", pref=crm_prefs, key='init_image', filled=True, page=page)
    batch_folder_name = TextField(label="3D Model Folder Name", value=crm_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    title = TextField(label="Project Title", value=crm_prefs['title'], expand=True, on_change=lambda e:changed(e,'title'))
    #batch_size = NumberPicker(label="Batch Size: ", min=1, max=5, value=crm_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    remove_background = Switcher(label="Remove Background", value=crm_prefs['remove_background'], on_change=lambda e:changed(e,'remove_background'), tooltip="You can clear the background yourself cleaner and save transparent png.")
    foreground_ratio = SliderRow(label="Foreground Ratio", min=0, max=1, divisions=20, round=2, expand=True, pref=crm_prefs, key='foreground_ratio')
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=32, multiple=32, suffix="px", pref=crm_prefs, key='max_size')
    steps = SliderRow(label="Inference Steps", min=0, max=100, divisions=100, pref=crm_prefs, key='steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=crm_prefs, key='guidance_scale')
    seed = TextField(label="Seed", width=90, value=str(crm_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🐈‍⬛  CRM Image-to-3D (under construction)", "Single Image to 3D Textured Mesh with Convolutional Reconstruction Model...", actions=[save_default(crm_prefs, exclude=['init_image']), IconButton(icon=icons.HELP, tooltip="Help with CRM Settings", on_click=crm_help)]),
        init_image,
        Row([remove_background, foreground_ratio]),
        steps,
        guidance,
        max_row,
        Row([seed, batch_folder_name, title]),
        ElevatedButton(content=Text("🚨  Run CRM 3D", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_crm(page)),
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

instant_ngp_prefs = {
    'train_steps': 2000, #Total number of training steps to perform.  If provided, overrides num_train_epochs.
    'sharpen': 0.0,
    'exposure': 0.0,
    'vr_mode': False,
    'name_of_your_model': '',
    'save_model': True,
    'where_to_save_model': 'Public HuggingFace',
    'resolution': 768,
    'image_path': '',
    'readme_description': '',
    'urls': [],
}

def buildInstantNGP(page):
    global prefs, instant_ngp_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            instant_ngp_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_instant_ngp_output(o):
        page.instant_ngp_output.controls.append(o)
        page.instant_ngp_output.update()
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.instant_ngp_output.controls = []
        page.instant_ngp_output.update()
        clear_button.visible = False
        clear_button.update()
    def instant_ngp_help(e):
        def close_instant_ngp_dlg(e):
          nonlocal instant_ngp_help_dlg
          instant_ngp_help_dlg.open = False
          page.update()
        instant_ngp_help_dlg = AlertDialog(title=Text("💁   Help with Instant-NGP"), content=Column([
            Text("Ever wanted to train a NeRF model of a fox in under 5 seconds? Or fly around a scene captured from photos of a factory robot? Of course you have! Here you will find an implementation of four neural graphics primitives, being neural radiance fields (NeRF), signed distance functions (SDFs), neural images, and neural volumes. In each case, we train and render a MLP with multiresolution hash input encoding using the tiny-cuda-nn framework."),
            Text("We demonstrate near-instant training of neural graphics primitives on a single GPU for multiple tasks. In gigapixel image we represent an image by a neural network. SDF learns a signed distance function in 3D space whose zero level-set represents a 2D surface. NeRF [Mildenhall et al. 2020] uses 2D images and their camera poses to reconstruct a volumetric radiance-and-density field that is visualized using ray marching. Lastly, neural volume learns a denoised radiance and density field directly from a volumetric path tracer. In all tasks, our encoding and its efficient implementation provide clear benefits: instant training, high quality, and simplicity. Our encoding is task-agnostic: we use the same implementation and hyperparameters across all tasks and only vary the hash table size which trades off quality and performance."),
            Text("Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations. A small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920x1080."),
            Markdown("[Project page](https://nvlabs.github.io/instant-ngp) / [Paper](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf) / [Video](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.mp4) / [Presentation](https://tom94.net/data/publications/mueller22instant/mueller22instant-gtc.mp4) / [Real-Time Live](https://tom94.net/data/publications/mueller22instant/mueller22instant-rtl.mp4) / [BibTeX](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.bib)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🌠  Like Magic... ", on_click=close_instant_ngp_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(instant_ngp_help_dlg)
        instant_ngp_help_dlg.open = True
        page.update()
    def delete_image(e):
        f = e.control.data
        if os.path.isfile(f):
          os.remove(f)
          for i, fl in enumerate(page.instant_ngp_file_list.controls):
            if fl.title.value == f:
              del page.instant_ngp_file_list.controls[i]
              page.instant_ngp_file_list.update()
              continue
    def delete_all_images(e):
        for fl in page.instant_ngp_file_list.controls:
          f = fl.title.value
          if os.path.isfile(f):
            os.remove(f)
        page.instant_ngp_file_list.controls.clear()
        page.instant_ngp_file_list.update()
    def image_details(e):
        img = e.control.data
        alert_msg(e.page, "Image Details", content=Image(src=img), sound=False)
    def add_file(fpath, update=True):
        page.instant_ngp_file_list.controls.append(ListTile(title=Text(fpath), dense=False, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.INFO, text="Image Details", on_click=image_details, data=fpath),
              PopupMenuItem(icon=icons.DELETE, text="Delete Image", on_click=delete_image, data=fpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All", on_click=delete_all_images, data=fpath),
          ]), data=fpath, on_click=image_details))
        if update: page.instant_ngp_file_list.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    save_dir = os.path.join(root_dir, 'my_ngp')
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
          save_file(e.file_name)
    def save_file(file_name):
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
          fpath = os.path.join(save_dir, file_name)
        else:
          fname = file_name
          fpath = os.path.join(save_dir, file_name.rpartition(slash)[2])
        original_img = PILImage.open(fname)
        width, height = original_img.size
        width, height = scale_dimensions(width, height, instant_ngp_prefs['resolution'])
        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
        original_img.save(fpath)
        if page.web:
          os.remove(fname)
        #shutil.move(fname, fpath)
        add_file(fpath)
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["png", "PNG", "jpg", "jpeg", 'obj', 'stl', 'nvdb'], dialog_title="Pick Image File to Enlarge")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def add_image(e):
        save_dir = os.path.join(root_dir, 'my_ngp')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        if image_path.value.startswith('http'):
          import requests
          from io import BytesIO
          response = requests.get(image_path.value)
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          model_image = PILImage.open(BytesIO(response.content)).convert("RGB")
          width, height = model_image.size
          width, height = scale_dimensions(width, height, instant_ngp_prefs['resolution'])
          model_image = model_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          model_image.save(fpath)
          add_file(fpath)
        elif os.path.isfile(image_path.value):
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(image_path.value)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, instant_ngp_prefs['resolution'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          #shutil.copy(image_path.value, fpath)
          add_file(fpath)
        elif os.path.isdir(image_path.value):
          for f in os.listdir(image_path.value):
            file_path = os.path.join(image_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg', 'obj', 'stl', 'nvdb')):
              fpath = os.path.join(save_dir, f)
              original_img = PILImage.open(file_path)
              width, height = original_img.size
              width, height = scale_dimensions(width, height, instant_ngp_prefs['resolution'])
              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              original_img.save(fpath)
              #shutil.copy(file_path, fpath)
              add_file(fpath)
        else:
          if bool(image_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        image_path.value = ""
        image_path.update()
    def load_images():
        if os.path.exists(save_dir):
          for f in os.listdir(save_dir):
            existing = os.path.join(save_dir, f)
            if os.path.isdir(existing): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg', 'obj', 'stl', 'nvdb')):
              add_file(existing, update=False)
    #instance_prompt = Container(content=Tooltip(message="The prompt with identifier specifying the instance", content=TextField(label="Instance Prompt Token Text", value=instant_ngp_prefs['instance_prompt'], on_change=lambda e:changed(e,'instance_prompt'))), col={'md':9})
    name_of_your_model = TextField(label="Name of your Model", value=instant_ngp_prefs['name_of_your_model'], on_change=lambda e:changed(e,'name_of_your_model'))
    train_steps = Tooltip(message="Total number of training steps to perform.  More the better, even around 35000..", content=TextField(label="Max Training Steps", value=instant_ngp_prefs['train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'train_steps', ptype='int'), width = 160))
    #save_model = Checkbox(label="Save Model to HuggingFace   ", tooltip="", value=instant_ngp_prefs['save_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_model'))
    #save_model = Tooltip(message="Requires WRITE access on API Key to Upload Checkpoint", content=Switcher(label="Save Model to HuggingFace    ", value=instant_ngp_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))
    #where_to_save_model = Dropdown(label="Where to Save Model", width=250, options=[dropdown.Option("Public HuggingFace"), dropdown.Option("Private HuggingFace")], value=instant_ngp_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))
    #class_data_dir = TextField(label="Prior Preservation Class Folder", value=instant_ngp_prefs['class_data_dir'], on_change=lambda e:changed(e,'class_data_dir'))
    #readme_description = TextField(label="Extra README Description", value=instant_ngp_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=12, multiple=64, suffix="px", pref=instant_ngp_prefs, key='resolution')

    sharpen = Row([Text(" Shapen Images:"), Slider(label="{value}", min=0, max=1, divisions=10, round=1, expand=True, value=instant_ngp_prefs['sharpen'], on_change=lambda e: changed(e, 'sharpen'))], col={'lg':6})
    exposure = Row([Text(" Image Exposure:"), Slider(label="{value}", min=0, max=1, divisions=10, round=1, expand=True, value=instant_ngp_prefs['exposure'], on_change=lambda e: changed(e, 'exposure'))], col={'lg':6})
    vr_mode = Checkbox(label="Output VR Mode", tooltip="Render to a VR headset", value=instant_ngp_prefs['vr_mode'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'vr_mode'))
    image_path = TextField(label="Image Files or Folder Path or URL to Train", value=instant_ngp_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    add_image_button = ElevatedButton(content=Text("Add File or Folder"), on_click=add_image)
    page.instant_ngp_file_list = Column([], tight=True, spacing=0)
    load_images()
    #where_to_save_model.visible = instant_ngp_prefs['save_model']
    #readme_description.visible = instant_ngp_prefs['save_model']
    #lambda_entropy = TextField(label="Lambda Entropy", value=dreamfusinstant_ngp_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    #max_steps = TextField(label="Max Steps", value=instant_ngp_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    page.instant_ngp_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.instant_ngp_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎑  Instant Neural Graphics Primitives by NVidia", "Convert series of images into 3D Models with Multiresolution Hash Encoding...", actions=[save_default(instant_ngp_prefs, exclude=['image_path']), IconButton(icon=icons.HELP, tooltip="Help with Instant NGP Settings", on_click=instant_ngp_help)]),
        Row([name_of_your_model]),
        Row([train_steps, vr_mode]),
        ResponsiveRow([sharpen, exposure]),
        #Row([save_model, where_to_save_model]),
        #readme_description,
        #Row([class_data_dir]),
        max_row,
        Text("The scene to load. Can be full path to the training data, multiple image angles, NeRF dataset, a *.obj/*.stl mesh for training a SDF, images, or a *.nvdb volume."),
        Row([image_path, add_image_button]),
        page.instant_ngp_file_list,
        Row([ElevatedButton(content=Text("🎇  Run Instant-NGP", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_instant_ngp(page))]),
        page.instant_ngp_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

meshy_prefs = {
    'meshy_mode': 'text-to-3d',
    'init_image': '',
    'init_model': '',
    'title': '',
    'prompt': '',
    'negative_prompt': 'low quality, low resolution, low poly, ugly',
    'style_prompt': '',
    'art_style_texture': 'Realistic',
    'art_style_3d': 'Realistic',
    'texture_richness': 'High',
    'resolution': '2048',
    'seed': '0',
    'meshy_api_key': '',
    'batch_folder_name': '',
}
def buildMeshy(page):
    global meshy_prefs, prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          meshy_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def changed_pref(e, pref=None):
      if pref is not None:
        prefs[pref] = e.control.value
        status['changed_prefs'] = True
    def meshy_help(e):
      def close_meshy_dlg(e):
        nonlocal meshy_help_dlg
        meshy_help_dlg.open = False
        page.update()
      meshy_help_dlg = AlertDialog(title=Text("🙅   Help with Meshy 3D"), content=Column([
          Text("Meshy is your 3D generative AI toolbox for effortlessly creating 3D assets from text or images, accelerating your 3D workflow. With Meshy, you can create high-quality textures and 3D models in minutes. Meshy is powered by the latest advances in AI and machine learning, and is built for designers, artists, and developers. Whether you're a 3D artist, a game developer, or a creative coder, Meshy can help you create 3D assets faster than ever before."),
          Markdown("[Meshy.ai](https://meshy.ai) | [Meshy API Docs](https://docs.meshy.ai) | [API Key](https://app.meshy.ai/settings/api) | [Cost](https://docs.meshy.ai/api-introduction#pricing) | [Pricing](https://www.meshy.ai/pricing)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🌚  It's Getting Meshy", on_click=close_meshy_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(meshy_help_dlg)
      meshy_help_dlg.open = True
      page.update()
    def change_mode(e):
        mode = e.data.split('"')[1]
        meshy_prefs['meshy_mode'] = mode
        init_image.show = mode == "image-to-3d"
        init_image.update()
        init_model.show = mode == "text-to-texture"
        init_model.update()
        style_prompt.visible = mode == "text-to-texture"
        style_prompt.update()
        art_style_texture.visible = mode == "text-to-texture"
        art_style_texture.update()
        art_style_3d.visible = mode == "text-to-3d"
        art_style_3d.update()
        seed.visible = mode == "text-to-3d"
        seed.update()
        resolution.visible = mode == "text-to-texture"
        resolution.update()
        prompt_container.visible = mode != "image-to-3d"
        prompt_container.update()
        texture_richness.visible = mode == "text-to-3d"
        texture_richness.update()
        credits = 10 if mode == "text-to-texture" else 3 if mode == "text-to-3d" else 20
        run_button.content.value = f"🚮  Run Meshy {mode.title()} ({credits} Credits)"
        run_button.update()
    selected_mode = ft.SegmentedButton(on_change=change_mode, selected={meshy_prefs['meshy_mode']}, allow_multiple_selection=False,
        segments=[
            ft.Segment(value="text-to-texture", label=ft.Text("Text-to-Texture"), icon=ft.Icon(ft.icons.TEXTURE)),
            ft.Segment(value="text-to-3d", label=ft.Text("Text-to-3D"), icon=ft.Icon(ft.icons.FORMAT_COLOR_TEXT)),
            ft.Segment(value="image-to-3d", label=ft.Text("Image-to-3D"), icon=ft.Icon(ft.icons.IMAGE)),
        ],
    )
    prompt = TextField(label="Object Prompt Text", value=meshy_prefs['prompt'], filled=True, col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=meshy_prefs['negative_prompt'], filled=True, col={'md':4}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    prompt_container = Container(content=ResponsiveRow([prompt, negative_prompt]), visible=meshy_prefs['meshy_mode']!="image-to-3d")
    init_image = FileInput(label="Initial Image", pref=meshy_prefs, key='init_image', visible=meshy_prefs['meshy_mode']=="image-to-3d", page=page)
    init_model = FileInput(label="Initial 3D Model Mesh", pref=meshy_prefs, key='init_model', ftype="model", visible=meshy_prefs['meshy_mode']=="text-to-texture", page=page)
    style_prompt = TextField(label="Style Prompt Text", value=meshy_prefs['style_prompt'], multiline=True, visible=meshy_prefs['meshy_mode']=="text-to-texture", on_change=lambda e:changed(e,'style_prompt'))
    art_style_texture = Dropdown(label="Art Style", width=210, options=[dropdown.Option(s) for s in ['Realistic', 'Voxel', 'Fake-3D-Cartoon', 'Japanese-Anime', 'Cartoon-Line-Art', 'Realistic-Hand-Drawn', 'Fake-3D-Hand-Drawn', 'Oriental-Comic-Ink']], value=meshy_prefs['art_style_texture'], visible=meshy_prefs['meshy_mode']=="text-to-texture", on_change=lambda e:changed(e,'art_style_texture'))
    art_style_3d = Dropdown(label="Art Style", width=140, options=[dropdown.Option(s) for s in ['Realistic', 'Cartoon', 'Low-Poly']], value=meshy_prefs['art_style_3d'], visible=meshy_prefs['meshy_mode']=="text-to-3d", on_change=lambda e:changed(e,'art_style_3d'))
    texture_richness = Dropdown(label="Texture Richness", width=140, options=[dropdown.Option(s) for s in ['High', 'Medium', 'Low', 'None']], value=meshy_prefs['texture_richness'], visible=meshy_prefs['meshy_mode']=="text-to-3d", on_change=lambda e:changed(e,'texture_richness'), tooltip="When Refining: high for realistic, medium for cartoon, none for low-poly")
    resolution = Dropdown(label="Resolution", width=100, options=[dropdown.Option(s) for s in ['1024', '2048', '4096']], value=meshy_prefs['resolution'], visible=meshy_prefs['meshy_mode']=="text-to-texture", on_change=lambda e:changed(e,'resolution'))
    title = TextField(label="Project Title", value=meshy_prefs['title'], expand=True, on_change=lambda e:changed(e,'title'))
    meshy_api_key = TextField(label="Meshy.ai API Key", value=prefs['meshy_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed_pref(e,'meshy_api_key'))
    api_instructions = Markdown("Sign-up and create your Meshy.ai API Key here [https://app.meshy.ai/settings/api](https://app.meshy.ai/settings/api) to get 200 credits a month free...", on_tap_link=lambda e: e.page.launch_url(e.data))
    seed = TextField(label="Seed", width=90, value=str(meshy_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", visible=meshy_prefs['meshy_mode']=="text-to-3d", on_change=lambda e:changed(e,'seed', ptype='int'))
    batch_folder_name = TextField(label="Batch Folder Name", value=meshy_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    run_button = ElevatedButton(content=Text("🚮  Run Meshy Text-To-3D (3 Credits)", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_meshy(page))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🍄  Meshy.ai 3D Generation API", "Uses credits from their servers to create quality mesh models. Can take 3-15 minutes per, but gives great results...", actions=[save_default(meshy_prefs, exclude=['init_image', 'init_model']), IconButton(icon=icons.HELP, tooltip="Help with Meshy API Settings", on_click=meshy_help)]),
        Row([Text("Meshy Mode:", weight=FontWeight.BOLD), selected_mode]),
        init_model,
        prompt_container,
        style_prompt,
        init_image,
        Row([art_style_texture, art_style_3d, texture_richness, resolution, seed]),
        Divider(thickness=2, height=4),
        api_instructions,
        meshy_api_key,
        Row([batch_folder_name, title]),
        run_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

luma_vid_to_3d_prefs = {
    'init_video': '',
    'title': '',
    'luma_api_key': '',
    'camera_type': 'Normal Perspective',
    'batch_folder_name': '',
}
def buildLuma(page):
    global luma_vid_to_3d_prefs, prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          luma_vid_to_3d_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def changed_pref(e, pref=None):
      if pref is not None:
        prefs[pref] = e.control.value
        status['changed_prefs'] = True
    def luma_vid_to_3d_help(e):
      def close_luma_vid_to_3d_dlg(e):
        nonlocal luma_vid_to_3d_help_dlg
        luma_vid_to_3d_help_dlg.open = False
        page.update()
      luma_vid_to_3d_help_dlg = AlertDialog(title=Text("🙅   Help with LumaLabs API"), content=Column([
          Text("Luma's NeRF and meshing models are available through their API, giving you access to the world's best 3D modeling and reconstruction capabilities. At a dollar a scene or object. Today it costs anywhere from $60-$1500 and 2-10wk, and rounds of back and forth to have 3D models created. At a dollar a model, and around 30 min of compute now we can imagine 3D models for entire inventories for e-commerce, and every previz scene for VFX. The API expects video walkthroughs of objects or scenes, looking outside in, from 2-3 levels. The output is an interactive 3D scene that can be embedded directly, coarse textured models to build interactions on in traditional 3D pipelines, and pre-rendered 360 images and videos."),
          Markdown("[Luma-API](https://lumalabs.ai/luma-api) | [Capture Practices](https://docs.lumalabs.ai/MCrGAEukR4orR9) | [Client Docs](https://documenter.getpostman.com/view/24305418/2s93CRMCas)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🌚  Who needs 3D scanner? ", on_click=close_luma_vid_to_3d_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(luma_vid_to_3d_help_dlg)
      luma_vid_to_3d_help_dlg.open = True
      page.update()
    init_video = FileInput(label="Video Walk-thru or Walk-around", pref=luma_vid_to_3d_prefs, key='init_video', ftype="video", page=page)
    camera_type = Dropdown(label="Camera Type", width=200, options=[dropdown.Option("Normal Perspective"), dropdown.Option("Fisheye Lens"), dropdown.Option("Equirectangular 360")], value=luma_vid_to_3d_prefs['camera_type'], on_change=lambda e:changed(e,'camera_type'))
    title = TextField(label="Project Title", value=luma_vid_to_3d_prefs['title'], on_change=lambda e:changed(e,'title'))
    luma_api_key = TextField(label="LumaLabs.ai API Key", value=prefs['luma_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed_pref(e,'luma_api_key'))
    api_instructions = Markdown("Sign-up and get your LumaLabs.ai API Key here [https://lumalabs.ai/dashboard/api](https://lumalabs.ai/dashboard/api), you should get 10 models free.", on_tap_link=lambda e: e.page.launch_url(e.data))
    batch_folder_name = TextField(label="Batch Folder Name", value=luma_vid_to_3d_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    page.luma_vid_to_3d_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.luma_vid_to_3d_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🌔  LumaLabs Video-to-3D API", "Costs $1 per Model, takes ~30min, but well worth it for these NeRF and meshing models in their cloud...", actions=[save_default(luma_vid_to_3d_prefs, exclude=['init_video']), IconButton(icon=icons.HELP, tooltip="Help with LumaLabs API Settings", on_click=luma_vid_to_3d_help)]),
        init_video,
        title,
        Divider(thickness=2, height=4),
        api_instructions,
        luma_api_key,
        Row([camera_type, batch_folder_name]),
        ElevatedButton(content=Text("🌜  Get Luma Vid-to-3D", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_luma_vid_to_3d(page)),
        page.luma_vid_to_3d_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c


repaint_prefs = {
    'original_image': '',
    'mask_image': '',
    'num_inference_steps': 500,
    'eta': 0.0,
    'jump_length': 10,
    'jump_n_sample': 10,
    'seed': 0,
    'file_name': '',
    'max_size': 1024,
    'invert_mask': False,
}
def buildRepainter(page):
    global repaint_prefs, prefs, pipe_repaint
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          repaint_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_repaint_output(o):
      page.repaint_output.controls.append(o)
      page.repaint_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.repaint_output.controls = []
      page.repaint_output.update()
      clear_button.visible = False
      clear_button.update()
    def repaint_help(e):
      def close_repaint_dlg(e):
        nonlocal repaint_help_dlg
        repaint_help_dlg.open = False
        page.update()
      repaint_help_dlg = AlertDialog(title=Text("💁   Help with Repainter"), content=Column([
          Text("It's difficult to explain exactly what all these parameters do, but keep it close to defaults, keep prompt simple, or experiment to see what's what, we don't know."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😪  Okay then... ", on_click=close_repaint_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(repaint_help_dlg)
      repaint_help_dlg.open = True
      page.update()
    def change_eta(e):
        changed(e, 'eta', ptype="float")
        eta_value.value = f" {repaint_prefs['eta']}"
        eta_value.update()
        eta_row.update()
    original_image = FileInput(label="Original Image", pref=repaint_prefs, key='original_image', expand=1, page=page)
    mask_image = FileInput(label="Mask Image", pref=repaint_prefs, key='mask_image', expand=1, page=page)
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=repaint_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
    jump_length = TextField(label="Jump Length", width=130, tooltip="The number of steps taken forward in time before going backward in time for a single jump", value=repaint_prefs['jump_length'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'jump_length', ptype='int'))
    jump_n_sample = TextField(label="Jump # of Sample", width=130, tooltip="The number of times we will make forward time jump for a given chosen time sample.", value=repaint_prefs['jump_n_sample'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'jump_n_sample', ptype='int'))
    seed = TextField(label="Seed", width=90, value=str(repaint_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    #num_inference_steps = TextField(label="Inference Steps", value=str(repaint_prefs['num_inference_steps']), keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_inference_steps', ptype='int'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=10, max=3000, divisions=2990, pref=repaint_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    #eta = TextField(label="ETA", value=str(repaint_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", round=2, value=float(repaint_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=change_eta)
    eta_value = Text(f" {repaint_prefs['eta']}", weight=FontWeight.BOLD)
    eta_row = Row([Text("ETA:"), eta_value, Text("  DDIM"), eta, Text("DDPM")])
    page.etas.append(eta_row)
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=repaint_prefs, key='max_size')
    page.repaint_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.repaint_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("💅  Repaint masked areas of an image", "Fills in areas of picture with what it thinks it should be, without a prompt...", actions=[save_default(repaint_prefs, exclude=['original_image', 'mask_image']), IconButton(icon=icons.HELP, tooltip="Help with Repainter Settings", on_click=repaint_help)]),
        Row([original_image, mask_image, invert_mask]),
        num_inference_row,
        eta_row,
        max_row,
        Row([jump_length, jump_n_sample, seed]),
        ElevatedButton(content=Text("🖌️  Run Repainter", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_repainter(page)),
        page.repaint_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

image_variation_prefs = {
    'init_image': '',
    'guidance_scale': 7.5,
    'num_inference_steps': 50,
    'eta': 0.4,
    'seed': 0,
    'num_images': 1,
    'file_name': '',
    'max_size': 1024,
    'width': 960,
    'height': 512,
}
def buildImageVariation(page):
    global image_variation_prefs, prefs, pipe_image_variation
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          image_variation_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_image_variation_output(o):
      page.image_variation_output.controls.append(o)
      page.image_variation_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_image_variation_output = add_to_image_variation_output
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.image_variation_output.controls = []
      page.image_variation_output.update()
      clear_button.visible = False
      clear_button.update()
    def image_variation_help(e):
      def close_image_variation_dlg(e):
        nonlocal image_variation_help_dlg
        image_variation_help_dlg.open = False
        page.update()
      image_variation_help_dlg = AlertDialog(title=Text("🙅   Help with Image Variations"), content=Column([
          Text("Give it any of your favorite images and create variations of it.... Simple as that, no prompt needed."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🤗  Sounds Fun... ", on_click=close_image_variation_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(image_variation_help_dlg)
      image_variation_help_dlg.open = True
      page.update()
    #init_image = TextField(label="Initial Image", value=image_variation_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    init_image = FileInput(label="Initial Image", pref=image_variation_prefs, key='init_image', page=page)
    seed = TextField(label="Seed", width=90, value=str(image_variation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=image_variation_prefs, key='guidance_scale')
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=image_variation_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    #eta = TextField(label="ETA", value=str(image_variation_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", round=2, value=float(image_variation_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))
    eta_row = Row([Text("DDIM ETA: "), eta])
    page.etas.append(eta_row)
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=image_variation_prefs, key='max_size')
    page.image_variation_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.image_variation_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🪩   Image Variations of any Init Image", "Creates a new version of your picture, without a prompt...", actions=[save_default(image_variation_prefs, exclude=['init_image']), IconButton(icon=icons.HELP, tooltip="Help with Image Variation Settings", on_click=image_variation_help)]),
        init_image,
        #Row([init_image, mask_image, invert_mask]),
        num_inference_row,
        guidance,
        eta_row,
        max_row,
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=image_variation_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed]),
        ElevatedButton(content=Text("🖍️  Get Image Variation", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_image_variation(page)),
        page.image_variation_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

background_remover_prefs = {
    'init_image': '',
    'threshold': 100,
    'file_name': '',
    'max_size': 1024,
    'transparent_png': False,
    'save_mask': False,
    'output_name': '',
    'batch_folder_name': '',
}
def buildBackgroundRemover(page):
    global background_remover_prefs, prefs, pipe_background_remover
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          background_remover_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_background_remover_output(o):
      page.background_remover_output.controls.append(o)
      page.background_remover_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_background_remover_output = add_to_background_remover_output
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.background_remover_output.controls = []
      page.background_remover_output.update()
      clear_button.visible = False
      clear_button.update()
    def background_remover_help(e):
      def close_background_remover_dlg(e):
        nonlocal background_remover_help_dlg
        background_remover_help_dlg.open = False
        page.update()
      background_remover_help_dlg = AlertDialog(title=Text("🙅   Help with Background Remover"), content=Column([
          Text("Give it any of your favorite images and it finds the main subject within the threshold value and gives you a cleaned up version back.... Simple as that, very useful to reuse as init image in another pipeline without needing to edit in Photoshop first.."),
          Markdown("[MODNet GitHub](https://github.com/Mazhar004/MODNet-BGRemover) | [HuggingFace Space](https://huggingface.co/spaces/nateraw/background-remover)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😸  Quite Convenient... ", on_click=close_background_remover_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(background_remover_help_dlg)
      background_remover_help_dlg.open = True
      page.update()
    init_image = FileInput(label="Initial Image", pref=background_remover_prefs, key='init_image', page=page)
    threshold = SliderRow(label="Mask Cutoff Threshold", min=0, max=250, divisions=250, round=0, pref=background_remover_prefs, key='threshold')
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=background_remover_prefs, key='max_size')
    save_mask = Switcher(label="Save B&W Mask", value=background_remover_prefs['save_mask'], tooltip="Gives you a Mask File you can reuse for Inpainting.", on_change=lambda e:changed(e,'save_mask'))
    output_name = TextField(label="Output File Name", value=background_remover_prefs['output_name'], on_change=lambda e:changed(e,'output_name'))
    batch_folder_name = TextField(label="Batch Folder Name", value=background_remover_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    page.background_remover_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.background_remover_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🖼   MODNet Background Remover", "A deep learning approach to clear the background of most images to isolate subject...", actions=[save_default(background_remover_prefs, exclude=['init_image']), IconButton(icon=icons.HELP, tooltip="Help with Background Remover Settings", on_click=background_remover_help)]),
        init_image,
        threshold,
        max_row,
        save_mask,
        Row([output_name, batch_folder_name]),
        ElevatedButton(content=Text("🏘  Get Background Remover", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_background_remover(page)),
        page.background_remover_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

horde_worker_regen_prefs = {
    'dreamer_name': '',
    'queue_size': 1,
    'max_threads': 1,
    'max_batch': 1,
    'safety_on_gpu': False,
    'require_upfront_kudos': False,
    'max_resolution': '768x768', #max_power 8,18,32,50
    'blacklist': '',
    'nsfw': True,
    'censor_nsfw': False,
    'censorlist': '',
    'allow_img2img': True,
    'allow_painting': True,
    'allow_post_processing': True,
    'allow_controlnet': False,
    'allow_lora': False,
    'stats_output_frequency': 10,
    'models_to_load': ['TOP 2'],
    'models_to_skip': ['pix2pix', 'SDXL_beta', 'A to Zovya RPG'],
    'scribe_name': '',
    'max_length': 80,
    'max_context_length': 1024,
    'alchemist_name': '',
    'alchemist_forms': ['caption', 'nsfw', 'interrogation', 'post-process'],
    'log_levels': ['SUCCESS', 'ERROR', 'INFO', 'DEBUG'],
    'log_lines': 35,
}

def buildHordeWorker(page):
    global prefs, horde_worker_regen_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            horde_worker_regen_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def horde_help(e):
        alert_msg(page, "💁   Help with reGen Horde Worker", [
            "Set up a AI Horde Worker to generate, post-process or analyze images for others. This will turn your graphics card(s) into a worker for the AI Horde where you will create images for others. You you will receive in turn earn 'kudos' which will give you priority for your own generations.",
            Markdown("[AI-Horde Page](https://aihorde.net/) | [GitHub Repo](https://github.com/Haidra-Org/horde-worker-reGen) | [A Division of Zer0 Discord](https://discord.com/channels/781145214752129095/1076124012305993768) | [dbZer0 Project](https://dbzer0.itch.io)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], "💑  Sharing is Caring", False)
    def models_AIHorde(e):
        model_request = "https://aihorde.net/api/v2/status/models"
        headers = {'apikey': prefs['AIHorde_api_key']}
        response = requests.get(model_request, headers=headers)
        if response != None:
            if response.status_code == 200:
              horde_models = json.loads(response.content)
              horde_models = sorted(horde_models, key=lambda x: (-x['count'], x['name']), reverse=False)
              model_info = [f"{model['name']} - Count: {model['count']}{f' Jobs: '+str(int(model['jobs'])) if model['jobs'] != 0.0 else ''}" for model in horde_models]
              alert_msg(e.page, "🏇  AI-Horde Current Model Stats", model_info, sound=False)
            else: print(response)
    horde_models_info = IconButton(icons.HELP_OUTLINE, tooltip="Show AI-Horde Models Stat List", on_click=models_AIHorde)
    max_resolution = Dropdown(label="Max Resolution", width=150, options=[dropdown.Option("512x512"), dropdown.Option("768x768"), dropdown.Option("1024x1024"), dropdown.Option("1280x1280")], value=horde_worker_regen_prefs["max_resolution"], on_change=lambda e: changed(e, 'max_resolution'))
    dreamer_name = TextField(label="Dreamer Instance Name", hint_text="An Awesome Dreamer", filled=True, expand=True, value=horde_worker_regen_prefs['dreamer_name'], on_change=lambda e:changed(e,'dreamer_name'))
    def changed_levels(e):
        if e.control.label in horde_worker_regen_prefs['log_levels']:
            horde_worker_regen_prefs['log_levels'].remove(e.control.label)
        else:
            horde_worker_regen_prefs['log_levels'].append(e.control.label)
    def changed_model(e):
        on = e.control.value
        if on is None:
            horde_worker_regen_prefs['models_to_skip'].append(e.control.data)
        else:
            if e.control.data in horde_worker_regen_prefs['models_to_skip']:
                horde_worker_regen_prefs['models_to_skip'].remove(e.control.data)
        if e.control.data in horde_worker_regen_prefs['models_to_load']:
            horde_worker_regen_prefs['models_to_load'].remove(e.control.data)
        else:
            if on is True:
                horde_worker_regen_prefs['models_to_load'].append(e.control.data)
    horde_models = ResponsiveRow(controls=[], spacing={'xs':0}, run_spacing={'xs':0})
    for m in ["ALL MODELS", "TOP 2", "TOP 3", "TOP 5", "TOP 10", "ALL SFW MODELS", "ALL NSFW MODELS", "ALL SD15 MODELS", "ALL SD21 MODELS", "ALL SDXL MODELS", "ALL INPAINTING MODELS"]:
        horde_models.controls.append(Checkbox(label=m, data=m, tristate=True, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, value=None if m in horde_worker_regen_prefs['models_to_skip'] else m in horde_worker_regen_prefs['models_to_load'], on_change=changed_model, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
    for m in AIHorde_models:
        horde_models.controls.append(Checkbox(label=m, data=m, tristate=True, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, value=None if m in horde_worker_regen_prefs['models_to_skip'] else m in horde_worker_regen_prefs['models_to_load'], on_change=changed_model, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
    queue_size = NumberPicker(label="Queue Size: ", min=1, max=3, value=horde_worker_regen_prefs['queue_size'], tooltip="We will keep this many requests in the queue so we can start working as soon as a thread is available. This generally should be or 1 or 2. You should never set this higher than 2 if your max_threads is 2.", on_change=lambda e: changed(e, 'queue_size'))
    max_threads = NumberPicker(label="Max Threads: ", min=1, max=2, value=horde_worker_regen_prefs['max_threads'], tooltip="The amount of parallel jobs to pick up for the horde. Only high end cards (e.g, 3080 or better) benefit from this setting. If you have a 20xx or earlier, or a xx60/xx70, do not change this setting from 1.", on_change=lambda e: changed(e, 'max_threads'))
    max_batch = NumberPicker(label="Max Batch: ", min=1, max=2, value=horde_worker_regen_prefs['max_threads'], tooltip="This will try to pull these many jobs per request and perform batched inference. This is way more optimized than doing them 1 by 1, but is slower. Keep in mind, that the horde will not give your max batch at your max resolution", on_change=lambda e: changed(e, 'max_batch'))
    safety_on_gpu = Switcher(label="Safety Check on GPU ", value=horde_worker_regen_prefs['safety_on_gpu'], on_change=lambda e:changed(e,'safety_on_gpu'), tooltip="Run CLIP model (Checking for potential CSAM or NSFW) on GPU insted of CPU. nable this on cards with 12gb or more VRAM to increase the rate you complete jobs. You can enable this on cards with less VRAM if you do not load SD2.0 or SDXL models, and keep your max_power low (<32)")
    require_upfront_kudos = Switcher(label="Require Upfront Kudos", value=horde_worker_regen_prefs['require_upfront_kudos'], on_change=lambda e:changed(e,'require_upfront_kudos'), tooltip="Worker will not only pick up jobs where the user has the required kudos upfront. Effectively this will exclude all anonymous accounts, and registered accounts who haven't contributed.")
    nsfw = Checkbox(label="Allow NSFW", value=horde_worker_regen_prefs['nsfw'], on_change=lambda e:changed(e,'nsfw'), tooltip="If you do not want to serve NSFW images, set this to false.", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2})
    censor_nsfw = Checkbox(label="Censor NSFW", value=horde_worker_regen_prefs['censor_nsfw'], on_change=lambda e:changed(e,'censor_nsfw'), tooltip="If you want to censor Not Safe For Work images.", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2})
    censorlist = TextField(label="Censor Word List (optional)", value=horde_worker_regen_prefs['censorlist'], on_change=lambda e:changed(e,'censorlist'), tooltip="A list of words for which you always want to censor, even if `nsfw` is true.", col={'xs':12, 'sm':6, 'md':6, 'lg':6, 'xl': 4})
    allow_img2img = Checkbox(label="Allow Img2Img", value=horde_worker_regen_prefs['allow_img2img'], on_change=lambda e:changed(e,'allow_img2img'), tooltip="Accept jobs which use a user-supplied image.", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2})
    allow_painting = Checkbox(label="Allow Painting", value=horde_worker_regen_prefs['allow_painting'], on_change=lambda e:changed(e,'allow_painting'), tooltip="Accept jobs which use a user-supplied image and an inpainting specific model.", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2})
    allow_post_processing = Checkbox(label="Allow Post-Processing", value=horde_worker_regen_prefs['allow_post_processing'], on_change=lambda e:changed(e,'allow_post_processing'), tooltip="Allow upscaling, facefixer and other post-generation features to be performed by the worker.", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2})
    allow_controlnet = Checkbox(label="Allow ControlNet", value=horde_worker_regen_prefs['allow_controlnet'], on_change=lambda e:changed(e,'allow_controlnet'), tooltip="Allow controlnet jobs to be done by this worker. Note: There is additional RAM/VRAM overhead with this option. Low VRAM cards (<6gb) should be cautious to enable this.", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2})
    allow_lora = Checkbox(label="Allow LoRA", value=horde_worker_regen_prefs['allow_lora'], on_change=lambda e:changed(e,'allow_lora'), tooltip="Allow LoRas to be used. This requires that you have a fast internet connection, LoRas will be downloaded on demand..", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2})
    log_success = Checkbox(label="SUCCESS", value="SUCCESS" in horde_worker_regen_prefs['log_levels'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}, on_change=changed_levels)
    log_info = Checkbox(label="INFO", value="INFO" in horde_worker_regen_prefs['log_levels'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}, on_change=changed_levels)
    log_debug = Checkbox(label="DEBUG", value="DEBUG" in horde_worker_regen_prefs['log_levels'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}, on_change=changed_levels)
    log_error = Checkbox(label="ERROR", value="ERROR" in horde_worker_regen_prefs['log_levels'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}, on_change=changed_levels)
    log_warning = Checkbox(label="WARNING", value="WARNING" in horde_worker_regen_prefs['log_levels'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}, on_change=changed_levels)
    log_lines = NumberPicker(label="  Max Lines: ", min=1, max=2000, value=horde_worker_regen_prefs['log_lines'], tooltip="How many Log Lines to display in UI before clearing oldest lines.", on_change=lambda e: changed(e, 'log_lines'))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("⛈️   AI-Horde Worker reGen Server", "Share your GPU in the AI-Horde SD Cloud and earn Kudos... Give back to the Stable Horde, thanks db0.", actions=[save_default(horde_worker_regen_prefs), IconButton(icon=icons.HELP, tooltip="Help with AI-Horde Worker Settings", on_click=horde_help)]),
        Row([dreamer_name]),
        ResponsiveRow([allow_img2img, allow_painting, allow_post_processing, allow_controlnet, allow_lora], spacing=0, run_spacing=0),
        ResponsiveRow([nsfw, censor_nsfw, censorlist]),
        Row([queue_size, max_threads, max_batch]),
        Row([max_resolution, safety_on_gpu, require_upfront_kudos]),
        Row([Text("Select one or more models to Share, or -- to Skip:", weight=FontWeight.BOLD, theme_style=TextThemeStyle.TITLE_MEDIUM), horde_models_info], vertical_alignment=CrossAxisAlignment.END),
        horde_models,
        Row([Text("Console Log Levels: ", weight=FontWeight.BOLD), log_success, log_info, log_debug, log_error, log_warning, log_lines]),
        ElevatedButton(content=Text("⛅  Start reGen Server", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_horde_worker_regen(page)),
      ]
    ))], scroll=ScrollMode.AUTO)
    return c


blip_diffusion_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "source_subject_category": '',
    "target_subject_category": '',
    "steps":50,
    #"ddim_eta":0.05,
    "width": 768,
    "height":768,
    "guidance_scale":7.5,
    "init_image": '',
    "control_image": '',
    "strength": 1.0,
    "prompt_reps": 20,
    "controlnet_type": 'None',
    "use_controlnet_canny": False,
    "seed": 0,
    "batch_folder_name": '',
    "file_prefix": "blip-",
    "num_images": 1,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildBLIPDiffusion(page):
    global prefs, blip_diffusion_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          blip_diffusion_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def blip_diffusion_help(e):
      def close_blip_diffusion_dlg(e):
        nonlocal blip_diffusion_help_dlg
        blip_diffusion_help_dlg.open = False
        page.update()
      blip_diffusion_help_dlg = AlertDialog(title=Text("🙅   Help with BLIP Diffusion Pipeline"), content=Column([
          Markdown("Blip Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation.", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("💖  Lovely... ", on_click=close_blip_diffusion_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(blip_diffusion_help_dlg)
      blip_diffusion_help_dlg.open = True
      page.update()

    def toggle_controlnet_image(e):
        blip_diffusion_prefs['use_controlnet_canny'] = e.control.value
        control_image_container.height=None if blip_diffusion_prefs['use_controlnet_canny'] else 0
        control_image_container.update()
    def change_controlnet_type(e):
        blip_diffusion_prefs['controlnet_type'] = e.control.value
        control_image_container.height=None if blip_diffusion_prefs['controlnet_type'] != "None" else 0
        control_image_container.update()
    prompt = TextField(label="Prompt Text", value=blip_diffusion_prefs['prompt'], multiline=True, filled=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=blip_diffusion_prefs['negative_prompt'], multiline=True, filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    source_subject_category = TextField(label="Source Subject Category List", value=blip_diffusion_prefs['source_subject_category'], multiline=True, col={'md':6}, on_change=lambda e:changed(e,'source_subject_category'))
    target_subject_category = TextField(label="Target Subject Category List", value=blip_diffusion_prefs['target_subject_category'], multiline=True, col={'md':6}, on_change=lambda e:changed(e,'target_subject_category'))
    batch_folder_name = TextField(label="Batch Folder Name", value=blip_diffusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=blip_diffusion_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    steps = TextField(label="Number of Steps", value=blip_diffusion_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype="int"))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=blip_diffusion_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=blip_diffusion_prefs, key='steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=blip_diffusion_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=blip_diffusion_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=blip_diffusion_prefs, key='height')
    init_image = FileInput(label="Reference Image", pref=blip_diffusion_prefs, key='init_image', page=page)
    image_pickers = Container(content=ResponsiveRow([init_image]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    #use_controlnet_image = Switcher(label="Use ControlNet Canny", value=blip_diffusion_prefs['use_controlnet_canny'], on_change=toggle_controlnet_image)
    controlnet_type = Dropdown(label="ControlNet Image Layer", width=177, options=[dropdown.Option("None"), dropdown.Option("Canny Edge"), dropdown.Option("HED")], value=blip_diffusion_prefs['controlnet_type'], on_change=change_controlnet_type)
    control_image = FileInput(label="ControlNet Image", pref=blip_diffusion_prefs, key='control_image', page=page)
    control_image_container = Container(animate_size=animation.Animation(700, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, expand=True, alignment = alignment.top_left, height = None if blip_diffusion_prefs['use_controlnet_canny'] else 0, padding=padding.only(top=4), content=Column([control_image]))
    strength_slider = SliderRow(label="Prompt Strength", min=0.1, max=2.0, divisions=38, round=2, pref=blip_diffusion_prefs, key='strength', col={'md':6}, tooltip="Specifies the number of times the prompt is repeated along with prompt_reps.")
    prompt_reps = SliderRow(label="Prompt Repetitions", min=0, max=50, divisions=50, pref=blip_diffusion_prefs, key='prompt_reps', col={'md':6}, tooltip="The number of times the prompt is repeated along with prompt_strength to amplify the prompt.")
    #img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    seed = TextField(label="Seed", width=90, value=str(blip_diffusion_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(blip_diffusion_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🦾   Run BLIP-Diffusion", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_blip_diffusion(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_blip_diffusion(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_blip_diffusion(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.blip_diffusion_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("📡  BLIP-Diffusion by Salesforce", "Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing...", actions=[save_default(blip_diffusion_prefs, ['init_image', 'control_image']), IconButton(icon=icons.HELP, tooltip="Help with BLIP Diffusion Settings", on_click=blip_diffusion_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            ResponsiveRow([source_subject_category, target_subject_category]),
            #img_block,
            image_pickers,
            ResponsiveRow([strength_slider, prompt_reps]),
            Row([controlnet_type, control_image_container]),
            #ResponsiveRow([prior_steps, prior_guidance_scale]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            #Row([batch_folder_name, file_prefix]),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
            parameters_row,
            page.blip_diffusion_output
        ],
    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed,
    return c

anytext_prefs = {
    "prompt": '',
    "negative_prompt": 'low-res, bad anatomy, cropped, worst quality, low quality, watermark, unreadable text, messy words, distorted text, disorganized writing',
    "a_prompt": 'best quality, extremely detailed,4k, HD, supper legible text, clear text edges, clear strokes, neat writing, no watermarks',
    "file_prefix": "anytext-",
    "num_images": 1,
    "width": 768,
    "height":768,
    "guidance_scale": 9.0,
    'num_inference_steps': 20,
    "strength": 1.0,
    "eta": 0.0,
    "seed": 0,
    'init_image': '',
    'mask_image': '',
    'init_image_strength': 0.8,
    'font_ttf': '',
    'sort_priority': "↕",# "↔"
    'revise_pos': True,
    "batch_folder_name": '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildAnyText(page):
    global prefs, anytext_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          anytext_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def anytext_help(e):
      def close_anytext_dlg(e):
        nonlocal anytext_help_dlg
        anytext_help_dlg.open = False
        page.update()
      anytext_help_dlg = AlertDialog(title=Text("🙅   Help with AnyText Pipeline"), content=Column([
          Text("AnyText comprises a diffusion pipeline with two primary elements: an auxiliary latent module and a text embedding module. The former uses inputs like text glyph, position, and masked image to generate latent features for text generation or editing. The latter employs an OCR model for encoding stroke data as embeddings, which blend with image caption embeddings from the tokenizer to generate texts that seamlessly integrate with the background. We employed text-control diffusion loss and text perceptual loss for training to further enhance writing accuracy. The drawing of text positions is crucial to the quality of the resulting image, please do not draw too casually or too small. The number of positions should match the number of text lines, and the size of each position should be matched as closely as possible to the length or width of the corresponding text line. When generating multiple lines, each position is matched with the text line according to a certain rule."),
          Text('Example Prompt: Photo of caramel macchiato coffee on the table, top-down perspective, with "Any" "Text" written on it'),
          Markdown("[GitHub](https://github.com/tyxsspa/AnyText) | [Paper](https://arxiv.org/abs/2311.03054) | [HuggingFace Space](https://huggingface.co/spaces/modelscope/AnyText) | [ModelScope](https://modelscope.cn/models/damo/cv_anytext_text_generation_editing/summary)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Markdown("Contributors include Yuxiang Tuo and Wangmeng Xiang and Jun-Yan He and Yifeng Geng and Xuansong Xie, and ModelScope Developers.", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🔠  Finally, words...", on_click=close_anytext_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(anytext_help_dlg)
      anytext_help_dlg.open = True
      page.update()
    prompt = TextField(label='Prompt with Text in "Double" "Quotes"', value=anytext_prefs['prompt'], filled=True, multiline=True, on_change=lambda e:changed(e,'prompt'))
    a_prompt = TextField(label="Additional Prompt Text", value=anytext_prefs['a_prompt'], multiline=True, col={'md':9}, on_change=lambda e:changed(e,'a_prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=anytext_prefs['negative_prompt'], multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = FileInput(label="Initial Image (optional)", pref=anytext_prefs, key='init_image', page=page, col={'md':6})
    mask_image = FileInput(label="Text Area Mask (optional)", pref=anytext_prefs, key='mask_image', page=page, col={'md':6})
    init_image_strength = SliderRow(label="Init-Image Strength", min=0.0, max=2.0, divisions=40, round=2, pref=anytext_prefs, key='init_image_strength', col={'md':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    font_ttf = FileInput(label="Font .ttf File (optional)", pref=anytext_prefs, key='font_ttf', page=page, ftype="font", col={'md':6})
    batch_folder_name = TextField(label="Batch Folder Name", value=anytext_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=anytext_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Images", min=1, max=8, step=1, value=anytext_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=1, max=50, divisions=49, pref=anytext_prefs, key='num_inference_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=anytext_prefs, key='guidance_scale')
    #strength = SliderRow(label="Strength", min=0, max=2, divisions=20, round=1, pref=anytext_prefs, key='strength', tooltip="How strong to influence")
    width_slider = SliderRow(label="Width", min=256, max=1024, divisions=12, multiple=64, suffix="px", pref=anytext_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1024, divisions=12, multiple=64, suffix="px", pref=anytext_prefs, key='height')
    eta = SliderRow(label="DDIM ETA", min=0, max=1, divisions=10, round=1, pref=anytext_prefs, key='eta', tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.")
    #anytext_model = Dropdown(label="AnyText Model", width=220, options=[dropdown.Option("Custom"), dropdown.Option("anytext-256"), dropdown.Option("anytext-512")], value=anytext_prefs['anytext_model'], on_change=changed_model)
    #anytext_custom_model = TextField(label="Custom AnyText Model (URL or Path)", value=anytext_prefs['custom_model'], expand=True, visible=anytext_prefs['anytext_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    def change_sort(e):
        anytext_prefs['sort_priority'] = e.data.split('"')[1]
    sort_priority = ft.SegmentedButton(on_change=change_sort, selected={anytext_prefs['sort_priority']}, allow_multiple_selection=False,
        segments=[
            ft.Segment(value="↕", label=ft.Text("Vertical ↕"), icon=ft.Icon(ft.icons.SWAP_VERTICAL_CIRCLE)),
            ft.Segment(value="↔", label=ft.Text("Horizontal ↔"), icon=ft.Icon(ft.icons.SWAP_HORIZONTAL_CIRCLE)),
        ], tooltip="When generating multiple lines, each position is matched with the text line according to a certain rule. This determines whether to prioritize sorting from top to bottom or from left to right."
    )
    revise_pos = Checkbox(label="Revise Position", value=anytext_prefs['revise_pos'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'revise_pos'), tooltip="Uses the bounding box of the rendered text as the revised position. However, it is occasionally found that the creativity of the generated text is slightly lower using this method.")
    seed = TextField(label="Seed", width=90, value=str(anytext_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(anytext_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="✍️   Run AnyText", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_anytext(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_anytext(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_anytext(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🔤  AnyText (under construction)", "Multilingual Visual Text Generation and Text Editing...", actions=[save_default(anytext_prefs, ['init_image', 'mask_image']), IconButton(icon=icons.HELP, tooltip="Help with AnyText Settings", on_click=anytext_help)]),
            prompt,
            ResponsiveRow([a_prompt, negative_prompt]),
            ResponsiveRow([init_image, mask_image]),
            init_image_strength,
            font_ttf,
            Row([Text("Sort Priority: "), sort_priority, revise_pos]),
            steps,
            guidance, eta,
            width_slider, height_slider, #Divider(height=9, thickness=2),
            #Row([anytext_model, anytext_custom_model]),
            #Row([cpu_offload, cpu_only]),
            upscaler,
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            parameters_row,
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

ip_adapter_models = [
    {'name': 'SD v1.5', 'path': 'h94/IP-Adapter', 'subfolder': 'models', 'weight_name': 'ip-adapter_sd15.bin'},
    {'name': 'Plus SD v1.5', 'path': 'h94/IP-Adapter', 'subfolder': 'models', 'weight_name': 'ip-adapter-plus_sd15.bin'},
    {'name': 'Plus Face SD v1.5', 'path': 'h94/IP-Adapter', 'subfolder': 'models', 'weight_name': 'ip-adapter-plus-face_sd15.bin'},
    {'name': 'Full Face SD v1.5', 'path': 'h94/IP-Adapter', 'subfolder': 'models', 'weight_name': 'ip-adapter-full-face_sd15.bin'},
    {'name': 'Light SD v1.5', 'path': 'h94/IP-Adapter', 'subfolder': 'models', 'weight_name': 'ip-adapter_sd15_light.bin'},
    {'name': 'FaceID SD v1.5', 'path': 'h94/IP-Adapter-FaceID', 'subfolder': '', 'weight_name': 'ip-adapter-faceid_sd15.bin'},
    {'name': 'Composition SD v1.5', 'path': 'ostris/ip-composition-adapter', 'subfolder': '', 'weight_name': 'ip_plus_composition_sd15.safetensors'},
]
ip_adapter_SDXL_models = [
    {'name': 'SDXL', 'path': 'h94/IP-Adapter', 'subfolder': 'sdxl_models', 'weight_name': 'ip-adapter_sdxl.bin'},
    {'name': 'Plus SDXL', 'path': 'h94/IP-Adapter', 'subfolder': 'sdxl_models', 'weight_name': 'ip-adapter-plus_sdxl_vit-h.bin'},
    {'name': 'Plus Face SDXL', 'path': 'h94/IP-Adapter', 'subfolder': 'sdxl_models', 'weight_name': 'ip-adapter-plus-face_sdxl_vit-h.bin'},
    {'name': 'SDXL ViT-H', 'path': 'h94/IP-Adapter', 'subfolder': 'sdxl_models', 'weight_name': 'ip-adapter_sdxl_vit-h.bin'},
    {'name': 'InstantID SDXL', 'path': 'InstantX/InstantID', 'subfolder': '', 'weight_name': 'ip-adapter.bin'},
    {'name': 'Composition SDXL', 'path': 'ostris/ip-composition-adapter', 'subfolder': '', 'weight_name': 'ip_plus_composition_sdxl.safetensors'},
    #{'name': 'Light SDXL', 'path': 'h94/IP-Adapter', 'subfolder': 'sdxl_models', 'weight_name': 'ip-adapter_sd15_light.bin'},
]
ip_adapter_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "sd-",
    "num_images": 1,
    "width": 1024,
    "height":1024,
    "guidance_scale":4.5,
    'num_inference_steps': 8,
    "seed": 0,
    'ip_adapter_image':'',
    'ip_adapter_strength': 0.8,
    'init_image': '',
    'init_image_strength': 0.8,
    'mask_image': '',
    'alpha_mask': False,
    'invert_mask': False,
    'use_SDXL': False,
    "cpu_offload": False,
    "cpu_only": False,
    "ip_adapter_model": "SD v1.5",
    "ip_adapter_SDXL_model": "SDXL",
    "custom_model": "",
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildIP_Adapter(page):
    global prefs, ip_adapter_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          ip_adapter_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def ip_adapter_help(e):
      def close_ip_adapter_dlg(e):
        nonlocal ip_adapter_help_dlg
        ip_adapter_help_dlg.open = False
        page.update()
      ip_adapter_help_dlg = AlertDialog(title=Text("🙅   Help with IP-Adapter Pipeline"), content=Column([
          Text("IP-Adapter is an effective and lightweight adapter that adds image prompting capabilities to a diffusion model. This adapter works by decoupling the cross-attention layers of the image and text features. All the other model components are frozen and only the embedded image features in the UNet are trained. As a result, IP-Adapter files are typically only ~100MBs."),
          Text("IP-Adapter works with most of our pipelines, including Stable Diffusion, Stable Diffusion XL (SDXL), ControlNet, T2I-Adapter, AnimateDiff. And you can use any custom models finetuned from the same base models. It also works with LCM-Lora out of box."),
          Markdown("[Project](https://ip-adapter.github.io/) | [Paper](https://arxiv.org/abs/2308.06721) | [GitHub Code](https://github.com/tencent-ailab/IP-Adapter) | [Checkpoint Models](https://huggingface.co/h94/IP-Adapter)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Markdown("IP-Adapter was contributed by [okotaku](https://github.com/okotaku), Tencent, okotaku, sayakpaul, yiyixuxu, Patrick von Platen and Steven Liu ..", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🏃  Adapt This...", on_click=close_ip_adapter_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(ip_adapter_help_dlg)
      ip_adapter_help_dlg.open = True
      page.update()
    def changed_model(e):
        ip_adapter_prefs['ip_adapter_model'] = e.control.value
        ip_adapter_custom_model.visible = e.control.value == "Custom"
        ip_adapter_custom_model.update()
    def toggle_SDXL(e):
        ip_adapter_prefs['use_SDXL'] = e.control.value
        ip_adapter_model.visible = not ip_adapter_prefs['use_SDXL']
        ip_adapter_SDXL_model.visible = ip_adapter_prefs['use_SDXL']
        ip_adapter_model.update()
        ip_adapter_SDXL_model.update()
    prompt = TextField(label="Prompt Text (optional)", value=ip_adapter_prefs['prompt'], filled=True, hint_text="Leave blank for Image Variation", multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=ip_adapter_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=ip_adapter_prefs, key='ip_adapter_image', expand=True, page=page)
    ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=ip_adapter_prefs, key='ip_adapter_strength', expand=True, col={'md':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    init_image = FileInput(label="Init Image (optional)", pref=ip_adapter_prefs, key='init_image', page=page, col={'md':6})
    mask_image = FileInput(label="Mask Image (optional)", pref=ip_adapter_prefs, key='mask_image', page=page, col={'md':6})
    init_image_strength = SliderRow(label="Init-Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=ip_adapter_prefs, key='init_image_strength', col={'md':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    batch_folder_name = TextField(label="Batch Folder Name", value=ip_adapter_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=ip_adapter_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=ip_adapter_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=40, divisions=40, pref=ip_adapter_prefs, key='num_inference_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=ip_adapter_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=ip_adapter_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=ip_adapter_prefs, key='height')
    use_SDXL = Switcher(label="Use Stable Diffusion XL", value=ip_adapter_prefs['use_SDXL'], on_change=toggle_SDXL, tooltip="SDXL uses Model Checkpoint set in Installation. Otherwise use selected 1.5 or 2.1 Inpainting Model.")
    ip_adapter_model = Dropdown(label="IP-Adapter SD Model", width=220, options=[], value=ip_adapter_prefs['ip_adapter_model'], visible=not ip_adapter_prefs['use_SDXL'], on_change=lambda e:changed(e,'ip_adapter_model'))
    for m in ip_adapter_models:
        ip_adapter_model.options.append(dropdown.Option(m['name']))
    ip_adapter_SDXL_model = Dropdown(label="IP-Adapter SDXL Model", width=220, options=[], value=ip_adapter_prefs['ip_adapter_SDXL_model'], visible=ip_adapter_prefs['use_SDXL'], on_change=lambda e:changed(e,'ip_adapter_SDXL_model'))
    for m in ip_adapter_SDXL_models:
        ip_adapter_SDXL_model.options.append(dropdown.Option(m['name']))
    ip_adapter_custom_model = TextField(label="Custom IP_Adapter Model (URL or Path)", value=ip_adapter_prefs['custom_model'], expand=True, visible=ip_adapter_prefs['ip_adapter_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    cpu_offload = Switcher(label="CPU Offload", value=ip_adapter_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    cpu_only = Switcher(label="CPU Only (not yet)", value=ip_adapter_prefs['cpu_only'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_only'), tooltip="If you don't have a good GPU, can run entirely on CPU")
    seed = TextField(label="Seed", width=90, value=str(ip_adapter_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(ip_adapter_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🪞   Run IP-Adapter", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_ip_adapter(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_ip_adapter(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_ip_adapter(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.ip_adapter_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🦊  IP-Adapter", "Image Prompting capabilities to Transfer Subject with or without Prompt...", actions=[save_default(ip_adapter_prefs, ['init_image', 'mask_image', 'ip_adapter_image']), IconButton(icon=icons.HELP, tooltip="Help with IP-Adapter Settings", on_click=ip_adapter_help)]),
            Row([ip_adapter_model, ip_adapter_SDXL_model, ip_adapter_image]),
            Row([use_SDXL, ip_adapter_strength]),
            ResponsiveRow([prompt, negative_prompt]),
            ResponsiveRow([init_image, mask_image]),
            init_image_strength,
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.ip_adapter_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

hd_painter_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "hd_painter-",
    "num_images": 1,
    "max_size":1024,
    "guidance_scale": 7.5,
    'num_inference_steps': 50,
    'eta': 0.01,
    "seed": 0,
    'init_image': '',
    'mask_image': '',
    'init_image_strength': 0.9,
    'use_rasg': True,
    'use_painta': True,
    "cpu_offload": False,
    "hd_painter_model": "stabilityai/stable-diffusion-2-inpainting",
    "custom_model": "",
    'use_ip_adapter': False,
    'ip_adapter_image': '',
    'ip_adapter_model': 'SD v1.5',
    'ip_adapter_strength': 0.8,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildHD_Painter(page):
    global prefs, hd_painter_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          hd_painter_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def hd_painter_help(e):
      def close_hd_painter_dlg(e):
        nonlocal hd_painter_help_dlg
        hd_painter_help_dlg.open = False
        page.update()
      hd_painter_help_dlg = AlertDialog(title=Text("🙅   Help with HD-Painter Pipeline"), content=Column([
          Text("Recent progress in text-guided image inpainting, based on the unprecedented success of text-to-image diffusion models, has led to exceptionally realistic and visually plausible results. However, there is still significant potential for improvement in current text-to-image inpainting models, particularly in better aligning the inpainted area with user prompts and performing high-resolution inpainting. Therefore, in this paper we introduce _HD-Painter_, a completely **training-free** approach that **accurately follows to prompts** and coherently **scales to high-resolution** image inpainting. To this end, we design the _Prompt-Aware Introverted Attention (PAIntA)_ layer enhancing self-attention scores by prompt information and resulting in better text alignment generations."),
          Text("To further improve the prompt coherence we introduce the _Reweighting Attention Score Guidance (RASG)_ mechanism seamlessly integrating a post-hoc sampling strategy into general form of DDIM to prevent out-of-distribution latent shifts. Moreover, HD-Painter allows extension to larger scales by introducing a specialized super-resolution technique customized for inpainting, enabling the completion of missing regions in images of up to 2K resolution.  Our experiments demonstrate that HD-Painter surpasses existing state-of-the-art approaches qualitatively and quantitatively, achieving an impressive generation accuracy improvement of **61.4** vs **51.9**. "),
          Markdown("[Github Project](https://github.com/Picsart-AI-Research/HD-Painter) | [Paper](https://arxiv.org/abs/2312.14091) | [HuggingFace Space](https://huggingface.co/spaces/PAIR/HD-Painter)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Markdown("Contributors include Hayk Manukyan, Andranik Sargsyan, Barsegh Atanyan, Zhangyang Wang, Shant Navasardyan and Humphrey Shi, Picsart AI Resarch (PAIR), UT Austin, Georgia Tech.", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🧑‍🎨️  Commission an Artist...", on_click=close_hd_painter_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(hd_painter_help_dlg)
      hd_painter_help_dlg.open = True
      page.update()
    def changed_model(e):
        hd_painter_prefs['hd_painter_model'] = e.control.value
        hd_painter_custom_model.visible = e.control.value == "Custom"
        hd_painter_custom_model.update()
    prompt = TextField(label="Prompt Text", value=hd_painter_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=hd_painter_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = FileInput(label="Init Image", pref=hd_painter_prefs, key='init_image', page=page, col={'md':6})
    mask_image = FileInput(label="Mask Image", pref=hd_painter_prefs, key='mask_image', page=page, col={'md':6})
    init_image_strength = SliderRow(label="Init-Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=hd_painter_prefs, key='init_image_strength', col={'md':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    batch_folder_name = TextField(label="Batch Folder Name", value=hd_painter_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=hd_painter_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Images", min=1, max=20, step=1, value=hd_painter_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=100, divisions=100, pref=hd_painter_prefs, key='num_inference_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=hd_painter_prefs, key='guidance_scale')
    eta = SliderRow(label="DDIM ETA", min=0, max=1, divisions=100, round=2, pref=hd_painter_prefs, key='eta', tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.")
    max_row = SliderRow(label="Max Resolution Size", min=256, max=2048, divisions=112, multiple=16, suffix="px", pref=hd_painter_prefs, key='max_size', tooltip="Resizes your Init and Mask Image to save memory.")
    def toggle_ip_adapter(e):
        hd_painter_prefs['use_ip_adapter'] = e.control.value
        ip_adapter_container.height = None if e.control.value else 0
        ip_adapter_container.update()
        ip_adapter_model.visible = e.control.value
        ip_adapter_model.update()
    use_ip_adapter = Switcher(label="Use IP-Adapter Reference Image", value=hd_painter_prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip="Uses both image and text to condition the image generation process.")
    ip_adapter_model = Dropdown(label="IP-Adapter SD Model", width=220, options=[], value=hd_painter_prefs['ip_adapter_model'], visible=hd_painter_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))
    for m in ip_adapter_models:
        ip_adapter_model.options.append(dropdown.Option(m['name']))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=hd_painter_prefs, key='ip_adapter_image', col={'lg':6}, page=page)
    ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=hd_painter_prefs, key='ip_adapter_strength', col={'lg':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    ip_adapter_container = Container(ResponsiveRow([ip_adapter_image, ip_adapter_strength]), height = None if hd_painter_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    hd_painter_model = Dropdown(label="Inpainting Model", width=376, options=[dropdown.Option(m) for m in ["stabilityai/stable-diffusion-2-inpainting", "runwayml/stable-diffusion-inpainting", "ImNoOne/f222-inpainting-diffusers", "Lykon/dreamshaper-8-inpainting", "parlance/dreamlike-diffusion-1.0-inpainting", "ghunkins/stable-diffusion-liberty-inpainting", "piyushaaryan011/realistic-vision-inpainting", "Custom"]], value=hd_painter_prefs['hd_painter_model'], on_change=changed_model)
    hd_painter_custom_model = TextField(label="Custom HD_Painter Model (URL or Path)", value=hd_painter_prefs['custom_model'], expand=True, visible=hd_painter_prefs['hd_painter_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    use_rasg = Switcher(label="Use RASG", value=hd_painter_prefs['use_rasg'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_rasg'), tooltip="Reweighting Attention Score Guidance improves the prompt coherence by seamlessly integrating a post-hoc sampling strategy into the general form of DDIM to prevent out-of-distribution latent shifts.")
    use_painta = Switcher(label="Use PAIntA", value=hd_painter_prefs['use_painta'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_painta'), tooltip="Prompt-Aware Introverted Attention layer enhancing self-attention scores by prompt information resulting in better text aligned generations.")
    cpu_offload = Switcher(label="CPU Offload", value=hd_painter_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    seed = TextField(label="Seed", width=90, value=str(hd_painter_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(hd_painter_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🖌   Run HD-Painter", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_hd_painter(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_hd_painter(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_hd_painter(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.hd_painter_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("💅  HD-Painter by Picsart AI-Research", "Prompt-Faithful and High-Resolution (up to 2k) Text-Guided Image Inpainting with Diffusion Models...", actions=[save_default(hd_painter_prefs, ['init_image', 'mask_image']), IconButton(icon=icons.HELP, tooltip="Help with HD-Painter Settings", on_click=hd_painter_help)]),
            ResponsiveRow([init_image, mask_image]),
            ResponsiveRow([prompt, negative_prompt]),
            init_image_strength,
            steps,
            guidance,
            eta,
            max_row, #Divider(height=9, thickness=2),
            Row([hd_painter_model, hd_painter_custom_model]),
            Row([use_rasg, use_painta]),
            Row([use_ip_adapter, ip_adapter_model], vertical_alignment=CrossAxisAlignment.START),
            ip_adapter_container,
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.hd_painter_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c


reference_prefs = {
    'ref_image': '',
    'prompt': '',
    'negative_prompt': '',
    'guidance_scale': 7.5,
    'num_inference_steps': 50,
    'seed': 0,
    'eta': 0.0,
    'attention_auto_machine_weight': 1.0,
    'gn_auto_machine_weight': 1.0,
    'style_fidelity': 0.5,
    'reference_attn': True,
    'reference_adain': True,
    'batch_size': 1,
    'num_images': 1,
    'max_size': 1024,
    'width': 960,
    'height': 768,
    'last_model': '',
    'use_SDXL': False,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": False,
}
def buildReference(page):
    global reference_prefs, prefs, pipe_reference
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          reference_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_reference_output(o):
      page.Reference.controls.append(o)
      page.Reference.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_reference_output = add_to_reference_output
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      for i, c in enumerate(page.Reference.controls):
        if i == 0: continue
        else: del page.Reference.controls[i]
      page.Reference.update()
      clear_button.visible = False
      clear_button.update()
    def reference_help(e):
      def close_reference_dlg(e):
        nonlocal reference_help_dlg
        reference_help_dlg.open = False
        page.update()
      reference_help_dlg = AlertDialog(title=Text("🙅   Help with Reference-Only"), content=Column([
          Text("This reference-only ControlNet can directly link the attention layers of your SD to any independent images, so that your SD will read arbitary images for reference. This preprocessor does not require any control models. It can guide the diffusion directly using images as references. This method is similar to inpaint-based reference but it does not make your image disordered."),
          Text("Note that this method is as 'non-opinioned' as possible. It only contains very basic connection codes, without any personal preferences, to connect the attention layers with your reference images. However, even if we tried best to not include any opinioned codes, we still need to write some subjective implementations to deal with weighting, cfg-scale, etc - tech report is on the way."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🧙  Powerful... ", on_click=close_reference_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(reference_help_dlg)
      reference_help_dlg.open = True
      page.update()
    prompt = TextField(label="Prompt Text", value=reference_prefs['prompt'], filled=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=reference_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    ref_image = FileInput(label="Reference Image", pref=reference_prefs, key='ref_image', page=page)
    use_SDXL = Switcher(label="Use Stable Diffusion XL Reference Pipeline", value=reference_prefs['use_SDXL'], on_change=lambda e:changed(e,'use_SDXL'), tooltip="Otherwise use standard Model Checkpoint set in Installation.")
    seed = TextField(label="Seed", width=90, value=str(reference_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=reference_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=reference_prefs, key='guidance_scale')
    eta = SliderRow(label="DDIM ETA", min=0, max=1, divisions=20, round=1, pref=reference_prefs, key='eta', tooltip="", visible=False)
    page.etas.append(eta)
    style_fidelity = SliderRow(label="Style Fidelity", min=0.0, max=1.0, divisions=20, round=2, pref=reference_prefs, key='style_fidelity', tooltip="Style fidelity of ref_uncond_xt. If style_fidelity=1.0, control more important, elif style_fidelity=0.0, prompt more important, else balanced.")
    attention_auto_machine_weight = SliderRow(label="Attention Auto Machine Weight", min=0.0, max=1.0, divisions=20, round=2, pref=reference_prefs, key='attention_auto_machine_weight', tooltip="Weight of using reference query for self attention's context. If attention_auto_machine_weight=1.0, use reference query for all self attention's context.")
    gn_auto_machine_weight = SliderRow(label="Gn Auto Machine Weight", min=0.0, max=2.0, divisions=20, round=1, pref=reference_prefs, key='gn_auto_machine_weight', tooltip="Weight of using reference adain. If gn_auto_machine_weight=2.0, use all reference adain plugins.")
    #reference_attn = Checkbox(label="Reference Attention", value=reference_prefs['reference_attn'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'reference_attn'), tooltip="Whether to use reference query for self attention's context.")
    #reference_adain = Checkbox(label="Reference Adain", value=reference_prefs['reference_adain'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'reference_adain'), tooltip="Whether to use reference Adain (Adaptive Instance Normalization).")
    reference_attn = Switcher(label="Reference Attention ", value=reference_prefs['reference_attn'], on_change=lambda e:changed(e,'reference_attn'), tooltip="Whether to use reference query for self attention's context.")
    reference_adain = Switcher(label="Reference Adain", value=reference_prefs['reference_adain'], on_change=lambda e:changed(e,'reference_adain'), tooltip="Whether to use reference Adain (Adaptive Instance Normalization).")
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=reference_prefs, key='max_size')
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=reference_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=reference_prefs, key='height')
    batch_size = NumberPicker(label="Batch Size: ", min=1, max=8, value=reference_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    num_images = NumberPicker(label="Number of Iterations: ", min=1, max=12, value=reference_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))
    batch_folder_name = TextField(label="Batch Folder Name", value=reference_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(reference_prefs)
    page.upscalers.append(upscaler)
    page.reference_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.reference_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎩  Reference-Only Image with Prompt", "ControlNet Pipeline for Transfering Ref Subject to new images...", actions=[save_default(reference_prefs, ['ref_image', 'last_model']), IconButton(icon=icons.HELP, tooltip="Help with Reference Settings", on_click=reference_help)]),
        ref_image,
        ResponsiveRow([prompt, negative_prompt]),
        use_SDXL,
        num_inference_row,
        guidance,
        eta,
        style_fidelity,
        ResponsiveRow([attention_auto_machine_weight, gn_auto_machine_weight]),
        Row([reference_attn, reference_adain]),
        max_row,
        width_slider,
        height_slider,
        ResponsiveRow([Row([batch_size, num_images], col={'lg':6}), Row([seed, batch_folder_name], col={'lg':6})]),
        upscaler,
        Row([ElevatedButton(content=Text("💗  Make Reference", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_reference(page)),
             ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_reference(page, from_list=True))]),
        page.reference_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

controlnet_qr_prefs = {
    'init_image': '',
    'ref_image': '',
    'qr_content': '',
    'border_thickness': 5,
    'use_image': False,
    'selected_mode': 'link',
    'prompt': '',
    'negative_prompt': 'ugly, disfigured, low quality, blurry, nsfw',
    'guidance_scale': 8.0,
    'conditioning_scale': 1.8,
    'control_guidance_start': 0.0,
    'control_guidance_end': 1.0,
    'strength': 0.9,
    'num_inference_steps': 50,
    'controlnet_version': "Stable Diffusion 2.1",
    'seed': 0,
    'batch_size': 1,
    'num_images': 1,
    'max_size': 768,
    'last_model': '',
    'last_controlnet_model': '',
    'use_ip_adapter': False,
    'ip_adapter_image': '',
    'ip_adapter_model': 'SD v1.5',
    'ip_adapter_SDXL_model': 'SDXL',
    'ip_adapter_strength': 0.8,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}
def buildControlNetQR(page):
    global controlnet_qr_prefs, prefs, pipe_controlnet_qr
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          controlnet_qr_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_controlnet_qr_output(o):
      page.ControlNetQR.controls.append(o)
      page.ControlNetQR.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_controlnet_qr_output = add_to_controlnet_qr_output
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      for i, c in enumerate(page.ControlNetQR.controls):
        if i == 0: continue
        else: del page.ControlNetQR.controls[i]
      page.ControlNetQR.update()
      clear_button.visible = False
      clear_button.update()
    def controlnet_qr_help(e):
      def close_controlnet_qr_dlg(e):
        nonlocal controlnet_qr_help_dlg
        controlnet_qr_help_dlg.open = False
        page.update()
      controlnet_qr_help_dlg = AlertDialog(title=Text("🙅   Help with ControlNet-QRCode"), content=Column([
          Text("These ControlNet models have been trained on a large dataset of 150,000 QR code + QR code artwork couples. They provide a solid foundation for generating QR code-based artwork that is aesthetically pleasing, while still maintaining the integral QR code shape."),
          Text("The Stable Diffusion 2.1 version is marginally more effective, as it was developed to address my specific needs. However, a 1.5 version model was also trained on the same dataset for those who are using the older version. You'll also get great results with QR Code Monster and QR Pattern, with or without SDXL. Experiment..."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🥃  Scantastic... ", on_click=close_controlnet_qr_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(controlnet_qr_help_dlg)
      controlnet_qr_help_dlg.open = True
      page.update()
    prompt = TextField(label="Prompt Text", value=controlnet_qr_prefs['prompt'], filled=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=controlnet_qr_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    def toggle_image(e):
        controlnet_qr_prefs['use_image'] = e.control.value
        qr_generator.height = None if not e.control.value else 0
        qr_generator.update()
        qr_content.visible = not e.control.value
        qr_content.update()
        ref_image.show = e.control.value
        ref_image.update()
    def change_mode(e):
        controlnet_qr_prefs['selected_mode'] = e.data
        image_mode = e.data.split('"')[1] == "image"
        qr_generator.height = None if not image_mode else 0
        qr_generator.update()
        qr_content.visible = not image_mode
        qr_content.update()
        ref_image.show = image_mode
        ref_image.update()
    def change_version(e):
        changed(e, 'controlnet_version')
        #controlnet_qr_prefs['controlnet_version'] = e.control.value
        sdxl = 'SDXL' in controlnet_qr_prefs['controlnet_version']
        ip_adapter_model.visible = not sdxl
        ip_adapter_SDXL_model.visible = sdxl
        ip_adapter_model.update()
        ip_adapter_SDXL_model.update()
    selected_mode = ft.SegmentedButton(on_change=change_mode, selected={controlnet_qr_prefs['selected_mode']}, allow_multiple_selection=False,
        segments=[
            ft.Segment(value="link", label=ft.Text("URL Text"), icon=ft.Icon(ft.icons.LINK)),
            ft.Segment(value="image", label=ft.Text("QR Image"), icon=ft.Icon(ft.icons.QR_CODE)),
        ],
    )
    qr_content = TextField(label="QR Code Content (URL or whatever text)", value=controlnet_qr_prefs['qr_content'], expand=True, visible=not controlnet_qr_prefs['use_image'], on_change=lambda e:changed(e,'qr_content'))
    init_image = FileInput(label="Initial Image (optional)", pref=controlnet_qr_prefs, key='init_image', expand=True, page=page)
    ref_image = FileInput(label="ControlNet QR Code Image", pref=controlnet_qr_prefs, key='ref_image', expand=True, visible=controlnet_qr_prefs['use_image'], page=page)
    use_image = Switcher(label="Use QR Image Instead", value=controlnet_qr_prefs['use_image'], tooltip="Provide your own QR Code made elsewhere, or generate here.", on_change=toggle_image)
    #use_image = Switcher(label="Use QR Image Instead", value=controlnet_qr_prefs['use_image'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_image)
    border_thickness = SliderRow(label="Border Thickness", min=0, max=20, divisions=20, round=0, pref=controlnet_qr_prefs, key='border_thickness', tooltip="The border is equal to the thickness of 5 tiny black boxes around QR.")
    qr_generator = Container(content=Column([border_thickness], alignment=MainAxisAlignment.START), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    qr_generator.height = None if not controlnet_qr_prefs['use_image'] else 0
    controlnet_version = Dropdown(label="ControlNet QRCode Version", width=250, options=[dropdown.Option("Stable Diffusion 2.1"), dropdown.Option("Stable Diffusion 1.5"), dropdown.Option("QR Code Monster v2 1.5"), dropdown.Option("QR Pattern v2 1.5"), dropdown.Option("SDXL QR Code Monster v1"), dropdown.Option("SDXL QR Pattern"), dropdown.Option("SDXL QR Pattern LLLite"), dropdown.Option("SDXL Canny"), dropdown.Option("SDXL Scribble")], value=controlnet_qr_prefs['controlnet_version'], on_change=change_version)
    seed = TextField(label="Seed", width=90, value=str(controlnet_qr_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=controlnet_qr_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=controlnet_qr_prefs, key='guidance_scale')
    conditioning_scale = SliderRow(label="Conditioning Scale", min=0.0, max=5.0, divisions=50, round=1, pref=controlnet_qr_prefs, key='conditioning_scale', tooltip="Strength of the ControlNet Mask.")
    control_guidance_start = SliderRow(label="Control Guidance Start", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_qr_prefs, key='control_guidance_start', tooltip="The percentage of total steps at which the controlnet starts applying.")
    control_guidance_end = SliderRow(label="Control Guidance End", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_qr_prefs, key='control_guidance_end', tooltip="The percentage of total steps at which the controlnet stops applying.")
    strength = SliderRow(label="Init Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_qr_prefs, key='strength', tooltip="How strong the Initial Image should be over the ControlNet. Higher value give less influence.")
    max_size = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_qr_prefs, key='max_size')
    def toggle_ip_adapter(e):
        controlnet_qr_prefs['use_ip_adapter'] = e.control.value
        ip_adapter_container.height = None if e.control.value else 0
        ip_adapter_container.update()
        sdxl = 'SDXL' in controlnet_qr_prefs['controlnet_version']
        ip_adapter_model.visible = e.control.value and not sdxl
        ip_adapter_model.update()
        ip_adapter_SDXL_model.visible = e.control.value and sdxl
        ip_adapter_SDXL_model.update()
    use_ip_adapter = Switcher(label="Use IP-Adapter Reference Image", value=controlnet_qr_prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip="Uses both image and text to condition the image generation process.")
    ip_adapter_model = Dropdown(label="IP-Adapter SD Model", width=220, options=[], value=controlnet_qr_prefs['ip_adapter_model'], visible=controlnet_qr_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))
    for m in ip_adapter_models:
        ip_adapter_model.options.append(dropdown.Option(m['name']))
    ip_adapter_SDXL_model = Dropdown(label="IP-Adapter SDXL Model", width=220, options=[], value=controlnet_qr_prefs['ip_adapter_SDXL_model'], visible=controlnet_qr_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))
    for m in ip_adapter_SDXL_models:
        ip_adapter_SDXL_model.options.append(dropdown.Option(m['name']))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=controlnet_qr_prefs, key='ip_adapter_image', col={'lg':6}, page=page)
    ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_qr_prefs, key='ip_adapter_strength', col={'lg':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    ip_adapter_container = Container(ResponsiveRow([ip_adapter_image, ip_adapter_strength]), height = None if controlnet_qr_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)

    batch_size = NumberPicker(label="Batch Size: ", min=1, max=8, value=controlnet_qr_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    num_images = NumberPicker(label="Number of Iterations: ", min=1, max=12, value=controlnet_qr_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))
    batch_folder_name = TextField(label="Batch Folder Name", value=controlnet_qr_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(controlnet_qr_prefs)
    page.upscalers.append(upscaler)
    page.controlnet_qr_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.controlnet_qr_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🔗  ControlNet QRCode Art Generator", "ControlNet Img2Img for Inpainting QR Code with Prompt and/or Init Image...", actions=[save_default(controlnet_qr_prefs, ['init_image', 'ref_image', 'ip_adapter_image']), IconButton(icon=icons.HELP, tooltip="Help with ControlNetQR Settings", on_click=controlnet_qr_help)]),
        Row([selected_mode, qr_content, ref_image]),
        qr_generator,
        ResponsiveRow([prompt, negative_prompt]),
        Row([controlnet_version, init_image]),
        Row([use_ip_adapter, ip_adapter_model, ip_adapter_SDXL_model], vertical_alignment=CrossAxisAlignment.START),
        ip_adapter_container,
        num_inference_row,
        guidance,
        conditioning_scale,
        strength,
        Row([control_guidance_start, control_guidance_end]),
        max_size,
        #batch_size, 
        ResponsiveRow([Row([num_images], col={'lg':6}), Row([seed, batch_folder_name], col={'lg':6})]),
        upscaler,
        Row([ElevatedButton(content=Text("🧑‍💻️  Make ControlNet QR", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_qr(page)),
             ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_qr(page, from_list=True), tooltip="Runs from global Prompt Queue using Image Parameters for Prompt, Neg Prompt, Steps, Guidance, Init Image, Strength and Seed.")]),
        page.controlnet_qr_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

controlnet_segment_prefs = {
    'ref_image': '',
    'prompt': '',
    'negative_prompt': 'low quality, low resolution, render, oversaturated, low contrast',
    'guidance_scale': 7.5,
    'num_inference_steps': 50,
    'seed': 0,
    'batch_size': 1,
    'num_images': 1,
    'max_size': 768,
    'width': 960,
    'height': 768,
    'last_model': '',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}
def buildControlNetSegmentAnything(page):
    global controlnet_segment_prefs, prefs, pipe_controlnet_segment
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          controlnet_segment_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_controlnet_segment_output(o):
      page.ControlNetSegmentAnything.controls.append(o)
      page.ControlNetSegmentAnything.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_controlnet_segment_output = add_to_controlnet_segment_output
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      for i, c in enumerate(page.ControlNetSegmentAnything.controls):
        if i == 0: continue
        else: del page.ControlNetSegmentAnything.controls[i]
      page.ControlNetSegmentAnything.update()
      clear_button.visible = False
      clear_button.update()
    def controlnet_segment_help(e):
      def close_controlnet_segment_dlg(e):
        nonlocal controlnet_segment_help_dlg
        controlnet_segment_help_dlg.open = False
        page.update()
      controlnet_segment_help_dlg = AlertDialog(title=Text("🙅   Help with ControlNet Segment-Anything"), content=Column([
          Text("This model is based on the ControlNet Model, which allow us to generate Images using some sort of condition image. For this model, we selected the segmentation maps produced by Meta’s new segmentation model called Segment Anything Model as the condition image. We then trained the model to generate images based on the structure of the segmentation maps and the text prompts given."),
          Text("For the training, we generated a segmented dataset based on the COYO-700M dataset. The dataset provided us with the images, and the text prompts. For the segmented images, we used Segment Anything Model. We then created 8k samples to train our model on, which isn’t a lot, but as a team, we have been very busy with many other responsibilities and time constraints, which made it challenging to dedicate a lot of time to generating a larger dataset. Despite the constraints we faced, we have still managed to achieve some nice results 🙌"),
          Markdown("[Project Page](https://segment-anything.com) | [Model Card](https://huggingface.co/mfidabel/controlnet-segment-anything) | [Segment-Anything Code](https://github.com/facebookresearch/segment-anything) | [HuggingFace Demo](https://huggingface.co/spaces/mfidabel/controlnet-segment-anything)", on_tap_link=lambda e: e.page.launch_url(e.data)),

        ], scroll=ScrollMode.AUTO), actions=[TextButton("🧩  Puzzle the Pieces... ", on_click=close_controlnet_segment_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(controlnet_segment_help_dlg)
      controlnet_segment_help_dlg.open = True
      page.update()
    prompt = TextField(label="Prompt Text", value=controlnet_segment_prefs['prompt'], filled=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=controlnet_segment_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    ref_image = FileInput(label="Initial Image to Segment", pref=controlnet_segment_prefs, key='ref_image', page=page)
    seed = TextField(label="Seed", width=90, value=str(controlnet_segment_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=controlnet_segment_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=controlnet_segment_prefs, key='guidance_scale')
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_segment_prefs, key='max_size')
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_segment_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_segment_prefs, key='height')
    batch_size = NumberPicker(label="Batch Size: ", min=1, max=8, value=controlnet_segment_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    num_images = NumberPicker(label="Number of Iterations: ", min=1, max=12, value=controlnet_segment_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))
    batch_folder_name = TextField(label="Batch Folder Name", value=controlnet_segment_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(controlnet_segment_prefs)
    page.upscalers.append(upscaler)
    page.controlnet_segment_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.controlnet_segment_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🥸  ControlNet on Meta's Segment-Anything", "Upload an Image, Segment it with Segment Anything, write a prompt, and generate images...", actions=[save_default(controlnet_segment_prefs, ['ref_image']), IconButton(icon=icons.HELP, tooltip="Help with ControlNet Segment Anything Settings", on_click=controlnet_segment_help)]),
        ref_image,
        ResponsiveRow([prompt, negative_prompt]),
        num_inference_row,
        guidance,
        max_row,
        #width_slider,
        #height_slider,
        ResponsiveRow([Row([batch_size, num_images], col={'lg':6}), Row([seed, batch_folder_name], col={'lg':6})]),
        upscaler,
        Row([ElevatedButton(content=Text("👹  Make ControlNet Segments", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_segment(page)),
             ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_segment(page, from_list=True))]),
        page.controlnet_segment_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

EDICT_prefs = {
    'target_prompt': '',
    'negative_prompt': '',
    'base_prompt': '',
    'init_image': '',
    'guidance_scale': 3.0,
    'num_inference_steps': 50,
    'strength': 0.8,
    'seed': 0,
    'num_images': 1,
    'batch_folder_name': '',
    'max_size': 512,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": False,
}

def buildEDICT(page):
    global EDICT_prefs, prefs, pipe_EDICT
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          EDICT_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_EDICT_output(o):
      page.EDICT_output.controls.append(o)
      page.EDICT_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_EDICT_output = add_to_EDICT_output
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.EDICT_output.controls = []
      page.EDICT_output.update()
      clear_button.visible = False
      clear_button.update()
    def EDICT_help(e):
      def close_EDICT_dlg(e):
        nonlocal EDICT_help_dlg
        EDICT_help_dlg.open = False
        page.update()
      EDICT_help_dlg = AlertDialog(title=Text("🙅   Help with EDICT Edit"), content=Column([
          Text("EDICT: Exact Diffusion Inversion via Coupled Transformations"),
          Text("Using the iterative denoising diffusion principle, denoising diffusion models (DDMs) trained with web-scale data can generate highly realistic images conditioned on input text, layouts, and scene graphs. After image generation, the next important application of DDMs being explored by the research community is that of image editing. Models such as DALL-E-2 and Stable Diffusion [24] can perform inpainting, allowing users to edit images through manual annotation. Methods such as SDEdit have demonstrated that both synthetic and real images can be edited using stroke or composite guidance via DDMs. However, the goal of a holistic image editing tool that can edit any real/artificial image using purely text has still not been achieved, until now."),
          Markdown("[Read Arxiv Paper](https://arxiv.org/pdf/2211.12446.pdf)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🥴  Not Complicated... ", on_click=close_EDICT_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(EDICT_help_dlg)
      EDICT_help_dlg.open = True
      page.update()
    base_prompt = TextField(label="Base Prompt Text (describe init image)", value=EDICT_prefs['base_prompt'], col={'md': 12}, multiline=True, on_change=lambda e:changed(e,'base_prompt'))
    target_prompt = TextField(label="Target Prompt Text", value=EDICT_prefs['target_prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'target_prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=EDICT_prefs['negative_prompt'], filled=True, col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = FileInput(label="Initial Image to Edit (crops square)", pref=EDICT_prefs, key='init_image', page=page)
    seed = TextField(label="Seed", width=90, value=str(EDICT_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=EDICT_prefs, key='guidance_scale')
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=EDICT_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    strength = SliderRow(label="Strength", min=0, max=1, divisions=20, round=2, pref=EDICT_prefs, key='strength')
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=EDICT_prefs, key='max_size')
    batch_folder_name = TextField(label="Batch Folder Name", value=EDICT_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(EDICT_prefs)
    page.upscalers.append(upscaler)
    page.EDICT_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.EDICT_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🤹  EDICT Image Editing", "Diffusion pipeline for text-guided image editing... Exact Diffusion Inversion via Coupled Transformations.", actions=[save_default(EDICT_prefs, ['init_image']), IconButton(icon=icons.HELP, tooltip="Help with Image Variation Settings", on_click=EDICT_help)]),
        init_image,
        base_prompt,
        ResponsiveRow([target_prompt, negative_prompt]),
        #Row([init_image, mask_image, invert_mask]),
        num_inference_row,
        guidance,
        strength,
        max_row,
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=EDICT_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        upscaler,
        ElevatedButton(content=Text("🧝  Run EDICT Edit", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_EDICT(page)),
        page.EDICT_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

DiffEdit_prefs = {
    'target_prompt': '',
    'negative_prompt': '',
    'source_prompt': '',
    'init_image': '',
    'guidance_scale': 3.0,
    'num_inference_steps': 50,
    'strength': 0.8,
    'seed': 0,
    'num_images': 1,
    'batch_folder_name': '',
    'max_size': 768,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": False,
}

def buildDiffEdit(page):
    global DiffEdit_prefs, prefs, pipe_DiffEdit
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          DiffEdit_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_DiffEdit_output(o):
      page.DiffEdit_output.controls.append(o)
      page.DiffEdit_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_DiffEdit_output = add_to_DiffEdit_output
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.DiffEdit_output.controls = []
      page.DiffEdit_output.update()
      clear_button.visible = False
      clear_button.update()
    def DiffEdit_help(e):
      def close_DiffEdit_dlg(e):
        nonlocal DiffEdit_help_dlg
        DiffEdit_help_dlg.open = False
        page.update()
      DiffEdit_help_dlg = AlertDialog(title=Text("🙅   Help with DiffEdit"), content=Column([
          Text("Image generation has recently seen tremendous advances, with diffusion models allowing to synthesize convincing images for a large variety of text prompts. In this article, we propose DiffEdit, a method to take advantage of text-conditioned diffusion models for the task of semantic image editing, where the goal is to edit an image based on a text query. Semantic image editing is an extension of image generation, with the additional constraint that the generated image should be as similar as possible to a given input image. Current editing methods based on diffusion models usually require to provide a mask, making the task much easier by treating it as a conditional inpainting task. In contrast, our main contribution is able to automatically generate a mask highlighting regions of the input image that need to be edited, by contrasting predictions of a diffusion model conditioned on different text prompts. Moreover, we rely on latent inference to preserve content in those regions of interest and show excellent synergies with mask-based diffusion. DiffEdit achieves state-of-the-art editing performance on ImageNet. In addition, we evaluate semantic image editing in more challenging settings, using images from the COCO dataset as well as text-based generated images."),
          #Text(""),
          Markdown("[Read Arxiv Paper](https://arxiv.org/abs/2210.11427) - [Blog Post with Demo](https://blog.problemsolversguild.com/technical/research/2022/11/02/DiffEdit-Implementation.html) - [GitHub](https://github.com/Xiang-cd/DiffEdit-stable-diffusion/)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😇  Not Difficult... ", on_click=close_DiffEdit_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(DiffEdit_help_dlg)
      DiffEdit_help_dlg.open = True
      page.update()
    source_prompt = TextField(label="Source Prompt Text (blank to auto-caption)", value=DiffEdit_prefs['source_prompt'], col={'md': 12}, multiline=True, on_change=lambda e:changed(e,'source_prompt'))
    target_prompt = TextField(label="Target Prompt Text (describe edits)", value=DiffEdit_prefs['target_prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'target_prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=DiffEdit_prefs['negative_prompt'], filled=True, col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = FileInput(label="Initial Image to Edit (crops square)", pref=DiffEdit_prefs, key='init_image', page=page)
    seed = TextField(label="Seed", width=90, value=str(DiffEdit_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=DiffEdit_prefs, key='guidance_scale')
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=DiffEdit_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    strength = SliderRow(label="Strength", min=0, max=1, divisions=20, round=2, pref=DiffEdit_prefs, key='strength')
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=DiffEdit_prefs, key='max_size')
    batch_folder_name = TextField(label="Batch Folder Name", value=DiffEdit_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(DiffEdit_prefs)
    page.upscalers.append(upscaler)
    page.DiffEdit_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.DiffEdit_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🍒  DiffEdit Image Editing", "Zero-shot Diffusion-based Semantic Image Editing with Mask Guidance...", actions=[save_default(DiffEdit_prefs, ['init_image']), IconButton(icon=icons.HELP, tooltip="Help with Image Variation Settings", on_click=DiffEdit_help)]),
        init_image,
        source_prompt,
        ResponsiveRow([target_prompt, negative_prompt]),
        #Row([init_image, mask_image, invert_mask]),
        num_inference_row,
        guidance,
        strength,
        max_row,
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=DiffEdit_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        upscaler,
        ElevatedButton(content=Text("😃  Run DiffEdit", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_DiffEdit(page)),
        page.DiffEdit_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

null_text_prefs = {
    'target_prompt': '',
    'negative_prompt': '',
    'base_prompt': '',
    'init_image': '',
    'guidance_scale': 7.5,
    'num_train_timesteps': 1000,
    'num_inference_steps': 50,
    'num_inner_steps': 10,
    'seed': 0,
    'num_images': 1,
    'batch_folder_name': '',
    'max_size': 1024 if prefs['higher_vram_mode'] else 720,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildNull_Text(page):
    global null_text_prefs, prefs, pipe_null_text
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          null_text_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def null_text_help(e):
      def close_null_text_dlg(e):
        nonlocal null_text_help_dlg
        null_text_help_dlg.open = False
        page.update()
      null_text_help_dlg = AlertDialog(title=Text("🙅   Help with Null-Text Inversion"), content=Column([
          Text("This pipeline provides null-text inversion for editing real images. It enables null-text optimization, and DDIM reconstruction via w, w/o null-text optimization. No prompt-to-prompt code is implemented as there is a Prompt2PromptPipeline."),
          Text("Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. With this pipeline, we pursue an intuitive prompt-toprompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts."),
          Markdown("[Read Arxiv Paper](https://arxiv.org/pdf/2208.01626.pdf) | [Null-Text Inversion](https://github.com/google/prompt-to-prompt/) | [Junsheng Luan](https://github.com/Junsheng121)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Credits go to Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel and HuggingFace team."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🙃  Edit Away... ", on_click=close_null_text_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(null_text_help_dlg)
      null_text_help_dlg.open = True
      page.update()
    base_prompt = TextField(label="Inverted Prompt Text (describe init-image)", value=null_text_prefs['base_prompt'], col={'md': 12}, multiline=True, on_change=lambda e:changed(e,'base_prompt'))
    target_prompt = TextField(label="Target Edited Prompt Text", value=null_text_prefs['target_prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'target_prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=null_text_prefs['negative_prompt'], filled=True, col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = FileInput(label="Initial Image to Edit", pref=null_text_prefs, key='init_image', page=page)
    #init_image = TextField(label="Initial Image to Edit", value=null_text_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    seed = TextField(label="Seed", width=90, value=str(null_text_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=null_text_prefs, key='guidance_scale')
    num_train_timesteps = SliderRow(label="Number of Train Timesteps", min=1, max=2000, divisions=1999, pref=null_text_prefs, key='num_train_timesteps', tooltip="Length of the Denoising process, or the Number of Timesteps required to process random Gaussian noise into a data sample.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=null_text_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    num_inner_steps = SliderRow(label="Number of Inner Steps", min=1, max=100, divisions=99, pref=null_text_prefs, key='num_inner_steps', tooltip="The number of steps to Invert init-image in the Null Optimization.")
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=null_text_prefs, key='max_size')
    batch_folder_name = TextField(label="Batch Folder Name", value=null_text_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(null_text_prefs)
    page.upscalers.append(upscaler)
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("😑  Null-Text Inversion Image Editing", "Editing Real Images using Guided Diffusion Models... Exact Diffusion Inversion via Coupled Transformations. Prompt-to-prompt image editing with cross attention control.", actions=[save_default(null_text_prefs, ['init_image']), IconButton(icon=icons.HELP, tooltip="Help with Image Variation Settings", on_click=null_text_help)]),
        init_image,
        base_prompt,
        ResponsiveRow([target_prompt, negative_prompt]),
        #Row([init_image, mask_image, invert_mask]),
        num_train_timesteps,
        num_inner_steps,
        num_inference_row,
        guidance,
        #strength,
        max_row,
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=null_text_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        upscaler,
        ElevatedButton(content=Text("⭕  Run Null-Text Inversion", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_null_text(page)),
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

unCLIP_prefs = {
    'prompt': '',
    'batch_folder_name': '',
    'prior_guidance_scale': 4.0,
    'decoder_guidance_scale': 8.0,
    'prior_num_inference_steps': 25,
    'decoder_num_inference_steps': 25,
    'super_res_num_inference_steps': 7,
    'seed': 0,
    'num_images': 1,
    'use_StableUnCLIP_pipeline': False,
    #'variance_type': 'learned_range',#fixed_small_log
    #'num_train_timesteps': 1000,
    #'prediction_type': 'epsilon',#sample
    #'clip_sample': True,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": True,
}
def buildUnCLIP(page):
    global unCLIP_prefs, prefs, pipe_unCLIP
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          unCLIP_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_unCLIP_output(o):
      page.unCLIP_output.controls.append(o)
      page.unCLIP_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_unCLIP_output = add_to_unCLIP_output
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.unCLIP_output.controls = []
      page.unCLIP_output.update()
      clear_button.visible = False
      clear_button.update()
    def unCLIP_help(e):
      def close_unCLIP_dlg(e):
        nonlocal unCLIP_help_dlg
        unCLIP_help_dlg.open = False
        page.update()
      unCLIP_help_dlg = AlertDialog(title=Text("🙅   Help with unCLIP Pipeline"), content=Column([
          Text("Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we implemented a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples."),
          Text("The scheduler is a modified DDPM that has some minor variations in how it calculates the learned range variance and dynamically re-calculates betas based off the timesteps it is skipping. The scheduler also uses a slightly different step ratio when computing timesteps to use for inference."),
          Markdown("The unCLIP model in diffusers comes from kakaobrain's karlo and the original codebase can be found [here](https://github.com/kakaobrain/karlo). Additionally, lucidrains has a DALL-E 2 recreation [here](https://github.com/lucidrains/DALLE2-pytorch)."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😕  Tricky... ", on_click=close_unCLIP_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(unCLIP_help_dlg)
      unCLIP_help_dlg.open = True
      page.update()
    def toggle_unCLIP(e):
        unCLIP_prefs['use_StableUnCLIP_pipeline'] = e.control.value
        if unCLIP_prefs['use_StableUnCLIP_pipeline']:
            decoder_num_inference_row.visible = False
            super_res_num_inference_row.visible = False
            decoder_guidance.visible = False
            decoder_num_inference_row.update()
            super_res_num_inference_row.update()
            decoder_guidance.update()
        else:
            decoder_num_inference_row.visible = True
            super_res_num_inference_row.visible = True
            decoder_guidance.visible = True
            decoder_guidance.update()
            super_res_num_inference_row.update()
            decoder_num_inference_row.update()
    prompt = TextField(label="Prompt Text", value=unCLIP_prefs['prompt'], filled=True, on_change=lambda e:changed(e,'prompt'))
    seed = TextField(label="Seed", width=90, value=str(unCLIP_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    prior_num_inference_row = SliderRow(label="Number of Prior Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_prefs, key='prior_num_inference_steps', tooltip="The number of Prior denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    decoder_num_inference_row = SliderRow(label="Number of Decoder Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_prefs, key='decoder_num_inference_steps', tooltip="The number of Decoder denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    super_res_num_inference_row = SliderRow(label="Number of Super-Res Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_prefs, key='decoder_num_inference_steps', tooltip="The number of Super-Res denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    prior_guidance = SliderRow(label="Prior Guidance Scale", min=0, max=50, divisions=100, round=1, pref=unCLIP_prefs, key='prior_guidance_scale')
    decoder_guidance = SliderRow(label="Decoder Guidance Scale", min=0, max=50, divisions=100, round=1, pref=unCLIP_prefs, key='decoder_guidance_scale')
    batch_folder_name = TextField(label="Batch Folder Name", value=unCLIP_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    use_StableUnCLIP_pipeline = Tooltip(message="Combines prior model (generate clip image embedding from text, UnCLIPPipeline) and decoder pipeline (decode clip image embedding to image, StableDiffusionImageVariationPipeline)", content=Switcher(label="Use Stable UnCLIP Pipeline as Primary", value=unCLIP_prefs['use_StableUnCLIP_pipeline'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_unCLIP))
    #eta = TextField(label="ETA", value=str(unCLIP_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(unCLIP_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta_row = Row([Text("DDIM ETA: "), eta])
    #max_size = Slider(min=256, max=1280, divisions=64, label="{value}px", value=int(unCLIP_prefs['max_size']), expand=True, on_change=lambda e:changed(e,'max_size', ptype='int'))
    #max_row = Row([Text("Max Resolution Size: "), max_size])
    upscaler = UpscaleBlock(unCLIP_prefs)
    page.upscalers.append(upscaler)
    page.unCLIP_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.unCLIP_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🌐  unCLIP Text-to-Image Generator", "Hierarchical Text-Conditional Image Generation with CLIP Latents.  Similar results to DALL-E 2...", actions=[save_default(unCLIP_prefs, []), IconButton(icon=icons.HELP, tooltip="Help with unCLIP Settings", on_click=unCLIP_help)]),
        prompt,
        #Row([prompt, mask_image, invert_mask]),
        prior_num_inference_row, decoder_num_inference_row, super_res_num_inference_row,
        prior_guidance, decoder_guidance,
        #eta_row, max_row,
        use_StableUnCLIP_pipeline,
        Row([NumberPicker(label="Number of Images: ", min=1, max=20, value=unCLIP_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        upscaler,
        Row([ElevatedButton(content=Text("🖇️   Get unCLIP Generation", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP(page)),
             ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP(page, from_list=True))]),
        page.unCLIP_output,
        clear_button,
      ]
    )),
    ], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

unCLIP_image_variation_prefs = {
    'init_image': '',
    'file_name': '',
    'batch_folder_name': '',
    'decoder_guidance_scale': 8.0,
    'decoder_num_inference_steps': 25,
    'super_res_num_inference_steps': 7,
    'seed': 0,
    'num_images': 1,
    'max_size': 768,
    #'variance_type': 'learned_range',#fixed_small_log
    #'num_train_timesteps': 1000,
    #'prediction_type': 'epsilon',#sample
    #'clip_sample': True,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": True,
}
def buildUnCLIP_ImageVariation(page):
    global unCLIP_image_variation_prefs, prefs, pipe_unCLIP_image_variation
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          unCLIP_image_variation_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_unCLIP_image_variation_output(o):
      page.unCLIP_image_variation_output.controls.append(o)
      page.unCLIP_image_variation_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_unCLIP_image_variation_output = add_to_unCLIP_image_variation_output
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.unCLIP_image_variation_output.controls = []
      page.unCLIP_image_variation_output.update()
      clear_button.visible = False
      clear_button.update()
    def unCLIP_image_variation_help(e):
      def close_unCLIP_image_variation_dlg(e):
        nonlocal unCLIP_image_variation_help_dlg
        unCLIP_image_variation_help_dlg.open = False
        page.update()
      unCLIP_image_variation_help_dlg = AlertDialog(title=Text("🙅   Help with unCLIP Image Variation Pipeline"), content=Column([
          Text("Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we implemented a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples."),
          Text("The scheduler is a modified DDPM that has some minor variations in how it calculates the learned range variance and dynamically re-calculates betas based off the timesteps it is skipping. The scheduler also uses a slightly different step ratio when computing timesteps to use for inference."),
          Markdown("The unCLIP Image Variation model in diffusers comes from kakaobrain's karlo and the original codebase can be found [here](https://github.com/kakaobrain/karlo). Additionally, lucidrains has a DALL-E 2 recreation [here](https://github.com/lucidrains/DALLE2-pytorch)."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🐇  We'll see... ", on_click=close_unCLIP_image_variation_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(unCLIP_image_variation_help_dlg)
      unCLIP_image_variation_help_dlg.open = True
      page.update()
    init_image = FileInput(label="Initial Image", pref=unCLIP_image_variation_prefs, key='init_image', page=page)
    #prompt = TextField(label="Prompt Text", value=unCLIP_image_variation_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))
    seed = TextField(label="Seed", width=90, value=str(unCLIP_image_variation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    decoder_num_inference_row = SliderRow(label="Number of Decoder Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_image_variation_prefs, key='decoder_num_inference_steps', tooltip="The number of Decoder denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    super_res_num_inference_row = SliderRow(label="Number of Super-Res Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_image_variation_prefs, key='decoder_num_inference_steps', tooltip="The number of Super-Res denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    decoder_guidance = SliderRow(label="Decoder Guidance Scale", min=0, max=50, divisions=100, round=1, pref=unCLIP_image_variation_prefs, key='decoder_guidance_scale')
    batch_folder_name = TextField(label="Batch Folder Name", value=unCLIP_image_variation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    #eta = TextField(label="ETA", value=str(unCLIP_image_variation_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(unCLIP_image_variation_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta_row = Row([Text("DDIM ETA: "), eta])
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=unCLIP_image_variation_prefs, key='max_size')
    upscaler = UpscaleBlock(unCLIP_image_variation_prefs)
    page.upscalers.append(upscaler)
    page.unCLIP_image_variation_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.unCLIP_image_variation_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎆  unCLIP Image Variation Generator", "Generate Variations from an input image using unCLIP...", actions=[save_default(unCLIP_image_variation_prefs, ['init_image']), IconButton(icon=icons.HELP, tooltip="Help with unCLIP Image Variation Settings", on_click=unCLIP_image_variation_help)]),
        init_image,
        #Row([prompt, mask_image, invert_mask]),
        decoder_num_inference_row, super_res_num_inference_row,
        decoder_guidance,
        #eta_row,
        max_row,
        Row([NumberPicker(label="Number of Images: ", min=1, max=20, value=unCLIP_image_variation_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        upscaler,
        Row([ElevatedButton(content=Text("🦄   Get unCLIP Image Variation", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_image_variation(page)),
             ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_image_variation(page, from_list=True))]),
        page.unCLIP_image_variation_output,
        clear_button,
      ]
    )),
    ], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

unCLIP_interpolation_prefs = {
    'prompt': '',
    'end_prompt': '',
    'steps': 5,
    'batch_folder_name': '',
    'prior_guidance_scale': 4.0,
    'decoder_guidance_scale': 8.0,
    'prior_num_inference_steps': 25,
    'decoder_num_inference_steps': 25,
    'super_res_num_inference_steps': 7,
    'seed': 0,
    'num_images': 1,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": True,
}
def buildUnCLIP_Interpolation(page):
    global unCLIP_interpolation_prefs, prefs, pipe_unCLIP_interpolation
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          unCLIP_interpolation_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_unCLIP_interpolation_output(o):
      page.unCLIP_interpolation_output.controls.append(o)
      page.unCLIP_interpolation_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_unCLIP_interpolation_output = add_to_unCLIP_interpolation_output
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.unCLIP_interpolation_output.controls = []
      page.unCLIP_interpolation_output.update()
      clear_button.visible = False
      clear_button.update()
    def unCLIP_interpolation_help(e):
      def close_unCLIP_interpolation_dlg(e):
        nonlocal unCLIP_interpolation_help_dlg
        unCLIP_interpolation_help_dlg.open = False
        page.update()
      unCLIP_interpolation_help_dlg = AlertDialog(title=Text("🙅   Help with unCLIP Text Interpolation Pipeline"), content=Column([
          Text("This Diffusion Pipeline takes two prompts and interpolates between the two input prompts using spherical interpolation ( slerp ). The input prompts are converted to text embeddings by the pipeline's text_encoder and the interpolation is done on the resulting text_embeddings over the number of steps specified. Defaults to 5 steps."),
          Text("Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we implemented a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples."),
          Text("The scheduler is a modified DDPM that has some minor variations in how it calculates the learned range variance and dynamically re-calculates betas based off the timesteps it is skipping. The scheduler also uses a slightly different step ratio when computing timesteps to use for inference."),
          Markdown("The unCLIP model in diffusers comes from kakaobrain's karlo and the original codebase can be found [here](https://github.com/kakaobrain/karlo). Additionally, lucidrains has a DALL-E 2 recreation [here](https://github.com/lucidrains/DALLE2-pytorch)."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😕  Transformative... ", on_click=close_unCLIP_interpolation_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(unCLIP_interpolation_help_dlg)
      unCLIP_interpolation_help_dlg.open = True
      page.update()
    prompt = TextField(label="Start Prompt Text", value=unCLIP_interpolation_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))
    end_prompt = TextField(label="End Prompt Text", value=unCLIP_interpolation_prefs['end_prompt'], on_change=lambda e:changed(e,'end_prompt'))
    seed = TextField(label="Seed", width=90, value=str(unCLIP_interpolation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    interpolation_steps_row = SliderRow(label="Number of Interpolation Step", min=1, max=50, divisions=49, pref=unCLIP_interpolation_prefs, key='steps', tooltip="The number of steps over which to interpolate from start_prompt to end_prompt.")
    prior_num_inference_row = SliderRow(label="Number of Prior Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_interpolation_prefs, key='prior_num_inference_steps', tooltip="The number of Prior denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    decoder_num_inference_row = SliderRow(label="Number of Decoder Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_interpolation_prefs, key='decoder_num_inference_steps', tooltip="The number of Decoder denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    super_res_num_inference_row = SliderRow(label="Number of Super-Res Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_interpolation_prefs, key='decoder_num_inference_steps', tooltip="The number of Super-Res denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    prior_guidance = SliderRow(label="Prior Guidance Scale", min=0, max=50, divisions=100, round=1, pref=unCLIP_interpolation_prefs, key='prior_guidance_scale')
    decoder_guidance = SliderRow(label="Decoder Guidance Scale", min=0, max=50, divisions=100, round=1, pref=unCLIP_interpolation_prefs, key='decoder_guidance_scale')
    batch_folder_name = TextField(label="Batch Folder Name", value=unCLIP_interpolation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    #use_StableunCLIP_interpolation_pipeline = Tooltip(message="Combines prior model (generate clip image embedding from text, UnCLIPPipeline) and decoder pipeline (decode clip image embedding to image, StableDiffusionImageVariationPipeline)", content=Switcher(label="Use Stable UnCLIP Pipeline Instead", value=unCLIP_interpolation_prefs['use_StableunCLIP_interpolation_pipeline'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_StableunCLIP_interpolation_pipeline')))
    #eta = TextField(label="ETA", value=str(unCLIP_interpolation_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(unCLIP_interpolation_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta_row = Row([Text("DDIM ETA: "), eta])
    #max_size = Slider(min=256, max=1280, divisions=64, label="{value}px", value=int(unCLIP_interpolation_prefs['max_size']), expand=True, on_change=lambda e:changed(e,'max_size', ptype='int'))
    #max_row = Row([Text("Max Resolution Size: "), max_size])
    upscaler = UpscaleBlock(unCLIP_interpolation_prefs)
    page.upscalers.append(upscaler)
    page.unCLIP_interpolation_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.unCLIP_interpolation_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🌌  unCLIP Text Interpolation Generator", "Takes two prompts and interpolates between the two input prompts using spherical interpolation...", actions=[save_default(unCLIP_interpolation_prefs), IconButton(icon=icons.HELP, tooltip="Help with unCLIP Settings", on_click=unCLIP_interpolation_help)]),
        prompt,
        end_prompt,
        #Row([prompt, mask_image, invert_mask]),
        interpolation_steps_row,
        prior_num_inference_row, decoder_num_inference_row, super_res_num_inference_row,
        prior_guidance, decoder_guidance,
        #eta_row, max_row,
        #use_StableunCLIP_interpolation_pipeline,
        #NumberPicker(label="Number of Images: ", min=1, max=20, value=unCLIP_interpolation_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')),
        Row([seed, batch_folder_name]),
        upscaler,
        Row([ElevatedButton(content=Text("🎆   Get unCLIP Interpolations", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_interpolation(page))]),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_interpolation(page, from_list=True))]),
        page.unCLIP_interpolation_output,
        clear_button,
      ]
    )),
    ], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

unCLIP_image_interpolation_prefs = {
    'init_image': '',
    'end_image': '',
    'file_name': '',
    'batch_folder_name': '',
    'interpolation_steps': 6,
    'decoder_guidance_scale': 8.0,
    'decoder_num_inference_steps': 25,
    'super_res_num_inference_steps': 7,
    'seed': 0,
    'num_images': 1,
    'max_size': 768,
    #'variance_type': 'learned_range',#fixed_small_log
    #'num_train_timesteps': 1000,
    #'prediction_type': 'epsilon',#sample
    #'clip_sample': True,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": True,
}
def buildUnCLIP_ImageInterpolation(page):
    global unCLIP_image_interpolation_prefs, prefs, pipe_unCLIP_image_interpolation
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          unCLIP_image_interpolation_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_unCLIP_image_interpolation_output(o):
      page.unCLIP_image_interpolation_output.controls.append(o)
      page.unCLIP_image_interpolation_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_unCLIP_image_interpolation_output = add_to_unCLIP_image_interpolation_output
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.unCLIP_image_interpolation_output.controls = []
      page.unCLIP_image_interpolation_output.update()
      clear_button.visible = False
      clear_button.update()
    def unCLIP_image_interpolation_help(e):
      def close_unCLIP_image_interpolation_dlg(e):
        nonlocal unCLIP_image_interpolation_help_dlg
        unCLIP_image_interpolation_help_dlg.open = False
        page.update()
      unCLIP_image_interpolation_help_dlg = AlertDialog(title=Text("🙅   Help with unCLIP Image Interpolation Pipeline"), content=Column([
          Text("This Diffusion Pipeline takes two images or an image_embeddings tensor of size 2 and interpolates between their embeddings using spherical interpolation ( slerp ). The input images/image_embeddings are converted to image embeddings by the pipeline's image_encoder and the interpolation is done on the resulting image_embeddings over the number of steps specified."),
          #Text(""),
          #Markdown("The unCLIP Image Interpolation model in diffusers comes from kakaobrain's karlo and the original codebase can be found [here](https://github.com/kakaobrain/karlo). Additionally, lucidrains has a DALL-E 2 recreation [here](https://github.com/lucidrains/DALLE2-pytorch)."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🦿  Transformers Activate... ", on_click=close_unCLIP_image_interpolation_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(unCLIP_image_interpolation_help_dlg)
      unCLIP_image_interpolation_help_dlg.open = True
      page.update()
    init_image = FileInput(label="Initial Image", pref=unCLIP_image_interpolation_prefs, key='init_image', page=page)
    end_image = FileInput(label="Ending Image", pref=unCLIP_image_interpolation_prefs, key='end_image', page=page)
    #prompt = TextField(label="Prompt Text", value=unCLIP_image_interpolation_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))
    seed = TextField(label="Seed", width=90, value=str(unCLIP_image_interpolation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    interpolation_steps = SliderRow(label="Interpolation Frames", min=1, max=100, divisions=99, pref=unCLIP_image_interpolation_prefs, key='interpolation_steps', tooltip="The number of interpolation images to generate.")
    decoder_num_inference_row = SliderRow(label="Number of Decoder Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_image_interpolation_prefs, key='decoder_num_inference_steps', tooltip="The number of Decoder denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    super_res_num_inference_row = SliderRow(label="Number of Super-Res Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_image_interpolation_prefs, key='decoder_num_inference_steps', tooltip="The number of Super-Res denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    decoder_guidance = SliderRow(label="Decoder Guidance Scale", min=0, max=50, divisions=100, round=1, pref=unCLIP_image_interpolation_prefs, key='decoder_guidance_scale')
    batch_folder_name = TextField(label="Batch Folder Name", value=unCLIP_image_interpolation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    #eta = TextField(label="ETA", value=str(unCLIP_image_interpolation_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(unCLIP_image_interpolation_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta_row = Row([Text("DDIM ETA: "), eta])
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=unCLIP_image_interpolation_prefs, key='max_size')
    upscaler = UpscaleBlock(unCLIP_image_interpolation_prefs)
    page.upscalers.append(upscaler)
    page.unCLIP_image_interpolation_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.unCLIP_image_interpolation_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🤖  unCLIP Image Interpolation Generator", "Pass two images and produces in-betweens while interpolating between their image-embeddings...", actions=[save_default(unCLIP_interpolation_prefs, ['init_image', 'end_image']), IconButton(icon=icons.HELP, tooltip="Help with unCLIP Image Interpolation Settings", on_click=unCLIP_image_interpolation_help)]),
        init_image, end_image,
        interpolation_steps,
        #Row([prompt, mask_image, invert_mask]),
        decoder_num_inference_row, super_res_num_inference_row,
        decoder_guidance,
        #eta_row,
        max_row,
        Row([NumberPicker(label="Number of Images: ", min=1, max=20, value=unCLIP_image_interpolation_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        upscaler,
        Row([ElevatedButton(content=Text("🦾   Get unCLIP Image Interpolation", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_image_interpolation(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_image_interpolation(page, from_list=True))
             ]),

      ]
    )), page.unCLIP_image_interpolation_output,
        clear_button,
    ], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c


magic_mix_prefs = {
    'init_image': '',
    'prompt': '',
    'guidance_scale': 7.5,
    'num_inference_steps': 50,
    'mix_factor': 0.5,
    'kmin': 0.3,
    'kmax': 0.6,
    'seed': 0,
    'num_images': 1,
    'max_size': 1024,
    'scheduler_mode': 'DDIM',
    'scheduler_last': '',
    'batch_folder_name': '',
    'file_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": False,
}
def buildMagicMix(page):
    global magic_mix_prefs, prefs, pipe_magic_mix
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          magic_mix_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_magic_mix_output(o):
      page.MagicMix.controls.append(o)
      page.MagicMix.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_magic_mix_output = add_to_magic_mix_output
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      for i, c in enumerate(page.MagicMix.controls):
        if i == 0: continue
        else: del page.MagicMix.controls[i]
      page.MagicMix.update()
      clear_button.visible = False
      clear_button.update()
    def magic_mix_help(e):
      def close_magic_mix_dlg(e):
        nonlocal magic_mix_help_dlg
        magic_mix_help_dlg.open = False
        page.update()
      magic_mix_help_dlg = AlertDialog(title=Text("🙅   Help with MagicMix"), content=Column([
          Text("Have you ever imagined what a corgi-alike coffee machine or a tiger-alike rabbit would look like? In this work, we attempt to answer these questions by exploring a new task called semantic mixing, aiming at blending two different semantics to create a new concept (e.g., corgi + coffee machine -- > corgi-alike coffee machine). Unlike style transfer, where an image is stylized according to the reference style without changing the image content, semantic blending mixes two different concepts in a semantic manner to synthesize a novel concept while preserving the spatial layout and geometry. To this end, we present MagicMix, a simple yet effective solution based on pre-trained text-conditioned diffusion models. Motivated by the progressive generation property of diffusion models where layout/shape emerges at early denoising steps while semantically meaningful details appear at later steps during the denoising process, our method first obtains a coarse layout (either by corrupting an image or denoising from a pure Gaussian noise given a text prompt), followed by injection of conditional prompt for semantic mixing. Our method does not require any spatial mask or re-training, yet is able to synthesize novel objects with high fidelity. To improve the mixing quality, we further devise two simple strategies to provide better control and flexibility over the synthesized content. With our method, we present our results over diverse downstream applications, including semantic style transfer, novel object synthesis, breed mixing, and concept removal, demonstrating the flexibility of our method."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🧙  Sounds like magic... ", on_click=close_magic_mix_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(magic_mix_help_dlg)
      magic_mix_help_dlg.open = True
      page.update()
    prompt = TextField(label="Prompt Text", value=magic_mix_prefs['prompt'], filled=True, on_change=lambda e:changed(e,'prompt'))
    init_image = FileInput(label="Initial Image", pref=magic_mix_prefs, key='init_image', page=page)
    seed = TextField(label="Seed", width=90, value=str(magic_mix_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    scheduler_mode = Dropdown(label="Scheduler/Sampler Mode", hint_text="They're very similar, with minor differences in the noise", width=200,
            options=[
                dropdown.Option("DDIM"),
                dropdown.Option("LMS Discrete"),
                dropdown.Option("PNDM"),
            ], value=magic_mix_prefs['scheduler_mode'], autofocus=False, on_change=lambda e:changed(e, 'scheduler_mode'),
        )
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=magic_mix_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=magic_mix_prefs, key='guidance_scale')
    mix_factor_row = SliderRow(label="Mix Factor", min=0.0, max=1.0, divisions=20, round=2, pref=magic_mix_prefs, key='mix_factor', tooltip="Interpolation constant used in the layout generation phase. The greater the value of `mix_factor`, the greater the influence of the prompt on the layout generation process.")
    kmin_row = SliderRow(label="k-Min", min=0.0, max=1.0, divisions=20, round=2, pref=magic_mix_prefs, key='kmin', tooltip="A higher value of kmin results in more steps for content generation process. Determine the range for the layout and content generation process.")
    kmax_row = SliderRow(label="k-Max", min=0.0, max=1.0, divisions=20, round=2, pref=magic_mix_prefs, key='kmax', tooltip="A higher value of kmax results in loss of more information about the layout of the original image. Determine the range for the layout and content generation process.")
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=magic_mix_prefs, key='max_size')
    batch_folder_name = TextField(label="Batch Folder Name", value=magic_mix_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(magic_mix_prefs)
    page.upscalers.append(upscaler)
    page.magic_mix_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.magic_mix_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🧚  MagicMix Init Image with Prompt", "Diffusion Pipeline for semantic mixing of an image and a text prompt...", actions=[save_default(magic_mix_prefs, ['init_image']), IconButton(icon=icons.HELP, tooltip="Help with MagicMix Settings", on_click=magic_mix_help)]),
        init_image,
        prompt,
        scheduler_mode,
        num_inference_row,
        guidance,
        mix_factor_row,
        ResponsiveRow([kmin_row, kmax_row]),
        max_row,
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=magic_mix_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        upscaler,
        Row([ElevatedButton(content=Text("🪄  Make MagicMix", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_magic_mix(page)),
             ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_magic_mix(page, from_list=True))]),
        page.magic_mix_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

paint_by_example_prefs = {
    'original_image': '',
    'mask_image': '',
    'example_image': '',
    'num_inference_steps': 50,
    'guidance_scale': 7.5,
    'eta': 0.0,
    'seed': 0,
    'max_size': 768,
    'alpha_mask': False,
    'invert_mask': False,
    'num_images': 1,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}
def buildPaintByExample(page):
    global paint_by_example_prefs, prefs, pipe_paint_by_example
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          paint_by_example_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_paint_by_example_output(o):
      page.paint_by_example_output.controls.append(o)
      page.paint_by_example_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.paint_by_example_output.controls = []
      page.paint_by_example_output.update()
      clear_button.visible = False
      clear_button.update()
    def paint_by_example_help(e):
      def close_paint_by_example_dlg(e):
        nonlocal paint_by_example_help_dlg
        paint_by_example_help_dlg.open = False
        page.update()
      paint_by_example_help_dlg = AlertDialog(title=Text("💁   Help with Paint-by-Example"), content=Column([
          Text("Language-guided image editing has achieved great success recently. In this pipeline, we use exemplar-guided image editing for more precise control. We achieve this goal by leveraging self-supervised training to disentangle and re-organize the source image and the exemplar. However, the naive approach will cause obvious fusing artifacts. We carefully analyze it and propose an information bottleneck and strong augmentations to avoid the trivial solution of directly copying and pasting the exemplar image. Meanwhile, to ensure the controllability of the editing process, we design an arbitrary shape mask for the exemplar image and leverage the classifier-free guidance to increase the similarity to the exemplar image. The whole framework involves a single forward of the diffusion model without any iterative optimization. We demonstrate that our method achieves an impressive performance and enables controllable editing on in-the-wild images with high fidelity.  Credit goes to https://github.com/Fantasy-Studio/Paint-by-Example"),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😸  Sweetness... ", on_click=close_paint_by_example_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(paint_by_example_help_dlg)
      paint_by_example_help_dlg.open = True
      page.update()
    def change_eta(e):
        changed(e, 'eta', ptype="float")
        eta_value.value = f" {paint_by_example_prefs['eta']}"
        eta_value.update()
        eta_row.update()
    original_image = FileInput(label="Original Image", pref=paint_by_example_prefs, key='original_image', expand=1, page=page)
    mask_image = FileInput(label="Mask Image", pref=paint_by_example_prefs, key='mask_image', expand=1, page=page)
    example_image = FileInput(label="Example Style Image", pref=paint_by_example_prefs, key='example_image', page=page)
    alpha_mask = Checkbox(label="Alpha Mask", value=paint_by_example_prefs['alpha_mask'], tooltip="Use Transparent Alpha Channel of Init as Mask", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=paint_by_example_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
    seed = TextField(label="Seed", width=90, value=str(paint_by_example_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=paint_by_example_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=paint_by_example_prefs, key='guidance_scale')
    #eta = TextField(label="ETA", value=str(paint_by_example_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    eta = Slider(min=0.0, max=1.0, divisions=20, round=2, label="{value}", value=float(paint_by_example_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=change_eta)
    eta_value = Text(f" {paint_by_example_prefs['eta']}", weight=FontWeight.BOLD)
    eta_row = Row([Text("ETA:"), eta_value, Text("  DDIM"), eta, Text("DDPM")])
    page.etas.append(eta_row)
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=paint_by_example_prefs, key='max_size')
    batch_folder_name = TextField(label="Batch Folder Name", value=paint_by_example_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(paint_by_example_prefs)
    page.upscalers.append(upscaler)
    page.paint_by_example_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.paint_by_example_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🦁  Paint-by-Example", "Image-guided Inpainting using an Example Image to Transfer Subject to Masked area...", actions=[save_default(paint_by_example_prefs, ['original_image', 'mask_image', 'example_image']), IconButton(icon=icons.HELP, tooltip="Help with Paint-by-Example Settings", on_click=paint_by_example_help)]),
        ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        example_image,
        num_inference_row,
        guidance,
        eta_row,
        max_row,
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=paint_by_example_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        upscaler,
        #Row([jump_length, jump_n_sample, seed]),
        ElevatedButton(content=Text("🐾  Run Paint-by-Example", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_paint_by_example(page)),
        page.paint_by_example_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

instruct_pix2pix_prefs = {
    'original_image': '',
    'prompt': '',
    'negative_prompt': '',
    'num_inference_steps': 100,
    'guidance_scale': 7.5,
    'image_guidance_scale': 1.5,
    'eta': 0.0,
    'seed': 0,
    'max_size': 768,
    'num_images': 1,
    'use_init_video': False,
    'init_video': '',
    'fps': 12,
    'start_time': 0,
    'end_time': 0,
    'control_v': 'v1.1',
    'use_SDXL': False,
    'use_ip_adapter': False,
    'ip_adapter_image': '',
    'ip_adapter_model': 'SD v1.5',
    'ip_adapter_SDXL_model': 'SDXL',
    'ip_adapter_strength': 0.8,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildInstructPix2Pix(page):
    global instruct_pix2pix_prefs, prefs, pipe_instruct_pix2pix
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          instruct_pix2pix_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_instruct_pix2pix_output(o):
      page.instruct_pix2pix_output.controls.append(o)
      page.instruct_pix2pix_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.instruct_pix2pix_output.controls = []
      page.instruct_pix2pix_output.update()
      clear_button.visible = False
      clear_button.update()
    def instruct_pix2pix_help(e):
      def close_instruct_pix2pix_dlg(e):
        nonlocal instruct_pix2pix_help_dlg
        instruct_pix2pix_help_dlg.open = False
        page.update()
      instruct_pix2pix_help_dlg = AlertDialog(title=Text("💁   Help with Instruct-Pix2Pix"), content=Column([
          Text("A method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models -- a language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😎  Fun2Fun... ", on_click=close_instruct_pix2pix_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(instruct_pix2pix_help_dlg)
      instruct_pix2pix_help_dlg.open = True
      page.update()
    def change_eta(e):
        changed(e, 'eta', ptype="float")
        eta_value.value = f" {instruct_pix2pix_prefs['eta']}"
        eta_value.update()
        eta_row.update()
    def toggle_init_video(e):
        changed(e, 'use_init_video')
        show = e.control.value
        original_image.visible = not show
        original_image.update()
        init_video.visible = show
        init_video.update()
        vid_params.height = None if show else 0
        vid_params.update()
        run_prompt_list.visible = not show
        run_prompt_list.update()
    original_image = FileInput(label="Original Image", pref=instruct_pix2pix_prefs, key='original_image', expand=True, page=page)
    prompt = TextField(label="Editing Instructions Prompt Text", value=instruct_pix2pix_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=instruct_pix2pix_prefs['negative_prompt'], filled=True, col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    seed = TextField(label="Seed", width=90, value=str(instruct_pix2pix_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    use_init_video = Tooltip(message="Input a short mp4 file to animate with.", content=Switcher(label="Use Init Video", value=instruct_pix2pix_prefs['use_init_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_init_video))
    init_video = FileInput(label="Init Video Clip", pref=instruct_pix2pix_prefs, key='init_video', ftype="video", expand=True, page=page)
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=instruct_pix2pix_prefs, key='fps', tooltip="The FPS to extract from the init video clip.")
    start_time = TextField(label="Start Time (s)", value=instruct_pix2pix_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype="float"))
    end_time = TextField(label="End Time (0 for all)", value=instruct_pix2pix_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype="float"))
    vid_params = Container(content=Column([fps, Row([start_time, end_time])]), animate_size=animation.Animation(800, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if instruct_pix2pix_prefs['use_init_video'] else 0)
    use_SDXL = Switcher(label="Use Stable Diffusion XL Instruct Pix2Pix Pipeline", value=instruct_pix2pix_prefs['use_SDXL'], on_change=lambda e:changed(e,'use_SDXL'), tooltip="Otherwise use standard 1.5/2.1 Model Checkpoint.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=instruct_pix2pix_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=instruct_pix2pix_prefs, key='guidance_scale')
    image_guidance = SliderRow(label="Image Guidance Scale", min=0, max=200, divisions=400, round=1, pref=instruct_pix2pix_prefs, key='image_guidance_scale', tooltip="Image guidance scale is to push the generated image towards the inital image `image`. Higher image guidance scale encourages to generate images that are closely linked to the source image `image`, usually at the expense of lower image quality.")
    #eta = TextField(label="ETA", value=str(instruct_pix2pix_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    eta = Slider(min=0.0, max=1.0, divisions=20, round=2, label="{value}", value=float(instruct_pix2pix_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=change_eta)
    eta_value = Text(f" {instruct_pix2pix_prefs['eta']}", weight=FontWeight.BOLD)
    eta_row = Row([Text("ETA:"), eta_value, Text("  DDIM"), eta, Text("DDPM")])
    page.etas.append(eta_row)
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=instruct_pix2pix_prefs, key='max_size')
    def toggle_ip_adapter(e):
        instruct_pix2pix_prefs['use_ip_adapter'] = e.control.value
        ip_adapter_container.height = None if e.control.value else 0
        ip_adapter_container.update()
        ip_adapter_model.visible = e.control.value
        ip_adapter_model.update()
        ip_adapter_SDXL_model.visible = e.control.value
        ip_adapter_SDXL_model.update()
    use_ip_adapter = Switcher(label="Use IP-Adapter Reference Image", value=instruct_pix2pix_prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip="Uses both image and text to condition the image generation process.")
    ip_adapter_model = Dropdown(label="IP-Adapter SD Model", width=220, options=[], value=instruct_pix2pix_prefs['ip_adapter_model'], visible=instruct_pix2pix_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))
    for m in ip_adapter_models:
        ip_adapter_model.options.append(dropdown.Option(m['name']))
    ip_adapter_SDXL_model = Dropdown(label="IP-Adapter SDXL Model", width=220, options=[], value=instruct_pix2pix_prefs['ip_adapter_SDXL_model'], visible=instruct_pix2pix_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_SDXL_model'))
    for m in ip_adapter_SDXL_models:
        ip_adapter_SDXL_model.options.append(dropdown.Option(m['name']))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=instruct_pix2pix_prefs, key='ip_adapter_image', col={'lg':6}, page=page)
    ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=instruct_pix2pix_prefs, key='ip_adapter_strength', col={'lg':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    ip_adapter_container = Container(ResponsiveRow([ip_adapter_image, ip_adapter_strength]), height = None if instruct_pix2pix_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    batch_folder_name = TextField(label="Batch Folder Name", value=instruct_pix2pix_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(instruct_pix2pix_prefs)
    page.upscalers.append(upscaler)
    page.instruct_pix2pix_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.instruct_pix2pix_output.controls) > 0
    run_prompt_list = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_instruct_pix2pix(page, from_list=True))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🏜️   Instruct-Pix2Pix", "Text-Based Image Editing - Follow Image Editing Instructions...", actions=[save_default(instruct_pix2pix_prefs, ['original_image', 'ip_adapter_image']), IconButton(icon=icons.HELP, tooltip="Help with Instruct-Pix2Pix Settings", on_click=instruct_pix2pix_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        Row([original_image, init_video, use_init_video]),
        vid_params,
        ResponsiveRow([prompt, negative_prompt]),
        num_inference_row,
        guidance,
        image_guidance,
        eta_row,
        max_row,
        use_SDXL,#, ip_adapter_SDXL_model
        Row([use_ip_adapter, ip_adapter_model, ip_adapter_SDXL_model], vertical_alignment=CrossAxisAlignment.START),
        ip_adapter_container,
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=instruct_pix2pix_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        upscaler,
        Row([ElevatedButton(content=Text("🏖️  Run Instruct Pix2Pix", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_instruct_pix2pix(page)),
             run_prompt_list]),
        page.instruct_pix2pix_output,
        clear_button,
      ]))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c


ledits_prefs = {
    'original_image': '',
    'source_prompt': '',
    'prompt': '',
    'negative_prompt': '',
    'num_inference_steps': 50,
    'source_guidance_scale': 3.5,
    'guidance_scale': 5.0,
    'edit_threshold': 0.75,
    'skip': 0.15,
    'seed': 0,
    'max_size': 768,
    'num_images': 1,
    'use_SDXL': False,
    'use_ip_adapter': False,
    'ip_adapter_image': '',
    'ip_adapter_model': 'SD v1.5',
    'ip_adapter_SDXL_model': 'SDXL',
    'ip_adapter_strength': 0.8,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildLEdits(page):
    global ledits_prefs, prefs, pipe_ledits
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          ledits_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def ledits_help(e):
      def close_ledits_dlg(e):
        nonlocal ledits_help_dlg
        ledits_help_dlg.open = False
        page.update()
      ledits_help_dlg = AlertDialog(title=Text("💁   Help with LEdits++"), content=Column([
          Text("Text-to-image diffusion models have recently received increasing interest for their astonishing ability to produce high-fidelity images from solely text inputs. Subsequent research efforts aim to exploit and apply their capabilities to real image editing. However, existing image-to-image methods are often inefficient, imprecise, and of limited versatility. They either require time-consuming fine-tuning, deviate unnecessarily strongly from the input image, and/or lack support for multiple, simultaneous edits. To address these issues, we introduce LEDITS++, an efficient yet versatile and precise textual image manipulation technique. LEDITS++'s novel inversion approach requires no tuning nor optimization and produces high-fidelity results with a few diffusion steps. Second, our methodology supports multiple simultaneous edits and is architecture-agnostic. Third, we use a novel implicit masking technique that limits changes to relevant image regions. We propose the novel TEdBench++ benchmark as part of our exhaustive evaluation. Our results demonstrate the capabilities of LEDITS++ and its improvements over previous methods."),
          Markdown("[Project](https://leditsplusplus-project.static.hf.space/index.html) | [HuggingFace Space](https://huggingface.co/spaces/editing-images/leditsplusplus) | [GitHub](https://github.com/ml-research/ledits_pp) | [Paper](https://huggingface.co/papers/2311.16711)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Credits go to Manuel Brack, Felix Friedrich, Katharina Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, Apolinário Passos"),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😽  Make a Change... ", on_click=close_ledits_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(ledits_help_dlg)
      ledits_help_dlg.open = True
      page.update()
    #original_image = TextField(label="Original Image", value=ledits_prefs['original_image'], expand=True, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))
    original_image = FileInput(label="Original Image to Edit", pref=ledits_prefs, key='original_image', page=page)
    source_prompt = TextField(label="Source Prompt Text", value=ledits_prefs['source_prompt'], col={'md': 12}, multiline=True, on_change=lambda e:changed(e,'source_prompt'))
    prompt = TextField(label="Editing Target Prompt", value=ledits_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=ledits_prefs['negative_prompt'], filled=True, col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    seed = TextField(label="Seed", width=90, value=str(ledits_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    use_SDXL = Switcher(label="Use Stable Diffusion XL LEdits++ Pipeline", value=ledits_prefs['use_SDXL'], on_change=lambda e:changed(e,'use_SDXL'), tooltip="Otherwise use standard 1.5/2.1 Model Checkpoint.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=ledits_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=ledits_prefs, key='guidance_scale')
    edit_threshold = SliderRow(label="Edit Threshold", min=0, max=1, divisions=20, round=2, pref=ledits_prefs, key='edit_threshold', tooltip="Masking threshold of guidance. Threshold should be proportional to the image region that is modified.")
    skip = SliderRow(label="Skip Amount", min=0, max=1, divisions=20, round=2, pref=ledits_prefs, key='skip', tooltip="Portion of initial steps that will be ignored for inversion and subsequent generation. Lower values will lead to stronger changes to the input image.")
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=ledits_prefs, key='max_size')
    def toggle_ip_adapter(e):
        ledits_prefs['use_ip_adapter'] = e.control.value
        ip_adapter_container.height = None if e.control.value else 0
        ip_adapter_container.update()
        ip_adapter_model.visible = e.control.value
        ip_adapter_model.update()
        ip_adapter_SDXL_model.visible = e.control.value
        ip_adapter_SDXL_model.update()
    use_ip_adapter = Switcher(label="Use IP-Adapter Reference Image", value=ledits_prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip="Uses both image and text to condition the image generation process.")
    ip_adapter_model = Dropdown(label="IP-Adapter SD Model", width=220, options=[], value=ledits_prefs['ip_adapter_model'], visible=ledits_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))
    for m in ip_adapter_models:
        ip_adapter_model.options.append(dropdown.Option(m['name']))
    ip_adapter_SDXL_model = Dropdown(label="IP-Adapter SDXL Model", width=220, options=[], value=ledits_prefs['ip_adapter_SDXL_model'], visible=ledits_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))
    for m in ip_adapter_SDXL_models:
        ip_adapter_SDXL_model.options.append(dropdown.Option(m['name']))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=ledits_prefs, key='ip_adapter_image', col={'lg':6}, page=page)
    ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=ledits_prefs, key='ip_adapter_strength', col={'lg':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    ip_adapter_container = Container(ResponsiveRow([ip_adapter_image, ip_adapter_strength]), height = None if ledits_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)

    batch_folder_name = TextField(label="Batch Folder Name", value=ledits_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(ledits_prefs)
    page.upscalers.append(upscaler)
    run_prompt_list = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_ledits(page, from_list=True))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🔀  LEDITS++ (under construction)", "Limitless Image Editing using Text-to-Image Models...", actions=[save_default(ledits_prefs, ['original_image', 'ip_adapter_image']), IconButton(icon=icons.HELP, tooltip="Help with LEdits++ Settings", on_click=ledits_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        original_image,
        source_prompt,
        ResponsiveRow([prompt, negative_prompt]),
        num_inference_row,
        guidance,
        edit_threshold,
        skip,
        max_row,
        use_SDXL,#, ip_adapter_SDXL_model
        Row([use_ip_adapter, ip_adapter_model, ip_adapter_SDXL_model], vertical_alignment=CrossAxisAlignment.START),
        ip_adapter_container,
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=ledits_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        upscaler,
        Row([ElevatedButton(content=Text("🪙  Run LEdits++", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_ledits(page)),
             run_prompt_list]),
      ]))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

controlnet_prefs = {
    'original_image': '',
    'prompt': '',
    'negative_prompt': 'lowres, text, watermark, cropped, low quality',
    'control_task': 'Scribble',
    'conditioning_scale': 1.0,
    'control_guidance_start': 0.0,
    'control_guidance_end': 1.0,
    'multi_controlnets': [],
    'batch_size': 1,
    'max_size': 768,
    'low_threshold': 100, #1-255
    'high_threshold': 200, #1-255
    'steps': 50, #100
    'guidance_scale': 9, #30
    'seed': 0,
    'eta': 0,
    'show_processed_image': False,
    'use_ip_adapter': False,
    'ip_adapter_image': '',
    'ip_adapter_model': 'SD v1.5',
    'ip_adapter_strength': 0.8,
    'use_init_video': False,
    'init_video': '',
    'fps': 12,
    'start_time': 0,
    'end_time': 0,
    'use_image2image': False,
    'init_image': '',
    'mask_image': '',
    'alpha_mask': False,
    'invert_mask': False,
    'file_prefix': 'controlnet-',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildControlNet(page):
    global controlnet_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          controlnet_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_controlnet_output(o):
      page.controlnet_output.controls.append(o)
      page.controlnet_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.controlnet_output.controls = []
      page.controlnet_output.update()
      clear_button.visible = False
      clear_button.update()
    def controlnet_help(e):
      def close_controlnet_dlg(e):
        nonlocal controlnet_help_dlg
        controlnet_help_dlg.open = False
        page.update()
      controlnet_help_dlg = AlertDialog(title=Text("💁   Help with ControlNet"), content=Column([
          Text('ControlNet is a neural network structure to control diffusion models by adding extra conditions. It copys the weights of neural network blocks into a "locked" copy and a "trainable" copy. The "trainable" one learns your condition. The "locked" one preserves your model. Thanks to this, training with small dataset of image pairs will not destroy the production-ready diffusion models. The "zero convolution" is 1×1 convolution with both weight and bias initialized as zeros. Before training, all zero convolutions output zeros, and ControlNet will not cause any distortion.  No layer is trained from scratch. You are still fine-tuning. Your original model is safe.  This allows training on small-scale or even personal devices. This is also friendly to merge/replacement/offsetting of models/weights/blocks/layers.'),
          Markdown("This is an interface for running the [official codebase](https://github.com/lllyasviel/ControlNet#readme) for models described in [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543).", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Scribble - A hand-drawn monochrome image with white outlines on a black background."),
          Text("Canny Map Edge - A monochrome image with white edges on a black background."),
          Text("OpenPose - A OpenPose bone image."),
          Text("Depth - A grayscale image with black representing deep areas and white representing shallow areas."),
          Text("HED - A monochrome image with white soft edges on a black background."),
          Text("M-LSD - A monochrome image composed only of white straight lines on a black background."),
          Text("Normal Map - A normal mapped image."),
          Text("Segmented - An ADE20K's semantic segmentation protocol image."),
          Text("LineArt - An image with line art, usually black lines on a white background."),
          Text("Shuffle - An image with shuffled patches or regions."),
          Text("Brightness - An image based on brightness of init."),
          Text("Mediapipe Face - An image based on face position & expression."),
          Text("Instruct Pix2Pix - Trained with pixel to pixel instruction."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🍄  Too much control... ", on_click=close_controlnet_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(controlnet_help_dlg)
      controlnet_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        save_file(e.file_name)
    def save_file(file_name):
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
          controlnet_prefs['file_name'] = file_name.rpartition('.')[0]
        else:
          fname = file_name
          controlnet_prefs['file_name'] = file_name.rpartition(slash)[2].rpartition('.')[0]
        if pick_type == "image":
          original_image.value = fname
          original_image.update()
          controlnet_prefs['original_image'] = fname
        elif pick_type == "video":
          init_video.value = fname
          init_video.update()
          controlnet_prefs['init_video'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=e.page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    def pick_original(e):
        nonlocal pick_type
        pick_type = "image"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Original Image File")
    def pick_video(e):
        nonlocal pick_type
        pick_type = "video"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["mp4", "avi"], dialog_title="Pick Initial Video File")
    def change_task(e):
        task = e.control.value
        show = task.startswith("Video")# or task == "Video OpenPose"
        update = controlnet_prefs['use_init_video'] != show
        changed(e,'control_task')
        threshold.height = None if controlnet_prefs['control_task'] == "Canny Map Edge" or controlnet_prefs['control_task'] == "Video Canny Edge" else 0
        threshold.update()
        if update:
            original_image.visible = not show
            original_image.update()
            init_video.visible = show
            init_video.update()
            vid_params.height = None if show else 0
            vid_params.update()
            conditioning_scale.visible = not show
            conditioning_scale.update()
            add_layer_btn.visible = not show
            add_layer_btn.update()
            multi_layers.visible = not show
            multi_layers.update()
            run_prompt_list.visible = not show
            run_prompt_list.update()
            controlnet_prefs['use_init_video'] = show
    def change_eta(e):
        changed(e, 'eta', ptype="float")
        eta_value.value = f" {controlnet_prefs['eta']}"
        eta_value.update()
        eta_row.update()
    def add_layer(e):
        layer = {'control_task': controlnet_prefs['control_task'], 'original_image': controlnet_prefs['original_image'], 'conditioning_scale': controlnet_prefs['conditioning_scale'], 'control_guidance_start': controlnet_prefs['control_guidance_start'], 'control_guidance_end': controlnet_prefs['control_guidance_end'], 'use_init_video': False}
        if controlnet_prefs['control_task'] == "Video Canny Edge" or controlnet_prefs['control_task'] == "Video OpenPose":
          layer['use_init_video'] = True
          layer['init_video'] = controlnet_prefs['init_video']
          layer['fps'] = controlnet_prefs['fps']
          layer['start_time'] = controlnet_prefs['start_time']
          layer['end_time'] = controlnet_prefs['end_time']
          controlnet_prefs['init_video'] = ""
          init_video.value = ""
          original_image.update()
        controlnet_prefs['multi_controlnets'].append(layer)
        multi_layers.controls.append(ListTile(title=Row([Text(layer['control_task'] + " - ", weight=FontWeight.BOLD), Text(layer['init_video'] if layer['use_init_video'] else layer['original_image']), Text(f"- Conditioning Scale: {layer['conditioning_scale']} - Start: {layer['control_guidance_start']}, End: {layer['control_guidance_end']}")]), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.DELETE, text="Delete Control Layer", on_click=delete_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
          ]), data=layer))
        multi_layers.update()
        controlnet_prefs['original_image'] = ""
        original_image.value = ""
        original_image.update()
    def delete_layer(e):
        controlnet_prefs['multi_controlnets'].remove(e.control.data)
        for c in multi_layers.controls:
          if c.data['original_image'] == e.control.data['original_image']:
             multi_layers.controls.remove(c)
             break
        multi_layers.update()
    def delete_all_layers(e):
        controlnet_prefs['multi_controlnets'].clear()
        multi_layers.controls.clear()
        multi_layers.update()
    def toggle_img2img(e):
        controlnet_prefs['use_image2image'] = e.control.value
        img2img_row.height = None if e.control.value else 0
        img2img_row.update()
    def toggle_ip_adapter(e):
        controlnet_prefs['use_ip_adapter'] = e.control.value
        ip_adapter_container.height = None if e.control.value else 0
        ip_adapter_container.update()
        ip_adapter_model.visible = e.control.value
        ip_adapter_model.update()
    #original_image = FileInput(label="Original Drawing", pref=controlnet_prefs, key='original_image', expand=True, page=page)
    original_image = TextField(label="Original Drawing", value=controlnet_prefs['original_image'], expand=True, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))
    prompt = TextField(label="Prompt Text", value=controlnet_prefs['prompt'], filled=True, col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    #a_prompt  = TextField(label="Added Prompt Text", value=controlnet_prefs['a_prompt'], col={'md':3}, on_change=lambda e:changed(e,'a_prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=controlnet_prefs['negative_prompt'], filled=True, col={'md':4}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    control_task = Dropdown(label="ControlNet Task", width=200, options=[dropdown.Option("Scribble"), dropdown.Option("Canny Map Edge"), dropdown.Option("OpenPose"), dropdown.Option("Depth"), dropdown.Option("Marigold Depth"), dropdown.Option("HED"), dropdown.Option("M-LSD"), dropdown.Option("Normal Map"), dropdown.Option("Segmentation"), dropdown.Option("LineArt"), dropdown.Option("Shuffle"), dropdown.Option("Instruct Pix2Pix"), dropdown.Option("Brightness"), dropdown.Option("Mediapipe Face"), dropdown.Option("Video Canny Edge"), dropdown.Option("Video OpenPose")], value=controlnet_prefs['control_task'], on_change=change_task)
    conditioning_scale = SliderRow(label="Conditioning Scale", min=0, max=2, divisions=20, round=1, pref=controlnet_prefs, key='conditioning_scale', tooltip="The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added to the residual in the original unet.")
    control_guidance_start = SliderRow(label="Control Guidance Start", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_prefs, key='control_guidance_start', tooltip="The percentage of total steps at which the controlnet starts applying.")
    control_guidance_end = SliderRow(label="Control Guidance End", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_prefs, key='control_guidance_end', tooltip="The percentage of total steps at which the controlnet stops applying.")
    #add_layer_btn = IconButton(icons.ADD, tooltip="Add Multi-ControlNet Layer", on_click=add_layer)
    add_layer_btn = ft.FilledButton("➕ Add Layer", width=140, on_click=add_layer)
    multi_layers = Column([], spacing=0)
    seed = TextField(label="Seed", width=90, value=str(controlnet_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    #use_init_video = Tooltip(message="Input a short mp4 file to animate with.", content=Switcher(label="Use Init Video", value=controlnet_prefs['use_init_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_init_video))
    #init_video = FileInput(label="Init Video Clip", pref=controlnet_prefs, key='init_video', ftype="video", visible=controlnet_prefs['use_init_video'], expand=True, page=page)
    init_video = TextField(label="Init Video Clip", value=controlnet_prefs['init_video'], expand=True, visible=controlnet_prefs['use_init_video'], on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=controlnet_prefs, key='fps', tooltip="The FPS to extract from the init video clip.")
    start_time = TextField(label="Start Time (s)", value=controlnet_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype="float"))
    end_time = TextField(label="End Time (0 for all)", value=controlnet_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype="float"))
    vid_params = Container(content=Column([fps, Row([start_time, end_time])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if controlnet_prefs['use_init_video'] else 0)
    num_inference_row = SliderRow(label="Number of Steps", min=1, max=100, divisions=99, pref=controlnet_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=30, divisions=60, round=1, pref=controlnet_prefs, key='guidance_scale')
    low_threshold_row = SliderRow(label="Canny Low Threshold", min=1, max=255, divisions=254, pref=controlnet_prefs, key='low_threshold', col={'lg':6}, tooltip="Lower increases sensitivity to weaker edges, higher gives fewer but more reliable edge detections.")
    high_threshold_row = SliderRow(label="Canny High Threshold", min=1, max=255, divisions=254, pref=controlnet_prefs, key='high_threshold', col={'lg':6}, tooltip="Higher value decreases the amount of noise but could result in missing some true edges.")
    threshold = Container(ResponsiveRow([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(bottom=8))
    threshold.height = None if controlnet_prefs['control_task'] == "Canny Map Edge" else 0
    eta = Slider(min=0.0, max=1.0, divisions=20, round=2, label="{value}", value=float(controlnet_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=change_eta)
    eta_value = Text(f" {controlnet_prefs['eta']}", weight=FontWeight.BOLD)
    eta_row = Row([Text("ETA:"), eta_value, Text("  DDIM"), eta])
    page.etas.append(eta_row)
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_prefs, key='max_size')
    use_ip_adapter = Switcher(label="Use IP-Adapter Reference Image", value=controlnet_prefs['use_ip_adapter'], on_change=toggle_ip_adapter)
    ip_adapter_model = Dropdown(label="IP-Adapter SD Model", width=220, options=[], value=controlnet_prefs['ip_adapter_model'], visible=controlnet_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))
    for m in ip_adapter_models:
        ip_adapter_model.options.append(dropdown.Option(m['name']))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=controlnet_prefs, key='ip_adapter_image', col={'lg':6}, page=page)
    ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_prefs, key='ip_adapter_strength', col={'lg':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    ip_adapter_container = Container(ResponsiveRow([ip_adapter_image, ip_adapter_strength]), height = None if controlnet_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    use_image2image = Switcher(label="Use Image2Image or Inpainting", value=controlnet_prefs['use_image2image'], on_change=toggle_img2img)
    init_image = FileInput(label="Init Image", pref=controlnet_prefs, key='init_image', expand=True, page=page)
    mask_image = FileInput(label="Mask Image (optional)", pref=controlnet_prefs, key='mask_image', expand=True, page=page)
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=controlnet_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
    alpha_mask = Checkbox(label="Alpha Mask", value=controlnet_prefs['alpha_mask'], tooltip="Use Transparent Alpha Channel of Init as Mask", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))
    img2img_row = Container(content=ResponsiveRow([Row([init_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]), height=None if controlnet_prefs['use_image2image'] else 0, animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    file_prefix = TextField(label="Filename Prefix",  value=controlnet_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))
    show_processed_image = Checkbox(label="Show Pre-Processed Image", value=controlnet_prefs['show_processed_image'], tooltip="Displays the Init-Image after being process by Canny, Depth, etc.", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'show_processed_image'))
    batch_folder_name = TextField(label="Batch Folder Name", value=controlnet_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(controlnet_prefs)
    page.upscalers.append(upscaler)
    page.controlnet_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.controlnet_output.controls) > 0
    run_prompt_list = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet(page, from_list=True))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🕸️  ControlNet Image+Text-to-Image+Video-to-Video", "Adding Input Conditions To Pretrained Text-to-Image Diffusion Models...", actions=[save_default(controlnet_prefs, ['original_image', 'init_image', 'mask_image', 'ip_adapter_image', 'multi_controlnets']), IconButton(icon=icons.HELP, tooltip="Help with ControlNet Settings", on_click=controlnet_help)]),
        Row([control_task, original_image, init_video, add_layer_btn]),
        conditioning_scale,
        Row([control_guidance_start, control_guidance_end]),
        multi_layers,
        vid_params,
        Divider(thickness=2, height=4),
        ResponsiveRow([prompt, negative_prompt]),
        threshold,
        num_inference_row,
        guidance,
        eta_row,
        max_row,
        use_image2image,
        img2img_row,
        Row([use_ip_adapter, ip_adapter_model], vertical_alignment=CrossAxisAlignment.START),
        ip_adapter_container,
        show_processed_image,
        Row([NumberPicker(label="Batch Size: ", min=1, max=8, value=controlnet_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size')), seed, batch_folder_name, file_prefix]),
        upscaler,
        Row([ElevatedButton(content=Text("🏸  Run ControlNet", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet(page)),
             run_prompt_list]),
        page.controlnet_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

controlnet_xl_prefs = {
    'original_image': '',
    'prompt': '',
    'negative_prompt': 'lowres, text, watermark, cropped, low quality',
    'control_task': 'Canny Map Edge',
    'conditioning_scale': 1.0,
    'control_guidance_start': 0.0,
    'control_guidance_end': 1.0,
    'multi_controlnets': [],
    'batch_size': 1,
    'max_size': 1024,
    'low_threshold': 100, #1-255
    'high_threshold': 200, #1-255
    'steps': 50, #100
    'guidance_scale': 7.5, #30
    'seed': 0,
    'eta': 0,
    'show_processed_image': False,
    'use_ip_adapter': False,
    'ip_adapter_image': '',
    'ip_adapter_SDXL_model': 'SDXL',
    'ip_adapter_strength': 0.8,
    'use_init_video': False,
    'init_video': '',
    'fps': 12,
    'start_time': 0,
    'end_time': 0,
    'use_image2image': False,
    'init_image': '',
    'mask_image': '',
    'alpha_mask': False,
    'invert_mask': False,
    'use_pag': False,
    'pag_scale': 5.0,
    'applied_layer_down': False,
    'applied_layer_mid': True,
    'applied_layer_up': False,
    'file_prefix': 'controlnet-',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildControlNetXL(page):
    global controlnet_xl_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          controlnet_xl_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_controlnet_xl_output(o):
      page.controlnet_xl_output.controls.append(o)
      page.controlnet_xl_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.controlnet_xl_output.controls = []
      page.controlnet_xl_output.update()
      clear_button.visible = False
      clear_button.update()
    def controlnet_xl_help(e):
      def close_controlnet_xl_dlg(e):
        nonlocal controlnet_xl_help_dlg
        controlnet_xl_help_dlg.open = False
        page.update()
      controlnet_xl_help_dlg = AlertDialog(title=Text("💁   Help with ControlNet SDXL"), content=Column([
          Text('ControlNet-XL is a neural network structure to control diffusion models by adding extra conditions, using the latest StableDiffusion XL models. It copys the weights of neural network blocks into a "locked" copy and a "trainable" copy. The "trainable" one learns your condition. The "locked" one preserves your model. Thanks to this, training with small dataset of image pairs will not destroy the production-ready diffusion models. The "zero convolution" is 1×1 convolution with both weight and bias initialized as zeros. Before training, all zero convolutions output zeros, and ControlNetXL will not cause any distortion.  No layer is trained from scratch. You are still fine-tuning. Your original model is safe.  This allows training on small-scale or even personal devices. This is also friendly to merge/replacement/offsetting of models/weights/blocks/layers.'),
          Markdown("This is an interface for running the [official codebase](https://github.com/lllyasviel/ControlNet#readme) for models described in [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543).", on_tap_link=lambda e: e.page.launch_url(e.data)),
          #Text("Scribble - A hand-drawn monochrome image with white outlines on a black background."),
          Text("Canny Map Edge - A monochrome image with white edges on a black background."),
          Text("OpenPose - A OpenPose bone image."),
          Text("Depth - A grayscale image with black representing deep areas and white representing shallow areas."),
          Text("Softedge - A monochrome image with white soft edges on a black background."),
          #Text("M-LSD - A monochrome image composed only of white straight lines on a black background."),
          #Text("Normal Map - A normal mapped image."),
          Text("Segmented - An ADE20K's semantic segmentation protocol image."),
          Text("LineArt - An image with line art, usually black lines on a white background."),
          Text("ControlNet++ Union - All-in-One supporting Canny, Depth, HED, LineArt, MLSD, Normal, Pose, Scribble, Segment & SoftEdge. Provide Post-Processed image, does not prepare from origional."),
          #Text("Shuffle - An image with shuffled patches or regions."),
          #Text("Brightness - An image based on brightness of init."),
          #Text("Instruct Pix2Pix - Trained with pixel to pixel instruction."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🍄  Too much control... ", on_click=close_controlnet_xl_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(controlnet_xl_help_dlg)
      controlnet_xl_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        save_file(e.file_name)
    def save_file(file_name):
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
          controlnet_xl_prefs['file_name'] = file_name.rpartition('.')[0]
        else:
          fname = file_name
          controlnet_xl_prefs['file_name'] = file_name.rpartition(slash)[2].rpartition('.')[0]
        if pick_type == "image":
          original_image.value = fname
          original_image.update()
          controlnet_xl_prefs['original_image'] = fname
        elif pick_type == "video":
          init_video.value = fname
          init_video.update()
          controlnet_xl_prefs['init_video'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=e.page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    def pick_original(e):
        nonlocal pick_type
        pick_type = "image"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Original Image File")
    def pick_video(e):
        nonlocal pick_type
        pick_type = "video"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["mp4", "avi"], dialog_title="Pick Initial Video File")
    def change_task(e):
        task = e.control.value
        show = task.startswith("Video")# or task == "Video OpenPose"
        update = controlnet_xl_prefs['use_init_video'] != show
        changed(e,'control_task')
        threshold.height = None if "Canny" in controlnet_xl_prefs['control_task'] else 0
        threshold.update()
        if update:
            original_image.visible = not show
            original_image.update()
            init_video.visible = show
            init_video.update()
            vid_params.height = None if show else 0
            vid_params.update()
            conditioning_scale.visible = not show
            conditioning_scale.update()
            add_layer_btn.visible = not show
            add_layer_btn.update()
            multi_layers.visible = not show
            multi_layers.update()
            run_prompt_list.visible = not show
            run_prompt_list.update()
            controlnet_xl_prefs['use_init_video'] = show
    def change_eta(e):
        changed(e, 'eta', ptype="float")
        eta_value.value = f" {controlnet_xl_prefs['eta']}"
        eta_value.update()
        eta_row.update()
    def add_layer(e):
        layer = {'control_task': controlnet_xl_prefs['control_task'], 'original_image': controlnet_xl_prefs['original_image'], 'conditioning_scale': controlnet_xl_prefs['conditioning_scale'], 'control_guidance_start': controlnet_xl_prefs['control_guidance_start'], 'control_guidance_end': controlnet_xl_prefs['control_guidance_end'], 'use_init_video': False}
        if controlnet_xl_prefs['control_task'] == "Video Canny Edge" or controlnet_xl_prefs['control_task'] == "Video OpenPose":
          layer['use_init_video'] = True
          layer['init_video'] = controlnet_xl_prefs['init_video']
          layer['fps'] = controlnet_xl_prefs['fps']
          layer['start_time'] = controlnet_xl_prefs['start_time']
          layer['end_time'] = controlnet_xl_prefs['end_time']
          controlnet_xl_prefs['init_video'] = ""
          init_video.value = ""
          original_image.update()
        controlnet_xl_prefs['multi_controlnets'].append(layer)
        multi_layers.controls.append(ListTile(title=Row([Text(layer['control_task'] + " - ", weight=FontWeight.BOLD), Text(layer['init_video'] if layer['use_init_video'] else layer['original_image']), Text(f"- Conditioning Scale: {layer['conditioning_scale']} - Start: {layer['control_guidance_start']}, End: {layer['control_guidance_end']}")]), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.DELETE, text="Delete Control Layer", on_click=delete_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
          ]), data=layer))
        multi_layers.update()
        controlnet_xl_prefs['original_image'] = ""
        original_image.value = ""
        original_image.update()
    def delete_layer(e):
        controlnet_xl_prefs['multi_controlnets'].remove(e.control.data)
        for c in multi_layers.controls:
          if c.data['original_image'] == e.control.data['original_image']:
             multi_layers.controls.remove(c)
             break
        multi_layers.update()
    def delete_all_layers(e):
        controlnet_xl_prefs['multi_controlnets'].clear()
        multi_layers.controls.clear()
        multi_layers.update()
    def toggle_img2img(e):
        controlnet_xl_prefs['use_image2image'] = e.control.value
        img2img_row.height = None if e.control.value else 0
        img2img_row.update()
    def toggle_ip_adapter(e):
        controlnet_xl_prefs['use_ip_adapter'] = e.control.value
        ip_adapter_container.height = None if e.control.value else 0
        ip_adapter_container.update()
        ip_adapter_SDXL_model.visible = e.control.value
        ip_adapter_SDXL_model.update()
    def toggle_pag(e):
        controlnet_xl_prefs['use_pag'] = e.control.value
        pag_container.height = None if e.control.value else 0
        pag_container.update()
    original_image = TextField(label="Original Drawing", value=controlnet_xl_prefs['original_image'], expand=True, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))
    prompt = TextField(label="Prompt Text", value=controlnet_xl_prefs['prompt'], filled=True, col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    #a_prompt  = TextField(label="Added Prompt Text", value=controlnet_xl_prefs['a_prompt'], col={'md':3}, on_change=lambda e:changed(e,'a_prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=controlnet_xl_prefs['negative_prompt'], filled=True, col={'md':4}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    control_task = Dropdown(label="ControlNet-SDXL Task", width=210, options=[dropdown.Option("Canny Map Edge"), dropdown.Option("Canny Map Edge mid"), dropdown.Option("Canny Map Edge small"), dropdown.Option("Depth"), dropdown.Option("Depth mid"), dropdown.Option("Depth small"), dropdown.Option("Marigold Depth"), dropdown.Option("Segmentation"), dropdown.Option("LineArt"), dropdown.Option("Softedge"), dropdown.Option("OpenPose"), dropdown.Option("Scribble"), dropdown.Option("Scribble Anime"), dropdown.Option("ControlNet++ Union")], value=controlnet_xl_prefs['control_task'], on_change=change_task)
    #, dropdown.Option("Scribble"), dropdown.Option("HED"), dropdown.Option("M-LSD"), dropdown.Option("Normal Map"), dropdown.Option("Shuffle"), dropdown.Option("Instruct Pix2Pix"), dropdown.Option("Brightness"), dropdown.Option("Video Canny Edge"), dropdown.Option("Video OpenPose")
    conditioning_scale = SliderRow(label="Conditioning Scale", min=0, max=2, divisions=20, round=1, pref=controlnet_xl_prefs, key='conditioning_scale', tooltip="The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added to the residual in the original unet.")
    control_guidance_start = SliderRow(label="Control Guidance Start", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_xl_prefs, key='control_guidance_start', tooltip="The percentage of total steps at which the controlnet starts applying.")
    control_guidance_end = SliderRow(label="Control Guidance End", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_xl_prefs, key='control_guidance_end', tooltip="The percentage of total steps at which the controlnet stops applying.")
    #add_layer_btn = IconButton(icons.ADD, tooltip="Add Multi-ControlNetXL Layer", on_click=add_layer)
    add_layer_btn = ft.FilledButton("➕ Add Layer", width=140, on_click=add_layer)
    multi_layers = Column([], spacing=0)
    seed = TextField(label="Seed", width=90, value=str(controlnet_xl_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    #use_init_video = Tooltip(message="Input a short mp4 file to animate with.", content=Switcher(label="Use Init Video", value=controlnet_xl_prefs['use_init_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_init_video))
    init_video = TextField(label="Init Video Clip", value=controlnet_xl_prefs['init_video'], expand=True, visible=controlnet_xl_prefs['use_init_video'], on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=controlnet_xl_prefs, key='fps', tooltip="The FPS to extract from the init video clip.")
    start_time = TextField(label="Start Time (s)", value=controlnet_xl_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype="float"))
    end_time = TextField(label="End Time (0 for all)", value=controlnet_xl_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype="float"))
    vid_params = Container(content=Column([fps, Row([start_time, end_time])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if controlnet_xl_prefs['use_init_video'] else 0)
    num_inference_row = SliderRow(label="Number of Steps", min=1, max=100, divisions=99, pref=controlnet_xl_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=30, divisions=60, round=1, pref=controlnet_xl_prefs, key='guidance_scale')
    low_threshold_row = SliderRow(label="Canny Low Threshold", min=1, max=255, divisions=254, pref=controlnet_xl_prefs, key='low_threshold', col={'lg':6}, tooltip="Lower increases sensitivity to weaker edges, higher gives fewer but more reliable edge detections.")
    high_threshold_row = SliderRow(label="Canny High Threshold", min=1, max=255, divisions=254, pref=controlnet_xl_prefs, key='high_threshold', col={'lg':6}, tooltip="Higher value decreases the amount of noise but could result in missing some true edges.")
    threshold = Container(ResponsiveRow([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(bottom=8))
    threshold.height = None if controlnet_xl_prefs['control_task'] == "Canny Map Edge" else 0
    eta = Slider(min=0.0, max=1.0, divisions=20, round=2, label="{value}", value=float(controlnet_xl_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=change_eta)
    eta_value = Text(f" {controlnet_xl_prefs['eta']}", weight=FontWeight.BOLD)
    eta_row = Row([Text("ETA:"), eta_value, Text("  DDIM"), eta])
    page.etas.append(eta_row)
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_xl_prefs, key='max_size')
    use_image2image = Switcher(label="Use Image2Image or Inpainting", value=controlnet_xl_prefs['use_image2image'], on_change=toggle_img2img)
    init_image = FileInput(label="Init Image", pref=controlnet_xl_prefs, key='init_image', expand=True, page=page)
    mask_image = FileInput(label="Mask Image (optional)", pref=controlnet_xl_prefs, key='mask_image', expand=True, page=page)
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=controlnet_xl_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
    alpha_mask = Checkbox(label="Alpha Mask", value=controlnet_xl_prefs['alpha_mask'], tooltip="Use Transparent Alpha Channel of Init as Mask", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))
    img2img_row = Container(content=ResponsiveRow([Row([init_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]), height=None if controlnet_xl_prefs['use_image2image'] else 0, animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    use_ip_adapter = Switcher(label="Use IP-Adapter Reference Image", value=controlnet_xl_prefs['use_ip_adapter'], on_change=toggle_ip_adapter)
    ip_adapter_SDXL_model = Dropdown(label="IP-Adapter SDXL Model", width=220, options=[], value=controlnet_xl_prefs['ip_adapter_SDXL_model'], visible=controlnet_xl_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_SDXL_model'))
    for m in ip_adapter_SDXL_models:
        ip_adapter_SDXL_model.options.append(dropdown.Option(m['name']))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=controlnet_xl_prefs, key='ip_adapter_image', col={'lg':6}, page=page)
    ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_xl_prefs, key='ip_adapter_strength', col={'lg':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    ip_adapter_container = Container(ResponsiveRow([ip_adapter_image, ip_adapter_strength]), height = None if controlnet_xl_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    use_pag = Switcher(label="Use PAG: Perturbed-Attention Guidance (doesn't apply using Image2Image/Inpainting)", value=controlnet_xl_prefs['use_pag'], on_change=toggle_pag, tooltip="Improves sample quality across both unconditional and conditional settings. Progressively enhance the structure of synthesized samples throughout the denoising process by considering the self-attention mechanisms' ability to capture structural information.")
    pag_scale = SliderRow(label="PAG Guidance Scale", min=0, max=50, divisions=50, pref=controlnet_xl_prefs, key='pag_scale', tooltip="Gain more semantically coherent structures and exhibit fewer artifacts. Large guidance scale can lead to smoother textures and slight saturation in the images.")
    applied_layer_down = Checkbox(label="Down", value=controlnet_xl_prefs['applied_layer_down'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'applied_layer_down'))
    applied_layer_mid = Checkbox(label="Mid", value=controlnet_xl_prefs['applied_layer_mid'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'applied_layer_mid'))
    applied_layer_up = Checkbox(label="Up", value=controlnet_xl_prefs['applied_layer_up'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'applied_layer_up'))
    pag_applied_layers = Container(Row([Text("PAG Applied Layers:"), applied_layer_down, applied_layer_mid, applied_layer_up]), tooltip="Specify which layers PAG is applied to. Changing this setting will significantly impact the output, so experiment.")
    pag_container = Container(Column([pag_scale, pag_applied_layers], spacing=0), padding=padding.only(left=32), height=None if controlnet_xl_prefs['use_pag'] else 0, animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    file_prefix = TextField(label="Filename Prefix",  value=controlnet_xl_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))
    show_processed_image = Checkbox(label="Show Pre-Processed Image", value=controlnet_xl_prefs['show_processed_image'], tooltip="Displays the Init-Image after being process by Canny, Depth, etc.", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'show_processed_image'))
    batch_folder_name = TextField(label="Batch Folder Name", value=controlnet_xl_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(controlnet_xl_prefs)
    page.upscalers.append(upscaler)
    page.controlnet_xl_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.controlnet_xl_output.controls) > 0
    run_prompt_list = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_xl(page, from_list=True))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🕷  ControlNet SDXL Image+Text-to-Image", "Adding Input Conditions To Pretrained Text-to-Image Diffusion Models...", actions=[save_default(controlnet_xl_prefs, ['original_image', 'init_image', 'mask_image', 'ip_adapter_image', 'multi_controlnets']), IconButton(icon=icons.HELP, tooltip="Help with ControlNetXL Settings", on_click=controlnet_xl_help)]),
        Row([control_task, original_image, init_video, add_layer_btn]),
        conditioning_scale,
        Row([control_guidance_start, control_guidance_end]),
        multi_layers,
        vid_params,
        Divider(thickness=2, height=4),
        ResponsiveRow([prompt, negative_prompt]),
        threshold,
        num_inference_row,
        guidance,
        eta_row,
        max_row,
        use_image2image,
        img2img_row,
        Row([use_ip_adapter, ip_adapter_SDXL_model], vertical_alignment=CrossAxisAlignment.START),
        ip_adapter_container,
        use_pag,
        pag_container,
        show_processed_image,
        Row([NumberPicker(label="Batch Size: ", min=1, max=8, value=controlnet_xl_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size')), seed, batch_folder_name, file_prefix]),
        upscaler,
        Row([ElevatedButton(content=Text("🛃  Run ControlNet-XL", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_xl(page)),
             run_prompt_list]),
        page.controlnet_xl_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

controlnet_sd3_prefs = {
    'original_image': '',
    'prompt': '',
    'negative_prompt': 'lowres, text, watermark, cropped, low quality',
    'control_task': 'Canny Map Edge',
    'conditioning_scale': 1.0,
    'control_guidance_start': 0.0,
    'control_guidance_end': 1.0,
    'multi_controlnets': [],
    'batch_size': 1,
    'max_size': 1024,
    'low_threshold': 100, #1-255
    'high_threshold': 200, #1-255
    'steps': 28, #100
    'guidance_scale': 5.5, #30
    'seed': 0,
    'eta': 0,
    'show_processed_image': False,
    'use_ip_adapter': False,
    'ip_adapter_image': '',
    'ip_adapter_SD3_model': 'SD3',
    'ip_adapter_strength': 0.8,
    'use_init_video': False,
    'init_video': '',
    'fps': 12,
    'start_time': 0,
    'end_time': 0,
    'use_image2image': False,
    'init_image': '',
    'mask_image': '',
    'alpha_mask': False,
    'invert_mask': False,
    'file_prefix': 'controlnetSD3-',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildControlNetSD3(page):
    global controlnet_sd3_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          controlnet_sd3_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_controlnet_sd3_output(o):
      page.controlnet_sd3_output.controls.append(o)
      page.controlnet_sd3_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.controlnet_sd3_output.controls = []
      page.controlnet_sd3_output.update()
      clear_button.visible = False
      clear_button.update()
    def controlnet_sd3_help(e):
      def close_controlnet_sd3_dlg(e):
        nonlocal controlnet_sd3_help_dlg
        controlnet_sd3_help_dlg.open = False
        page.update()
      controlnet_sd3_help_dlg = AlertDialog(title=Text("💁   Help with ControlNet SD3"), content=Column([
          Text('ControlNet-SD3 is a neural network structure to control diffusion models by adding extra conditions, using the latest StableDiffusion XL models. It copys the weights of neural network blocks into a "locked" copy and a "trainable" copy. The "trainable" one learns your condition. The "locked" one preserves your model. Thanks to this, training with small dataset of image pairs will not destroy the production-ready diffusion models. The "zero convolution" is 1×1 convolution with both weight and bias initialized as zeros. Before training, all zero convolutions output zeros, and ControlNetSD3 will not cause any distortion.  No layer is trained from scratch. You are still fine-tuning. Your original model is safe.  This allows training on small-scale or even personal devices. This is also friendly to merge/replacement/offsetting of models/weights/blocks/layers.'),
          Markdown("This is an interface for running the [official codebase](https://github.com/lllyasviel/ControlNet#readme) for models described in [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543).", on_tap_link=lambda e: e.page.launch_url(e.data)),
          #Text("Scribble - A hand-drawn monochrome image with white outlines on a black background."),
          Text("Canny Map Edge - A monochrome image with white edges on a black background."),
          Text("OpenPose - A OpenPose bone image."),
          Text("Tile - Adds more detail, makes image sharper and apply new styles."),
          #Text("Depth - A grayscale image with black representing deep areas and white representing shallow areas."),
          #Text("Softedge - A monochrome image with white soft edges on a black background."),
          #Text("M-LSD - A monochrome image composed only of white straight lines on a black background."),
          #Text("Normal Map - A normal mapped image."),
          #Text("Segmented - An ADE20K's semantic segmentation protocol image."),
          #Text("LineArt - An image with line art, usually black lines on a white background."),
          #Text("Shuffle - An image with shuffled patches or regions."),
          #Text("Brightness - An image based on brightness of init."),
          #Text("Instruct Pix2Pix - Trained with pixel to pixel instruction."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🍄  Some Control... ", on_click=close_controlnet_sd3_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(controlnet_sd3_help_dlg)
      controlnet_sd3_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        save_file(e.file_name)
    def save_file(file_name):
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
          controlnet_sd3_prefs['file_name'] = file_name.rpartition('.')[0]
        else:
          fname = file_name
          controlnet_sd3_prefs['file_name'] = file_name.rpartition(slash)[2].rpartition('.')[0]
        if pick_type == "image":
          original_image.value = fname
          original_image.update()
          controlnet_sd3_prefs['original_image'] = fname
        elif pick_type == "video":
          init_video.value = fname
          init_video.update()
          controlnet_sd3_prefs['init_video'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=e.page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    def pick_original(e):
        nonlocal pick_type
        pick_type = "image"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Original Image File")
    def pick_video(e):
        nonlocal pick_type
        pick_type = "video"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["mp4", "avi"], dialog_title="Pick Initial Video File")
    def change_task(e):
        task = e.control.value
        show = task.startswith("Video")# or task == "Video OpenPose"
        update = controlnet_sd3_prefs['use_init_video'] != show
        changed(e,'control_task')
        threshold.height = None if "Canny" in controlnet_sd3_prefs['control_task'] else 0
        threshold.update()
        if update:
            original_image.visible = not show
            original_image.update()
            init_video.visible = show
            init_video.update()
            vid_params.height = None if show else 0
            vid_params.update()
            conditioning_scale.visible = not show
            conditioning_scale.update()
            add_layer_btn.visible = not show
            add_layer_btn.update()
            multi_layers.visible = not show
            multi_layers.update()
            run_prompt_list.visible = not show
            run_prompt_list.update()
            controlnet_sd3_prefs['use_init_video'] = show
    def add_layer(e):
        layer = {'control_task': controlnet_sd3_prefs['control_task'], 'original_image': controlnet_sd3_prefs['original_image'], 'conditioning_scale': controlnet_sd3_prefs['conditioning_scale'], 'control_guidance_start': controlnet_sd3_prefs['control_guidance_start'], 'control_guidance_end': controlnet_sd3_prefs['control_guidance_end'], 'use_init_video': False}
        if controlnet_sd3_prefs['control_task'] == "Video Canny Edge" or controlnet_sd3_prefs['control_task'] == "Video OpenPose":
          layer['use_init_video'] = True
          layer['init_video'] = controlnet_sd3_prefs['init_video']
          layer['fps'] = controlnet_sd3_prefs['fps']
          layer['start_time'] = controlnet_sd3_prefs['start_time']
          layer['end_time'] = controlnet_sd3_prefs['end_time']
          controlnet_sd3_prefs['init_video'] = ""
          init_video.value = ""
          original_image.update()
        controlnet_sd3_prefs['multi_controlnets'].append(layer)
        multi_layers.controls.append(ListTile(title=Row([Text(layer['control_task'] + " - ", weight=FontWeight.BOLD), Text(layer['init_video'] if layer['use_init_video'] else layer['original_image']), Text(f"- Conditioning Scale: {layer['conditioning_scale']} - Start: {layer['control_guidance_start']}, End: {layer['control_guidance_end']}")]), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.DELETE, text="Delete Control Layer", on_click=delete_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
          ]), data=layer))
        multi_layers.update()
        controlnet_sd3_prefs['original_image'] = ""
        original_image.value = ""
        original_image.update()
    def delete_layer(e):
        controlnet_sd3_prefs['multi_controlnets'].remove(e.control.data)
        for c in multi_layers.controls:
          if c.data['original_image'] == e.control.data['original_image']:
             multi_layers.controls.remove(c)
             break
        multi_layers.update()
    def delete_all_layers(e):
        controlnet_sd3_prefs['multi_controlnets'].clear()
        multi_layers.controls.clear()
        multi_layers.update()
    def toggle_img2img(e):
        controlnet_sd3_prefs['use_image2image'] = e.control.value
        img2img_row.height = None if e.control.value else 0
        img2img_row.update()
    def toggle_ip_adapter(e):
        controlnet_sd3_prefs['use_ip_adapter'] = e.control.value
        ip_adapter_container.height = None if e.control.value else 0
        ip_adapter_container.update()
        ip_adapter_SD3_model.visible = e.control.value
        ip_adapter_SD3_model.update()
    original_image = TextField(label="Original Drawing", value=controlnet_sd3_prefs['original_image'], expand=True, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))
    prompt = TextField(label="Prompt Text", value=controlnet_sd3_prefs['prompt'], filled=True, col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=controlnet_sd3_prefs['negative_prompt'], filled=True, col={'md':4}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    control_task = Dropdown(label="ControlNet-SD3 Task", width=210, options=[dropdown.Option("Canny Map Edge"), dropdown.Option("OpenPose"), dropdown.Option("Tile")], value=controlnet_sd3_prefs['control_task'], on_change=change_task)
    #control_task = Dropdown(label="ControlNet-SD3 Task", width=210, options=[dropdown.Option("Canny Map Edge"), dropdown.Option("Canny Map Edge mid"), dropdown.Option("Canny Map Edge small"), dropdown.Option("Depth"), dropdown.Option("Depth mid"), dropdown.Option("Depth small"), dropdown.Option("Marigold Depth"), dropdown.Option("Segmentation"), dropdown.Option("LineArt"), dropdown.Option("Softedge"), dropdown.Option("OpenPose"), dropdown.Option("Scribble"), dropdown.Option("Scribble Anime")], value=controlnet_sd3_prefs['control_task'], on_change=change_task)
    #, dropdown.Option("Scribble"), dropdown.Option("HED"), dropdown.Option("M-LSD"), dropdown.Option("Normal Map"), dropdown.Option("Shuffle"), dropdown.Option("Instruct Pix2Pix"), dropdown.Option("Brightness"), dropdown.Option("Video Canny Edge"), dropdown.Option("Video OpenPose")
    conditioning_scale = SliderRow(label="Conditioning Scale", min=0, max=2, divisions=20, round=1, pref=controlnet_sd3_prefs, key='conditioning_scale', tooltip="The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added to the residual in the original unet.")
    control_guidance_start = SliderRow(label="Control Guidance Start", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_sd3_prefs, key='control_guidance_start', tooltip="The percentage of total steps at which the controlnet starts applying.")
    control_guidance_end = SliderRow(label="Control Guidance End", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_sd3_prefs, key='control_guidance_end', tooltip="The percentage of total steps at which the controlnet stops applying.")
    #add_layer_btn = IconButton(icons.ADD, tooltip="Add Multi-ControlNetSD3 Layer", on_click=add_layer)
    add_layer_btn = ft.FilledButton("➕ Add Layer", width=140, on_click=add_layer)
    multi_layers = Column([], spacing=0)
    seed = TextField(label="Seed", width=90, value=str(controlnet_sd3_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    #use_init_video = Tooltip(message="Input a short mp4 file to animate with.", content=Switcher(label="Use Init Video", value=controlnet_sd3_prefs['use_init_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_init_video))
    init_video = TextField(label="Init Video Clip", value=controlnet_sd3_prefs['init_video'], expand=True, visible=controlnet_sd3_prefs['use_init_video'], on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=controlnet_sd3_prefs, key='fps', tooltip="The FPS to extract from the init video clip.")
    start_time = TextField(label="Start Time (s)", value=controlnet_sd3_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype="float"))
    end_time = TextField(label="End Time (0 for all)", value=controlnet_sd3_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype="float"))
    vid_params = Container(content=Column([fps, Row([start_time, end_time])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if controlnet_sd3_prefs['use_init_video'] else 0)
    num_inference_row = SliderRow(label="Number of Steps", min=1, max=100, divisions=99, pref=controlnet_sd3_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=30, divisions=60, round=1, pref=controlnet_sd3_prefs, key='guidance_scale')
    low_threshold_row = SliderRow(label="Canny Low Threshold", min=1, max=255, divisions=254, pref=controlnet_sd3_prefs, key='low_threshold', col={'lg':6}, tooltip="Lower increases sensitivity to weaker edges, higher gives fewer but more reliable edge detections.")
    high_threshold_row = SliderRow(label="Canny High Threshold", min=1, max=255, divisions=254, pref=controlnet_sd3_prefs, key='high_threshold', col={'lg':6}, tooltip="Higher value decreases the amount of noise but could result in missing some true edges.")
    threshold = Container(ResponsiveRow([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(bottom=8))
    threshold.height = None if controlnet_sd3_prefs['control_task'] == "Canny Map Edge" else 0
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_sd3_prefs, key='max_size')
    use_image2image = Switcher(label="Use Image2Image or Inpainting", value=controlnet_sd3_prefs['use_image2image'], on_change=toggle_img2img)
    init_image = FileInput(label="Init Image", pref=controlnet_sd3_prefs, key='init_image', expand=True, page=page)
    mask_image = FileInput(label="Mask Image (optional)", pref=controlnet_sd3_prefs, key='mask_image', expand=True, page=page)
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=controlnet_sd3_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
    alpha_mask = Checkbox(label="Alpha Mask", value=controlnet_sd3_prefs['alpha_mask'], tooltip="Use Transparent Alpha Channel of Init as Mask", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))
    img2img_row = Container(content=ResponsiveRow([Row([init_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]), height=None if controlnet_sd3_prefs['use_image2image'] else 0, animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    use_ip_adapter = Switcher(label="Use IP-Adapter Reference Image", value=controlnet_sd3_prefs['use_ip_adapter'], on_change=toggle_ip_adapter)
    ip_adapter_SD3_model = Dropdown(label="IP-Adapter SD3 Model", width=220, options=[], value=controlnet_sd3_prefs['ip_adapter_SD3_model'], visible=controlnet_sd3_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_SD3_model'))
    for m in ip_adapter_SDXL_models:
        ip_adapter_SD3_model.options.append(dropdown.Option(m['name']))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=controlnet_sd3_prefs, key='ip_adapter_image', col={'lg':6}, page=page)
    ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_sd3_prefs, key='ip_adapter_strength', col={'lg':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    ip_adapter_container = Container(ResponsiveRow([ip_adapter_image, ip_adapter_strength]), height = None if controlnet_sd3_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    file_prefix = TextField(label="Filename Prefix",  value=controlnet_sd3_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))
    show_processed_image = Checkbox(label="Show Pre-Processed Image", value=controlnet_sd3_prefs['show_processed_image'], tooltip="Displays the Init-Image after being process by Canny, Depth, etc.", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'show_processed_image'))
    batch_folder_name = TextField(label="Batch Folder Name", value=controlnet_sd3_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(controlnet_sd3_prefs)
    page.upscalers.append(upscaler)
    page.controlnet_sd3_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.controlnet_sd3_output.controls) > 0
    run_prompt_list = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_sd3(page, from_list=True))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🕋  ControlNet Stable Diffusion 3 Image+Text-to-Image", "Adding Input Conditions To Pretrained Text-to-Image Diffusion Models...", actions=[save_default(controlnet_sd3_prefs, ['original_image', 'init_image', 'mask_image', 'ip_adapter_image', 'multi_controlnets']), IconButton(icon=icons.HELP, tooltip="Help with ControlNetSD3 Settings", on_click=controlnet_sd3_help)]),
        Row([control_task, original_image, init_video, add_layer_btn]),
        conditioning_scale,
        Row([control_guidance_start, control_guidance_end]),
        multi_layers,
        vid_params,
        Divider(thickness=2, height=4),
        ResponsiveRow([prompt, negative_prompt]),
        threshold,
        num_inference_row,
        guidance,
        #eta_row,
        max_row,
        #use_image2image,
        #img2img_row,
        #Row([use_ip_adapter, ip_adapter_SD3_model], vertical_alignment=CrossAxisAlignment.START),
        #ip_adapter_container,
        show_processed_image,
        Row([NumberPicker(label="Batch Size: ", min=1, max=8, value=controlnet_sd3_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size')), seed, batch_folder_name, file_prefix]),
        upscaler,
        Row([ElevatedButton(content=Text("✊  Run ControlNet-SD3", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_sd3(page)),
             run_prompt_list]),
        page.controlnet_sd3_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

controlnet_xs_prefs = {
    'original_image': '',
    'prompt': '',
    'negative_prompt': 'lowres, text, watermark, cropped, low quality',
    'use_SDXL': True,
    'cpu_offload': True,
    'control_task': 'Canny Map Edge',
    'conditioning_scale': 1.0,
    'control_guidance_start': 0.0,
    'control_guidance_end': 1.0,
    'multi_controlnets': [],
    'batch_size': 1,
    'max_size': 1024,
    'low_threshold': 100, #1-255
    'high_threshold': 200, #1-255
    'steps': 50, #100
    'guidance_scale': 7.5, #30
    'seed': 0,
    'eta': 0,
    'show_processed_image': False,
    'use_ip_adapter': False,
    'ip_adapter_image': '',
    'ip_adapter_SDXL_model': 'SDXL',
    'ip_adapter_strength': 0.8,
    'use_init_video': False,
    'init_video': '',
    'fps': 12,
    'start_time': 0,
    'end_time': 0,
    'use_image2image': False,
    'init_image': '',
    'mask_image': '',
    'alpha_mask': False,
    'invert_mask': False,
    'file_prefix': 'controlnet-',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildControlNetXS(page):
    global controlnet_xs_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          controlnet_xs_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_controlnet_xs_output(o):
      page.controlnet_xs_output.controls.append(o)
      page.controlnet_xs_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.controlnet_xs_output.controls = []
      page.controlnet_xs_output.update()
      clear_button.visible = False
      clear_button.update()
    def controlnet_xs_help(e):
      def close_controlnet_xs_dlg(e):
        nonlocal controlnet_xs_help_dlg
        controlnet_xs_help_dlg.open = False
        page.update()
      controlnet_xs_help_dlg = AlertDialog(title=Text("💁   Help with ControlNet XS"), content=Column([
          Markdown("ControlNet-XS was introduced in [ControlNet-XS](https://vislearn.github.io/ControlNet-XS/) by Denis Zavadski and Carsten Rother. It is based on the observation that the control model in the [original ControlNet](https://huggingface.co/papers/2302.05543) can be made much smaller and still produce good results. Like the original ControlNet model, you can provide an additional control image to condition and control Stable Diffusion generation. For example, if you provide a depth map, the ControlNet model generates an image that'll preserve the spatial information from the depth map. It is a more flexible and accurate way to control the image generation process. ControlNet-XS generates images with comparable quality to a regular ControlNet, but it is 20-25% faster ([see benchmark](https://github.com/UmerHA/controlnet-xs-benchmark/blob/main/Speed%20Benchmark.ipynb)) and uses ~45% less memory.", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text('With increasing computing capabilities, current model architectures appear to follow the trend of simply upscaling all components without validating the necessity for doing so. In this project we investigate the size and architectural design of ControlNet [Zhang et al., 2023] for controlling the image generation process with stable diffusion-based models. We show that a new architecture with as little as 1% of the parameters of the base model achieves state-of-the art results, considerably better than ControlNet in terms of FID score. Hence we call it ControlNet-XS. We provide the code for controlling StableDiffusion-XL [Podell et al., 2023] (Model B, 48M Parameters) and StableDiffusion 2.1 [Rombach et al. 2022] (Model B, 14M Parameters), all under openrail license.'),
          Markdown("[XS Project Page](https://vislearn.github.io/ControlNet-XS/)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Canny Map Edge - A monochrome image with white edges on a black background."),
          Text("Depth - A grayscale image with black representing deep areas and white representing shallow areas."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🍄  Too much control... ", on_click=close_controlnet_xs_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(controlnet_xs_help_dlg)
      controlnet_xs_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        save_file(e.file_name)
    def save_file(file_name):
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
          controlnet_xs_prefs['file_name'] = file_name.rpartition('.')[0]
        else:
          fname = file_name
          controlnet_xs_prefs['file_name'] = file_name.rpartition(slash)[2].rpartition('.')[0]
        if pick_type == "image":
          original_image.value = fname
          original_image.update()
          controlnet_xs_prefs['original_image'] = fname
        elif pick_type == "video":
          init_video.value = fname
          init_video.update()
          controlnet_xs_prefs['init_video'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=e.page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    def pick_original(e):
        nonlocal pick_type
        pick_type = "image"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Original Image File")
    def pick_video(e):
        nonlocal pick_type
        pick_type = "video"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["mp4", "avi"], dialog_title="Pick Initial Video File")
    def change_task(e):
        task = e.control.value
        show = task.startswith("Video")# or task == "Video OpenPose"
        update = controlnet_xs_prefs['use_init_video'] != show
        changed(e,'control_task')
        threshold.height = None if "Canny" in controlnet_xs_prefs['control_task'] else 0
        threshold.update()
        if update:
            original_image.visible = not show
            original_image.update()
            init_video.visible = show
            init_video.update()
            vid_params.height = None if show else 0
            vid_params.update()
            conditioning_scale.visible = not show
            conditioning_scale.update()
            add_layer_btn.visible = not show
            add_layer_btn.update()
            multi_layers.visible = not show
            multi_layers.update()
            run_prompt_list.visible = not show
            run_prompt_list.update()
            controlnet_xs_prefs['use_init_video'] = show
    def change_eta(e):
        changed(e, 'eta', ptype="float")
        eta_value.value = f" {controlnet_xs_prefs['eta']}"
        eta_value.update()
        eta_row.update()
    def add_layer(e):
        layer = {'control_task': controlnet_xs_prefs['control_task'], 'original_image': controlnet_xs_prefs['original_image'], 'conditioning_scale': controlnet_xs_prefs['conditioning_scale'], 'control_guidance_start': controlnet_xs_prefs['control_guidance_start'], 'control_guidance_end': controlnet_xs_prefs['control_guidance_end'], 'use_init_video': False}
        if controlnet_xs_prefs['control_task'] == "Video Canny Edge" or controlnet_xs_prefs['control_task'] == "Video OpenPose":
          layer['use_init_video'] = True
          layer['init_video'] = controlnet_xs_prefs['init_video']
          layer['fps'] = controlnet_xs_prefs['fps']
          layer['start_time'] = controlnet_xs_prefs['start_time']
          layer['end_time'] = controlnet_xs_prefs['end_time']
          controlnet_xs_prefs['init_video'] = ""
          init_video.value = ""
          original_image.update()
        controlnet_xs_prefs['multi_controlnets'].append(layer)
        multi_layers.controls.append(ListTile(title=Row([Text(layer['control_task'] + " - ", weight=FontWeight.BOLD), Text(layer['init_video'] if layer['use_init_video'] else layer['original_image']), Text(f"- Conditioning Scale: {layer['conditioning_scale']} - Start: {layer['control_guidance_start']}, End: {layer['control_guidance_end']}")]), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.DELETE, text="Delete Control Layer", on_click=delete_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
          ]), data=layer))
        multi_layers.update()
        controlnet_xs_prefs['original_image'] = ""
        original_image.value = ""
        original_image.update()
    def delete_layer(e):
        controlnet_xs_prefs['multi_controlnets'].remove(e.control.data)
        for c in multi_layers.controls:
          if c.data['original_image'] == e.control.data['original_image']:
             multi_layers.controls.remove(c)
             break
        multi_layers.update()
    def delete_all_layers(e):
        controlnet_xs_prefs['multi_controlnets'].clear()
        multi_layers.controls.clear()
        multi_layers.update()
    def toggle_img2img(e):
        controlnet_xs_prefs['use_image2image'] = e.control.value
        img2img_row.height = None if e.control.value else 0
        img2img_row.update()
    def toggle_ip_adapter(e):
        controlnet_xs_prefs['use_ip_adapter'] = e.control.value
        ip_adapter_container.height = None if e.control.value else 0
        ip_adapter_container.update()
        ip_adapter_SDXL_model.visible = e.control.value
        ip_adapter_SDXL_model.update()
    original_image = TextField(label="Original Drawing", value=controlnet_xs_prefs['original_image'], expand=True, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))
    prompt = TextField(label="Prompt Text", value=controlnet_xs_prefs['prompt'], filled=True, col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    use_SDXL = Switcher(label="Use Stable Diffusion XL", value=controlnet_xs_prefs['use_SDXL'], on_change=lambda e:changed(e,'use_SDXL'), tooltip="SDXL uses Model Checkpoint set in Installation. Otherwise use selected 1.5 or 2.1 Inpainting Model.")
    cpu_offload = Switcher(label="CPU Offload", value=controlnet_xs_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    #a_prompt  = TextField(label="Added Prompt Text", value=controlnet_xs_prefs['a_prompt'], col={'md':3}, on_change=lambda e:changed(e,'a_prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=controlnet_xs_prefs['negative_prompt'], filled=True, col={'md':4}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    control_task = Dropdown(label="ControlNet Task", width=210, options=[dropdown.Option("Canny Map Edge"), dropdown.Option("Depth"), dropdown.Option("Marigold Depth")], value=controlnet_xs_prefs['control_task'], on_change=change_task)
    #, dropdown.Option("Scribble"), dropdown.Option("HED"), dropdown.Option("M-LSD"), dropdown.Option("Normal Map"), dropdown.Option("Shuffle"), dropdown.Option("Instruct Pix2Pix"), dropdown.Option("Brightness"), dropdown.Option("Video Canny Edge"), dropdown.Option("Video OpenPose")
    conditioning_scale = SliderRow(label="Conditioning Scale", min=0, max=2, divisions=20, round=1, pref=controlnet_xs_prefs, key='conditioning_scale', tooltip="The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added to the residual in the original unet.")
    control_guidance_start = SliderRow(label="Control Guidance Start", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_xs_prefs, key='control_guidance_start', tooltip="The percentage of total steps at which the controlnet starts applying.")
    control_guidance_end = SliderRow(label="Control Guidance End", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_xs_prefs, key='control_guidance_end', tooltip="The percentage of total steps at which the controlnet stops applying.")
    #add_layer_btn = IconButton(icons.ADD, tooltip="Add Multi-ControlNetXS Layer", on_click=add_layer)
    add_layer_btn = ft.FilledButton("➕ Add Layer", width=140, on_click=add_layer)
    multi_layers = Column([], spacing=0)
    seed = TextField(label="Seed", width=90, value=str(controlnet_xs_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    #use_init_video = Tooltip(message="Input a short mp4 file to animate with.", content=Switcher(label="Use Init Video", value=controlnet_xs_prefs['use_init_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_init_video))
    init_video = TextField(label="Init Video Clip", value=controlnet_xs_prefs['init_video'], expand=True, visible=controlnet_xs_prefs['use_init_video'], on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=controlnet_xs_prefs, key='fps', tooltip="The FPS to extract from the init video clip.")
    start_time = TextField(label="Start Time (s)", value=controlnet_xs_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype="float"))
    end_time = TextField(label="End Time (0 for all)", value=controlnet_xs_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype="float"))
    vid_params = Container(content=Column([fps, Row([start_time, end_time])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if controlnet_xs_prefs['use_init_video'] else 0)
    num_inference_row = SliderRow(label="Number of Steps", min=1, max=100, divisions=99, pref=controlnet_xs_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=30, divisions=60, round=1, pref=controlnet_xs_prefs, key='guidance_scale')
    low_threshold_row = SliderRow(label="Canny Low Threshold", min=1, max=255, divisions=254, pref=controlnet_xs_prefs, key='low_threshold', col={'lg':6}, tooltip="Lower increases sensitivity to weaker edges, higher gives fewer but more reliable edge detections.")
    high_threshold_row = SliderRow(label="Canny High Threshold", min=1, max=255, divisions=254, pref=controlnet_xs_prefs, key='high_threshold', col={'lg':6}, tooltip="Higher value decreases the amount of noise but could result in missing some true edges.")
    threshold = Container(ResponsiveRow([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(bottom=8))
    threshold.height = None if controlnet_xs_prefs['control_task'] == "Canny Map Edge" else 0
    eta = Slider(min=0.0, max=1.0, divisions=20, round=2, label="{value}", value=float(controlnet_xs_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=change_eta)
    eta_value = Text(f" {controlnet_xs_prefs['eta']}", weight=FontWeight.BOLD)
    eta_row = Row([Text("ETA:"), eta_value, Text("  DDIM"), eta])
    page.etas.append(eta_row)
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_xs_prefs, key='max_size')
    use_image2image = Switcher(label="Use Image2Image or Inpainting", value=controlnet_xs_prefs['use_image2image'], on_change=toggle_img2img)
    init_image = FileInput(label="Init Image", pref=controlnet_xs_prefs, key='init_image', expand=True, page=page)
    mask_image = FileInput(label="Mask Image (optional)", pref=controlnet_xs_prefs, key='mask_image', expand=True, page=page)
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=controlnet_xs_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
    alpha_mask = Checkbox(label="Alpha Mask", value=controlnet_xs_prefs['alpha_mask'], tooltip="Use Transparent Alpha Channel of Init as Mask", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))
    img2img_row = Container(content=ResponsiveRow([Row([init_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]), height=None if controlnet_xs_prefs['use_image2image'] else 0, animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    use_ip_adapter = Switcher(label="Use IP-Adapter Reference Image", value=controlnet_xs_prefs['use_ip_adapter'], on_change=toggle_ip_adapter)
    ip_adapter_SDXL_model = Dropdown(label="IP-Adapter SDXL Model", width=220, options=[], value=controlnet_xs_prefs['ip_adapter_SDXL_model'], visible=controlnet_xs_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_SDXL_model'))
    for m in ip_adapter_SDXL_models:
        ip_adapter_SDXL_model.options.append(dropdown.Option(m['name']))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=controlnet_xs_prefs, key='ip_adapter_image', col={'lg':6}, page=page)
    ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_xs_prefs, key='ip_adapter_strength', col={'lg':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    ip_adapter_container = Container(ResponsiveRow([ip_adapter_image, ip_adapter_strength]), height = None if controlnet_xs_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    file_prefix = TextField(label="Filename Prefix",  value=controlnet_xs_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))
    show_processed_image = Checkbox(label="Show Pre-Processed Image", value=controlnet_xs_prefs['show_processed_image'], tooltip="Displays the Init-Image after being process by Canny, Depth, etc.", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'show_processed_image'))
    batch_folder_name = TextField(label="Batch Folder Name", value=controlnet_xs_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(controlnet_xs_prefs)
    page.upscalers.append(upscaler)
    page.controlnet_xs_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.controlnet_xs_output.controls) > 0
    run_prompt_list = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_xs(page, from_list=True))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🕸  ControlNet-XS Image+Text-to-Image", "Faster & Smaller Controlnet Image Conditioning Text-to-Image Diffusion Models...", actions=[save_default(controlnet_xs_prefs, ['original_image', 'init_image', 'mask_image', 'ip_adapter_image', 'multi_controlnets']), IconButton(icon=icons.HELP, tooltip="Help with ControlNetXS Settings", on_click=controlnet_xs_help)]),
        Row([control_task, original_image, init_video, add_layer_btn]),
        conditioning_scale,
        Row([control_guidance_start, control_guidance_end]),
        multi_layers,
        vid_params,
        Divider(thickness=2, height=4),
        ResponsiveRow([prompt, negative_prompt]),
        threshold,
        num_inference_row,
        guidance,
        eta_row,
        max_row,
        Row([use_SDXL, cpu_offload]),
        #use_image2image,
        #img2img_row,
        #Row([use_ip_adapter, ip_adapter_SDXL_model], vertical_alignment=CrossAxisAlignment.START),
        #ip_adapter_container,
        show_processed_image,
        Row([NumberPicker(label="Batch Size: ", min=1, max=8, value=controlnet_xs_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size')), seed, batch_folder_name, file_prefix]),
        upscaler,
        Row([ElevatedButton(content=Text("⛹️  Run ControlNet-XS", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_xs(page)),
             run_prompt_list]),
        page.controlnet_xs_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c


controlnet_video2video_prefs = {
    'init_video': '',
    'prompt': '',
    'negative_prompt': '',
    'control_task': 'Canny21',
    'controlnet_strength': 1.0,
    'init_image_strength': 0.5,
    'feedthrough_strength': 0.0,
    'motion_alpha': 0.1,
    'motion_sigma': 0.3,
    'color_fix': 'Lab', #'none', 'rgb', 'hsv', 'lab'
    'color_amount': 0.0,
    'max_size': 768,
    'low_threshold': 100, #1-255 canny
    'high_threshold': 200, #1-255
    'mlsd_score_thr': 0.1, #mlsd line detector v threshold
    'mlsd_dist_thr': 0.1, #mlsd line detector d threshold
    'steps': 25,
    'prompt_strength': 7.5,
    'start_time': 0.0,
    'end_time': 0.0,
    'duration': 0.0,
    'max_dimension': 832,
    'min_dimension': 512,
    'round_dims_to': 64,
    'no_audio': False,
    'skip_dumped_frames': False,
    'save_frames': False,
    'show_console': True,
    'fix_orientation': True,
    'file_prefix': 'controlnet-',
    'output_name': '',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildControlNet_Video2Video(page):
    global controlnet_video2video_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          controlnet_video2video_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.controlnet_video2video_output.controls = []
      page.controlnet_video2video_output.update()
      clear_button.visible = False
      clear_button.update()
    def controlnet_video2video_help(e):
      def close_controlnet_video2video_dlg(e):
        nonlocal controlnet_video2video_help_dlg
        controlnet_video2video_help_dlg.open = False
        page.update()
      controlnet_video2video_help_dlg = AlertDialog(title=Text("💁   Help with ControlNet Video2Video"), content=Column([
          Text("Can apply Stable Diffusion to a video, while maintaining frame-to-frame consistency. It is based on the Stable Diffusion img2img model, but adds a motion estimator and motion compensator to maintain consistency between frames. It will process each input frame with some preprocessing (motion transfer/compensation of the output feedback), followed by a detector and diffusion models in a pipeline configured by the ControlNet Type option."),
          Text("Feedback strength, set with init-image-strength controls frame-to-frame consistency, by changing how much the motion-compensated previous output frame is fed into the next frame's diffusion pipeline in place of initial latent noise, a la img2img latent diffusion (citation needed). Values around 0.3 to 0.5 and sometimes much higher (closer to 1.0, the maximum which means no noise is added and no denoising steps will be run)."),
          Markdown("This is an interface for running the [ControlNet Video codebase](https://github.com/un1tz3r0/controlnetvideo) by [Victor Condino](un1tz3r0@gmail.com).", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text('ControlNet is a neural network structure to control diffusion models by adding extra conditions. It copys the weights of neural network blocks into a "locked" copy and a "trainable" copy. The "trainable" one learns your condition. The "locked" one preserves your model. Thanks to this, training with small dataset of image pairs will not destroy the production-ready diffusion models. The "zero convolution" is 1×1 convolution with both weight and bias initialized as zeros. Before training, all zero convolutions output zeros, and ControlNet will not cause any distortion.  No layer is trained from scratch. You are still fine-tuning. Your original model is safe.  This allows training on small-scale or even personal devices. This is also friendly to merge/replacement/offsetting of models/weights/blocks/layers.'),
          Text("Aesthetic - Uses image features extracted using a Canny edge detector trained on a large aesthetic dataset."),
          Text("Canny Map Edge - A monochrome image with white edges on a black background."),
          Text("OpenPose - A OpenPose bone image."),
          Text("Depth - A grayscale image with black representing deep areas and white representing shallow areas."),
          Text("HED - A monochrome image with white soft edges on a black background."),
          Text("M-LSD - A monochrome image composed only of white straight lines on a black background."),
          Text("Normal Map - A normal mapped image."),
          Text("LineArt - An image with line art, usually black lines on a white background."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😸  Could get crazy... ", on_click=close_controlnet_video2video_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(controlnet_video2video_help_dlg)
      controlnet_video2video_help_dlg.open = True
      page.update()
    def change_task(e):
        changed(e,'control_task')
        canny_threshold.height = None if controlnet_video2video_prefs['control_task'] == "Canny" or controlnet_video2video_prefs['control_task'] == "Canny21" else 0
        canny_threshold.update()
        mlsd_threshold.height = None if controlnet_video2video_prefs['control_task'] == "MLSD" else 0
        mlsd_threshold.update()
    prompt = TextField(label="Prompt Text", value=controlnet_video2video_prefs['prompt'], filled=True, col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=controlnet_video2video_prefs['negative_prompt'], filled=True, col={'md':4}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    #'aesthetic', 'lineart21', 'hed', 'hed21', 'canny', 'canny21', 'openpose', 'openpose21', 'depth', 'depth21', 'normal', 'mlsd'
    control_task = Dropdown(label="ControlNet Task", width=150, options=[dropdown.Option("Aesthetic"), dropdown.Option("Lineart21"), dropdown.Option("HED"), dropdown.Option("HED21"), dropdown.Option("HED"), dropdown.Option("Canny"), dropdown.Option("Canny21"), dropdown.Option("OpenPose"), dropdown.Option("OpenPose21"), dropdown.Option("Depth"), dropdown.Option("Depth21"), dropdown.Option("Normal"), dropdown.Option("MLSD")], value=controlnet_video2video_prefs['control_task'], on_change=change_task)
    #conditioning_scale = SliderRow(label="Conditioning Scale", min=0, max=2, divisions=20, round=1, pref=controlnet_video2video_prefs, key='conditioning_scale', tooltip="The outputs of the controlnet are multiplied by `controlnet_video2video_conditioning_scale` before they are added to the residual in the original unet.")
    controlnet_strength = SliderRow(label="ControlNet Strength", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_video2video_prefs, key='controlnet_strength', tooltip="How much influence the controlnet annotator's output is used to guide the denoising process.")
    init_image_strength = SliderRow(label="Init-Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_video2video_prefs, key='init_image_strength', tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    feedthrough_strength = SliderRow(label="Feedthrough Strength", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_video2video_prefs, key='feedthrough_strength', tooltip="The ratio of input to motion compensated prior output to feed through to the next frame.")
    init_video = FileInput(label="Init Video Clip", pref=controlnet_video2video_prefs, key='init_video', ftype="video", expand=True, page=page)
    #init_video = TextField(label="Init Video Clip", value=controlnet_video2video_prefs['init_video'], expand=True, on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))
    #fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=controlnet_video2video_prefs, key='fps', tooltip="The FPS to extract from the init video clip.")
    start_time = TextField(label="Start Time (s)", value=controlnet_video2video_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype="float"))
    end_time = TextField(label="End Time (0 for all)", value=controlnet_video2video_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype="float"))
    duration = TextField(label="Duration (0 for all)", value=controlnet_video2video_prefs['duration'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'duration', ptype="float"))
    vid_params = Container(content=Column([Row([start_time, end_time, duration])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(top=5))
    num_inference_row = SliderRow(label="Number of Steps", min=1, max=100, divisions=99, pref=controlnet_video2video_prefs, key='steps', tooltip="Number of inference steps, depends on the scheduler, trades off speed for quality.")
    prompt_strength = SliderRow(label="Prompt Strength", min=0, max=30, divisions=60, round=1, pref=controlnet_video2video_prefs, key='prompt_strength', tooltip="How much influence the prompt has on the output. Guidance Scale.")
    low_threshold_row = SliderRow(label="Canny Low Threshold", min=1, max=255, divisions=254, pref=controlnet_video2video_prefs, key='low_threshold', expand=True, col={'lg':6}, tooltip="Lower increases sensitivity to weaker edges, higher gives fewer but more reliable edge detections.")
    high_threshold_row = SliderRow(label="Canny High Threshold", min=1, max=255, divisions=254, pref=controlnet_video2video_prefs, key='high_threshold', expand=True, col={'lg':6}, tooltip="Higher value decreases the amount of noise but could result in missing some true edges.")
    canny_threshold = Container(ResponsiveRow([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(bottom=8))
    canny_threshold.height = None if "Canny" in controlnet_video2video_prefs['control_task'] else 0
    mlsd_score_thr = SliderRow(label="MLSD Score Threshold", min=0.0, max=1.0, divisions=10, round=1, pref=controlnet_video2video_prefs, key='mlsd_score_thr', expand=True, col={'lg':6})
    mlsd_dist_thr = SliderRow(label="MLSD Dist Threshold", min=0.0, max=1.0, divisions=10, round=1, pref=controlnet_video2video_prefs, key='mlsd_dist_thr', expand=True, col={'lg':6})
    mlsd_threshold = Container(ResponsiveRow([mlsd_score_thr, mlsd_dist_thr]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(bottom=8))
    mlsd_threshold.height = None if controlnet_video2video_prefs['control_task'] == "MLSD" else 0
    motion_alpha = SliderRow(label="Motion Alpha", min=0.0, max=1.0, divisions=10, round=1, pref=controlnet_video2video_prefs, key='motion_alpha', expand=True, col={'lg':6}, tooltip="Smooth the motion vectors over time, 0.0 is no smoothing, 1.0 is maximum smoothing.")
    motion_sigma = SliderRow(label="Motion Sigma", min=0.0, max=1.0, divisions=10, round=1, pref=controlnet_video2video_prefs, key='motion_sigma', expand=True, col={'lg':6}, tooltip="Smooth the motion estimate spatially, 0.0 is no smoothing, used as sigma for gaussian blur.")
    max_dimension = SliderRow(label="Max Dimension", min=256, max=1280, divisions=32, multiple=32, suffix="px", pref=controlnet_video2video_prefs, key='max_dimension', expand=True, col={'lg':6})
    min_dimension = SliderRow(label="Mix Dimension", min=256, max=1280, divisions=32, multiple=32, suffix="px", pref=controlnet_video2video_prefs, key='min_dimension', expand=True, col={'lg':6})
    color_fix = Dropdown(label="Color Fix", width=150, options=[dropdown.Option("None"), dropdown.Option("RGB"), dropdown.Option("HSV"), dropdown.Option("Lab")], value=controlnet_video2video_prefs['color_fix'], on_change=lambda e:changed(e,'color_fix'), tooltip="Prevent color from drifting due to feedback and model bias by fixing the histogram to the first frame. Specify colorspace for histogram matching")
    color_amount = SliderRow(label="Color Amount", min=0.0, max=1.0, divisions=10, round=1, pref=controlnet_video2video_prefs, key='color_amount', expand=True, tooltip="Blend between the original color and the color matched version.")
    #max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_video2video_prefs, key='max_size')
    no_audio = Switcher(label="No Audio", value=controlnet_video2video_prefs['no_audio'], tooltip="Don't include audio in the output video, even if the input video has audio", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'no_audio'))
    skip_dumped_frames = Switcher(label="Skip Dumped Frames", value=controlnet_video2video_prefs['skip_dumped_frames'], tooltip="Read dumped frames from a previous run instead of processing the input video.", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'skip_dumped_frames'))
    save_frames = Switcher(label="Save all Frames", value=controlnet_video2video_prefs['save_frames'], tooltip="Save the dumped frames to images_out batch folder. Otherwise only saves final video, keeping pngs in temp folder.", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'save_frames'))
    fix_orientation = Switcher(label="Fix Orientation", value=controlnet_video2video_prefs['fix_orientation'], tooltip="Resize videos shot in portrait mode on some devices to fix incorrect aspect ratio bug.", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'fix_orientation'))
    show_console = Switcher(label="Show Console Output", value=controlnet_video2video_prefs['show_console'], tooltip="Outputs the progress run log in the console window. Gets messy, but useful.", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'show_console'))
    file_prefix = TextField(label="Filename Prefix",  value=controlnet_video2video_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))
    output_name = TextField(label="Output Name", value=controlnet_video2video_prefs['output_name'], on_change=lambda e:changed(e,'output_name'))
    batch_folder_name = TextField(label="Batch Folder Name", value=controlnet_video2video_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(controlnet_video2video_prefs)
    page.upscalers.append(upscaler)
    page.controlnet_video2video_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.controlnet_video2video_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🤪  ControlNet Video2Video", "Apply Stable Diffusion to a video, while maintaining frame-to-frame consistency with motion estimator & compensator...", actions=[save_default(controlnet_video2video_prefs, ['init_video']), IconButton(icon=icons.HELP, tooltip="Help with ControlNet Vid2Vid Settings", on_click=controlnet_video2video_help)]),
        ResponsiveRow([prompt, negative_prompt]),
        Row([control_task, init_video]),
        canny_threshold,
        mlsd_threshold,
        vid_params,
        controlnet_strength,
        init_image_strength,
        feedthrough_strength,
        prompt_strength,
        num_inference_row,
        Row([color_fix, color_amount]),
        ResponsiveRow([motion_alpha, motion_sigma]),
        ResponsiveRow([max_dimension, min_dimension]),
        Row([no_audio, skip_dumped_frames, save_frames, fix_orientation, show_console]),
        Row([output_name, batch_folder_name, file_prefix]),
        upscaler,
        Row([ElevatedButton(content=Text("🍃  Run ControlNet Vid2Vid", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_video2video(page))]),
        page.controlnet_video2video_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

deepfloyd_prefs = {
    'prompt': '',
    'negative_prompt': '',
    'init_image': '',
    'mask_image': '',
    'alpha_mask': False,
    'invert_mask': False,
    'num_inference_steps': 100,
    'guidance_scale': 10.0,
    'image_strength': 0.7,
    'superres_num_inference_steps': 50,
    'superres_guidance_scale': 4.0,
    'upscale_num_inference_steps': 75,
    'upscale_guidance_scale': 9.0,
    'eta': 0.0,
    'seed': 0,
    'max_size': 768,
    'model_size': 'L (900M)',
    'apply_watermark': True,
    'low_memory': True,
    'keep_pipelines': False,
    'num_images': 1,
    'batch_folder_name': '',
    'file_prefix': 'IF-',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildDeepFloyd(page):
    global deepfloyd_prefs, prefs, pipe_deepfloyd
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          deepfloyd_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_deepfloyd_output(o):
      page.deepfloyd_output.controls.append(o)
      page.deepfloyd_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.deepfloyd_output.controls = []
      page.deepfloyd_output.update()
      clear_button.visible = False
      clear_button.update()
    def deepfloyd_help(e):
      def close_deepfloyd_dlg(e):
        nonlocal deepfloyd_help_dlg
        deepfloyd_help_dlg.open = False
        page.update()
      deepfloyd_help_dlg = AlertDialog(title=Text("💁   Help with DeepFloyd-IF"), content=Column([
          Markdown("**You must accept the license on the model card of [DeepFloyd/IF-I-XL-v1.0](https://huggingface.co/DeepFloyd/IF-I-XL-v1.0) before using.**", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text('DeepFloyd IF is a novel state-of-the-art open-source text-to-image model with a high degree of photorealism and language understanding. Built by the team behind ruDALL-E (the Russian-language version of OpenAI\'s DALL-E algorithm), inspired by Google\'s "Imagen", and backed by the company behind Stable Diffusion, DeepFloyd\'s IF outperforms all of those algorithms. DeepFloyd IF is particularly good at understanding complex prompts and relationships between objects. It is also very good at inserting legible text into images - even more so than Stable Diffusion XL. It can even understand prompts in multiple languages. IF, or "Intelligent Fiction", is a text2image generator that is designed to create text and captions in the images in response to a prompt. The model is a modular composed of a frozen text encoder and three cascaded pixel diffusion modules:'),
          Markdown("""* Stage 1: a base model that generates 64x64 px image based on text prompt,
* Stage 2: a 64x64 px => 256x256 px super-resolution model, and a
* Stage 3: a 256x256 px => 1024x1024 px super-resolution model Stage 1 and Stage 2 utilize a frozen text encoder based on the T5 transformer to extract text embeddings, which are then fed into a UNet architecture enhanced with cross-attention and attention pooling. Stage 3 is Stability's x4 Upscaling model. The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID score of 6.66 on the COCO dataset. Our work underscores the potential of larger UNet architectures in the first stage of cascaded diffusion models and depicts a promising future for text-to-image synthesis.""", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😌  Let's go Deep... ", on_click=close_deepfloyd_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(deepfloyd_help_dlg)
      deepfloyd_help_dlg.open = True
      page.update()
    def change_eta(e):
        changed(e, 'eta', ptype="float")
        eta_value.value = f" {deepfloyd_prefs['eta']}"
        eta_value.update()
        eta_row.update()
    init_image = FileInput(label="Original Image (optional)", pref=deepfloyd_prefs, key='init_image', expand=True, page=page)
    #init_image = TextField(label="Original Image (optional)", value=deepfloyd_prefs['init_image'], expand=True, on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    mask_image = FileInput(label="Mask Image (optional)", pref=deepfloyd_prefs, key='mask_image', expand=1, page=page)
    #mask_image = TextField(label="Mask Image (optional)", value=deepfloyd_prefs['mask_image'], expand=1, on_change=lambda e:changed(e,'mask_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=deepfloyd_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
    alpha_mask = Checkbox(label="Alpha Mask", value=deepfloyd_prefs['alpha_mask'], tooltip="Use Transparent Alpha Channel of Init as Mask", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))
    prompt = TextField(label="Prompt Text", value=deepfloyd_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=deepfloyd_prefs['negative_prompt'], filled=True, col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    seed = TextField(label="Seed", width=90, value=str(deepfloyd_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=200, divisions=199, pref=deepfloyd_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=deepfloyd_prefs, key='guidance_scale')
    superres_num_inference_row = SliderRow(label="Super Res Inference Steps", min=1, max=200, divisions=199, pref=deepfloyd_prefs, key='superres_num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    superres_guidance = SliderRow(label="Super Res Guidance Scale", min=0, max=50, divisions=100, round=1, pref=deepfloyd_prefs, key='superres_guidance_scale')
    upscale_num_inference_row = SliderRow(label="Upscale Inference Steps", min=1, max=200, divisions=199, pref=deepfloyd_prefs, key='upscale_num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    upscale_guidance = SliderRow(label="Upscale Guidance Scale", min=0, max=50, divisions=100, round=1, pref=deepfloyd_prefs, key='upscale_guidance_scale')
    image_strength = SliderRow(label="Image Strength", min=0, max=1, divisions=20, round=2, pref=deepfloyd_prefs, key='image_strength', tooltip="Conceptually, indicates how much to transform the reference `image`. Denoising steps depends on the amount of noise initially added.")
    eta = Slider(min=0.0, max=1.0, divisions=20, round=2, label="{value}", value=float(deepfloyd_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=change_eta)
    eta_value = Text(f" {deepfloyd_prefs['eta']}", weight=FontWeight.BOLD)
    eta_row = Row([Text("ETA:"), eta_value, Text("  DDIM"), eta, Text("DDPM")])
    #page.etas.append(eta_row)
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=deepfloyd_prefs, key='max_size', tooltip="Resizes your Init and Mask Image to save memory.")
    apply_watermark = Tooltip(message="Under the license, you are legally required to include the watermark on bottom right corner.", content=Switcher(label="Apply IF Watermark", value=deepfloyd_prefs['apply_watermark'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'apply_watermark')))
    low_memory = Tooltip(message="Needed for < 16GB VRAM to run. If you have more power, disable for faster runs.", content=Switcher(label="Lower Memory", value=deepfloyd_prefs['low_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'low_memory')))
    keep_pipelines = Tooltip(message="Keeps all 3 Pipelines loaded persistantly. Faster reuse, but needs a lot of VRAM.", content=Switcher(label="Keep Pipelines Loaded", value=deepfloyd_prefs['keep_pipelines'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'keep_pipelines')))
    model_size = Dropdown(label="Model Size", hint_text="Amount of Parameters trained on. Depends on your available memory.", width=240, options=[dropdown.Option("XL (4.3B)"), dropdown.Option("L (900M)"), dropdown.Option("M (400M)")], value=deepfloyd_prefs['model_size'], autofocus=False, on_change=lambda e:changed(e, 'model_size'))
    file_prefix = TextField(label="Filename Prefix",  value=deepfloyd_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))
    batch_folder_name = TextField(label="Batch Folder Name", value=deepfloyd_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(deepfloyd_prefs)
    page.upscalers.append(upscaler)
    page.deepfloyd_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.deepfloyd_output.controls) > 0
    run_prompt_list = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_deepfloyd(page, from_list=True))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🌈  DeepFloyd IF (under construction, may not work)", "A new AI image generator that achieves state-of-the-art results on numerous image-generation tasks...", actions=[save_default(deepfloyd_prefs, ['init_image', 'mask_image']), IconButton(icon=icons.HELP, tooltip="Help with IF-DeepFloyd Settings", on_click=deepfloyd_help)]),
        ResponsiveRow([prompt, negative_prompt]),
        ResponsiveRow([Row([init_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        #Row([init_image, mask_image, invert_mask]),
        image_strength,
        num_inference_row,
        guidance,
        superres_num_inference_row,
        superres_guidance,
        upscale_num_inference_row,
        upscale_guidance,
        eta_row,
        max_row,
        Row([apply_watermark, low_memory, keep_pipelines, model_size]),
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=deepfloyd_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name, file_prefix]),
        upscaler,
        Row([ElevatedButton(content=Text("🎈  Run DeepFloyd", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_deepfloyd(page)),
             run_prompt_list]),
        page.deepfloyd_output,
        clear_button,
      ]))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c


amused_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "amused-",
    "num_images": 1,
    "width": 1024,
    "height":1024,
    "guidance_scale": 10.0,
    'num_inference_steps': 12,
    "seed": 0,
    'init_image': '',
    'mask_image': '',
    'init_image_strength': 0.8,
    "cpu_offload": False,
    "cpu_only": False,
    "amused_model": "amused-256",
    "custom_model": "",
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildAmused(page):
    global prefs, amused_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          amused_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def amused_help(e):
      def close_amused_dlg(e):
        nonlocal amused_help_dlg
        amused_help_dlg.open = False
        page.update()
      amused_help_dlg = AlertDialog(title=Text("🙅   Help with Amused Pipeline"), content=Column([
          Text("Amused is a lightweight text to image model based off of the muse architecture. Amused is particularly useful in applications that require a lightweight and fast model such as generating many images quickly at once. Amused is a vqvae token based transformer that can generate an image in fewer forward passes than many diffusion models. In contrast with muse, it uses the smaller text encoder CLIP-L/14 instead of t5-xxl. Due to its small parameter count and few forward pass generation process, amused can generate many images quickly. This benefit is seen particularly at larger batch sizes."),
          Text("Muse is a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality, etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing."),
          Markdown("[Project](https://muse-model.github.io) | [Paper](https://huggingface.co/papers/2401.01808) | [HuggingFace Model](https://huggingface.co/amused/amused-256)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Markdown("Contributors include Suraj Patil, William Berman, Patrick von Platen, Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang Ming-Hsuan Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li and Dilip Krishnan.", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🎢  Very Amusing...", on_click=close_amused_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(amused_help_dlg)
      amused_help_dlg.open = True
      page.update()
    def changed_model(e):
        amused_prefs['amused_model'] = e.control.value
        amused_custom_model.visible = e.control.value == "Custom"
        amused_custom_model.update()
    prompt = TextField(label="Prompt Text", value=amused_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=amused_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = FileInput(label="Init Image (optional)", pref=amused_prefs, key='init_image', page=page, col={'md':6})
    mask_image = FileInput(label="Mask Image (optional)", pref=amused_prefs, key='mask_image', page=page, col={'md':6})
    init_image_strength = SliderRow(label="Init-Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=amused_prefs, key='init_image_strength', col={'md':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    batch_folder_name = TextField(label="Batch Folder Name", value=amused_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=amused_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Images", min=1, max=20, step=1, value=amused_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=40, divisions=40, pref=amused_prefs, key='num_inference_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=amused_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=amused_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=amused_prefs, key='height')
    amused_model = Dropdown(label="Amused Model", width=220, options=[dropdown.Option("Custom"), dropdown.Option("amused-256"), dropdown.Option("amused-512")], value=amused_prefs['amused_model'], on_change=changed_model)
    amused_custom_model = TextField(label="Custom Amused Model (URL or Path)", value=amused_prefs['custom_model'], expand=True, visible=amused_prefs['amused_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    cpu_offload = Switcher(label="CPU Offload", value=amused_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    #cpu_only = Switcher(label="CPU Only (not yet)", value=amused_prefs['cpu_only'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_only'), tooltip="If you don't have a good GPU, can run entirely on CPU")
    seed = TextField(label="Seed", width=90, value=str(amused_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(amused_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🎠   Run aMUSEd", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_amused(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_amused(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_amused(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.amused_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🎡  aMUSEd Open-MUSE", "Lightweight and Fast vqVAE Masked Generative Transformer Model to make many images quickly at once...", actions=[save_default(amused_prefs, ['init_image', 'mask_image']), IconButton(icon=icons.HELP, tooltip="Help with Amused Settings", on_click=amused_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            ResponsiveRow([init_image, mask_image]),
            init_image_strength,
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            Row([amused_model, amused_custom_model]),
            #Row([cpu_offload, cpu_only]),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.amused_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

wuerstchen_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "wuerstchen-",
    "num_images": 1,
    "steps":12,
    "width": 1024,
    "height":1024,
    "guidance_scale":4.0,
    'prior_guidance_scale': 4.0,
    'prior_steps': 60,
    "seed": 0,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildWuerstchen(page):
    global prefs, wuerstchen_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          wuerstchen_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def wuerstchen_help(e):
      def close_wuerstchen_dlg(e):
        nonlocal wuerstchen_help_dlg
        wuerstchen_help_dlg.open = False
        page.update()
      wuerstchen_help_dlg = AlertDialog(title=Text("🙅   Help with Würstchen Pipeline"), content=Column([
          Text("Würstchen: Efficient Pretraining of Text-to-Image Models is by Pablo Pernias, Dominic Rampas, and Marc Aubreville."),
          Text("We introduce Würstchen, a novel technique for text-to-image synthesis that unites competitive performance with unprecedented cost-effectiveness and ease of training on constrained hardware. Building on recent advancements in machine learning, our approach, which utilizes latent diffusion strategies at strong latent image compression rates, significantly reduces the computational burden, typically associated with state-of-the-art models, while preserving, if not enhancing, the quality of generated images. Wuerstchen achieves notable speed improvements at inference time, thereby rendering real-time applications more viable. One of the key advantages of our method lies in its modest training requirements of only 9,200 GPU hours, slashing the usual costs significantly without compromising the end performance. In a comparison against the state-of-the-art, we found the approach to yield strong competitiveness. This paper opens the door to a new line of research that prioritizes both performance and computational accessibility, hence democratizing the use of sophisticated AI technologies. Through Wuerstchen, we demonstrate a compelling stride forward in the realm of text-to-image synthesis, offering an innovative path to explore in future research."),
          Markdown("[Paper](https://huggingface.co/papers/2306.00637) | [Original GitHub](https://github.com/dome272/Wuerstchen)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🐗  Some Brätwurst? ", on_click=close_wuerstchen_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(wuerstchen_help_dlg)
      wuerstchen_help_dlg.open = True
      page.update()
    prompt = TextField(label="Prompt Text", value=wuerstchen_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=wuerstchen_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=wuerstchen_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=wuerstchen_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=wuerstchen_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Decoder Steps", min=0, max=100, divisions=100, pref=wuerstchen_prefs, key='steps', col={'xs':12, 'md':6})
    prior_guidance_scale = SliderRow(label="Prior Guidance Scale", min=0, max=10, divisions=20, round=1, expand=True, pref=wuerstchen_prefs, key='prior_guidance_scale', col={'xs':12, 'md':6})
    prior_steps = SliderRow(label="Prior Steps", min=0, max=100, divisions=100, expand=True, pref=wuerstchen_prefs, key='prior_steps', col={'xs':12, 'md':6})
    guidance = SliderRow(label="Decoder Guidance Scale", min=0, max=10, divisions=20, round=1, pref=wuerstchen_prefs, key='guidance_scale', col={'xs':12, 'md':6})
    width_slider = SliderRow(label="Width", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=wuerstchen_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=wuerstchen_prefs, key='height')
    seed = TextField(label="Seed", width=90, value=str(wuerstchen_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(wuerstchen_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🐷   Run Würstchen", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_wuerstchen(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_wuerstchen(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_wuerstchen(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.wuerstchen_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🌭  Würstchen", "Text-to-Image Synthesis uniting competitive performance, cost-effectiveness and ease of training on constrained hardware.", actions=[save_default(wuerstchen_prefs), IconButton(icon=icons.HELP, tooltip="Help with Würstchen Settings", on_click=wuerstchen_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            ResponsiveRow([prior_steps, prior_guidance_scale]),
            ResponsiveRow([steps, guidance]), 
            width_slider, height_slider, #Divider(height=9, thickness=2),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.wuerstchen_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

stable_cascade_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "cascade-",
    "num_images": 1,
    "steps":12,
    "guidance_scale": 1.0,
    'prior_guidance_scale': 4.0,
    'prior_steps': 60,
    "width": 1024,
    "height":1024,
    "seed": 0,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildStableCascade(page):
    global prefs, stable_cascade_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          stable_cascade_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def stable_cascade_help(e):
      def close_stable_cascade_dlg(e):
        nonlocal stable_cascade_help_dlg
        stable_cascade_help_dlg.open = False
        page.update()
      stable_cascade_help_dlg = AlertDialog(title=Text("🙅   Help with Stable Cascade Pipeline"), content=Column([
          Text("This model is built upon the Würstchen architecture and its main difference to other models, like Stable Diffusion, is that it is working at a much smaller latent space. Why is this important? The smaller the latent space, the faster you can run inference and the cheaper the training becomes. How small is the latent space? Stable Diffusion uses a compression factor of 8, resulting in a 1024x1024 image being encoded to 128x128. Stable Cascade achieves a compression factor of 42, meaning that it is possible to encode a 1024x1024 image to 24x24, while maintaining crisp reconstructions. The text-conditional model is then trained in the highly compressed latent space. Previous versions of this architecture, achieved a 16x cost reduction over Stable Diffusion 1.5."),
          Text("Stable Cascade achieves impressive results, both visually and evaluation wise. According to our evaluation, Stable Cascade performs best in both prompt alignment and aesthetic quality in almost all comparisons. Stable Cascade´s focus on efficiency is evidenced through its architecture and a higher compressed latent space. Despite the largest model containing 1.4 billion parameters more than Stable Diffusion XL, it still features faster inference times. Stable Cascade consists of three models: Stage A, Stage B and Stage C, representing a cascade for generating images, hence the name 'Stable Cascade'. Stage A & B are used to compress images, similarly to what the job of the VAE is in Stable Diffusion. However, as mentioned before, with this setup a much higher compression of images can be achieved. Furthermore, Stage C is responsible for generating the small 24 x 24 latents given a text prompt."),
          Markdown("[Paper](https://openreview.net/forum?id=gU58d5QeGv) | [Original GitHub](https://github.com/Stability-AI/StableCascade) | [Model Card](https://huggingface.co/stabilityai/stable-cascade)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("💆  Cascading Benefits ", on_click=close_stable_cascade_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(stable_cascade_help_dlg)
      stable_cascade_help_dlg.open = True
      page.update()
    prompt = TextField(label="Prompt Text", value=stable_cascade_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=stable_cascade_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=stable_cascade_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=stable_cascade_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #steps = TextField(label="Decoder Steps", value=stable_cascade_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype="int"))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=stable_cascade_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    prior_steps = SliderRow(label="Prior Steps", min=0, max=100, divisions=100, expand=True, pref=stable_cascade_prefs, key='prior_steps', col={'xs':12, 'md':6})
    prior_guidance_scale = SliderRow(label="Prior Guidance Scale", min=0, max=10, divisions=20, round=1, expand=True, pref=stable_cascade_prefs, key='prior_guidance_scale', col={'xs':12, 'md':6})
    steps = SliderRow(label="Decoder Steps", min=0, max=100, divisions=100, pref=stable_cascade_prefs, key='steps', col={'xs':12, 'md':6})
    guidance = SliderRow(label="Decoder Guidance Scale", min=0, max=10, divisions=20, round=1, pref=stable_cascade_prefs, key='guidance_scale', col={'xs':12, 'md':6})
    width_slider = SliderRow(label="Width", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=stable_cascade_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=stable_cascade_prefs, key='height')
    seed = TextField(label="Seed", width=90, value=str(stable_cascade_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(stable_cascade_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🍂   Run Stable Cascade", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_stable_cascade(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_stable_cascade(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_stable_cascade(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.stable_cascade_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🌷  Stable Cascade", "Efficient Text-to-Image Synthesis using Würstchen 3 Enhanced... Excelent prompt understanding & text writing.", actions=[save_default(stable_cascade_prefs), IconButton(icon=icons.HELP, tooltip="Help with Stable Cascade Settings", on_click=stable_cascade_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            ResponsiveRow([prior_steps, prior_guidance_scale]),
            ResponsiveRow([steps, guidance]), 
            width_slider, height_slider, #Divider(height=9, thickness=2),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.stable_cascade_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

pixart_alpha_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "pixart-",
    "num_images": 1,
    "width": 1024,
    "height":1024,
    "guidance_scale":4.5,
    'num_inference_steps': 30,
    "seed": 0,
    "clean_caption": True,
    "resolution_binning": True,
    #"mask_feature": True,
    "cpu_offload": True,
    "use_8bit": False,
    "pixart_model": "PixArt-XL-2-1024-MS",
    "custom_model": "",
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildPixArtAlpha(page):
    global prefs, pixart_alpha_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          pixart_alpha_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def pixart_alpha_help(e):
      def close_pixart_alpha_dlg(e):
        nonlocal pixart_alpha_help_dlg
        pixart_alpha_help_dlg.open = False
        page.update()
      pixart_alpha_help_dlg = AlertDialog(title=Text("🙅   Help with PixArt-α Pipeline"), content=Column([
          Text("The most advanced text-to-image (T2I) models require significant training costs (e.g., millions of GPU hours), seriously hindering the fundamental innovation for the AIGC community while increasing CO2 emissions. This paper introduces PIXART-α, a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), reaching near-commercial application standards. Additionally, it supports high-resolution image synthesis up to 1024px resolution with low training cost, as shown in Figure 1 and 2. To achieve this goal, three core designs are proposed: (1) Training strategy decomposition: We devise three distinct training steps that separately optimize pixel dependency, text-image alignment, and image aesthetic quality; (2) Efficient T2I Transformer: We incorporate cross-attention modules into Diffusion Transformer (DiT) to inject text conditions and streamline the computation-intensive class-condition branch; (3) High-informative data: We emphasize the significance of concept density in text-image pairs and leverage a large Vision-Language model to auto-label dense pseudo-captions to assist text-image alignment learning. As a result, PIXART-α's training speed markedly surpasses existing large-scale T2I models, e.g., PIXART-α only takes 10.8% of Stable Diffusion v1.5's training time (675 vs. 6,250 A100 GPU days), saving nearly $300,000 ($26,000 vs. $320,000) and reducing 90% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training cost is merely 1%. Extensive experiments demonstrate that PIXART-α excels in image quality, artistry, and semantic control. We hope PIXART-α will provide new insights to the AIGC community and startups to accelerate building their own high-quality yet low-cost generative models from scratch."),
          Text("It uses a Transformer backbone (instead of a UNet) for denoising. As such it has a similar architecture as DiT. It was trained using text conditions computed from T5. This aspect makes the pipeline better at following complex text prompts with intricate details. It is good at producing high-resolution images at different aspect ratios. It rivals the quality of state-of-the-art text-to-image generation systems (as of this writing) such as Stable Diffusion XL, Imagen, and DALL-E 2, while being more efficient than them."),
          Markdown("[Paper](https://huggingface.co/papers/2310.00426) | [PixArt-alpha GitHub](https://github.com/PixArt-alpha/PixArt-alpha) | [PixArt-alpha Checkpoints](https://huggingface.co/PixArt-alpha) | [Recomended Sizes](https://github.com/PixArt-alpha/PixArt-alpha/blob/master/diffusion/data/datasets/utils.py)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🐗  Pix that Art ", on_click=close_pixart_alpha_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(pixart_alpha_help_dlg)
      pixart_alpha_help_dlg.open = True
      page.update()
    def changed_model(e):
        pixart_alpha_prefs['pixart_model'] = e.control.value
        pixart_custom_model.visible = e.control.value == "Custom"
        pixart_custom_model.update()
    prompt = TextField(label="Prompt Text", value=pixart_alpha_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=pixart_alpha_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=pixart_alpha_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=pixart_alpha_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #num_inference_steps = TextField(label="Number of Steps", value=pixart_alpha_prefs['num_inference_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_inference_steps', ptype="int"))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=pixart_alpha_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=pixart_alpha_prefs, key='num_inference_steps')
    #prior_guidance_scale = SliderRow(label="Prior Guidance Scale", min=0, max=10, divisions=20, round=1, expand=True, pref=pixart_alpha_prefs, key='prior_guidance_scale', col={'xs':12, 'md':6})
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=pixart_alpha_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=pixart_alpha_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=pixart_alpha_prefs, key='height')
    pixart_model = Dropdown(label="PixArt-α Model", width=260, options=[dropdown.Option("Custom"), dropdown.Option("PixArt-XL-2-1024-MS"), dropdown.Option("PixArt-XL-2-512x512"), dropdown.Option("PixArt-LCM-XL-2-1024-MS"), dropdown.Option("TwistedReality-PixArt-1024ms"), dropdown.Option("TwistedReality-PixArt-512ms")], value=pixart_alpha_prefs['pixart_model'], on_change=changed_model)
    pixart_custom_model = TextField(label="Custom PixArt-α Model (URL or Path)", value=pixart_alpha_prefs['custom_model'], expand=True, visible=pixart_alpha_prefs['pixart_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    clean_caption = Switcher(label="Clean Caption", value=pixart_alpha_prefs['clean_caption'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'clean_caption'), tooltip="Whether or not to clean the caption before creating embeddings.")
    resolution_binning = Switcher(label="Resolution Binning", value=pixart_alpha_prefs['resolution_binning'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'resolution_binning'), tooltip="The requested height and width are first mapped to the closest resolutions using `ASPECT_RATIO_1024_BIN`. After the produced latents are decoded into images, they are resized back to the requested resolution. Useful for generating non-square images.")
    #mask_feature = Switcher(label="Feature Mask", value=pixart_alpha_prefs['mask_feature'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'mask_feature'), tooltip="If enabled, the text embeddings will be masked.")
    cpu_offload = Switcher(label="CPU Offload", value=pixart_alpha_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    use_8bit = Switcher(label="Use 8-bit Precision", value=pixart_alpha_prefs['use_8bit'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_8bit'), tooltip="Runs with under 8GB VRAM by loading the text encoder in 8-bit numerical precision. Reduces quality & loads slower.")
    seed = TextField(label="Seed", width=90, value=str(pixart_alpha_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(pixart_alpha_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🏂   Run PixArt-α", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_pixart_alpha(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_pixart_alpha(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_pixart_alpha(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.pixart_alpha_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🧚  PixArt-αlpha", "Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis... Note: Uses a lot of RAM & Space, may run out.", actions=[save_default(pixart_alpha_prefs), IconButton(icon=icons.HELP, tooltip="Help with PixArt-α Settings", on_click=pixart_alpha_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            #ResponsiveRow([num_inference_steps]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            Row([pixart_model, pixart_custom_model]),
            Row([clean_caption, resolution_binning, cpu_offload, use_8bit]),
            #Can't get wrap to work!! Container(Row([Container(clean_caption), Container(resolution_binning), Container(cpu_offload), Container(use_8bit)], wrap=True, expand=True, width=page.width, alignment=ft.MainAxisAlignment.START), width=800),#], expand=True),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.pixart_alpha_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

pixart_sigma_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "pixart-",
    "num_images": 1,
    "width": 1024,
    "height":1024,
    "guidance_scale":4.5,
    'num_inference_steps': 30,
    "seed": 0,
    "clean_caption": True,
    "resolution_binning": True,
    #"mask_feature": True,
    "cpu_offload": True,
    "use_8bit": False,
    'use_refiner': False,
    'SDXL_high_noise_frac': 0.7,
    "pixart_model": "PixArt-Sigma-XL-2-512-MS",
    "custom_model": "",
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildPixArtSigma(page):
    global prefs, pixart_sigma_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          pixart_sigma_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def pixart_sigma_help(e):
      def close_pixart_sigma_dlg(e):
        nonlocal pixart_sigma_help_dlg
        pixart_sigma_help_dlg.open = False
        page.update()
      pixart_sigma_help_dlg = AlertDialog(title=Text("🙅   Help with PixArt-Σ Pipeline"), content=Column([
          Text("PixArt-Σ is a Diffusion Transformer model (DiT) capable of directly generating images at 4K resolution. PixArt-Σ represents a significant advancement over its predecessor, PixArt-α, offering images of markedly higher fidelity and improved alignment with text prompts. A key feature of PixArt-Σ is its training efficiency. Leveraging the foundational pre-training of PixArt-α, it evolves from the ‘weaker’ baseline to a ‘stronger’ model via incorporating higher quality data, a process we term “weak-to-strong training”. The advancements in PixArt-Σ are twofold: (1) High-Quality Training Data: PixArt-Σ incorporates superior-quality image data, paired with more precise and detailed image captions. (2) Efficient Token Compression: we propose a novel attention module within the DiT framework that compresses both keys and values, significantly improving efficiency and facilitating ultra-high-resolution image generation. Thanks to these improvements, PixArt-Σ achieves superior image quality and user prompt adherence capabilities with significantly smaller model size (0.6B parameters) than existing text-to-image diffusion models, such as SDXL (2.6B parameters) and SD Cascade (5.1B parameters). Moreover, PixArt-Σ’s capability to generate 4K images supports the creation of high-resolution posters and wallpapers, efficiently bolstering the production of high-quality visual content in industries such as film and gaming."),
          Text("PixArt-Sigma achieves superior image quality and alignment with prompts compared to previous models like PixArt-alpha. It does so efficiently, evolving from PixArt-alpha through a process termed weak-to-strong training - leveraging higher quality data and an improved attention mechanism. With just 0.6 billion parameters, PixArt-Sigma reaches new heights in text-to-image generation. Output stunning, intricate 4K images for posters, wallpapers, concept art, and more. Guide the model with descriptive prompts and fine-tune parameters like guidance scale and number of inference steps."),
          Markdown("[Project Page](https://pixart-alpha.github.io/PixArt-sigma-project/) | [Paper](https://arxiv.org/abs/2403.04692) | [PixArt-sigma GitHub](https://pixart-alpha.github.io/PixArt-sigma-project/) | [PixArt-sigma Checkpoints](https://huggingface.co/PixArt-alpha/PixArt-Sigma-XL-2-1024-MS) | [HuggingFace Space](https://huggingface.co/spaces/PixArt-alpha/PixArt-Sigma) | [Recomended Sizes](https://github.com/PixArt-sigma/PixArt-sigma/blob/master/diffusion/data/datasets/utils.py)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Credits go to Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, Zhenguo Li"),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🐗  Pix that Art ", on_click=close_pixart_sigma_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(pixart_sigma_help_dlg)
      pixart_sigma_help_dlg.open = True
      page.update()
    def changed_model(e):
        pixart_sigma_prefs['pixart_model'] = e.control.value
        pixart_custom_model.visible = e.control.value == "Custom"
        pixart_custom_model.update()
    def toggle_refiner(e):
        pixart_sigma_prefs['use_refiner'] = e.control.value
        SDXL_high_noise_frac.show = e.control.value
    prompt = TextField(label="Prompt Text", value=pixart_sigma_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=pixart_sigma_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=pixart_sigma_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=pixart_sigma_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #num_inference_steps = TextField(label="Number of Steps", value=pixart_sigma_prefs['num_inference_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_inference_steps', ptype="int"))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=pixart_sigma_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=pixart_sigma_prefs, key='num_inference_steps')
    #prior_guidance_scale = SliderRow(label="Prior Guidance Scale", min=0, max=10, divisions=20, round=1, expand=True, pref=pixart_sigma_prefs, key='prior_guidance_scale', col={'xs':12, 'md':6})
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=pixart_sigma_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=pixart_sigma_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=pixart_sigma_prefs, key='height')
    pixart_model = Dropdown(label="PixArt-Σ Model", width=260, options=[dropdown.Option("Custom"), dropdown.Option("PixArt-Sigma-XL-2-512-MS"), dropdown.Option("PixArt-Sigma-XL-2-1024-MS"), dropdown.Option("PixArt-Sigma-XL-2-2K-MS"), dropdown.Option("SigmaJourney-1024ms")], value=pixart_sigma_prefs['pixart_model'], on_change=changed_model)
    pixart_custom_model = TextField(label="Custom PixArt-Σ Model (URL or Path)", value=pixart_sigma_prefs['custom_model'], expand=True, visible=pixart_sigma_prefs['pixart_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    clean_caption = Switcher(label="Clean Caption", value=pixart_sigma_prefs['clean_caption'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'clean_caption'), tooltip="Whether or not to clean the caption before creating embeddings.")
    resolution_binning = Switcher(label="Resolution Binning", value=pixart_sigma_prefs['resolution_binning'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'resolution_binning'), tooltip="The requested height and width are first mapped to the closest resolutions using `ASPECT_RATIO_1024_BIN`. After the produced latents are decoded into images, they are resized back to the requested resolution. Useful for generating non-square images.")
    #mask_feature = Switcher(label="Feature Mask", value=pixart_sigma_prefs['mask_feature'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'mask_feature'), tooltip="If enabled, the text embeddings will be masked.")
    cpu_offload = Switcher(label="CPU Offload", value=pixart_sigma_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    use_8bit = Switcher(label="Use 8-bit Precision", value=pixart_sigma_prefs['use_8bit'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_8bit'), tooltip="Runs with under 8GB VRAM by loading the text encoder in 8-bit numerical precision. Reduces quality & loads slower.")
    use_refiner = Switcher(label="Use SDXL Refiner Pass", value=pixart_sigma_prefs['use_refiner'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_refiner, tooltip="Uses Expert Ensemble Refiner to clean-up after generation.")
    SDXL_high_noise_frac = SliderRow(label="Refiner High Noise Fraction", min=0, max=1, divisions=20, round=2, pref=pixart_sigma_prefs, key='SDXL_high_noise_frac', visible=pixart_sigma_prefs['use_refiner'], tooltip="Percentage of Steps to use Base model, then Refiner model. Known as an Ensemble of Expert Denoisers. Value of 1 skips Refine steps.", on_change=lambda e:changed(e,'SDXL_high_noise_frac'))
    seed = TextField(label="Seed", width=90, value=str(pixart_sigma_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(pixart_sigma_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🏂   Run PixArt-Σ", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_pixart_sigma(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_pixart_sigma(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_pixart_sigma(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.pixart_sigma_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🧚  PixArt-Σ Sigma", "Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation... Note: Uses a lot of RAM & Space, may run out.", actions=[save_default(pixart_sigma_prefs), IconButton(icon=icons.HELP, tooltip="Help with PixArt-Σ Settings", on_click=pixart_sigma_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            #ResponsiveRow([num_inference_steps]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            Row([pixart_model, pixart_custom_model]),
            #use_refiner,
            #SDXL_high_noise_frac,
            Row([clean_caption, resolution_binning, cpu_offload, use_8bit]),
            #Can't get wrap to work!! Container(Row([Container(clean_caption), Container(resolution_binning), Container(cpu_offload), Container(use_8bit)], wrap=True, expand=True, width=page.width, alignment=ft.MainAxisAlignment.START), width=800),#], expand=True),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.pixart_sigma_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

hunyuan_dit_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "hunyuan-",
    "num_images": 1,
    "steps":50,
    "width": 1024,
    "height":1024,
    "guidance_scale":5.0,
    "distilled_model": False,
    "hunyuan_model": "HunyuanDiT-v1.2-Diffusers",
    "custom_model": "",
    "cpu_offload": False,
    "use_controlnet": False,
    "control_task": "Canny Map Edge",
    "original_image": "",
    "conditioning_scale": 1.0,
    'multi_controlnets': [],
    'low_threshold': 100, #1-255
    'high_threshold': 200, #1-255
    'show_processed_image': False,
    'use_refiner': False,
    'SDXL_high_noise_frac': 0.7,
    "seed": 0,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildHunyuanDiT(page):
    global prefs, hunyuan_dit_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          hunyuan_dit_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def hunyuan_dit_help(e):
      def close_hunyuan_dit_dlg(e):
        nonlocal hunyuan_dit_help_dlg
        hunyuan_dit_help_dlg.open = False
        page.update()
      hunyuan_dit_help_dlg = AlertDialog(title=Text("🙅   Help with Hunyuan Pipeline"), content=Column([
          Text("We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models."),
          Markdown("HunyuanDiT uses two text encoders: [mT5](https://huggingface.co/google/mt5-base) and bilingual CLIP (fine-tuned by ourselves)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Markdown("[Paper](https://arxiv.org/abs/2405.08748) | [Project Page](https://dit.hunyuan.tencent.com/) | [Github](https://github.com/Tencent/HunyuanDiT) | [Model Checkpoint](https://huggingface.co/Tencent-Hunyuan/HunyuanDiT) | [HF Space](https://huggingface.co/spaces/Tencent-Hunyuan/HunyuanDiT)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🥡  Tasty Aesthetics... ", on_click=close_hunyuan_dit_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(hunyuan_dit_help_dlg)
      hunyuan_dit_help_dlg.open = True
      page.update()
    def toggle_refiner(e):
        hunyuan_dit_prefs['use_refiner'] = e.control.value
        SDXL_high_noise_frac.show = e.control.value
    def toggle_controlnet(e):
        controlnet_container.height = None if e.control.value else 0
        hunyuan_dit_prefs['use_controlnet'] = e.control.value
        controlnet_container.update()
    def add_layer(e):
        layer = {'control_task': hunyuan_dit_prefs['control_task'], 'original_image': hunyuan_dit_prefs['original_image'], 'conditioning_scale': hunyuan_dit_prefs['conditioning_scale']}
        hunyuan_dit_prefs['multi_controlnets'].append(layer)
        multi_layers.controls.append(ListTile(title=Row([Text(layer['control_task'] + " - ", weight=FontWeight.BOLD), Text(layer['original_image']), Text(f"- Conditioning Scale: {layer['conditioning_scale']}")]), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.DELETE, text="Delete Control Layer", on_click=delete_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
          ]), data=layer))
        multi_layers.update()
        hunyuan_dit_prefs['original_image'] = ""
        original_image.value = ""
        original_image.update()
    def delete_layer(e):
        hunyuan_dit_prefs['multi_controlnets'].remove(e.control.data)
        for c in multi_layers.controls:
          if c.data['original_image'] == e.control.data['original_image']:
             multi_layers.controls.remove(c)
             break
        multi_layers.update()
    def delete_all_layers(e):
        hunyuan_dit_prefs['multi_controlnets'].clear()
        multi_layers.controls.clear()
        multi_layers.update()
    def change_task(e):
        changed(e,'control_task')
        threshold.height = None if "Canny" in hunyuan_dit_prefs['control_task'] else 0
        threshold.update()
    def changed_model(e):
        hunyuan_dit_prefs['hunyuan_model'] = e.control.value
        hunyuan_custom_model.visible = e.control.value == "Custom"
        hunyuan_custom_model.update()
    prompt = TextField(label="Prompt Text", value=hunyuan_dit_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=hunyuan_dit_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=hunyuan_dit_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=hunyuan_dit_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=hunyuan_dit_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=hunyuan_dit_prefs, key='steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=hunyuan_dit_prefs, key='guidance_scale')
    low_threshold_row = SliderRow(label="Canny Low Threshold", min=1, max=255, divisions=254, pref=hunyuan_dit_prefs, key='low_threshold', col={'lg':6}, tooltip="Lower increases sensitivity to weaker edges, higher gives fewer but more reliable edge detections.")
    high_threshold_row = SliderRow(label="Canny High Threshold", min=1, max=255, divisions=254, pref=hunyuan_dit_prefs, key='high_threshold', col={'lg':6}, tooltip="Higher value decreases the amount of noise but could result in missing some true edges.")
    threshold = Container(ResponsiveRow([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(bottom=8))
    threshold.height = None if "Canny" in hunyuan_dit_prefs['control_task'] else 0
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=hunyuan_dit_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=hunyuan_dit_prefs, key='height')
    hunyuan_model = Dropdown(label="HunyuanDiT Model", width=310, options=[dropdown.Option("Custom"), dropdown.Option("HunyuanDiT-v1.2-Diffusers"), dropdown.Option("HunyuanDiT-v1.2-Diffusers-Distilled"), dropdown.Option("HunyuanDiT-v1.1-Diffusers"), dropdown.Option("HunyuanDiT-v1.1-Diffusers-Distilled")], value=hunyuan_dit_prefs['hunyuan_model'], on_change=changed_model)
    hunyuan_custom_model = TextField(label="Custom Hunyuan Model (URL or Path)", value=hunyuan_dit_prefs['custom_model'], expand=True, visible=hunyuan_dit_prefs['hunyuan_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    cpu_offload = Switcher(label="CPU Offload", value=hunyuan_dit_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 16GB VRAM. Otherwise can run out of memory.")
    distilled_model = Switcher(label="Use Distilled Model", value=hunyuan_dit_prefs['distilled_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'distilled_model'), tooltip="Generate images even faster in around 25 steps.")
    use_controlnet = Switcher(label="Use ControlNet with Canny, Pose or Depth", value=hunyuan_dit_prefs['use_controlnet'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_controlnet, tooltip="Provide an additional control image to condition and control Hunyuan-DiT generation. For example, if you provide a depth map, the ControlNet model generates an image that'll preserve the spatial information from the depth map.")
    control_task = Dropdown(label="ControlNet Task", width=180, options=[dropdown.Option("Canny Map Edge"), dropdown.Option("OpenPose"), dropdown.Option("Depth"), dropdown.Option("Marigold Depth")], value=hunyuan_dit_prefs['control_task'], on_change=change_task)
    original_image = FileInput(label="Init Image", pref=hunyuan_dit_prefs, key='original_image', expand=True, page=page)
    conditioning_scale = SliderRow(label="Conditioning Scale", min=0, max=2, divisions=20, round=1, pref=hunyuan_dit_prefs, key='conditioning_scale', expand=True, tooltip="The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added to the residual in the original unet.")
    add_layer_btn = ft.FilledButton("➕ Add Layer", width=140, on_click=add_layer)
    multi_layers = Column([], spacing=0)
    show_processed_image = Checkbox(label="Show Pre-Processed Image", value=hunyuan_dit_prefs['show_processed_image'], tooltip="Displays the Init-Image after being process by Canny, Depth, etc.", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'show_processed_image'))
    controlnet_container = Container(Column([Row([control_task, original_image, add_layer_btn]), Row([conditioning_scale]), multi_layers, threshold, show_processed_image]), height=None if hunyuan_dit_prefs['use_controlnet'] else 0, animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(top=4, left=10))
    use_refiner = Switcher(label="Use SDXL Refiner Pass", value=hunyuan_dit_prefs['use_refiner'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_refiner, tooltip="Uses Expert Ensemble Refiner to clean-up after generation.")
    SDXL_high_noise_frac = SliderRow(label="Refiner High Noise Fraction", min=0, max=1, divisions=20, round=2, pref=hunyuan_dit_prefs, key='SDXL_high_noise_frac', visible=hunyuan_dit_prefs['use_refiner'], tooltip="Percentage of Steps to use Base model, then Refiner model. Known as an Ensemble of Expert Denoisers. Value of 1 skips Refine steps.", on_change=lambda e:changed(e,'SDXL_high_noise_frac'))
    seed = TextField(label="Seed", width=90, value=str(hunyuan_dit_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(hunyuan_dit_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🏮   Run Hunyuan-DiT", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_hunyuan(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_hunyuan(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_hunyuan(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.Hunyuan_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([#ft.OutlinedButton(content=Text("Switch to 2.1", size=18), on_click=switch_version)
            Header("🉐️  Tencent Hunyuan-DiT", "Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding from Tencent Hunyuan....", actions=[save_default(hunyuan_dit_prefs), IconButton(icon=icons.HELP, tooltip="Help with Hunyuan Settings", on_click=hunyuan_dit_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            steps,
            guidance, width_slider, height_slider,
            use_controlnet,
            controlnet_container,
            use_refiner,
            SDXL_high_noise_frac,
            Row([hunyuan_model, hunyuan_custom_model]),
            upscaler,
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            parameters_row,
            page.Hunyuan_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

lumina_next_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "lumina-",
    "num_images": 1,
    "steps":30,
    "width": 1024,
    "height":1024,
    "guidance_scale":4.0,
    "cpu_offload": True,
    "seed": 0,
    "lumina_model": "Lumina-Next-SFT-diffusers",
    "custom_model": "",
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildLuminaNext(page):
    global prefs, lumina_next_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          lumina_next_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def lumina_next_help(e):
      def close_lumina_next_dlg(e):
        nonlocal lumina_next_help_dlg
        lumina_next_help_dlg.open = False
        page.update()
      lumina_next_help_dlg = AlertDialog(title=Text("🙅   Help with Lumina Pipeline"), content=Column([
          Text("Lumina-Next is a next-generation Diffusion Transformer that significantly enhances text-to-image generation, multilingual generation, and multitask performance by introducing the Next-DiT architecture, 3D RoPE, and frequency- and time-aware RoPE, among other improvements. Lumina-T2X is a nascent family of Flow-based Large Diffusion Transformers (Flag-DiT) that establishes a unified framework for transforming noise into various modalities, such as images and videos, conditioned on text instructions. Despite its promising capabilities, Lumina-T2X still encounters challenges including training instability, slow inference, and extrapolation artifacts. In this paper, we present Lumina-Next, an improved version of Lumina-T2X, showcasing stronger generation performance with increased training and inference efficiency. We begin with a comprehensive analysis of the Flag-DiT architecture and identify several suboptimal components, which we address by introducing the Next-DiT architecture with 3D RoPE and sandwich normalizations. To enable better resolution extrapolation, we thoroughly compare different context extrapolation methods applied to text-to-image generation with 3D RoPE, and propose Frequency- and Time-Aware Scaled RoPE tailored for diffusion transformers. Additionally, we introduce a sigmoid time discretization schedule to reduce sampling steps in solving the Flow ODE and the Context Drop method to merge redundant visual tokens for faster network evaluation, effectively boosting the overall sampling speed. Thanks to these improvements, Lumina-Next not only improves the quality and efficiency of basic text-to-image generation but also demonstrates superior resolution extrapolation capabilities and multilingual generation using decoder-based LLMs as the text encoder, all in a zero-shot manner. To further validate Lumina-Next as a versatile generative framework, we instantiate it on diverse tasks including visual recognition, multi-view, audio, music, and point cloud generation, showcasing strong performance across these domains."),
          Markdown("[Lumina-Next : Making Lumina-T2X Stronger and Faster with Next-DiT](https://github.com/Alpha-VLLM/Lumina-T2X/blob/main/assets/lumina-next.pdf) from Alpha-VLLM, OpenGVLab, Shanghai AI Laboratory.", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Markdown("[Paper](https://arxiv.org/abs/2405.05945) | [Github](https://github.com/Alpha-VLLM/Lumina-T2X) | [Model Checkpoint](https://huggingface.co/collections/Alpha-VLLM/lumina-family-66423205bedb81171fd0644b) | [HF Space](https://huggingface.co/spaces/Alpha-VLLM/Lumina-Next-T2I)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😎  Extra Special... ", on_click=close_lumina_next_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(lumina_next_help_dlg)
      lumina_next_help_dlg.open = True
      page.update()
    def changed_model(e):
        lumina_next_prefs['lumina_model'] = e.control.value
        lumina_custom_model.visible = e.control.value == "Custom"
        lumina_custom_model.update()
    prompt = TextField(label="Prompt Text", value=lumina_next_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=lumina_next_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=lumina_next_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=lumina_next_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=lumina_next_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=lumina_next_prefs, key='steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=lumina_next_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=lumina_next_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=lumina_next_prefs, key='height')
    lumina_model = Dropdown(label="Lumina-Next Model", width=256, options=[dropdown.Option("Custom"), dropdown.Option("Lumina-Next-SFT-diffusers")], value=lumina_next_prefs['lumina_model'], on_change=changed_model)
    lumina_custom_model = TextField(label="Custom Lumina Model (URL or Path)", value=lumina_next_prefs['custom_model'], expand=True, visible=lumina_next_prefs['lumina_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    cpu_offload = Switcher(label="CPU Offload", value=lumina_next_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 16GB VRAM. Otherwise can run out of memory.")
    #distilled_model = Switcher(label="Use Distilled Model", value=lumina_next_prefs['distilled_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'distilled_model'), tooltip="Generate images even faster in around 25 steps.")
    seed = TextField(label="Seed", width=90, value=str(lumina_next_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(lumina_next_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🌈   Run Lumina-Next-DiT", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_lumina(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_lumina(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_lumina(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.Lumina_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([#ft.OutlinedButton(content=Text("Switch to 2.1", size=18), on_click=switch_version)
            Header("🌞  Lumina-Next-DiT", "Next-Gen Diffusion Transformer that Enhances Text-to-Image Generation, Multilingual, Multitasked Performance, and really good style...", actions=[save_default(lumina_next_prefs), IconButton(icon=icons.HELP, tooltip="Help with Lumina Settings", on_click=lumina_next_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            steps,
            guidance, width_slider, height_slider,
            Row([lumina_model, lumina_custom_model]),
            upscaler,
            ResponsiveRow([Row([n_images, seed, cpu_offload], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            parameters_row,
            page.Lumina_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

kolors_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "kolors-",
    "num_images": 1,
    "width": 1024,
    "height":1024,
    "guidance_scale":5.0,
    'num_inference_steps': 50,
    "seed": 0,
    'init_image': '',
    'init_image_strength': 0.3,
    "cpu_offload": True,
    "kolors_model": "Kwai-Kolors/Kolors-diffusers",
    "custom_model": "",
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildKolors(page):
    global prefs, kolors_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          kolors_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def kolors_help(e):
      def close_kolors_dlg(e):
        nonlocal kolors_help_dlg
        kolors_help_dlg.open = False
        page.update()
      kolors_help_dlg = AlertDialog(title=Text("🙅   Help with Kolors Pipeline"), content=Column([
          Text("Kolors is a large-scale text-to-image generation model based on latent diffusion, developed by the Kuaishou Kolors team. Trained on billions of text-image pairs, Kolors exhibits significant advantages over both open-source and closed-source models in visual quality, complex semantic accuracy, and text rendering for both Chinese and English characters. Furthermore, Kolors supports both Chinese and English inputs, demonstrating strong performance in understanding and generating Chinese-specific content."),
          Text("We present Kolors, a latent diffusion model for text-to-image synthesis, characterized by its profound understanding of both English and Chinese, as well as an impressive degree of photorealism. There are three key insights contributing to the development of Kolors. Firstly, unlike large language model T5 used in Imagen and Stable Diffusion 3, Kolors is built upon the General Language Model (GLM), which enhances its comprehension capabilities in both English and Chinese. Moreover, we employ a multimodal large language model to recaption the extensive training dataset for fine-grained text understanding. These strategies significantly improve Kolors’ ability to comprehend intricate semantics, particularly those involving multiple entities, and enable its advanced text rendering capabilities. Secondly, we divide the training of Kolors into two phases: the concept learning phase with broad knowledge and the quality improvement phase with specifically curated high-aesthetic data. Furthermore, we investigate the critical role of the noise schedule and introduce a novel schedule to optimize high-resolution image generation. These strategies collectively enhance the visual appeal of the generated high-resolution images. Lastly, we propose a category-balanced benchmark KolorsPrompts, which serves as a guide for the training and evaluation of Kolors. Consequently, even when employing the commonly used U-Net backbone, Kolors has demonstrated remarkable performance in human evaluations, surpassing the existing open-source models and achieving Midjourney-v6 level performance, especially in terms of visual appeal."),
          Markdown("[Project](https://kolors.kuaishou.com/) | [GitHub](https://github.com/Kwai-Kolors/Kolors) | [Paper](https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/Kolors_paper.pdf) | [Checkpoint](https://huggingface.co/Kwai-Kolors/Kolors-diffusers)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          #Markdown("The pipelines were contributed by [luosiallen](https://luosiallen.github.io/), [nagolinc](https://github.com/nagolinc), and [dg845](https://github.com/dg845).", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🖼  For real real?", on_click=close_kolors_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(kolors_help_dlg)
      kolors_help_dlg.open = True
      page.update()
    def changed_model(e):
        kolors_prefs['kolors_model'] = e.control.value
        kolors_custom_model.visible = e.control.value == "Custom"
        kolors_custom_model.update()
    prompt = TextField(label="Prompt Text", value=kolors_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=kolors_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = FileInput(label="Init Image (optional)", pref=kolors_prefs, key='init_image', page=page, col={'md':6})
    init_image_strength = SliderRow(label="Init-Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=kolors_prefs, key='init_image_strength', col={'md':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    batch_folder_name = TextField(label="Batch Folder Name", value=kolors_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=kolors_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=kolors_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=70, divisions=70, pref=kolors_prefs, key='num_inference_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=kolors_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=kolors_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=kolors_prefs, key='height')
    kolors_model = Dropdown(label="Kolors Model", width=280, options=[dropdown.Option("Custom"), dropdown.Option("Kwai-Kolors/Kolors-diffusers")], value=kolors_prefs['kolors_model'], on_change=changed_model)
    kolors_custom_model = TextField(label="Custom Kolors Model (URL or Path)", value=kolors_prefs['custom_model'], expand=True, visible=kolors_prefs['kolors_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    cpu_offload = Switcher(label="CPU Offload", value=kolors_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    seed = TextField(label="Seed", width=90, value=str(kolors_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(kolors_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🦜   Run Kolors", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kolors(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kolors(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kolors(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.kolors_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🎨  Kolors", "Diffusion Model for Photorealistic Text-to-Image Synthesis...", actions=[save_default(kolors_prefs, ['init_image', 'ip_adapter_image']), IconButton(icon=icons.HELP, tooltip="Help with Kolors Settings", on_click=kolors_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            ResponsiveRow([init_image, init_image_strength]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            Row([kolors_model, kolors_custom_model]),
            #Row([use_ip_adapter, ip_adapter_model], vertical_alignment=CrossAxisAlignment.START),
            #ip_adapter_container,
            Row([cpu_offload]),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.kolors_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

auraflow_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "auraflow-",
    "num_images": 1,
    "steps":50,
    "width": 1024,
    "height":1024,
    "guidance_scale":4.0,
    "cpu_offload": True,
    "seed": 0,
    "auraflow_model": "fal/AuraFlow",
    "custom_model": "",
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildAuraFlow(page):
    global prefs, auraflow_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          auraflow_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def auraflow_help(e):
      def close_auraflow_dlg(e):
        nonlocal auraflow_help_dlg
        auraflow_help_dlg.open = False
        page.update()
      auraflow_help_dlg = AlertDialog(title=Text("🙅   Help with AuraFlow Pipeline"), content=Column([
          Markdown("AuraFlow is inspired by [Stable Diffusion 3](../pipelines/stable_diffusion/stable_diffusion_3.md) and is by far the largest text-to-image generation model that comes with an Apache 2.0 license. This model achieves state-of-the-art results on the [GenEval](https://github.com/djghosh13/geneval) benchmark. It was developed by the Fal team and more details about it can be found in [this blog post](https://blog.fal.ai/auraflow/). AuraFlow can be quite expensive to run on consumer hardware devices. However, you can perform a suite of optimizations to run it faster and in a more memory-friendly manner. Check out [this section](https://huggingface.co/blog/sd3#memory-optimizations-for-sd3) for more details.", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("AuraFlow v0.1 is the fully open-sourced largest flow-based text-to-image generation model. We are excited to present you the first release of our AuraFlow model series, the largest yet completely open sourced flow-based generation model that is capable of text-to-image generation. AuraFlow is a reaffirmation of the open-source community's resilience and relentless determination. We wanted to bring serious resources and compute to scale up the model. We were aligned very well, and thus begun the collaboration. AuraFlow demonstrates that collaborative, transparent AI development is not only alive but thriving, ready to tackle the challenges and opportunities of tomorrow's AI landscape.."),
          Markdown("[Paper/Blog](https://blog.fal.ai/auraflow/) | [Model Checkpoint](https://huggingface.co/fal/AuraFlow) | [HF Space](https://huggingface.co/spaces/multimodalart/AuraFlow) | [fal.ai](https://fal.ai/models/fal-ai/aura-flow)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🔅  It just Glows... ", on_click=close_auraflow_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(auraflow_help_dlg)
      auraflow_help_dlg.open = True
      page.update()
    def changed_model(e):
        auraflow_prefs['auraflow_model'] = e.control.value
        auraflow_custom_model.visible = e.control.value == "Custom"
        auraflow_custom_model.update()
    prompt = TextField(label="Prompt Text", value=auraflow_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=auraflow_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=auraflow_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=auraflow_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=auraflow_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=auraflow_prefs, key='steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=auraflow_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=auraflow_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=auraflow_prefs, key='height')
    auraflow_model = Dropdown(label="AuraFlow Model", width=256, options=[dropdown.Option("Custom"), dropdown.Option("fal/AuraFlow")], value=auraflow_prefs['auraflow_model'], on_change=changed_model)
    auraflow_custom_model = TextField(label="Custom AuraFlow Model (URL or Path)", value=auraflow_prefs['custom_model'], expand=True, visible=auraflow_prefs['auraflow_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    cpu_offload = Switcher(label="CPU Offload", value=auraflow_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 16GB VRAM. Otherwise can run out of memory.")
    #distilled_model = Switcher(label="Use Distilled Model", value=auraflow_prefs['distilled_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'distilled_model'), tooltip="Generate images even faster in around 25 steps.")
    seed = TextField(label="Seed", width=90, value=str(auraflow_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(auraflow_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="👶   Run AuraFlow", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_auraflow(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_auraflow(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_auraflow(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.AuraFlow_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([#ft.OutlinedButton(content=Text("Switch to 2.1", size=18), on_click=switch_version)
            Header("🌗  AuraFlow SD3+", "Open Exploration of Large Rectified Flow Models. Achieves State-of-the-Art Results on GenEval...", actions=[save_default(auraflow_prefs), IconButton(icon=icons.HELP, tooltip="Help with AuraFlow Settings", on_click=auraflow_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            steps,
            guidance, width_slider, height_slider,
            Row([auraflow_model, auraflow_custom_model]),
            upscaler,
            ResponsiveRow([Row([n_images, seed, cpu_offload], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            parameters_row,
            page.AuraFlow_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

layer_diffusion_prefs = {
    "prompt": '',
    "negative_prompt": 'face asymmetry, eyes asymmetry, deformed eyes, open mouth',
    "batch_folder_name": '',
    "file_prefix": "ld-",
    "num_images": 1,
    "steps":30,
    "width": 1024,
    "height":1024,
    "guidance_scale":5.0,
    'init_image': '',
    'init_image_strength': 0.7,
    "cpu_offload": False,
    "seed": 0,
    "layer_diffusion_model": "Layer Diffusion-SFT-diffusers",
    "custom_model": "",
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildLayerDiffusion(page):
    global prefs, layer_diffusion_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          layer_diffusion_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def layer_diffusion_help(e):
      def close_layer_diffusion_dlg(e):
        nonlocal layer_diffusion_help_dlg
        layer_diffusion_help_dlg.open = False
        page.update()
      layer_diffusion_help_dlg = AlertDialog(title=Text("🙅   Help with LayerDiffusion Pipeline"), content=Column([
          Text("You can see that the native transparent diffusion can process transparent glass, semi-transparent glowing effects, etc, that are not possible with simple background removal methods. Native transparent diffusion also gives you detailed fur, hair, whiskers, and detailed structure like that skeleton. SDXL models that can generate foreground and background together and SDXL's one step conditional model. (Note that all joint models for SD1.5 are already released) I put this model on hold because of these reasons: (1) the other released models can already achieve all functionalities and this model does not bring more functionalities. (2) the inference speed of this model is 3x slower than others and requires 4x more VRAM than other released model, and I am working on reducing the VRAM of this model and speed up the inference. (3) This model will involve more hyperparameters and if demanded, I will investigate the best practice for inference/training before release it."),
          Markdown("[Project](https://github.com/layerdiffusion/sd-forge-layerdiffuse) | [Github](https://github.com/lllyasviel/LayerDiffuse_DiffusersCLI) | [Model Checkpoint](https://huggingface.co/lllyasviel/LayerDiffuse_Diffusers) | [HF Space](https://huggingface.co/spaces/radames/LayerDiffuse-gradio-unofficial)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🔦  Clear enough... ", on_click=close_layer_diffusion_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(layer_diffusion_help_dlg)
      layer_diffusion_help_dlg.open = True
      page.update()
    def changed_model(e):
        layer_diffusion_prefs['layer_diffusion_model'] = e.control.value
        layer_diffusion_custom_model.visible = e.control.value == "Custom"
        layer_diffusion_custom_model.update()
    prompt = TextField(label="Prompt Text", value=layer_diffusion_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=layer_diffusion_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=layer_diffusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=layer_diffusion_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=layer_diffusion_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=layer_diffusion_prefs, key='steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=layer_diffusion_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=layer_diffusion_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=layer_diffusion_prefs, key='height')
    init_image = FileInput(label="Source Image (optional)", pref=layer_diffusion_prefs, key='init_image', page=page, col={'md':6})
    init_image_strength = SliderRow(label="Init-Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=layer_diffusion_prefs, key='init_image_strength', col={'md':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    layer_diffusion_model = Dropdown(label="Layer Diffusion Model", width=250, options=[dropdown.Option("Custom"), dropdown.Option("Layer Diffusion-SFT-diffusers")], value=layer_diffusion_prefs['layer_diffusion_model'], on_change=changed_model)
    layer_diffusion_custom_model = TextField(label="Custom LayerDiffusion Model (URL or Path)", value=layer_diffusion_prefs['custom_model'], expand=True, visible=layer_diffusion_prefs['layer_diffusion_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    #cpu_offload = Switcher(label="CPU Offload", value=layer_diffusion_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 16GB VRAM. Otherwise can run out of memory.")
    #distilled_model = Switcher(label="Use Distilled Model", value=layer_diffusion_prefs['distilled_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'distilled_model'), tooltip="Generate images even faster in around 25 steps.")
    seed = TextField(label="Seed", width=90, value=str(layer_diffusion_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(layer_diffusion_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="💎   Run Layer Diffusion", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_layer_diffusion(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_layer_diffusion(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_layer_diffusion(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.LayerDiffusion_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([#ft.OutlinedButton(content=Text("Switch to 2.1", size=18), on_click=switch_version)
            Header("🫥  Layer Diffusion SDXL", "Transparent Image Layer Generation using Latent Transparency...", actions=[save_default(layer_diffusion_prefs), IconButton(icon=icons.HELP, tooltip="Help with LayerDiffusion Settings", on_click=layer_diffusion_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            ResponsiveRow([init_image, init_image_strength]),
            steps,
            guidance, width_slider, height_slider,
            #Row([layer_diffusion_model, layer_diffusion_custom_model]),
            upscaler,
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            parameters_row,
            page.LayerDiffusion_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

differential_diffusion_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "differential-",
    "num_images": 1,
    "max_size":1024,
    "guidance_scale": 7.5,
    'num_inference_steps': 12,
    'eta': 0.0,
    "seed": 0,
    'init_image': '',
    'mask_image': '',
    'init_image_strength': 0.9,
    "differential_diffusion_model": "stabilityai/stable-diffusion-2-inpainting",
    "custom_model": "",
    'use_SD3': False,
    'use_ip_adapter': False,
    'ip_adapter_image': '',
    'ip_adapter_model': 'SDXL',
    'ip_adapter_strength': 0.8,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildDifferential_Diffusion(page):
    global prefs, differential_diffusion_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          differential_diffusion_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def differential_diffusion_help(e):
      def close_differential_diffusion_dlg(e):
        nonlocal differential_diffusion_help_dlg
        differential_diffusion_help_dlg.open = False
        page.update()
      differential_diffusion_help_dlg = AlertDialog(title=Text("🙅   Help with Differential Diffusion Pipeline"), content=Column([
          Text("Diffusion models have revolutionized image generation and editing, producing state-of-the-art results in conditioned and unconditioned image synthesis. While current techniques enable user control over the degree of change in an image edit, the controllability is limited to global changes over an entire edited region. This paper introduces a novel framework that enables customization of the amount of change per pixel or per image region. Our framework can be integrated into any existing diffusion model, enhancing it with this capability. Such granular control on the quantity of change opens up a diverse array of new editing capabilities, such as control of the extent to which individual objects are modified, or the ability to introduce gradual spatial changes. Furthermore, we showcase the framework's effectiveness in soft-inpainting---the completion of portions of an image while subtly adjusting the surrounding areas to ensure seamless integration. Additionally, we introduce a new tool for exploring the effects of different change quantities. Our framework operates solely during inference, requiring no model training or fine-tuning. We demonstrate our method with the current open state-of-the-art models, and validate it via both quantitative and qualitative comparisons, and a user study."),
          Markdown("[Project Page](https://differential-diffusion.github.io/) | [Github](https://github.com/exx8/differential-diffusion) | [Paper](https://differential-diffusion.github.io/paper.pdf) | [HuggingFace Space](https://huggingface.co/spaces/exx8/differential-diffusion) | [Colab Notebook](https://colab.research.google.com/github/exx8/differential-diffusion/blob/main/examples/SD2.ipynb)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Markdown("Contributors include [Eran Levin](https://github.com/exx8), [Ohad Fried](https://www.ohadf.com/), Tel Aviv University, Reichman University", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🐻  That Different...", on_click=close_differential_diffusion_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(differential_diffusion_help_dlg)
      differential_diffusion_help_dlg.open = True
      page.update()
    def changed_model(e):
        differential_diffusion_prefs['differential_diffusion_model'] = e.control.value
        differential_diffusion_custom_model.visible = e.control.value == "Custom"
        differential_diffusion_custom_model.update()
    prompt = TextField(label="Prompt Text", value=differential_diffusion_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=differential_diffusion_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = FileInput(label="Init Input Image", pref=differential_diffusion_prefs, key='init_image', page=page, col={'md':6})
    mask_image = FileInput(label="Greyscale Map Gradient Mask", pref=differential_diffusion_prefs, key='mask_image', page=page, col={'md':6})
    init_image_strength = SliderRow(label="Init-Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=differential_diffusion_prefs, key='init_image_strength', col={'md':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    batch_folder_name = TextField(label="Batch Folder Name", value=differential_diffusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=differential_diffusion_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Images", min=1, max=20, step=1, value=differential_diffusion_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=40, divisions=40, pref=differential_diffusion_prefs, key='num_inference_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=differential_diffusion_prefs, key='guidance_scale')
    eta = SliderRow(label="DDIM ETA", min=0, max=1, divisions=20, round=2, pref=differential_diffusion_prefs, key='eta', tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.")
    max_row = SliderRow(label="Max Resolution Size", min=256, max=2048, divisions=112, multiple=16, suffix="px", pref=differential_diffusion_prefs, key='max_size', tooltip="Resizes your Init and Mask Image to save memory.")
    def toggle_SD3(e):
        differential_diffusion_prefs['use_SD3'] = e.control.value
        use_ip_adapter.visible = not differential_diffusion_prefs['use_SD3']
        use_ip_adapter.update()
        ip_adapter_container.visible = not differential_diffusion_prefs['use_SD3'] and differential_diffusion_prefs['use_ip_adapter']
        ip_adapter_container.update()
        ip_adapter_SDXL_model.visible = not differential_diffusion_prefs['use_SD3'] and differential_diffusion_prefs['use_ip_adapter']
        ip_adapter_SDXL_model.update()
    use_SD3 = Switcher(label="Use Stable Diffusion 3 Differential Diffusion Img2Img Pipeline", value=differential_diffusion_prefs['use_SD3'], on_change=toggle_SD3, tooltip="SD3 uses Model Checkpoint set in Installation. Otherwise use selected SDXL Inpainting Model.")
    def toggle_ip_adapter(e):
        differential_diffusion_prefs['use_ip_adapter'] = e.control.value
        ip_adapter_container.height = None if e.control.value else 0
        ip_adapter_container.update()
        ip_adapter_SDXL_model.visible = e.control.value
        ip_adapter_SDXL_model.update()
    use_ip_adapter = Switcher(label="Use IP-Adapter Reference Image", value=differential_diffusion_prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip="Uses both image and text to condition the image generation process.")
    ip_adapter_SDXL_model = Dropdown(label="IP-Adapter SDXL Model", width=220, options=[], value=differential_diffusion_prefs['ip_adapter_model'], visible=differential_diffusion_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))
    for m in ip_adapter_SDXL_models:
        ip_adapter_SDXL_model.options.append(dropdown.Option(m['name']))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=differential_diffusion_prefs, key='ip_adapter_image', col={'lg':6}, page=page)
    ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=differential_diffusion_prefs, key='ip_adapter_strength', col={'lg':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    ip_adapter_container = Container(ResponsiveRow([ip_adapter_image, ip_adapter_strength]), height = None if differential_diffusion_prefs['use_ip_adapter'] and not differential_diffusion_prefs['use_SD3'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    differential_diffusion_model = Dropdown(label="Inpainting Model", width=386, options=[dropdown.Option(m) for m in ["stabilityai/stable-diffusion-2-inpainting", "runwayml/stable-diffusion-inpainting", "ImNoOne/f222-inpainting-diffusers", "Lykon/dreamshaper-8-inpainting", "parlance/dreamlike-diffusion-1.0-inpainting", "ghunkins/stable-diffusion-liberty-inpainting", "piyushaaryan011/realistic-vision-inpainting", "Custom"]], value=differential_diffusion_prefs['differential_diffusion_model'], on_change=changed_model)
    differential_diffusion_custom_model = TextField(label="Custom Differential_Diffusion Model (URL or Path)", value=differential_diffusion_prefs['custom_model'], expand=True, visible=differential_diffusion_prefs['differential_diffusion_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    #cpu_offload = Switcher(label="CPU Offload", value=differential_diffusion_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    seed = TextField(label="Seed", width=90, value=str(differential_diffusion_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(differential_diffusion_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="😕   Run Differential Diffusion", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_differential_diffusion(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_differential_diffusion(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_differential_diffusion(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.differential_diffusion_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🙈  Differential Diffusion SDXL & SD3 Image2Image", "Modifies an image according to a text prompt, and according to a map that specifies the amount of change in each region...", actions=[save_default(differential_diffusion_prefs, ['init_image', 'mask_image']), IconButton(icon=icons.HELP, tooltip="Help with Differential Diffusion Settings", on_click=differential_diffusion_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            ResponsiveRow([init_image, mask_image]),
            init_image_strength,
            steps,
            guidance,
            #eta,
            max_row, #Divider(height=9, thickness=2),
            #Row([differential_diffusion_model, differential_diffusion_custom_model]),
            #Row([cpu_offload, cpu_only]),
            use_SD3,
            Row([use_ip_adapter, ip_adapter_SDXL_model], vertical_alignment=CrossAxisAlignment.START),
            ip_adapter_container,
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.differential_diffusion_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c


lmd_plus_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "lmd-",
    "num_images": 1,
    "width": 512,
    "height":512,
    "guidance_scale":7.5,
    'num_inference_steps': 50,
    'gligen_scheduled_sampling_beta': 0.4,
    'AI_engine': 'ChatGPT-3.5 Turbo',
    "temperature": 0.7,
    "seed": 0,
    'init_image': '',
    "cpu_offload": False,
    "lmd_plus_model": "longlian/lmd_plus",
    "custom_model": "",
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildLMD_Plus(page):
    global prefs, lmd_plus_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          lmd_plus_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def lmd_plus_help(e):
      def close_lmd_plus_dlg(e):
        nonlocal lmd_plus_help_dlg
        lmd_plus_help_dlg.open = False
        page.update()
      lmd_plus_help_dlg = AlertDialog(title=Text("🙅   Help with LMD+ Pipeline"), content=Column([
          Text("LMD+ greatly improves the prompt understanding ability of text-to-image generation models by introducing an LLM as a front-end prompt parser and layout planner. It improves spatial reasoning, the understanding of negation, attribute binding, generative numeracy, etc. in a unified manner without explicitly aiming for each. LMD is completely training-free (i.e., uses SD model off-the-shelf). LMD+ takes in additional adapters for better control. This is a reproduction of LMD+ model used in our work."),
          Text("We provide a parser that parses LLM outputs to the layouts. You can obtain the prompt to input to the LLM for layout generation"),
          Markdown("[Paper](https://arxiv.org/pdf/2305.13655.pdf) | [Project Page](https://llm-grounded-diffusion.github.io/) | [GitHub](https://github.com/TonyLianLong/LLM-groundedDiffusion)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🐗  Understandable ", on_click=close_lmd_plus_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(lmd_plus_help_dlg)
      lmd_plus_help_dlg.open = True
      page.update()
    def changed_model(e):
        lmd_plus_prefs['lmd_plus_model'] = e.control.value
        lmd_plus_custom_model.visible = e.control.value == "Custom"
        lmd_plus_custom_model.update()
    prompt = TextField(label="Prompt Text", value=lmd_plus_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=lmd_plus_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=lmd_plus_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=lmd_plus_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #num_inference_steps = TextField(label="Number of Steps", value=lmd_plus_prefs['num_inference_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_inference_steps', ptype="int"))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=lmd_plus_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=lmd_plus_prefs, key='num_inference_steps')
    #prior_guidance_scale = SliderRow(label="Prior Guidance Scale", min=0, max=10, divisions=20, round=1, expand=True, pref=lmd_plus_prefs, key='prior_guidance_scale', col={'xs':12, 'md':6})
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=lmd_plus_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=lmd_plus_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=lmd_plus_prefs, key='height')
    lmd_plus_model = Dropdown(label="LMD+ Model", width=250, options=[dropdown.Option("Custom"), dropdown.Option("longlian/lmd_plus")], value=lmd_plus_prefs['lmd_plus_model'], on_change=changed_model)
    lmd_plus_custom_model = TextField(label="Custom LMD_Plus Model (URL or Path)", value=lmd_plus_prefs['custom_model'], expand=True, visible=lmd_plus_prefs['lmd_plus_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    AI_engine = Dropdown(label="AI Engine", width=250, options=[dropdown.Option("OpenAI GPT-3"), dropdown.Option("ChatGPT-3.5 Turbo"), dropdown.Option("OpenAI GPT-4"), dropdown.Option("GPT-4 Turbo"), dropdown.Option("GPT-4o"), dropdown.Option("Google Gemini")], value=lmd_plus_prefs['AI_engine'], on_change=lambda e: changed(e, 'AI_engine'))
    temperature = SliderRow(label="AI Temperature", min=0, max=1, divisions=10, round=1, expand=True, pref=lmd_plus_prefs, key='temperature', tooltip="Softmax value used to module the next token probabilities", col={'lg':6})
    gligen_scheduled_sampling_beta = SliderRow(label="Gligen Scheduled Sampling Beta", min=0, max=1, divisions=10, round=1, pref=lmd_plus_prefs, key='gligen_scheduled_sampling_beta', tooltip="Scheduled Sampling factor from GLIGEN: Open-Set Grounded Text-to-Image Generation. Scheduled Sampling factor is only varied for scheduled sampling during inference for improved quality and controllability.", col={'lg':6})
    cpu_offload = Switcher(label="CPU Offload", value=lmd_plus_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    seed = TextField(label="Seed", width=90, value=str(lmd_plus_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(lmd_plus_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🦔   Run LMD+", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_lmd_plus(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_lmd_plus(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_lmd_plus(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.lmd_plus_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🐆  LMD+ LLM-grounded Diffusion", "Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models.", actions=[save_default(lmd_plus_prefs, ['init_image']), IconButton(icon=icons.HELP, tooltip="Help with LMD Plus Settings", on_click=lmd_plus_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            #ResponsiveRow([num_inference_steps]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            gligen_scheduled_sampling_beta,
            Row([AI_engine, temperature]),
            Row([lmd_plus_model, lmd_plus_custom_model, cpu_offload]),
            #Row([clean_caption, mask_feature, cpu_offload]),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.lmd_plus_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

lcm_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "lcm-",
    "num_images": 1,
    "width": 1024,
    "height":1024,
    "guidance_scale":4.5,
    'num_inference_steps': 8,
    "seed": 0,
    'init_image': '',
    'init_image_strength': 0.8,
    "cpu_offload": False,
    "cpu_only": False,
    "lcm_model": "LCM_Dreamshaper_v7",
    "custom_model": "",
    'use_ip_adapter': False,
    'ip_adapter_image': '',
    'ip_adapter_model': 'SD v1.5',
    'ip_adapter_strength': 0.8,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildLCM(page):
    global prefs, lcm_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          lcm_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def lcm_help(e):
      def close_lcm_dlg(e):
        nonlocal lcm_help_dlg
        lcm_help_dlg.open = False
        page.update()
      lcm_help_dlg = AlertDialog(title=Text("🙅   Help with LCM Pipeline"), content=Column([
          Text("Latent Diffusion Models (LDMs) have achieved remarkable results in synthesizing high-resolution images. However, the iterative sampling process is computationally intensive and leads to slow generation. Inspired by Consistency Models (song et al.), we propose Latent Consistency Models (LCMs), enabling swift inference with minimal steps on any pre-trained LDMs, including Stable Diffusion (rombach et al). Viewing the guided reverse diffusion process as solving an augmented probability flow ODE (PF-ODE), LCMs are designed to directly predict the solution of such ODE in latent space, mitigating the need for numerous iterations and allowing rapid, high-fidelity sampling. Efficiently distilled from pre-trained classifier-free guided diffusion models, a high-quality 768 x 768 2~4-step LCM takes only 32 A100 GPU hours for training. Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method that is tailored for fine-tuning LCMs on customized image datasets. Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-art text-to-image generation performance with few-step inference."),
          #Text(""),
          Markdown("[Project](https://latent-consistency-models.github.io/) | [Paper](https://arxiv.org/pdf/2310.04378.pdf) | [SimianLuo/LCM_Dreamshaper_v7](https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7) | [Checkpoint](https://huggingface.co/spaces/SimianLuo/Latent_Consistency_Model)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Markdown("The pipelines were contributed by [luosiallen](https://luosiallen.github.io/), [nagolinc](https://github.com/nagolinc), and [dg845](https://github.com/dg845).", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🏃  How many steps?", on_click=close_lcm_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(lcm_help_dlg)
      lcm_help_dlg.open = True
      page.update()
    def changed_model(e):
        lcm_prefs['lcm_model'] = e.control.value
        lcm_custom_model.visible = e.control.value == "Custom"
        lcm_custom_model.update()
    prompt = TextField(label="Prompt Text", value=lcm_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=lcm_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = FileInput(label="Init Image (optional)", pref=lcm_prefs, key='init_image', page=page, col={'md':6})
    init_image_strength = SliderRow(label="Init-Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=lcm_prefs, key='init_image_strength', col={'md':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    batch_folder_name = TextField(label="Batch Folder Name", value=lcm_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=lcm_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=lcm_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=40, divisions=40, pref=lcm_prefs, key='num_inference_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=lcm_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=lcm_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=lcm_prefs, key='height')
    def toggle_ip_adapter(e):
        lcm_prefs['use_ip_adapter'] = e.control.value
        ip_adapter_container.height = None if e.control.value else 0
        ip_adapter_container.update()
        ip_adapter_model.visible = e.control.value
        ip_adapter_model.update()
    use_ip_adapter = Switcher(label="Use IP-Adapter Reference Image", value=lcm_prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip="Uses both image and text to condition the image generation process.")
    ip_adapter_model = Dropdown(label="IP-Adapter SD Model", width=220, options=[], value=lcm_prefs['ip_adapter_model'], visible=lcm_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))
    for m in ip_adapter_models:
        ip_adapter_model.options.append(dropdown.Option(m['name']))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=lcm_prefs, key='ip_adapter_image', col={'lg':6}, page=page)
    ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=lcm_prefs, key='ip_adapter_strength', col={'lg':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    ip_adapter_container = Container(ResponsiveRow([ip_adapter_image, ip_adapter_strength]), height = None if lcm_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    lcm_model = Dropdown(label="LCM Model", width=220, options=[dropdown.Option("Custom"), dropdown.Option("LCM_Dreamshaper_v7"), dropdown.Option("LCM_Dreamshaper_v8")], value=lcm_prefs['lcm_model'], on_change=changed_model)
    lcm_custom_model = TextField(label="Custom LCM Model (URL or Path)", value=lcm_prefs['custom_model'], expand=True, visible=lcm_prefs['lcm_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    cpu_offload = Switcher(label="CPU Offload", value=lcm_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    cpu_only = Switcher(label="CPU Only (not yet)", value=lcm_prefs['cpu_only'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_only'), tooltip="If you don't have a good GPU, can run entirely on CPU")
    seed = TextField(label="Seed", width=90, value=str(lcm_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(lcm_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🏎   Run LCM", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_lcm(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_lcm(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_lcm(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.lcm_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("💻  Latent Consistency Model (LCM)", "Synthesizing High-Resolution Images with Few-Step Inference.", actions=[save_default(lcm_prefs, ['init_image', 'ip_adapter_image']), IconButton(icon=icons.HELP, tooltip="Help with LCM Settings", on_click=lcm_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            ResponsiveRow([init_image, init_image_strength]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            Row([lcm_model, lcm_custom_model]),
            Row([use_ip_adapter, ip_adapter_model], vertical_alignment=CrossAxisAlignment.START),
            ip_adapter_container,
            Row([cpu_offload, cpu_only]),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.lcm_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

lcm_interpolation_prefs = {
    "prompt": '',
    "batch_folder_name": '',
    "file_prefix": "lcm-",
    "num_images": 1,
    "mixes": [],
    "num_interpolation_steps": 60,
    "steps":8,
    "width": 512,
    "height":512,
    "guidance_scale":8,
    'process_batch_size': 4,
    "embedding_interpolation_type": "lerp",
    "latent_interpolation_type": "slerp",
    "save_video": False,
    "interpolate_video": True,
    "source_fps": 8,
    "target_fps": 24,
    "seed": 0,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildLCMInterpolation(page):
    global prefs, lcm_interpolation_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          lcm_interpolation_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def lcm_interpolation_help(e):
      def close_lcm_interpolation_dlg(e):
        nonlocal lcm_interpolation_help_dlg
        lcm_interpolation_help_dlg.open = False
        page.update()
      lcm_interpolation_help_dlg = AlertDialog(title=Text("🙅   Help with LCM Interpolation Pipeline"), content=Column([
          Text("This pipeline extends the Latent Consistency Pipeline to allow for interpolation of the latent space between multiple prompts. It is similar to the Stable Diffusion Interpolate and unCLIP Interpolate community pipelines."),
          #Text(""),
          Markdown(""),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🔲  The Space Between... ", on_click=close_lcm_interpolation_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(lcm_interpolation_help_dlg)
      lcm_interpolation_help_dlg.open = True
      page.update()
    def add_prompt(e):
        if not bool(lcm_interpolation_prefs['prompt']): return
        layer = {'prompt': lcm_interpolation_prefs['prompt']}
        lcm_interpolation_prefs['mixes'].append(layer)
        fuse_layers.controls.append(ListTile(title=Row([Text(layer['prompt'], weight=FontWeight.BOLD)], alignment=MainAxisAlignment.SPACE_BETWEEN), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.EDIT, text="Edit Text Layer", on_click=edit_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE, text="Delete Text Layer", on_click=delete_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
              PopupMenuItem(icon=icons.ARROW_UPWARD, text="Move Up", on_click=move_up, data=layer),
              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text="Move Down", on_click=move_down, data=layer),
          ]), data=layer, on_click=edit_layer))
        fuse_layers.update()
        lcm_interpolation_prefs['prompt'] = ""
        prompt.value = ""
        prompt.update()
    def delete_layer(e):
        lcm_interpolation_prefs['mixes'].remove(e.control.data)
        for c in fuse_layers.controls:
          if c.data['prompt'] == e.control.data['prompt']:
              fuse_layers.controls.remove(c)
              break
        fuse_layers.update()
    def delete_all_layers(e):
        lcm_interpolation_prefs['mixes'].clear()
        fuse_layers.controls.clear()
        fuse_layers.update()
    def move_down(e):
        idx = lcm_interpolation_prefs['mixes'].index(e.control.data)
        if idx < (len(lcm_interpolation_prefs['mixes']) - 1):
          d = lcm_interpolation_prefs['mixes'].pop(idx)
          lcm_interpolation_prefs['mixes'].insert(idx+1, d)
          dr = fuse_layers.controls.pop(idx)
          fuse_layers.controls.insert(idx+1, dr)
          fuse_layers.update()
    def move_up(e):
        idx = lcm_interpolation_prefs['mixes'].index(e.control.data)
        if idx > 0:
          d = lcm_interpolation_prefs['mixes'].pop(idx)
          lcm_interpolation_prefs['mixes'].insert(idx-1, d)
          dr = fuse_layers.controls.pop(idx)
          fuse_layers.controls.insert(idx-1, dr)
          fuse_layers.update()
    def edit_layer(e):
        data = e.control.data
        prompt_value = data["prompt"]
        image_value = ""
        def close_dlg(e):
            dlg_edit.open = False
            page.update()
        def save_layer(e):
            layer = None
            for l in lcm_interpolation_prefs['mixes']:
                if data["prompt"] == l["prompt"]:
                    layer = l
                    layer['prompt'] = prompt_text.value
                    break
            for c in fuse_layers.controls:
                if 'prompt' not in data: continue
                if c.data['prompt'] == data['prompt']:
                    c.title.controls[0].value = layer['prompt']
                    c.update()
                    break
            layer['prompt'] = prompt_text.value
            dlg_edit.open = False
            e.control.update()
            page.update()
        prompt_text = TextField(label="Interpolation Prompt Text", value=prompt_value, multiline=True)
        dlg_edit = AlertDialog(modal=False, title=Text(f"🧳 Edit Interpolation Prompt"), content=Container(Column([prompt_text], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO, width=(page.width if page.web else page.window.width) - 100)), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Layer ", size=19, weight=FontWeight.BOLD), on_click=save_layer)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(dlg_edit)
        dlg_edit.open = True
        page.update()
    def toggle_video(e):
        lcm_interpolation_prefs['save_video'] = e.control.value
        video_container.visible = lcm_interpolation_prefs['save_video']
        video_container.update()
    add_prompt_btn = ft.FilledButton("➕ Add Prompt", width=150, on_click=add_prompt)
    prompt = TextField(label="Interpolation Prompt Text", value=lcm_interpolation_prefs['prompt'], filled=True, expand=True, multiline=True, on_submit=add_prompt, on_change=lambda e:changed(e,'prompt'))
    prompt_row = Row([prompt, add_prompt_btn])
    fuse_layers = Column([], spacing=0)
    batch_folder_name = TextField(label="Batch Folder Name", value=lcm_interpolation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=lcm_interpolation_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    num_interpolation_steps = SliderRow(label="Interpolation Frames", min=0, max=200, divisions=200, pref=lcm_interpolation_prefs, key='num_interpolation_steps', tooltip="Number of Image Frames between each Prompt to Interpolate.")
    process_batch_size = SliderRow(label="Process Batch Size", min=0, max=20, divisions=20, pref=lcm_interpolation_prefs, key='process_batch_size', tooltip="The batch size to use for processing the images. This is useful when generating a large number of images and you want to avoid running out of memory.")
    steps = SliderRow(label="Number of Inference Steps", min=0, max=40, divisions=40, pref=lcm_interpolation_prefs, key='steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=lcm_interpolation_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=lcm_interpolation_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=lcm_interpolation_prefs, key='height')
    embedding_interpolation_type = Dropdown(label="Embedding Interpolation Type", tooltip="The type of interpolation to use for interpolating between text embeddings.", width=220, options=[dropdown.Option("lerp"), dropdown.Option("slerp")], value=lcm_interpolation_prefs['embedding_interpolation_type'], autofocus=False, on_change=lambda e:changed(e, 'embedding_interpolation_type'))
    latent_interpolation_type = Dropdown(label="Latent Interpolation Type", tooltip="The type of interpolation to use for interpolating between latents.", width=220, options=[dropdown.Option("lerp"), dropdown.Option("slerp")], value=lcm_interpolation_prefs['latent_interpolation_type'], autofocus=False, on_change=lambda e:changed(e, 'latent_interpolation_type'))
    save_video = Switcher(label="Save Video", value=lcm_interpolation_prefs['save_video'], on_change=toggle_video)
    interpolate_vid = Switcher(label="Interpolate", value=lcm_interpolation_prefs['interpolate_video'], tooltip="Use Google FiLM Interpolation to transition between frames.", on_change=lambda e:changed(e,'interpolate_video'))
    source_fps = SliderRow(label="Source FPS", min=0, max=30, suffix="fps", divisions=30, expand=1, pref=lcm_interpolation_prefs, key='source_fps')
    target_fps = SliderRow(label="Target FPS", min=0, max=30, suffix="fps", divisions=30, expand=1, pref=lcm_interpolation_prefs, key='target_fps')
    video_container = Container(content=Row([interpolate_vid, source_fps, target_fps], expand=True), expand=True, visible=lcm_interpolation_prefs['save_video'])
    seed = TextField(label="Seed", width=90, value=str(lcm_interpolation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(lcm_interpolation_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🧶   Run LCM Interpolation", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_lcm_interpolation(page))
    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)
    page.lcm_interpolation_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("👪  LCM Interpolation", "Transition the Latent Consistency between multiple text prompts... Good fast results in only 4 Steps!", actions=[save_default(lcm_interpolation_prefs, ['mixes']), IconButton(icon=icons.HELP, tooltip="Help with LCM Settings", on_click=lcm_interpolation_help)]),
            prompt_row,
            fuse_layers,
            Divider(height=5, thickness=4),
            num_interpolation_steps,
            process_batch_size,
            steps,
            guidance, width_slider, height_slider,
            Row([embedding_interpolation_type, latent_interpolation_type]),
            Row([save_video, video_container]),
            ResponsiveRow([Row([seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.lcm_interpolation_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

instaflow_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "instaflow-",
    "num_images": 1,
    "width": 1024,
    "height":1024,
    "guidance_scale":0.0,
    'num_inference_steps': 1,
    "seed": 0,
    "cpu_offload": False,
    "cpu_only": False,
    "instaflow_model": "instaflow_0_9B_from_sd_1_5",
    "custom_model": "",
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildInstaFlow(page):
    global prefs, instaflow_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          instaflow_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def instaflow_help(e):
      def close_instaflow_dlg(e):
        nonlocal instaflow_help_dlg
        instaflow_help_dlg.open = False
        page.update()
      instaflow_help_dlg = AlertDialog(title=Text("🙅   Help with InstaFlow Pipeline"), content=Column([
          Text("InstaFlow is an ultra-fast, one-step image generator that achieves image quality close to Stable Diffusion, significantly reducing the demand of computational resources. This efficiency is made possible through a recent Rectified Flow technique, which trains probability flows with straight trajectories, hence inherently requiring only a single step for fast inference."),
          Text("Rectified Flow is a novel method for learning transport maps between two distributions by connecting straight paths between the samples and learning an ODE model. Then, by a reflow operation, we iteratively straighten the ODE trajectories to eventually achieve one-step generation, with higher diversity than GAN and better FID than fast diffusion models."),
          Text("Diffusion models have revolutionized text-to-image generation with its exceptional quality and creativity. However, its multi-step sampling process is known to be slow, often requiring tens of inference steps to obtain satisfactory results. Previous attempts to improve its sampling speed and reduce computational costs through distillation have been unsuccessful in achieving a functional one-step model. In this paper, we explore a recent method called Rectified Flow, which, thus far, has only been applied to small datasets. The core of Rectified Flow lies in its reflow procedure, which straightens the trajectories of probability flows, refines the coupling between noises and images, and facilitates the distillation process with student models. We propose a novel text-conditioned pipeline to turn Stable Diffusion (SD) into an ultra-fast one-step model, in which we find reflow plays a critical role in improving the assignment between noise and images. Leveraging our new pipeline, we create, to the best of our knowledge, the first one-step diffusion-based text-to-image generator with SD-level image quality, achieving an FID (Frechet Inception Distance) of 23.3 on MS COCO 2017-5k, surpassing the previous state-of-the-art technique, progressive distillation, by a significant margin (37.2 → 23.3 in FID). By utilizing an expanded network with 1.7B parameters, we further improve the FID to 22.4."),
          Markdown("[Rectified Flow](https://github.com/gnobitab/RectifiedFlow) | [Paper](https://arxiv.org/abs/2309.06380) | [Checkpoint](https://huggingface.co/XCLIU/instaflow_0_9B_from_sd_1_5)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Markdown("The pipelines were contributed by [Ayush Mangal](https://github.com/ayushtues), Sayak Paul, and Patrick von Platen.", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🏇  Faster, faster!", on_click=close_instaflow_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(instaflow_help_dlg)
      instaflow_help_dlg.open = True
      page.update()
    def changed_model(e):
        instaflow_prefs['instaflow_model'] = e.control.value
        instaflow_custom_model.visible = e.control.value == "Custom"
        instaflow_custom_model.update()
    prompt = TextField(label="Prompt Text", value=instaflow_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=instaflow_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    #init_image = FileInput(label="Init Image (optional)", pref=instaflow_prefs, key='init_image', page=page, col={'md':6})
    #init_image_strength = SliderRow(label="Init-Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=instaflow_prefs, key='init_image_strength', col={'md':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    batch_folder_name = TextField(label="Batch Folder Name", value=instaflow_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=instaflow_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=instaflow_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=10, divisions=10, pref=instaflow_prefs, key='num_inference_steps')
    #guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=instaflow_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=instaflow_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=instaflow_prefs, key='height')
    instaflow_model = Dropdown(label="InstaFlow Model", width=255, options=[dropdown.Option("Custom"), dropdown.Option("instaflow_0_9B_from_sd_1_5")], value=instaflow_prefs['instaflow_model'], on_change=changed_model)
    instaflow_custom_model = TextField(label="Custom InstaFlow Model (URL or Path)", value=instaflow_prefs['custom_model'], expand=True, visible=instaflow_prefs['instaflow_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    cpu_offload = Switcher(label="CPU Offload", value=instaflow_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    #cpu_only = Switcher(label="CPU Only (not yet)", value=instaflow_prefs['cpu_only'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_only'), tooltip="If you don't have a good GPU, can run entirely on CPU")
    seed = TextField(label="Seed", width=90, value=str(instaflow_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(instaflow_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="⛹️   Run InstaFlow", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_instaflow(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_instaflow(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_instaflow(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.instaflow_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("⚡️  InstaFlow One-Step", "Ultra-Fast One-Step High-Quality Diffusion-Based Text-to-Image Generation...", actions=[save_default(instaflow_prefs), IconButton(icon=icons.HELP, tooltip="Help with InstaFlow Settings", on_click=instaflow_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            #ResponsiveRow([init_image, init_image_strength]),
            steps,
            #guidance,
            width_slider, height_slider, #Divider(height=9, thickness=2),
            Row([instaflow_model, instaflow_custom_model, cpu_offload]),
            #Row([]),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.instaflow_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

pag_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "pag-",
    "num_images": 1,
    "width": 1024,
    "height":1024,
    "guidance_scale":5.0,
    'num_inference_steps': 50,
    'pag_scale': 5.0,
    'pag_adaptive_scaling': 0.0,
    'pag_drop_rate': 0.5,
    'applied_layer_down': False,
    'applied_layer_mid': True,
    'applied_layer_up': False,
    'pag_applied_layers': "down",  # ['down', 'mid', 'up']
    'pag_applied_layers_index': "m0",  # ['d4', 'd5', 'm0']
    'init_image': '',
    'mask_image': '',
    'alpha_mask': False,
    'invert_mask': False,
    'image_strength': 0.7,
    "seed": 0,
    'init_image': '',
    'init_image_strength': 0.8,
    "cpu_offload": True,
    "cpu_only": False,
    "pag_model": "PAG_Dreamshaper_v7",
    "custom_model": "",
    'use_SDXL': True,
    'use_ip_adapter': False,
    'ip_adapter_image': '',
    'ip_adapter_model': 'SD v1.5',
    'ip_adapter_SDXL_model': 'SDXL',
    'ip_adapter_strength': 0.8,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildPAG(page):
    global prefs, pag_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          pag_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def pag_help(e):
        alert_msg(page, "💁   Help with Perturbed-Attention Guidance", [
            "Perturbed-Attention Guidance significantly improves sample quality in unconditional and conditional generation, without additional training or external conditions. It can be easily used with any kinds of diffusion models. Qualitative comparisons between unguided (baseline) and perturbed-attention-guided (PAG) diffusion samples. Without any external conditions, e.g., class labels or text prompts, or additional training, our PAG dramatically elevates the quality of diffusion samples even in unconditional generation, where classifier-free guidance (CFG) is inapplicable. Our guidance can also enhance the baseline performance in various downstream tasks such as ControlNet with empty prompt and image restoration such as inpainting and deblurring.",
            "Recent studies prove that diffusion models can generate high-quality samples, but their quality is often highly reliant on sampling guidance techniques such as classifier guidance (CG) and classifier-free guidance (CFG), which are inapplicable in unconditional generation or various downstream tasks such as image restoration. In this paper, we propose a novel diffusion sampling guidance, called Perturbed-Attention Guidance (PAG), which improves sample quality across both unconditional and conditional settings, achieving this without requiring further training or the integration of external modules. PAG is designed to progressively enhance the structure of synthesized samples throughout the denoising process by considering the self-attention mechanisms' ability to capture structural information. It involves generating intermediate samples with degraded structure by substituting selected self-attention maps in diffusion U-Net with an identity matrix, and guiding the denoising process away from these degraded samples.",
            Markdown("[Project Page](https://ku-cvlab.github.io/Perturbed-Attention-Guidance/) | [GitHub Repo](https://github.com/KU-CVLAB/Perturbed-Attention-Guidance) | [Paper](https://arxiv.org/abs/2403.17377) | [HuggingFace](https://huggingface.co/hyoungwoncho/sd_perturbed_attention_guidance)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], "💑  Attention is Perturbed", False)
    def changed_model(e):
        pag_prefs['pag_model'] = e.control.value
        pag_custom_model.visible = e.control.value == "Custom"
        pag_custom_model.update()
    prompt = TextField(label="Prompt Text", value=pag_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=pag_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    #init_image = FileInput(label="Init Image (optional)", pref=pag_prefs, key='init_image', page=page, col={'md':6})
    #init_image_strength = SliderRow(label="Init-Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=pag_prefs, key='init_image_strength', col={'md':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    batch_folder_name = TextField(label="Batch Folder Name", value=pag_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=pag_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=pag_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=60, divisions=60, pref=pag_prefs, key='num_inference_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=pag_prefs, key='guidance_scale')
    pag_scale = SliderRow(label="PAG Guidance Scale", min=0, max=50, divisions=50, pref=pag_prefs, key='pag_scale', tooltip="Gain more semantically coherent structures and exhibit fewer artifacts. Large guidance scale can lead to smoother textures and slight saturation in the images.")
    pag_adaptive_scaling = SliderRow(label="PAG Adaptive Scaling", min=0.0, max=1.0, divisions=20, round=2, pref=pag_prefs, key='pag_adaptive_scaling', col={'lg':6}, tooltip="Scales the Diffusion Adaptivly (I don't know)")
    #pag_drop_rate = SliderRow(label="PAG Drop Rate", min=0.0, max=1.0, divisions=20, round=2, pref=pag_prefs, key='pag_drop_rate', col={'lg':6}, visible = not pag_prefs['use_SDXL'], tooltip="Experiment...")
    applied_layer_down = Checkbox(label="Down", value=pag_prefs['applied_layer_down'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'applied_layer_down'))
    applied_layer_mid = Checkbox(label="Mid", value=pag_prefs['applied_layer_mid'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'applied_layer_mid'))
    applied_layer_up = Checkbox(label="Up", value=pag_prefs['applied_layer_up'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'applied_layer_up'))
    pag_applied_layers = Container(Row([Text("PAG Applied Layers:"), applied_layer_down, applied_layer_mid, applied_layer_up]), tooltip="Specify which layers PAG is applied to. Changing this setting will significantly impact the output, so experiment.")
    #pag_applied_layers = Dropdown(label="PAG Applied Layers", width=200, options=[dropdown.Option(o) for o in ['down', 'mid', 'up']], value=pag_prefs['pag_applied_layers'], on_change=lambda e:changed(e,'pag_applied_layers'))
    #pag_applied_layers_index = Dropdown(label="PAG Applied Layers Index", width=200, options=[dropdown.Option(o) for o in ['d4', 'd5', 'm0']], value=pag_prefs['pag_applied_layers_index'], visible = not pag_prefs['use_SDXL'], on_change=lambda e:changed(e,'pag_applied_layers_index'))
    width_slider = SliderRow(label="Width", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=pag_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=pag_prefs, key='height')
    def toggle_SDXL(e):
        pag_prefs['use_SDXL'] = e.control.value
        #pag_applied_layers_index.visible = not pag_prefs['use_SDXL']
        #pag_applied_layers_index.update()
        #pag_drop_rate.visible = not pag_prefs['use_SDXL']
        #pag_drop_rate.update()
        ip_adapter_model.visible = not pag_prefs['use_SDXL'] and pag_prefs['use_ip_adapter']
        ip_adapter_model.update()
        ip_adapter_SDXL_model.visible = pag_prefs['use_SDXL'] and pag_prefs['use_ip_adapter']
        ip_adapter_SDXL_model.update()
        #cpu_offload.visible = pag_prefs['use_SDXL']
        #cpu_offload.update()
        image_container.height = None if pag_prefs['use_SDXL'] else 0
        image_container.update()
    use_SDXL = Switcher(label="Use Stable Diffusion XL PAG Pipeline", value=pag_prefs['use_SDXL'], on_change=toggle_SDXL, tooltip="SDXL uses Model Checkpoint set in Installation. Otherwise use selected 1.5 or 2.1 Inpainting Model.")
    def toggle_ip_adapter(e):
        pag_prefs['use_ip_adapter'] = e.control.value
        ip_adapter_container.height = None if e.control.value else 0
        ip_adapter_container.update()
        ip_adapter_model.visible = e.control.value and not pag_prefs['use_SDXL']
        ip_adapter_model.update()
        ip_adapter_SDXL_model.visible = e.control.value and pag_prefs['use_SDXL']
        ip_adapter_SDXL_model.update()
    use_ip_adapter = Switcher(label="Use IP-Adapter Reference Image", value=pag_prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip="Uses both image and text to condition the image generation process.")
    ip_adapter_model = Dropdown(label="IP-Adapter SD Model", width=220, options=[], value=pag_prefs['ip_adapter_model'], visible=pag_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))
    for m in ip_adapter_models:
        ip_adapter_model.options.append(dropdown.Option(m['name']))
    ip_adapter_SDXL_model = Dropdown(label="IP-Adapter SDXL Model", width=220, options=[], value=pag_prefs['ip_adapter_SDXL_model'], visible=pag_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_SDXL_model'))
    for m in ip_adapter_SDXL_models:
        ip_adapter_SDXL_model.options.append(dropdown.Option(m['name']))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=pag_prefs, key='ip_adapter_image', col={'lg':6}, page=page)
    ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=pag_prefs, key='ip_adapter_strength', col={'lg':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    ip_adapter_container = Container(ResponsiveRow([ip_adapter_image, ip_adapter_strength]), height = None if pag_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    init_image = FileInput(label="Original Image (optional)", pref=pag_prefs, key='init_image', expand=True, page=page)
    mask_image = FileInput(label="Mask Image (optional)", pref=pag_prefs, key='mask_image', expand=1, page=page)
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=pag_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
    alpha_mask = Checkbox(label="Alpha Mask", value=pag_prefs['alpha_mask'], tooltip="Use Transparent Alpha Channel of Init as Mask", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))
    image_strength = SliderRow(label="Image Strength", min=0, max=1, divisions=20, round=2, pref=pag_prefs, key='image_strength', tooltip="Conceptually, indicates how much to transform the reference `image`. Denoising steps depends on the amount of noise initially added.")
    image_container = Container(Column([ResponsiveRow([Row([init_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]), image_strength]), height = None if pag_prefs['use_SDXL'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    pag_model = Dropdown(label="PAG Model", width=220, options=[dropdown.Option("Custom"), dropdown.Option("PAG_Dreamshaper_v7"), dropdown.Option("PAG_Dreamshaper_v8")], value=pag_prefs['pag_model'], on_change=changed_model)
    pag_custom_model = TextField(label="Custom PAG Model (URL or Path)", value=pag_prefs['custom_model'], expand=True, visible=pag_prefs['pag_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    cpu_offload = Checkbox(label="CPU Offload", tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.", value=pag_prefs['cpu_offload'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'cpu_offload'))
    #cpu_offload = Switcher(label="CPU Offload", value=pag_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), visible=pag_prefs['use_SDXL'], tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    cpu_only = Switcher(label="CPU Only", value=pag_prefs['cpu_only'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_only'), tooltip="If you don't have a good GPU, can run entirely on CPU")
    seed = TextField(label="Seed", width=90, value=str(pag_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(pag_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🍝   Run PAG", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_pag(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_pag(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_pag(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.pag_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("💫  Perturbed-Attention Guidance (PAG)", "Self-Rectifying Diffusion Sampling. Uses SD Model in Installation settings...", actions=[save_default(pag_prefs, ['init_image', 'ip_adapter_image']), IconButton(icon=icons.HELP, tooltip="Help with PAG Settings", on_click=pag_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            #ResponsiveRow([init_image, init_image_strength]),
            steps,
            guidance,
            pag_scale,
            pag_adaptive_scaling,
            pag_applied_layers,
            width_slider, height_slider, #Divider(height=9, thickness=2),
            #Row([pag_model, pag_custom_model]),
            Row([use_SDXL, cpu_offload]),
            image_container,
            Row([use_ip_adapter, ip_adapter_model, ip_adapter_SDXL_model], vertical_alignment=CrossAxisAlignment.START),
            ip_adapter_container,
            #Row([cpu_offload, cpu_only]),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.pag_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c


ldm3d_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "ldm3d-",
    "num_images": 1,
    "width": 1024,
    "height":512,
    "guidance_scale":5.0,
    'num_inference_steps': 50,
    "seed": 0,
    "cpu_offload": True,
    "use_upscale": False,
    "ldm3d_model": "Intel/ldm3d-4c",
    "custom_model": "",
    'use_ip_adapter': False,
    'ip_adapter_image': '',
    'ip_adapter_model': 'SD v1.5',
    'ip_adapter_strength': 0.8,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildLDM3D(page):
    global prefs, ldm3d_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          ldm3d_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def ldm3d_help(e):
      def close_ldm3d_dlg(e):
        nonlocal ldm3d_help_dlg
        ldm3d_help_dlg.open = False
        page.update()
      ldm3d_help_dlg = AlertDialog(title=Text("🙅   Help with LDM3D Pipeline"), content=Column([
          Text("A Latent Diffusion Model for 3D (LDM3D) that generates both image and depth map data from a given text prompt, allowing users to generate RGBD images from text prompts. The LDM3D model is fine-tuned on a dataset of tuples containing an RGB image, depth map and caption, and validated through extensive experiments. We also develop an application called DepthFusion, which uses the generated RGB images and depth maps to create immersive and interactive 360-degree-view experiences using TouchDesigner. This technology has the potential to transform a wide range of industries, from entertainment and gaming to architecture and design. Overall, this paper presents a significant contribution to the field of generative AI and computer vision, and showcases the potential of LDM3D and DepthFusion to revolutionize content creation and digital experiences."),
          Markdown("You can use this model to generate RGB and depth map given a text prompt. A short video summarizing the approach can be found at [this url](https://t.ly/tdi2) and a VR demo can be found [here](https://www.youtube.com/watch?v=3hbUo-hwAs0). A demo is also accessible on [Spaces](https://huggingface.co/spaces/Intel/ldm3d)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Markdown("The LDM3D model was proposed in [LDM3D: Latent Diffusion Model for 3D](https://arxiv.org/abs/2305.10853) by Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-Yen Tseng, Fabio Nonato, Matthias Muller, Vasudev Lal.", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Markdown("[Paper](https://huggingface.co/papers/2305.10853) | [Video](https://t.ly/tdi2) | [Intel/ldm3d-4c Model](https://huggingface.co/Intel/ldm3d-4c) | [Checkpoint](https://huggingface.co/spaces/SimianLuo/Latent_Consistency_Model)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🕳  Going Deep", on_click=close_ldm3d_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(ldm3d_help_dlg)
      ldm3d_help_dlg.open = True
      page.update()
    def changed_model(e):
        ldm3d_prefs['ldm3d_model'] = e.control.value
        ldm3d_custom_model.visible = e.control.value == "Custom"
        ldm3d_custom_model.update()
    prompt = TextField(label="Prompt Text", value=ldm3d_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=ldm3d_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=ldm3d_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=ldm3d_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=ldm3d_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=ldm3d_prefs, key='num_inference_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=ldm3d_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=ldm3d_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=ldm3d_prefs, key='height')
    def toggle_ip_adapter(e):
        ldm3d_prefs['use_ip_adapter'] = e.control.value
        ip_adapter_container.height = None if e.control.value else 0
        ip_adapter_container.update()
        ip_adapter_model.visible = e.control.value
        ip_adapter_model.update()
    use_ip_adapter = Switcher(label="Use IP-Adapter Reference Image", value=ldm3d_prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip="Uses both image and text to condition the image generation process.")
    ip_adapter_model = Dropdown(label="IP-Adapter SD Model", width=220, options=[], value=ldm3d_prefs['ip_adapter_model'], visible=ldm3d_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))
    for m in ip_adapter_models:
        ip_adapter_model.options.append(dropdown.Option(m['name']))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=ldm3d_prefs, key='ip_adapter_image', col={'lg':6}, page=page)
    ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=ldm3d_prefs, key='ip_adapter_strength', col={'lg':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    ip_adapter_container = Container(ResponsiveRow([ip_adapter_image, ip_adapter_strength]), height = None if ldm3d_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    ldm3d_model = Dropdown(label="LDM3D Model", width=220, options=[dropdown.Option("Custom"), dropdown.Option("Intel/ldm3d-4c"), dropdown.Option("Intel/ldm3d-pano"), dropdown.Option("Intel/ldm3d")], value=ldm3d_prefs['ldm3d_model'], on_change=changed_model)
    ldm3d_custom_model = TextField(label="Custom LDM3D Model (URL or Path)", value=ldm3d_prefs['custom_model'], expand=True, visible=ldm3d_prefs['ldm3d_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    cpu_offload = Switcher(label="CPU Offload", value=ldm3d_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    use_upscale = Switcher(label="Use SR Upscale", value=ldm3d_prefs['use_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_upscale'), tooltip="Uses ldm3d-sr to double the size of rgb & depth.")
    seed = TextField(label="Seed", width=90, value=str(ldm3d_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(ldm3d_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🏎   Run LDM3D", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_ldm3d(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_ldm3d(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_ldm3d(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.ldm3d_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🍋  Latent Diffusion Model for 3D (LDM3D)", "Generate RGB Images and 3D Depth Maps given a text prompt... Made with Intel.", actions=[save_default(ldm3d_prefs, ['ip_adapter_image']), IconButton(icon=icons.HELP, tooltip="Help with LDM3D Settings", on_click=ldm3d_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            Row([ldm3d_model, ldm3d_custom_model]),
            Row([use_ip_adapter, ip_adapter_model], vertical_alignment=CrossAxisAlignment.START),
            ip_adapter_container,
            Row([cpu_offload, use_upscale]),
            upscaler,
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            parameters_row,
            page.ldm3d_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

task_matrix_prefs = {
    'prompt': '',
    'image_path': '',
    'modules': ['ImageEditing', 'Text2Image', 'ImageCaptioning'],
}
task_matrix_modules = ['ImageEditing', 'InstructPix2Pix', 'Text2Image', 'ImageCaptioning', 'Image2Canny', 'CannyText2Image', 'Image2Line', 'LineText2Image', 'Image2Hed', 'HedText2Image', 'Image2Scribble', 'ScribbleText2Image', 'Image2Pose', 'PoseText2Image', 'Image2Seg', 'SegText2Image', 'Image2Depth', 'DepthText2Image', 'Image2Normal', 'NormalText2Image', 'VisualQuestionAnswering']

def buildTaskMatrix(page):
    def changed(e, pref=None):
        if pref is not None:
          task_matrix_prefs[pref] = e.control.value
    def add_to_task_matrix_output(o):
      task_matrix_output.controls.append(o)
      task_matrix_output.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      task_matrix_output.controls = []
      task_matrix_output.update()
      clear_button.visible = False
      clear_button.update()
    def task_matrix_help(e):
      def close_task_matrix_dlg(e):
        nonlocal task_matrix_help_dlg
        task_matrix_help_dlg.open = False
        page.update()
      task_matrix_help_dlg = AlertDialog(title=Text("🙅   Help with TaskMatrix Pipeline"), content=Column([
          Text("TaskMatrix connects ChatGPT and a series of Visual Foundation Models to enable sending and receiving images during chatting. Made in partnership with Microsoft Reserarch."),
          Text("ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called \textbf{Visual ChatGPT}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models."),
          Text("Supports tasks for Image Editing, Instruct Pix2Pix, Text2Image, Image Captioning, Image2Canny, Canny Text2Image, Image2Line, Line Text2Image, Image2Hed, HED Text2Image, Image2Scribble, Scribble Text2Image, Image2Pose, Pose Text2Image, Image2Seg, Seg Text2Image, Image2Depth, Depth Text2Image, Image2Normal, Normal Text2Image and Visual Question Answering..	That's a lot!"),
          Markdown("[Paper](https://arxiv.org/abs/2303.04671) | [GitHub](https://github.com/moymix/TaskMatrix) | [HuggingFace Space](https://huggingface.co/spaces/microsoft/visual_chatgpt) | [Colab](https://colab.research.google.com/drive/1P3jJqKEWEaeNcZg8fODbbWeQ3gxOHk2-?usp=sharing)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Credits go to Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan and Microsoft")
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🧑‍🚀️  Wowsers! ", on_click=close_task_matrix_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(task_matrix_help_dlg)
      task_matrix_help_dlg.open = True
      page.update()
    def change_modules(e):
        if e.control.data in task_matrix_prefs['modules']:
            task_matrix_prefs['modules'].remove(e.control.data)
        else:
            task_matrix_prefs['modules'].append(e.control.data)
    page.add_to_task_matrix_output = add_to_task_matrix_output
    modules_list = ResponsiveRow(controls=[])
    for v in task_matrix_modules:
        modules_list.controls.append(Checkbox(label=v, data=v, value=v in task_matrix_prefs['modules'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=change_modules, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
    prompt = TextField(label="Conversational Prompt Request", value=task_matrix_prefs['prompt'], filled=True, col={'md': 9}, on_change=lambda e:changed(e,'prompt'))
    image_path = FileInput(label="Input Image", pref=task_matrix_prefs, key='image_path', filled=True, col={'md': 3}, page=page)
    task_matrix_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(task_matrix_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🧑‍💻️  TaskMatrix Visual ChatGPT (under construction)", "Talking, Drawing and Editing with Visual Foundation Models. Conversational requests for image editing & creating using OpenAI brain...", actions=[save_default(task_matrix_prefs, ['image_path']), IconButton(icon=icons.HELP, tooltip="Help with TaskMatrix Settings", on_click=task_matrix_help)]),
        Text("Active Pipeline Modules: (uses up VRAM)", weight=FontWeight.BOLD),
        modules_list,
        ResponsiveRow([prompt, image_path]),
        ElevatedButton(content=Text("🧑‍🏭️  Ask Art Bot", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_task_matrix(page)),
        task_matrix_output,
        clear_button,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c


text_to_video_prefs = {
    'prompt': '',
    'negative_prompt': 'text, words, watermark, shutterstock',
    'num_inference_steps': 50,
    'guidance_scale': 9.0,
    'export_to_video': True,
    "interpolate_video": True,
    'eta': 0.0,
    'seed': 0,
    'width': 256,
    'height': 256,
    'num_frames': 16,
    'model': 'damo-vilab/text-to-video-ms-1.7b',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
    "lower_memory": True,
}

def buildTextToVideo(page):
    global text_to_video_prefs, prefs, pipe_text_to_video, editing_prompt
    editing_prompt = {'editing_prompt':'', 'edit_warmup_steps':10, 'edit_guidance_scale':5, 'edit_threshold':0.9, 'edit_weights':1, 'reverse_editing_direction': False}
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          text_to_video_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.text_to_video_output.controls = []
      page.text_to_video_output.update()
      clear_button.visible = False
      clear_button.update()
    def text_to_video_help(e):
      def close_text_to_video_dlg(e):
        nonlocal text_to_video_help_dlg
        text_to_video_help_dlg.open = False
        page.update()
      text_to_video_help_dlg = AlertDialog(title=Text("💁   Help with Text-To-Video"), content=Column([
          Text("Text-to-video synthesis from [ModelScope](https://modelscope.cn/) can be considered the same as Stable Diffusion structure-wise but it is extended to videos instead of static images. More specifically, this system allows us to generate videos from a natural language text prompt."),
          Markdown("""From the [model summary](https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis):
*This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.*
Resources:
* [Website](https://modelscope.cn/models/damo/text-to-video-synthesis/summary)
* [GitHub repository](https://github.com/modelscope/modelscope/)""", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🎞  What'll be next... ", on_click=close_text_to_video_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(text_to_video_help_dlg)
      text_to_video_help_dlg.open = True
      page.update()
    prompt = TextField(label="Animation Prompt Text", value=text_to_video_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=text_to_video_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    num_frames = SliderRow(label="Number of Frames", min=1, max=300, divisions=299, pref=text_to_video_prefs, key='num_frames', tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=text_to_video_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=text_to_video_prefs, key='guidance_scale')
    eta_slider = SliderRow(label="ETA", min=0, max=1.0, divisions=20, round=1, pref=text_to_video_prefs, key='eta', tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.")
    #width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=text_to_video_prefs, key='width')
    #height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=text_to_video_prefs, key='height')
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=text_to_video_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    interpolate_vid = Switcher(label="Interpolate", value=text_to_video_prefs['interpolate_video'], tooltip="Use Google FiLM Interpolation to transition between frames.", on_change=lambda e:changed(e,'interpolate_video'))
    lower_memory = Tooltip(message="Enable CPU offloading, VAE Tiling & Stitching", content=Switcher(label="Lower Memory Mode", value=text_to_video_prefs['lower_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'lower_memory')))
    model = Dropdown(label="Video Model", hint_text="", expand=True, options=[dropdown.Option("damo-vilab/text-to-video-ms-1.7b"), dropdown.Option("modelscope-damo-text2video-synthesis"), dropdown.Option("modelscope-damo-text2video-pruned-weights"), dropdown.Option("cerspense/zeroscope_v2_XL"), dropdown.Option("cerspense/zeroscope_v2_576w")], value=text_to_video_prefs['model'], autofocus=False, on_change=lambda e:changed(e, 'model'))
    batch_folder_name = TextField(label="Batch Folder Name", value=text_to_video_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    seed = TextField(label="Seed", width=90, value=str(text_to_video_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(text_to_video_prefs)
    page.upscalers.append(upscaler)
    page.text_to_video_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.text_to_video_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎥  Text-To-Video Synthesis", "Modelscope's Text-to-video-synthesis Model to Animate Diffusion", actions=[save_default(text_to_video_prefs), IconButton(icon=icons.HELP, tooltip="Help with Text-to-Video Settings", on_click=text_to_video_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        ResponsiveRow([prompt, negative_prompt]),
        #Row([NumberPicker(label="Number of Frames: ", min=1, max=8, value=text_to_video_prefs['num_frames'], tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.", on_change=lambda e: changed(e, 'num_frames')), seed, batch_folder_name]),
        Row([export_to_video, interpolate_vid, lower_memory, model]),
        num_frames,
        num_inference_row,
        guidance,
        eta_slider,
        #width_slider, height_slider,
        upscaler,
        Row([seed, batch_folder_name]),
        #Row([jump_length, jump_n_sample, seed]),
        Row([
            ElevatedButton(content=Text("📹  Run Text-To-Video", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_text_to_video(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_text_to_video(page, from_list=True))
        ]),
        page.text_to_video_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

text_to_video_zero_prefs = {
    'prompt': '',
    'negative_prompt': 'text, words, watermark, shutterstock',
    'num_inference_steps': 50,
    'guidance_scale': 9.0,
    'export_to_video': True,
    'eta': 0.0,
    'seed': 0,
    'width': 1024,
    'height': 1024,
    'num_frames': 8,
    'motion_field_strength_x': 12,
    'motion_field_strength_y': 12,
    't0': 42,
    't1': 47,
    'input_video': '',
    'prep_type': 'Zero Shot',
    'use_SDXL': False,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
    "lower_memory": True,
}

def buildTextToVideoZero(page):
    global text_to_video_zero_prefs, prefs, pipe_text_to_video_zero
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          text_to_video_zero_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def text_to_video_zero_help(e):
      def close_text_to_video_zero_dlg(e):
        nonlocal text_to_video_zero_help_dlg
        text_to_video_zero_help_dlg.open = False
        page.update()
      text_to_video_zero_help_dlg = AlertDialog(title=Text("💁   Help with Text-To-Video Zero"), content=Column([
          Text("Recent text-to-video generation approaches rely on computationally heavy training and require large-scale video datasets. In this paper, we introduce a new task of zero-shot text-to-video generation and propose a low-cost approach (without any training or optimization) by leveraging the power of existing text-to-image synthesis methods (e.g., Stable Diffusion), making them suitable for the video domain. Our key modifications include (i) enriching the latent codes of the generated frames with motion dynamics to keep the global scene and the background time consistent; and (ii) reprogramming frame-level self-attention using a new cross-frame attention of each frame on the first frame, to preserve the context, appearance, and identity of the foreground object."),
          Text("Experiments show that this leads to low overhead, yet high-quality and remarkably consistent video generation. Moreover, our approach is not limited to text-to-video synthesis but is also applicable to other tasks such as conditional and content-specialized video generation, and Video Instruct-Pix2Pix, i.e., instruction-guided video editing. As experiments show, our method performs comparably or sometimes better than recent approaches, despite not being trained on additional video data."),
          Markdown("""* [Project Page](https://text2video-zero.github.io/)
* [Paper](https://arxiv.org/abs/2303.13439)
* [Original Code](https://github.com/Picsart-AI-Research/Text2Video-Zero)""", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🎞  Let's go crazy... ", on_click=close_text_to_video_zero_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(text_to_video_zero_help_dlg)
      text_to_video_zero_help_dlg.open = True
      page.update()
    def change_steps(e):
        steps = e.control.value
        t0.set_max(steps - 1)
        t0.set_max(steps - 1)
        t0.set_divisions(steps - 1)
        if text_to_video_zero_prefs['t0'] > steps - 1:
            text_to_video_zero_prefs['t0'] = steps - 1
            t0.set_value(text_to_video_zero_prefs['t0'])
        t1.set_min(text_to_video_zero_prefs['t0'] + 1)
        t1.set_max(steps - 1)
        if text_to_video_zero_prefs['t1'] > steps - 1:
            text_to_video_zero_prefs['t1'] = steps - 1
            t1.set_value(text_to_video_zero_prefs['t1'])
        if text_to_video_zero_prefs['t1'] < t1.min:
            text_to_video_zero_prefs['t1'] = t1.min
            t1.set_value(text_to_video_zero_prefs['t1'])
        t1.set_divisions(t1.max - t1.min)
        t0.update_slider()
        t1.update_slider()
    def change_t0(e):
        t0_value = e.control.value
        steps = num_inference_row.value
        t1.set_min(t0_value + 1)
        if text_to_video_zero_prefs['t1'] > steps - 1:
            text_to_video_zero_prefs['t1'] = steps - 1
            t1.set_value(text_to_video_zero_prefs['t1'])
        if text_to_video_zero_prefs['t1'] < t1.min:
            text_to_video_zero_prefs['t1'] = t1.min
            t1.set_value(text_to_video_zero_prefs['t1'])
        t1.set_divisions(t1.max - t1.min)
        t1.update_slider()
    prompt = TextField(label="Animation Prompt Text", value=text_to_video_zero_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=text_to_video_zero_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    num_frames = SliderRow(label="Number of Frames", min=1, max=300, divisions=299, pref=text_to_video_zero_prefs, key='num_frames', tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=text_to_video_zero_prefs, key='num_inference_steps', on_change=change_steps, tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=text_to_video_zero_prefs, key='guidance_scale')
    eta_slider = SliderRow(label="ETA", min=0, max=1.0, divisions=20, round=1, pref=text_to_video_zero_prefs, key='eta', tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.")
    motion_field_strength_x = SliderRow(label="Motion Field Strength X", min=1, max=30, divisions=29, pref=text_to_video_zero_prefs, key='motion_field_strength_x', tooltip="Strength of motion in generated video along x-axis")
    motion_field_strength_y = SliderRow(label="Motion Field Strength Y", min=1, max=30, divisions=29, pref=text_to_video_zero_prefs, key='motion_field_strength_y', tooltip="Strength of motion in generated video along y-axis")
    t0 = SliderRow(label="Timestep t0", min=0, max=50, divisions=50, pref=text_to_video_zero_prefs, key='t0', on_change=change_t0, tooltip="Should be in the range [0, num_inference_steps - 1]")
    t1 = SliderRow(label="Timestep t1", min=43, max=50, divisions=7, pref=text_to_video_zero_prefs, key='t1', tooltip="Should be in the range [t0 + 1, num_inference_steps - 1]")
    #width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=text_to_video_zero_prefs, key='width')
    #height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=text_to_video_zero_prefs, key='height')
    use_SDXL = Switcher(label="Use Stable Diffusion XL", value=text_to_video_zero_prefs['use_SDXL'], on_change=lambda e:changed(e,'use_SDXL'), tooltip="SDXL uses Model Checkpoint set in Installation. Otherwise use selected 1.5 or 2.1 Model.")
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=text_to_video_zero_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    #lower_memory = Tooltip(message="Enable CPU offloading, VAE Tiling & Stitching", content=Switcher(label="Lower Memory Mode", value=text_to_video_zero_prefs['lower_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'lower_memory')))
    batch_folder_name = TextField(label="Batch Folder Name", value=text_to_video_zero_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    seed = TextField(label="Seed", width=90, value=str(text_to_video_zero_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(text_to_video_zero_prefs)
    page.upscalers.append(upscaler)
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎥  Text-To-Video Zero", "Text-to-Image Diffusion Models for Zero-Shot Video Generators", actions=[save_default(text_to_video_zero_prefs, ['input_video']), IconButton(icon=icons.HELP, tooltip="Help with Text-To-Video Zero Settings", on_click=text_to_video_zero_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        ResponsiveRow([prompt, negative_prompt]),
        #Row([NumberPicker(label="Number of Frames: ", min=1, max=8, value=text_to_video_zero_prefs['num_frames'], tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.", on_change=lambda e: changed(e, 'num_frames')), seed, batch_folder_name]),
        Row([export_to_video, use_SDXL]),
        num_frames,
        num_inference_row,
        guidance,
        eta_slider,
        motion_field_strength_x, motion_field_strength_y,
        t0, t1,
        #width_slider, height_slider,
        upscaler,
        Row([seed, batch_folder_name]),
        #Row([jump_length, jump_n_sample, seed]),
        Row([
            ElevatedButton(content=Text("📹  Run Text2Video-Zero", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_text_to_video_zero(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_text_to_video_zero(page, from_list=True))
        ]),
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

video_to_video_prefs = {
    'init_video': '',
    'fps': 12,
    'start_time': 0,
    'end_time': 0,
    'prompt': '',
    'negative_prompt': '',
    'num_inference_steps': 50,
    'guidance_scale': 15.0,
    'strength': 0.6,
    'export_to_video': True,
    'eta': 0.0,
    'seed': 0,
    'max_size': 1024,
    'width': 256,
    'height': 256,
    'num_frames': 16,
    'model': 'cerspense/zeroscope_v2_576w',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
    "lower_memory": True,
}

def buildVideoToVideo(page):
    global video_to_video_prefs, prefs, pipe_video_to_video, editing_prompt
    editing_prompt = {'editing_prompt':'', 'edit_warmup_steps':10, 'edit_guidance_scale':5, 'edit_threshold':0.9, 'edit_weights':1, 'reverse_editing_direction': False}
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          video_to_video_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.video_to_video_output.controls = []
      page.video_to_video_output.update()
      clear_button.visible = False
      clear_button.update()
    def video_to_video_help(e):
      def close_video_to_video_dlg(e):
        nonlocal video_to_video_help_dlg
        video_to_video_help_dlg.open = False
        page.update()
      video_to_video_help_dlg = AlertDialog(title=Text("💁   Help with Video-To-Video"), content=Column([
          Text("Cerspense Zeroscope are watermark-free model and have been trained on images that are 1024x576. Text-to-video synthesis from [ModelScope](https://modelscope.cn/) can be considered the same as Stable Diffusion structure-wise but it is extended to videos instead of static images. More specifically, this system allows us to generate videos from a natural language text prompt."),
          Markdown("""From the [model summary](https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis):
*This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.*
Resources:
* [Website](https://modelscope.cn/models/damo/text-to-video-synthesis/summary)
* [GitHub repository](https://github.com/modelscope/modelscope/)""", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🎞  What'll be next... ", on_click=close_video_to_video_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(video_to_video_help_dlg)
      video_to_video_help_dlg.open = True
      page.update()
    prompt = TextField(label="Video Prompt Text", value=video_to_video_prefs['prompt'], col={'md': 9}, filled=True, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=video_to_video_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    init_video = FileInput(label="Init Video Clip", pref=video_to_video_prefs, key='init_video', ftype="video", page=page)
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=video_to_video_prefs, key='fps', tooltip="The FPS to extract from the init video clip.")
    start_time = TextField(label="Start Time (s)", value=controlnet_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype="float"))
    end_time = TextField(label="End Time (0 for all)", value=controlnet_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype="float"))
    vid_params = Container(content=Column([fps, Row([start_time, end_time])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if controlnet_prefs['use_init_video'] else 0)
    #num_frames = SliderRow(label="Number of Frames", min=1, max=300, divisions=299, pref=video_to_video_prefs, key='num_frames', tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=video_to_video_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=video_to_video_prefs, key='guidance_scale')
    strength = SliderRow(label="Init Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=video_to_video_prefs, key='strength', tooltip="Conceptually, indicates how much to transform the Reference Image over the Vid Generation. Higher value give less influence.")
    eta_slider = SliderRow(label="ETA", min=0, max=1.0, divisions=20, round=1, pref=video_to_video_prefs, key='eta', tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.")
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=video_to_video_prefs, key='max_size')
    #width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=video_to_video_prefs, key='width')
    #height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=video_to_video_prefs, key='height')
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=video_to_video_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    lower_memory = Tooltip(message="Enable CPU offloading, VAE Tiling & Stitching", content=Switcher(label="Lower Memory Mode", value=video_to_video_prefs['lower_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'lower_memory')))
    model = Dropdown(label="Video Model", hint_text="", expand=True, options=[dropdown.Option("damo-vilab/text-to-video-ms-1.7b"), dropdown.Option("modelscope-damo-text2video-synthesis"), dropdown.Option("modelscope-damo-text2video-pruned-weights"), dropdown.Option("cerspense/zeroscope_v2_XL"), dropdown.Option("cerspense/zeroscope_v2_576w")], value=video_to_video_prefs['model'], autofocus=False, on_change=lambda e:changed(e, 'model'))
    batch_folder_name = TextField(label="Batch Folder Name", value=video_to_video_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    seed = TextField(label="Seed", width=90, value=str(video_to_video_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(video_to_video_prefs)
    page.upscalers.append(upscaler)
    page.video_to_video_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.video_to_video_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("📽   Video-To-Video Synthesis", "Note: Uses more than 16GB VRAM, may crash session. Video-to-video-synthesis Model to Reanimate Video Clips", actions=[save_default(video_to_video_prefs, ['init_video']), IconButton(icon=icons.HELP, tooltip="Help with Video-to-Video Settings", on_click=video_to_video_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        ResponsiveRow([prompt, negative_prompt]),
        init_video,
        fps,
        Row([start_time, end_time]),
        #Row([NumberPicker(label="Number of Frames: ", min=1, max=8, value=video_to_video_prefs['num_frames'], tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.", on_change=lambda e: changed(e, 'num_frames')), seed, batch_folder_name]),
        Row([export_to_video, lower_memory, model]),
        #num_frames,
        num_inference_row,
        guidance,
        strength,
        eta_slider,
        max_row,
        #width_slider, height_slider,
        upscaler,
        Row([seed, batch_folder_name]),
        #Row([jump_length, jump_n_sample, seed]),
        Row([
            ElevatedButton(content=Text("🎦  Run Video-To-Video", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_video_to_video(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_video_to_video(page, from_list=True))
        ]),
        page.video_to_video_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

controlnet_temporalnet_prefs = {
    'init_video': '',
    'init_image': '',
    'prompt': '',
    'negative_prompt': '',
    'num_inference_steps': 50,
    'guidance_scale': 7.0,
    'temporalnet_strength': 0.6,
    'canny_strength': 0.7,
    'export_to_video': True,
    'save_frames': False,
    'save_canny': False,
    "lower_memory": True,
    'seed': 0,
    'max_size': 1024,
    'low_threshold': 25,
    'high_threshold': 200,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildTemporalNet_XL(page):
    global controlnet_temporalnet_prefs, prefs, pipe_controlnet, editing_prompt
    editing_prompt = {'editing_prompt':'', 'edit_warmup_steps':10, 'edit_guidance_scale':5, 'edit_threshold':0.9, 'edit_weights':1, 'reverse_editing_direction': False}
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          controlnet_temporalnet_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.controlnet_temporalnet_output.controls = []
      page.controlnet_temporalnet_output.update()
      clear_button.visible = False
      clear_button.update()
    def controlnet_temporalnet_help(e):
      def close_controlnet_temporalnet_dlg(e):
        nonlocal controlnet_temporalnet_help_dlg
        controlnet_temporalnet_help_dlg.open = False
        page.update()
      controlnet_temporalnet_help_dlg = AlertDialog(title=Text("💁   Help with Controlnet TemporalNet XL"), content=Column([
          Text("This is TemporalNet-XL, it is a re-train of the controlnet TemporalNet1 with Stable Diffusion XL. This does not use the control mechanism of TemporalNet2 as it would require some additional work to adapt the diffusers pipeline to work with a 6-channel input. While it does not eliminate all flickering, it significantly reduces it, particularly at higher denoise levels. For optimal results, it is recommended to use TemporalNet in combination with other methods."),
          Markdown("Credit goes to Ciara Rowles - [Huggingface Model](https://huggingface.co/CiaraRowles/controlnet-temporalnet-sdxl-1.0)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🎞  Temporal Anomaly... ", on_click=close_controlnet_temporalnet_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(controlnet_temporalnet_help_dlg)
      controlnet_temporalnet_help_dlg.open = True
      page.update()
    prompt = TextField(label="Video Prompt Text", value=controlnet_temporalnet_prefs['prompt'], col={'md': 9}, filled=True, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=controlnet_temporalnet_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    init_video = FileInput(label="Init Video Clip", pref=controlnet_temporalnet_prefs, key='init_video', ftype="video", page=page)
    init_image = FileInput(label="Frame 0 Init Image (optional)", pref=controlnet_temporalnet_prefs, key='init_image', ftype="image", page=page)
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=controlnet_temporalnet_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=controlnet_temporalnet_prefs, key='guidance_scale')
    #strength = SliderRow(label="Init Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_temporalnet_prefs, key='strength', tooltip="Conceptually, indicates how much to transform the Reference Image over the Vid Generation. Higher value give less influence.")
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_temporalnet_prefs, key='max_size')
    #width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_temporalnet_prefs, key='width')
    #height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_temporalnet_prefs, key='height')
    low_threshold_row = SliderRow(label="Canny Low Threshold", min=1, max=255, divisions=254, pref=controlnet_temporalnet_prefs, key='low_threshold', expand=True, col={'lg':6}, tooltip="Lower increases sensitivity to weaker edges, higher gives fewer but more reliable edge detections.")
    high_threshold_row = SliderRow(label="Canny High Threshold", min=1, max=255, divisions=254, pref=controlnet_temporalnet_prefs, key='high_threshold', expand=True, col={'lg':6}, tooltip="Higher value decreases the amount of noise but could result in missing some true edges.")
    canny_threshold = Container(ResponsiveRow([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(bottom=8))
    temporalnet_strength = SliderRow(label="TemporalNet Strength", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_temporalnet_prefs, key='temporalnet_strength', col={'lg':6}, tooltip="")
    canny_strength = SliderRow(label="Canny Strength", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_temporalnet_prefs, key='canny_strength', col={'lg':6}, tooltip="")
    lower_memory = Tooltip(message="Enable CPU offloading, VAE Tiling & Stitching", content=Switcher(label="Lower Memory Mode", value=controlnet_temporalnet_prefs['lower_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'lower_memory')))
    save_frames = Tooltip(message="Save Frames", content=Switcher(label="Save Frames", value=controlnet_temporalnet_prefs['save_frames'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'save_frames')))
    save_canny = Tooltip(message="Save Canny", content=Switcher(label="Save Canny Frames", value=controlnet_temporalnet_prefs['save_canny'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'save_canny')))
    #model = Dropdown(label="Video Model", hint_text="", expand=True, options=[dropdown.Option("damo-vilab/text-to-video-ms-1.7b"), dropdown.Option("modelscope-damo-text2video-synthesis"), dropdown.Option("modelscope-damo-text2video-pruned-weights"), dropdown.Option("cerspense/zeroscope_v2_XL"), dropdown.Option("cerspense/zeroscope_v2_576w")], value=controlnet_temporalnet_prefs['model'], autofocus=False, on_change=lambda e:changed(e, 'model'))
    batch_folder_name = TextField(label="Batch Folder Name", value=controlnet_temporalnet_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    seed = TextField(label="Seed", width=90, value=str(controlnet_temporalnet_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(controlnet_temporalnet_prefs)
    page.upscalers.append(upscaler)
    page.controlnet_temporalnet_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.controlnet_temporalnet_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("⌛  Controlnet TemporalNet-XL", "Video2Video ControlNet model designed to enhance the temporal consistency of video frames...", actions=[save_default(controlnet_temporalnet_prefs, ['init_video', 'init_image']), IconButton(icon=icons.HELP, tooltip="Help with TemporalNet-XL Settings", on_click=controlnet_temporalnet_help)]),
        ResponsiveRow([prompt, negative_prompt]),
        init_video,
        init_image,
        num_inference_row,
        guidance,
        ResponsiveRow([temporalnet_strength, canny_strength]),
        canny_threshold,
        max_row,
        Row([lower_memory, save_frames, save_canny]),
        upscaler,
        Row([seed, batch_folder_name]),
        Row([ElevatedButton(content=Text("⏲️  Run TemporalNet-XL", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_temporalnet(page)),]),
        page.controlnet_temporalnet_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

infinite_zoom_prefs = {
    'prompt': '',
    'negative_prompt': '',
    'frame': '0',
    'guidance_scale': 6.0,
    'num_inference_steps': 40,
    'editing_prompts': [],
    'animation_prompts': {},
    'init_image': '',
    'fps': 30,
    'mask_width': 128,
    'num_outpainting_steps': 20,
    'frame_dupe_amount': 15,
    'num_interpol_frames': 30,
    'inpainting_model': 'stabilityai/stable-diffusion-2-inpainting',
    'use_SDXL': False,
    'max_size': 512,
    'width': 512,
    'height': 512,
    'seed': 0,
    'save_frames': True,
    'save_gif': True,
    'save_video': True,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 1.5,
    "display_upscaled_image": False,
}

def buildInfiniteZoom(page):
    global infinite_zoom_prefs, prefs, pipe_infinite_zoom, editing_prompt
    editing_prompt = {'prompt':'', 'negative_prompt':'', 'seed':0}
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          infinite_zoom_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_infinite_zoom_output(o):
      page.infinite_zoom_output.controls.append(o)
      page.infinite_zoom_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.infinite_zoom_output.controls = []
      page.infinite_zoom_output.update()
      clear_button.visible = False
      clear_button.update()
    def infinite_zoom_help(e):
      def close_infinite_zoom_dlg(e):
        nonlocal infinite_zoom_help_dlg
        infinite_zoom_help_dlg.open = False
        page.update()
      infinite_zoom_help_dlg = AlertDialog(title=Text("💁   Help with Infinite-Zoom"), content=Column([
          Text('We shrink the init image from the previous block and outpaint its outer frame using the same concept (e.g. prompt, negative prompt, inference steps) but with a different seed. To generate an "inifinte zoom" video this is repeated num_outpainting_steps times and then rendered in reversed order. To keep the outpainted part coherent and full of new content its width has to be relatively large (e.g. mask_width = 128 pixels if resolution is 512*512). This on the other hand means that the generated video would be too fast and aestetically unpleasant. To slow down and smoothen the video we generate num_interpol_frames additional images between outpainted images using simple "interpolation".'),
          Text('Notes: Length of the video is proportional to num_outpainting_steps * num_interpol_frames. The time to generate the video is proportional to num_outpainting_steps. On a T4 GPU it takes about ~7 minutes to generate the video of width = 512, num_inference_steps = 20, num_outpainting_steps = 100. With fps = 24 and num_interpol_frames = 24 the video will be about 1:40 minutes long.'),
          #Text(""),
          Markdown("[Github Project](https://github.com/v8hid/infinite-zoom-stable-diffusion) | [Google Colab](https://colab.research.google.com/github/v8hid/infinite-zoom-stable-diffusion/blob/main/smooth_infinite_zoom.ipynb)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🕵  It's Endlessness... ", on_click=close_infinite_zoom_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(infinite_zoom_help_dlg)
      infinite_zoom_help_dlg.open = True
      page.update()
    def infinite_zoom_tile(animate_prompt):
        params = []
        for k, v in animate_prompt.items():
            if k == 'prompt': continue
            params.append(f'{to_title(k)}: {v}')
        sub = ', '.join(params)
        return ListTile(title=Text(animate_prompt['prompt'], max_lines=6, theme_style=TextThemeStyle.BODY_LARGE), subtitle=Text(sub), dense=True, data=animate_prompt, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[PopupMenuItem(icon=icons.EDIT, text="Edit Animation Prompt", on_click=lambda e: edit_infinite_zoom(animate_prompt), data=animate_prompt),
                 PopupMenuItem(icon=icons.DELETE, text="Delete Animation Prompt", on_click=lambda e: del_infinite_zoom(animate_prompt), data=animate_prompt)]), on_click=lambda e: edit_infinite_zoom(animate_prompt))
    def edit_infinite_zoom(edit=None):
        infinite_zoom_prompt = edit if bool(edit) else editing_prompt.copy()
        edit_prompt = edit['prompt'] if bool(edit) else infinite_zoom_prefs['prompt']
        if not bool(edit):
            infinite_zoom_prompt['prompt'] = infinite_zoom_prefs['prompt']
            infinite_zoom_prompt['negative_prompt'] = infinite_zoom_prefs['negative_prompt']
            infinite_zoom_prompt['seed'] = infinite_zoom_prefs['seed']
        def close_dlg(e):
            dlg_edit.open = False
            page.update()
        def changed_p(e, pref=None):
            if pref is not None:
                infinite_zoom_prompt[pref] = e.control.value
        def save_infinite_zoom_prompt(e):
            if edit == None:
                infinite_zoom_prefs['editing_prompts'].append(infinite_zoom_prompt)
                page.infinite_zoom_prompts.controls.append(infinite_zoom_tile(infinite_zoom_prompt))
                page.infinite_zoom_prompts.update()
            else:
                for s in infinite_zoom_prefs['editing_prompts']:
                    if s['prompt'] == edit_prompt:
                        s = infinite_zoom_prompt
                        break
                for t in page.infinite_zoom_prompts.controls:
                    #print(f"{t.data['prompt']} == {edit_prompt} - {t.title.value}")
                    if t.title.value == edit_prompt: #t.data['prompt']
                        params = []
                        for k, v in infinite_zoom_prompt.items():
                            if k == 'prompt': continue
                            params.append(f'{to_title(k)}: {v}')
                        sub = ', '.join(params)
                        t.title = Text(infinite_zoom_prompt['prompt'], max_lines=6, theme_style=TextThemeStyle.BODY_LARGE)
                        t.subtitle = Text(sub)
                        t.data = infinite_zoom_prompt
                        t.update()
                        break
                dlg_edit.open = False
                e.control.update()
                page.update()
        infinite_zoom_editing_prompt = TextField(label="InfiniteZoom Prompt Modifier", value=infinite_zoom_prompt['prompt'], autofocus=True, on_change=lambda e:changed_p(e,'prompt'))
        infinite_zoom_negative_prompt = TextField(label="Negative Prompt", value=infinite_zoom_prompt['negative_prompt'], autofocus=True, on_change=lambda e:changed_p(e,'negative_prompt'))
        infinite_zoom_seed = TextField(label="Seed", width=90, value=str(infinite_zoom_prompt['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed_p(e,'seed'), col={'md':1})
        if edit != None:
            dlg_edit = AlertDialog(modal=False, title=Text(f"♟️ {'Edit' if bool(edit) else 'Add'} Animated Prompt"), content=Container(Column([
                infinite_zoom_editing_prompt,
                infinite_zoom_negative_prompt, infinite_zoom_seed,
            ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO), width=(page.width if page.web else page.window.width) - 180), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Prompt ", size=19, weight=FontWeight.BOLD), on_click=save_infinite_zoom_prompt)], actions_alignment=MainAxisAlignment.END)
            page.overlay.append(dlg_edit)
            dlg_edit.open = True
            page.update()
        else:
            save_infinite_zoom_prompt(None)
    def del_infinite_zoom(edit=None):
        for s in infinite_zoom_prefs['editing_prompts']:
            if s['prompt'] == edit['prompt']:
                infinite_zoom_prefs['editing_prompts'].remove(s)
                break
        for t in page.infinite_zoom_prompts.controls:
            if t.data['prompt'] == edit['prompt']:
                page.infinite_zoom_prompts.controls.remove(t)
                break
        page.infinite_zoom_prompts.update()
        play_snd(Snd.DELETE, page)
    def edit_prompt(e):
      nonlocal animation_prompts
      f = int(e.control.data)
      edit_prompt = infinite_zoom_prefs['animation_prompts'][str(f)]
      def close_dlg(e):
        dlg_edit.open = False
        page.update()
      def save_prompt(e):
        fr = int(editing_frame.value)
        pline = f'{fr}: "{editing_prompt.value}"'
        for p in animation_prompts.controls:
          if p.data == f:
            p.title.value = pline
            p.data = fr
            for button in p.trailing.items:
              button.data = fr
            break
        infinite_zoom_prefs['animation_prompts'][str(editing_frame.value)] = editing_prompt.value.strip()
        if f != int(editing_frame.value):
          del infinite_zoom_prefs['animation_prompts'][str(f)]
          sorted_dict = {}
          for key in sorted(infinite_zoom_prefs['animation_prompts'].keys()):
              sorted_dict[key] = infinite_zoom_prefs['animation_prompts'][key]
          infinite_zoom_prefs['animation_prompts'] = sorted_dict
          animation_prompts.controls = sorted(animation_prompts.controls, key=lambda tile: tile.data)
        animation_prompts.update()
        close_dlg(e)
      editing_frame = TextField(label="Frame", width=90, value=str(f), keyboard_type=KeyboardType.NUMBER, tooltip="")
      editing_prompt = TextField(label="Keyframe Prompt Animation", expand=True, multiline=True, value=edit_prompt, autofocus=True)
      dlg_edit = AlertDialog(modal=False, title=Text(f"♟️ Edit Prompt Keyframe"), content=Container(Column([
          Row([editing_frame, editing_prompt])
      ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO), width=(page.width if page.web else page.window.width) - 180), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Prompt ", size=19, weight=FontWeight.BOLD), on_click=save_prompt)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(dlg_edit)
      dlg_edit.open = True
      page.update()
    def del_prompt(e):
      f = e.control.data
      for i, p in enumerate(animation_prompts.controls):
        if p.data == f:
          del animation_prompts.controls[i]
          break
      animation_prompts.update()
      del infinite_zoom_prefs['animation_prompts'][str(f)]
      play_snd(Snd.DELETE, page)
    def clear_prompts(e):
      animation_prompts.controls.clear()
      animation_prompts.update()
      infinite_zoom_prefs['animation_prompts'] = {}
      clear_prompt(e)
      play_snd(Snd.DELETE, page)
    def clear_prompt(e):
      prompt.value = ""
      prompt.update()
    def copy_prompt(e):
      p = infinite_zoom_prefs['animation_prompts'][str(e.control.data)]
      page.set_clipboard(p)
      toast_msg(page, f"📋  Prompt Text copied to clipboard...")
    def add_prompt(e, f=None, p=None, sound=True):
      if (not bool(prompt.value) or not bool(frame.value)) and f == None: return
      if f == None: f = int(frame.value)
      if p == None: p = prompt.value.strip()
      pline = f'{f}: "{p}"'
      if str(f) in infinite_zoom_prefs['animation_prompts']:
        for i, pr in enumerate(animation_prompts.controls):
          if pr.data == f:
            pr.title.value = pline
      else:
        animation_prompts.controls.append(ListTile(title=Text(pline, size=14), data=f, trailing=PopupMenuButton(icon=icons.MORE_VERT,
            items=[PopupMenuItem(icon=icons.EDIT, text="Edit Animation Prompt", on_click=edit_prompt, data=f),
                  PopupMenuItem(icon=icons.COPY, text="Copy Prompt Text", on_click=copy_prompt, data=f),
                  PopupMenuItem(icon=icons.DELETE, text="Delete Animation Prompt", on_click=del_prompt, data=f), PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Prompts", on_click=clear_prompts)]), dense=True, on_click=edit_prompt))
      infinite_zoom_prefs['animation_prompts'][str(f)] = p
      sorted_dict = {}
      for key in sorted(infinite_zoom_prefs['animation_prompts'].keys()):
          sorted_dict[key] = infinite_zoom_prefs['animation_prompts'][key]
      infinite_zoom_prefs['animation_prompts'] = sorted_dict
      animation_prompts.controls = sorted(animation_prompts.controls, key=lambda tile: tile.data)
      animation_prompts.update()
      if prefs['enable_sounds'] and sound: page.snd_drop.play()

    prompt = TextField(label="Animation Prompt Text", value=infinite_zoom_prefs['prompt'], filled=True, expand=True, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=infinite_zoom_prefs['negative_prompt'], col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    seed = TextField(label="Seed", width=76, value=str(infinite_zoom_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'), col={'md':1})
    frame = TextField(label="Frame", width=76, value="0", filled=True, keyboard_type=KeyboardType.NUMBER, tooltip="")
    add_prompt_keyframe = ft.FilledButton("➕  Add Keyframe", on_click=add_prompt)
    num_interpol_frames = SliderRow(label="Interpolation Frames", min=1, max=150, divisions=149, pref=infinite_zoom_prefs, key='num_interpol_frames', tooltip="Number of images to be interpolated between each outpainting step.")
    num_inference_row = SliderRow(label="Inference Steps", min=1, max=150, divisions=149, pref=infinite_zoom_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=infinite_zoom_prefs, key='guidance_scale')
    mask_width = SliderRow(label="Mask Width", min=1, max=256, divisions=255, suffix="px", pref=infinite_zoom_prefs, key='mask_width', col={'md': 6}, tooltip="Width of the border in pixels to be outpainted during each step. Make sure: mask_width < image resolution / 2")
    num_outpainting_steps = SliderRow(label="Outpainting Steps", min=1, max=100, divisions=99, pref=infinite_zoom_prefs, key='num_outpainting_steps', col={'md': 6}, tooltip="Number of outpainting images between each interpolation.")
    frame_dupe_amount = SliderRow(label="Frame Dupe Amount", min=1, max=90, divisions=89, pref=infinite_zoom_prefs, key='frame_dupe_amount', tooltip="Duplicates the first and last frames, use to add a delay before animation based on playback fps (15 = 0.5 seconds @ 30fps)")
    fps = SliderRow(label="Frames per Second", min=1, max=60, divisions=59, suffix='fps', pref=infinite_zoom_prefs, key='fps', tooltip="The FPS to save video clip.")
    #max_size = SliderRow(label="Max Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=infinite_zoom_prefs, key='max_size')
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=infinite_zoom_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=infinite_zoom_prefs, key='height')
    use_SDXL = Switcher(label="Use Stable Diffusion XL", value=infinite_zoom_prefs['use_SDXL'], on_change=lambda e:changed(e,'use_SDXL'), tooltip="SDXL uses Model Checkpoint set in Installation. Otherwise use selected 1.5 or 2.1 Inpainting Model.")
    inpainting_model = Dropdown(label="Inpainting Model", width=386, options=[dropdown.Option(m) for m in ["stabilityai/stable-diffusion-2-inpainting", "runwayml/stable-diffusion-inpainting", "ImNoOne/f222-inpainting-diffusers", "Lykon/dreamshaper-8-inpainting", "parlance/dreamlike-diffusion-1.0-inpainting", "ghunkins/stable-diffusion-liberty-inpainting", "piyushaaryan011/realistic-vision-inpainting"]], value=infinite_zoom_prefs['inpainting_model'], on_change=lambda e: changed(e, 'inpainting_model'))
    save_frames = Switcher(label="Save Frames", value=infinite_zoom_prefs['save_frames'], on_change=lambda e:changed(e,'save_frames'))
    save_gif = Switcher(label="Save Animated GIF", value=infinite_zoom_prefs['save_gif'], on_change=lambda e:changed(e,'save_gif'))
    save_video = Switcher(label="Save Video", value=infinite_zoom_prefs['save_video'], on_change=lambda e:changed(e,'save_video'))
    init_image = FileInput(label="Initial Image (optional)", pref=infinite_zoom_prefs, key='init_image', page=page)
    batch_folder_name = TextField(label="Batch Folder Name (required)", value=infinite_zoom_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(infinite_zoom_prefs)
    page.upscalers.append(upscaler)
    page.infinite_zoom_prompts = Column([], spacing=0)
    page.infinite_zoom_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.infinite_zoom_output.controls) > 0
    animation_prompts = Column([], spacing=0)
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🔍  Infinite Zoom Text-to-Video", "Animate your Keyframe Prompts with an Endless Zooming Effect...", actions=[save_default(infinite_zoom_prefs, ['init_image', 'editing_prompts', 'animation_prompts']), IconButton(icon=icons.HELP, tooltip="Help with InfiniteZoom Settings", on_click=infinite_zoom_help)]),
        Row([frame, prompt, add_prompt_keyframe]),
        animation_prompts,
        Divider(thickness=2, height=4),
        negative_prompt,
        init_image,
        num_inference_row,
        guidance,
        #max_size,
        width_slider, height_slider,
        mask_width,
        num_outpainting_steps,
        num_interpol_frames,
        fps,
        frame_dupe_amount,
        Divider(thickness=4, height=4),
        Row([inpainting_model, use_SDXL]),
        Row([seed, batch_folder_name]),
        Row([save_frames, save_gif, save_video]),
        upscaler,
        Row([ElevatedButton(content=Text("💫  Run Infinite Zoom", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_infinite_zoom(page)),]),
        page.infinite_zoom_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

potat1_prefs = {
    'prompt': '',
    'negative_prompt': 'text, watermark, copyright, blurry, low resolution, blur, low quality',
    'num_inference_steps': 25,
    'guidance_scale': 23.0,
    'fps': 24,
    'num_frames': 16,
    'export_to_video': False,
    'remove_watermark': True,
    'eta': 0.0,
    'seed': 0,
    'width': 1024,
    'height': 576,
    'init_video': '',
    'init_weight': 0.5,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildPotat1(page):
    global potat1_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          potat1_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.potat1_output.controls = []
      page.potat1_output.update()
      clear_button.visible = False
      clear_button.update()
    def potat1_help(e):
      def close_potat1_dlg(e):
        nonlocal potat1_help_dlg
        potat1_help_dlg.open = False
        page.update()
      potat1_help_dlg = AlertDialog(title=Text("💁   Help with Potat1 Text-To-Video"), content=Column([
          Text("This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The text-to-video generation diffusion model consists of three sub-networks: text feature extraction, text feature-to-video latent space diffusion model, and video latent space to video visual space. The overall model parameters are about 1.7 billion. Support English input. The diffusion model adopts the Unet3D structure, and realizes the function of video generation through the iterative denoising process from the pure Gaussian noise video. Trained with https://lambdalabs.com ❤ 1xA100 (40GB) 2197 clips, 68388 tagged frames ( salesforce/blip2-opt-6.7b-coco ) train_steps: 10000"),
          Markdown("[Huggingface Website](https://huggingface.co/camenduru/potat1) | [GitHub repository](https://github.com/camenduru/text-to-video-synthesis-colab)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🍠  Spuds up... ", on_click=close_potat1_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(potat1_help_dlg)
      potat1_help_dlg.open = True
      page.update()
    prompt = TextField(label="Animation Prompt Text", value=potat1_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=potat1_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    num_frames = SliderRow(label="Number of Frames", min=1, max=300, divisions=299, pref=potat1_prefs, key='num_frames', tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=potat1_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=potat1_prefs, key='guidance_scale')
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=potat1_prefs, key='fps')
    #eta_slider = SliderRow(label="ETA", min=0, max=1.0, divisions=20, round=1, pref=potat1_prefs, key='eta', tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.")
    #width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=potat1_prefs, key='width')
    #height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=potat1_prefs, key='height')
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=potat1_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    width_slider = SliderRow(label="Width", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=potat1_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=potat1_prefs, key='height')
    batch_folder_name = TextField(label="Video Folder Name", value=potat1_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    seed = TextField(label="Seed", width=90, value=str(potat1_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    #upscaler = UpscaleBlock(potat1_prefs)
    #page.upscalers.append(upscaler)
    page.potat1_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.potat1_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🥔  Potat1️⃣ Text-To-Video Synthesis", "CamenDuru's Open-Source 1024x576 Text-To-Video Model ", actions=[save_default(potat1_prefs), IconButton(icon=icons.HELP, tooltip="Help with Potat1 Settings", on_click=potat1_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        ResponsiveRow([prompt, negative_prompt]),
        #Row([export_to_video, lower_memory]),
        num_frames,
        fps,
        num_inference_row,
        guidance,
        #eta_slider,
        width_slider, height_slider,
        #upscaler,
        Row([seed, batch_folder_name]),
        Row([
            ElevatedButton(content=Text("🍟  Run Potat1", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_potat1(page)),
        ]),
        page.potat1_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

stable_animation_prefs = {
    #'animation_prompt': '{\n0:"",\n}',
    'animation_prompts': {},
    'negative_prompt': 'text, words, watermark, shutterstock',
    'negative_prompt_weight': -1.0,
    #'prompt': '',
    #'frame': 0,
    'num_inference_steps': 50,
    'guidance_scale': 9.0,
    'export_to_video': True,
    'seed': 0,
    'width': 512,
    'height': 512,
    'max_frames': 300,
    'steps_curve': "0:(30)",
    'model': 'stable-diffusion-v1-5',
    'style_preset': "None",
    'sampler': 'K_dpmpp_2m',
    'clip_guidance': "None",
    'steps_strength_adj': True,
    'interpolate_prompts': False,
    'locked_seed': False,
    'noise_add_curve': "0:(0.02)",
    'noise_scale_curve': "0:(0.99)",
    'strength_curve': "0:(0.65)",
    'diffusion_cadence_curve': "0:(1.0)",
    'cadence_interp': 'Mix',
    'cadence_spans': False,
    'inpaint_border': False,
    'border': "replicate",
    'use_inpainting_model': False,
    'fps': 12,
    'mask_min_value': "0:(0.25)",
    'mask_binarization_thr': 0.5,
    #Colour & Depth Parameters
    'color_coherence': 'LAB',
    'brightness_curve': "0:(1.0)",
    'contrast_curve': "0:(1.0)",
    'hue_curve': "0:(0.0)",
    'saturation_curve': "0:(1.0)",
    'lightness_curve': "0:(0.0)",
    'color_match_animate': True,
    'depth_model_weight': 0.3,
    'near_plane': 200,
    'far_plane': 10000,
    'fov_curve': "0:(25)", #-180
    'depth_blur_curve': "0:(0.0)", #-7
    'depth_warp_curve': "0:(1.0)", #-1
    #2D & 3D Parameters
    'translation_x': "0:(0)",
    'translation_y': "0:(0)",
    'translation_z': "0:(0)",
    'angle': "0:(0)",
    'zoom': "0:(1)",
    'rotation_x': "0:(0)",
    'rotation_y': "0:(0)",
    'rotation_z': "0:(0)",
    'camera_type': "Perspective",#Orthographic
    'render_mode': "Mesh",#Pointcloud
    'animation_mode': "3D warp",
    'mask_power': 0.3, #-4
    #Input Parameters
    'init_image': "",
    'init_sizing': "Stretch",
    'mask_image': "",
    'mask_invert': False,
    'video_init_path': "",
    'video_flow_warp': True,
    'video_mix_in_curve': "0:(0.02)",
    'extract_nth_frame': 1,
    'video_init_fps': 12,
    'resume': False,
    'resume_from': '-1',
    #Post-Processor Parameters
    'output_fps': 24,
    'frame_interpolation_mode': "Rife", #film, none
    'frame_interpolation_factor': "2", #4, 8
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
    "lower_memory": True,
}
stable_animation_prefs_default = stable_animation_prefs.copy()

def buildStableAnimation(page):
    global stable_animation_prefs, prefs, pipe_stable_animation
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          stable_animation_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.stable_animation_output.controls = []
      page.stable_animation_output.update()
      clear_button.visible = False
      clear_button.update()
    def stable_animation_help(e):
      def close_stable_animation_dlg(e):
        nonlocal stable_animation_help_dlg
        stable_animation_help_dlg.open = False
        page.update()
      stable_animation_help_dlg = AlertDialog(title=Text("💁   Help with Stable Animation"), content=Column([
          Text("With our Animation SDK, artists and developers have the ability to use the Stable Diffusion family of models to generate stunning animations. Create animations purely from prompts, start with an initial image, or drive an animation from a source video. "),
          Text("Artists have the ability to use all of our available inference models to generate animations. We currently support text-to-animation, image-to-animation, and video-to-animation. To see animated previews of how these parameters affect the resulting animation, please check out our Animation Handbook."),
          Markdown(" [Project Page](https://platform.stability.ai/docs/features/animation) | [Colab Gradio](https://colab.research.google.com/github/Stability-AI/stability-sdk/blob/animation/nbs/animation_gradio.ipynb) | [Animation Handbook](https://docs.google.com/document/d/1iHcAu_5rG11guGFie8sXBXPGuM4yKzqdd13MJ_1LU8U/edit?usp=sharing)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("💸  Worth it... ", on_click=close_stable_animation_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(stable_animation_help_dlg)
      stable_animation_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      nonlocal pick_type
      if e.progress == 1:
        save_file(e.file_name)
    def save_file(file_name):
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
          stable_animation_prefs['file_name'] = file_name.rpartition('.')[0]
        else:
          fname = file_name
          stable_animation_prefs['file_name'] = file_name.rpartition(slash)[2].rpartition('.')[0]
        if pick_type == "image":
          init_image.value = fname
          init_image.update()
          stable_animation_prefs['init_image'] = fname
        if pick_type == "mask":
          mask_image.value = fname
          mask_image.update()
          stable_animation_prefs['mask_image'] = fname
        elif pick_type == "video":
          init_video.value = fname
          init_video.update()
          stable_animation_prefs['init_video'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    def pick_init(e):
        nonlocal pick_type
        pick_type = "image"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Init Image File")
    def pick_mask(e):
        nonlocal pick_type
        pick_type = "mask"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Mask Image File")
    def pick_video(e):
        nonlocal pick_type
        pick_type = "video"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["mp4", "avi"], dialog_title="Pick Initial Video File")
    def save_preset(e):
      def copy_preset(pl):
        nonlocal text_list, enter_text
        page.set_clipboard(enter_text.value)
        toast_msg(page, f"📋   Animation Preset copied to clipboard...")
        close_dlg(e)
      def close_dlg(e):
          dlg_copy.open = False
          page.update()
      stable_animation_prefs['animation_prompts'] = {str(k):v for k,v in stable_animation_prefs['animation_prompts'].items()}
      text_list = json.dumps(stable_animation_prefs, indent = 4)
      enter_text = TextField(label="Stable Animation Preset JSON", value=text_list.strip(), expand=True, filled=True, multiline=True, autofocus=True)
      dlg_copy = AlertDialog(modal=False, title=Text("📝   Stable Animation as JSON"), content=Container(Column([enter_text], alignment=MainAxisAlignment.START, tight=True, width=(page.width if page.web else page.window.width) - 180, height=(page.height if page.web else page.window.height) - 100, scroll="none"), width=(page.width if page.web else page.window.width) - 180, height=(page.height if page.web else page.window.height) - 100), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Copy Preset JSON to Clipboard", size=19, weight=FontWeight.BOLD), data=text_list, on_click=lambda ev: copy_preset(text_list))], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(dlg_copy)
      dlg_copy.open = True
      page.update()
    def toggle_resume(e):
      stable_animation_prefs['resume'] = e.control.value
      resume_from.visible = stable_animation_prefs['resume']
      resume_from.update()
    def paste_preset(e):
      def save_preset(e):
        try:
          preset_json = json.loads(enter_text.value.strip())
        except UnicodeDecodeError:
          close_dlg(e)
          alert_msg("Error Parsing JSON Data...")
          return
        load_preset(preset_json)
        close_dlg(e)
      def paste_clipboard(e):
          enter_text.value = page.get_clipboard()
          enter_text.update()
      def close_dlg(e):
          dlg_paste.open = False
          page.update()
      enter_text = TextField(label="Enter Stable Animation Preset JSON", expand=True, filled=True, min_lines=30, multiline=True, autofocus=True)
      dlg_paste = AlertDialog(modal=False, title=Text("📝  Paste Saved Preset JSON"), content=Container(Column([enter_text], alignment=MainAxisAlignment.START, tight=True, width=(page.width if page.web else page.window.width) - 180, height=(page.height if page.web else page.window.height) - 100, scroll="none"), width=(page.width if page.web else page.window.width) - 180, height=(page.height if page.web else page.window.height) - 100), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), TextButton(content=Text("📋  Paste from Clipboard", size=18), on_click=paste_clipboard), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Load Preset JSON Values ", size=19, weight=FontWeight.BOLD), on_click=save_preset)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(dlg_paste)
      dlg_paste.open = True
      page.update()
    def load_preset(d):
      global stable_animation_prefs
      stable_animation_prefs = d.copy()
      stable_animation_prefs['animation_prompts'] = {}
      #animation_prompt.value = d['animation_prompt']
      animation_prompts.controls.clear()
      prompts = d['animation_prompts'].copy()
      for k, v in prompts.items():
        add_prompt(d, int(k), v, False)
      negative_prompt.value = d['negative_prompt']
      negative_prompt_weight.value = d['negative_prompt_weight']
      max_frames.value = d['max_frames']
      fps.set_value(d['fps'])
      num_inference_row.set_value(d['num_inference_steps'])
      guidance.set_value(d['guidance_scale'])
      model_checkpoint.value = d['model']
      generation_sampler.value = d['sampler']
      style_preset.value = d['style_preset']
      clip_guidance.value = d['clip_guidance']
      steps_strength_adj.value = d['steps_strength_adj']
      interpolate_prompts.value = d['interpolate_prompts']
      locked_seed.value = d['locked_seed']
      noise_add_curve.value = d['noise_add_curve']
      noise_scale_curve.value = d['noise_scale_curve']
      strength_curve.value = d['strength_curve']
      diffusion_cadence_curve.value = d['diffusion_cadence_curve']
      cadence_interp.value = d['cadence_interp']
      cadence_spans.value = d['cadence_spans']
      inpaint_border.value = d['inpaint_border']
      border.value = d['border']
      use_inpainting_model.value = d['use_inpainting_model']
      mask_min_value.value = d['mask_min_value']
      mask_binarization_thr.set_value(d['mask_binarization_thr'])
      color_coherence.value = d['color_coherence']
      brightness_curve.value = d['brightness_curve']
      contrast_curve.value = d['contrast_curve']
      hue_curve.value = d['hue_curve']
      saturation_curve.value = d['saturation_curve']
      lightness_curve.value = d['lightness_curve']
      color_match_animate.value = d['color_match_animate']
      depth_model_weight.set_value(d['depth_model_weight'])
      fov_curve.value = d['fov_curve']
      depth_blur_curve.value = d['depth_blur_curve']
      depth_warp_curve.value = d['depth_warp_curve']
      translation_x.value = d['translation_x']
      translation_y.value = d['translation_y']
      translation_z.value = d['translation_z']
      angle.value = d['angle']
      zoom.value = d['zoom']
      rotation_x.value = d['rotation_x']
      rotation_y.value = d['rotation_y']
      rotation_z.value = d['rotation_z']
      camera_type.value = d['camera_type']
      render_mode.value = d['render_mode']
      mask_power.set_value(d['mask_power'])
      init_image.value = d['init_image']
      init_sizing.value = d['init_sizing']
      mask_image.value = d['mask_image']
      mask_invert.value = d['mask_invert']
      video_mix_in_curve.value = d['video_mix_in_curve']
      animation_mode.value = d['animation_mode']
      init_video.value = d['video_init_path']
      video_mix_in_curve.value = d['video_mix_in_curve']
      video_flow_warp.value = d['video_flow_warp']
      extract_nth_frame.value = d['extract_nth_frame']
      video_init_fps.value = d['video_init_fps']
      output_fps.value = d['output_fps']
      resume.value = d['resume']
      resume_from.value = d['resume_from']
      frame_interpolation_mode.value = d['frame_interpolation_mode']
      frame_interpolation_factor.value = d['frame_interpolation_factor']
      width_slider.set_value(d['width'])
      height_slider.set_value(d['height'])
      seed.value = d['seed']
      batch_folder_name.value = d['batch_folder_name']
      #.set_value(d[''])
      page.update()
    def load_default(e):
      global stable_animation_prefs_default
      load_preset(stable_animation_prefs_default)
    def edit_prompt(e):
      nonlocal animation_prompts
      f = int(e.control.data)
      edit_prompt = stable_animation_prefs['animation_prompts'][str(f)]
      def close_dlg(e):
        dlg_edit.open = False
        page.update()
      def save_prompt(e):
        fr = int(editing_frame.value)
        pline = f'{fr}: "{editing_prompt.value}"'
        for p in animation_prompts.controls:
          if p.data == f:
            p.title.value = pline
            p.data = fr
            for button in p.trailing.items:
              button.data = fr
            break
        stable_animation_prefs['animation_prompts'][str(editing_frame.value)] = editing_prompt.value.strip()
        if f != int(editing_frame.value):
          del stable_animation_prefs['animation_prompts'][str(f)]
          sorted_dict = {}
          for key in sorted(stable_animation_prefs['animation_prompts'].keys()):
              sorted_dict[key] = stable_animation_prefs['animation_prompts'][key]
          stable_animation_prefs['animation_prompts'] = sorted_dict
          animation_prompts.controls = sorted(animation_prompts.controls, key=lambda tile: tile.data)
        animation_prompts.update()
        close_dlg(e)
      editing_frame = TextField(label="Frame", width=90, value=str(f), keyboard_type=KeyboardType.NUMBER, tooltip="")
      editing_prompt = TextField(label="Keyframe Prompt Animation", expand=True, multiline=True, value=edit_prompt, autofocus=True)
      dlg_edit = AlertDialog(modal=False, title=Text(f"♟️ Edit Prompt Keyframe"), content=Container(Column([
          Row([editing_frame, editing_prompt])
      ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO), width=(page.width if page.web else page.window.width) - 180), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Prompt ", size=19, weight=FontWeight.BOLD), on_click=save_prompt)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(dlg_edit)
      dlg_edit.open = True
      page.update()
    def del_prompt(e):
      f = e.control.data
      for i, p in enumerate(animation_prompts.controls):
        if p.data == f:
          del animation_prompts.controls[i]
          break
      animation_prompts.update()
      del stable_animation_prefs['animation_prompts'][str(f)]
      play_snd(Snd.DELETE, page)
    def clear_prompts(e):
      animation_prompts.controls.clear()
      animation_prompts.update()
      stable_animation_prefs['animation_prompts'] = {}
      clear_prompt(e)
      play_snd(Snd.DELETE, page)
    def clear_prompt(e):
      prompt.value = ""
      prompt.update()
    def copy_prompt(e):
      p = stable_animation_prefs['animation_prompts'][str(e.control.data)]
      page.set_clipboard(p)
      toast_msg(page, f"📋  Prompt Text copied to clipboard...")
    def add_prompt(e, f=None, p=None, sound=True):
      if (not bool(prompt.value) or not bool(frame.value)) and f == None: return
      if f == None: f = int(frame.value)
      if p == None: p = prompt.value.strip()
      pline = f'{f}: "{p}"'
      if f in stable_animation_prefs['animation_prompts']:
        for i, p in enumerate(animation_prompts.controls):
          if p.data == f:
            p.title.value = pline
      else:
        animation_prompts.controls.append(ListTile(title=Text(pline, size=14), data=f, trailing=PopupMenuButton(icon=icons.MORE_VERT,
            items=[PopupMenuItem(icon=icons.EDIT, text="Edit Animation Prompt", on_click=edit_prompt, data=f),
                  PopupMenuItem(icon=icons.COPY, text="Copy Prompt Text", on_click=copy_prompt, data=f),
                  PopupMenuItem(icon=icons.DELETE, text="Delete Animation Prompt", on_click=del_prompt, data=f), PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Prompts", on_click=clear_prompts)]), dense=True, on_click=edit_prompt))
      stable_animation_prefs['animation_prompts'][str(f)] = p
      #stable_animation_prefs['animation_prompts'] = {int(k):v for k,v in stable_animation_prefs['animation_prompts'].items()}
      #stable_animation_prefs['animation_prompts'] = sorted(stable_animation_prefs['animation_prompts'].keys())
      #stable_animation_prefs['animation_prompts'] = {i: stable_animation_prefs['animation_prompts'][i] for i in list(stable_animation_prefs['animation_prompts'].keys()).sort()}
      sorted_dict = {}
      for key in sorted(stable_animation_prefs['animation_prompts'].keys()):
          sorted_dict[key] = stable_animation_prefs['animation_prompts'][key]
      stable_animation_prefs['animation_prompts'] = sorted_dict
      animation_prompts.controls = sorted(animation_prompts.controls, key=lambda tile: tile.data)
      animation_prompts.update()
      if prefs['enable_sounds'] and sound: page.snd_drop.play()
    copy_preset_button = IconButton(icons.COPY_ALL, tooltip="Save Animation Presets as JSON", on_click=save_preset)
    paste_preset_button = IconButton(icons.CONTENT_PASTE, tooltip="Load Animation Presets as JSON", on_click=paste_preset)
    default_preset_button = IconButton(icons.REFRESH, tooltip="Reset Animation Presets to Default", on_click=load_default) #, suffix=IconButton(icons.CLEAR, on_click=clear_prompt), width=50, height=50
    prompt = TextField(label="Keyframe Prompt Text", value="", filled=True, expand=True, multiline=True)
    frame = TextField(label="Frame", width=76, value="0", filled=True, keyboard_type=KeyboardType.NUMBER, tooltip="")
    negative_prompt_weight = TextField(label="Negative Weight", width=120, value=stable_animation_prefs['negative_prompt_weight'], keyboard_type=KeyboardType.NUMBER, tooltip="", on_change=lambda e:changed(e,'negative_prompt_weight'))
    add_prompt_keyframe = ft.FilledButton("➕  Add Keyframe", on_click=add_prompt)
    #animation_prompt = TextField(label="Animation Prompt Text", value=stable_animation_prefs['animation_prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'animation_prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=stable_animation_prefs['negative_prompt'], col={'md':3}, expand=True, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    max_frames  = TextField(label="Max Number of Frames", width=200, hint_text="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.", value=stable_animation_prefs['max_frames'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'max_frames', ptype='int'))
    #max_frames = SliderRow(label="Max Number of Frames", min=1, max=300, divisions=299, pref=stable_animation_prefs, key='max_frames', tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.")
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', expand=True, pref=stable_animation_prefs, key='fps', tooltip="The FPS to extract from the init video clip.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=stable_animation_prefs, key='num_inference_steps', on_change=lambda e:changed(e,'num_inference_steps'), tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=stable_animation_prefs, key='guidance_scale')
    #eta_slider = SliderRow(label="ETA", min=0, max=1.0, divisions=20, round=1, pref=stable_animation_prefs, key='eta', tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.")
    model_checkpoint = Dropdown(label="Model Checkpoint", hint_text="", width=270, options=[dropdown.Option("stable-diffusion-xl-beta-v2-2-2"), dropdown.Option("stable-diffusion-768-v2-1"), dropdown.Option("stable-diffusion-512-v2-1"), dropdown.Option("stable-diffusion-768-v2-0"), dropdown.Option("stable-diffusion-512-v2-0"), dropdown.Option("stable-diffusion-v1-5"), dropdown.Option("stable-diffusion-v1"), dropdown.Option("stable-inpainting-512-v2-0"), dropdown.Option("stable-inpainting-v1-0")], value=stable_animation_prefs['model'], autofocus=False, on_change=lambda e:changed(e, 'model'))
    generation_sampler = Dropdown(label="Generation Sampler", hint_text="", width=230, options=[dropdown.Option("DDIM"), dropdown.Option("PLMS"), dropdown.Option("K_euler"), dropdown.Option("K_euler_ancestral"), dropdown.Option("K_heun"), dropdown.Option("K_dpmpp_2m"), dropdown.Option("K_dpm_2_ancestral"), dropdown.Option("K_lms"), dropdown.Option("K_dpmpp_2s_ancestral"), dropdown.Option("K_dpm_2")], value=stable_animation_prefs['sampler'], autofocus=False, on_change=lambda e:changed(e, 'sampler'))
    #DDIM, PLMS, K_euler, K_euler_ancestral, K_heun, K_dpm_2, K_dpm_2_ancestral, K_lms, K_dpmpp_2m, K_dpmpp_2s_ancestral
    style_preset = Dropdown(label="Style Preset", hint_text="", width=240, options=[dropdown.Option("None"), dropdown.Option("3d-model"), dropdown.Option("analog-film"), dropdown.Option("anime"), dropdown.Option("cinematic"), dropdown.Option("comic-book"), dropdown.Option("digital-art"), dropdown.Option("enhance fantasy-art"), dropdown.Option("isometric"), dropdown.Option("line-art"), dropdown.Option("low-poly"), dropdown.Option("modeling-compound"), dropdown.Option("neon-punk"), dropdown.Option("origami"), dropdown.Option("photographic"), dropdown.Option("pixel-art")], value=stable_animation_prefs['style_preset'], autofocus=False, on_change=lambda e:changed(e, 'style_preset'))
    clip_guidance = Dropdown(label="Clip Guidance", hint_text="", width=240, options=[dropdown.Option("None"), dropdown.Option("Simple"), dropdown.Option("FastBlue"), dropdown.Option("FastGreen")], value=stable_animation_prefs['clip_guidance'], autofocus=False, on_change=lambda e:changed(e, 'clip_guidance'))
    steps_strength_adj = Checkbox(label="Steps Strength Adjustment", tooltip="Adjusts number of diffusion steps based on current previous frame strength value.", value=stable_animation_prefs['steps_strength_adj'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'steps_strength_adj'), col={'xs':12, 'md':6, 'lg':4, 'xl':3})
    interpolate_prompts = Checkbox(label="Interpolate Prompts", tooltip="Smoothly interpolate prompts between keyframes.", value=stable_animation_prefs['interpolate_prompts'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'interpolate_prompts'), col={'xs':12, 'md':6, 'lg':4, 'xl':3})
    locked_seed = Checkbox(label="Locked Seed", tooltip="Keep the same seed for all frames.", value=stable_animation_prefs['locked_seed'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'locked_seed'), col={'xs':12, 'md':6, 'lg':4, 'xl':3})
    noise_add_curve  = TextField(label="Noise Add Curve", value=stable_animation_prefs['noise_add_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'noise_add_curve'))
    noise_scale_curve  = TextField(label="Noise Scale Curve", value=stable_animation_prefs['noise_scale_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'noise_scale_curve'))
    strength_curve  = TextField(label="Strength Curve", value=stable_animation_prefs['strength_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'strength_curve'))
    diffusion_cadence_curve  = TextField(label="Diffusion Cadence Curve", hint_text="One greater than the number of frames between diffusion operations. A cadence of 1 performs diffusion on each frame. Values greater than one will generate frames using interpolation methods.", value=stable_animation_prefs['diffusion_cadence_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'diffusion_cadence_curve'))
    cadence_interp = Dropdown(label="Cadence Interpolation", hint_text="", width=200, options=[dropdown.Option("Mix"), dropdown.Option("RIFE"), dropdown.Option("VAE-LERP"), dropdown.Option("VAE-SLERP")], value=stable_animation_prefs['cadence_interp'], autofocus=False, on_change=lambda e:changed(e, 'cadence_interp'))
    cadence_spans = Checkbox(label="Cadence Spans", tooltip="Experimental diffusion cadence mode for better outpainting", value=stable_animation_prefs['cadence_spans'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'cadence_spans'))
    inpaint_border = Checkbox(label="Inpaint Border", tooltip="Use inpainting on top of border regions for 2D and 3D warp modes.", value=stable_animation_prefs['inpaint_border'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'inpaint_border'))
    border = Dropdown(label="Border", hint_text="Method that will be used to fill empty regions, e.g. after a rotation transform.", width=200, options=[dropdown.Option("reflect"), dropdown.Option("replicate"), dropdown.Option("wrap"), dropdown.Option("zero"), dropdown.Option("prefill")], value=stable_animation_prefs['border'], autofocus=False, on_change=lambda e:changed(e, 'border'))
    use_inpainting_model = Checkbox(label="Use Inpainting Model", tooltip="", value=stable_animation_prefs['use_inpainting_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'use_inpainting_model'))
    mask_min_value  = TextField(label="Mask Minimum", value=stable_animation_prefs['mask_min_value'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'mask_min_value'))
    mask_binarization_thr = SliderRow(label="Mask Binarization Threshold", min=0, max=1.0, divisions=20, round=1, pref=stable_animation_prefs, key='mask_binarization_thr', expand=True, tooltip="Grayscale mask values lower than this value will be set to 0, values that are higher — to 1.")
    color_coherence = Dropdown(label="Color Coherance", hint_text="Color space that will be used for inter-frame color adjustments.", width=350, col={'xs':12, 'md':6, 'lg':4, 'xl':3}, options=[dropdown.Option("None"), dropdown.Option("HSV"), dropdown.Option("LAB"), dropdown.Option("RGB")], value=stable_animation_prefs['color_coherence'], autofocus=False, on_change=lambda e:changed(e, 'color_coherence'))
    brightness_curve  = TextField(label="Brightness Curve", value=stable_animation_prefs['brightness_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'brightness_curve'))
    contrast_curve  = TextField(label="Contrast Curve", value=stable_animation_prefs['contrast_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'contrast_curve'))
    hue_curve  = TextField(label="Hue Curve", value=stable_animation_prefs['hue_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'hue_curve'))
    saturation_curve  = TextField(label="Saturation Curve", value=stable_animation_prefs['saturation_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'saturation_curve'))
    lightness_curve  = TextField(label="Lightness Curve", value=stable_animation_prefs['lightness_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'lightness_curve'))
    color_match_animate = Checkbox(label="Color Match Animate", tooltip="Animate color match between key frames.", value=stable_animation_prefs['color_match_animate'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'color_match_animate'))
    depth_model_weight = SliderRow(label="Depth Model Weight", min=0, max=1.0, divisions=20, round=1, pref=stable_animation_prefs, key='depth_model_weight', expand=True, tooltip="Blend factor between AdaBins and MiDaS depth models.")
    fov_curve  = TextField(label="FOV Curve", hint_text="FOV angle of camera volume in degrees. Max 180.", value=stable_animation_prefs['fov_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'fov_curve'))
    depth_blur_curve  = TextField(label="Depth Blur Curve", hint_text="", value=stable_animation_prefs['depth_blur_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'depth_blur_curve'))
    depth_warp_curve  = TextField(label="Depth Warp Curve", hint_text="", value=stable_animation_prefs['depth_warp_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'depth_warp_curve'))
    translation_x  = TextField(label="Translation X", hint_text="", value=stable_animation_prefs['translation_x'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'translation_x'))
    translation_y  = TextField(label="Translation Y", hint_text="", value=stable_animation_prefs['translation_y'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'translation_y'))
    translation_z  = TextField(label="Translation Z", hint_text="", value=stable_animation_prefs['translation_z'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'translation_z'))
    angle  = TextField(label="Angle", hint_text="", value=stable_animation_prefs['angle'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'angle'))
    zoom  = TextField(label="Zoom", hint_text="", value=stable_animation_prefs['zoom'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'zoom'))
    rotation_x  = TextField(label="Rotation X", hint_text="", value=stable_animation_prefs['rotation_x'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'rotation_x'))
    rotation_y  = TextField(label="Rotation Y", hint_text="", value=stable_animation_prefs['rotation_y'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'rotation_y'))
    rotation_z  = TextField(label="Rotation Z", hint_text="", value=stable_animation_prefs['rotation_z'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'rotation_z'))
    camera_type = Dropdown(label="Camera Type", hint_text="", width=200, options=[dropdown.Option("Perspective"), dropdown.Option("Orthographic")], value=stable_animation_prefs['camera_type'], autofocus=False, on_change=lambda e:changed(e, 'camera_type'))
    render_mode = Dropdown(label="Render Mode", hint_text="", width=200, options=[dropdown.Option("Mesh"), dropdown.Option("Pointcloud")], value=stable_animation_prefs['render_mode'], autofocus=False, col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e, 'render_mode'))
    mask_power = SliderRow(label="Mesh Power", min=0, max=4.0, divisions=80, round=1, expand=True, pref=stable_animation_prefs, key='mask_power', tooltip="")
    init_image = TextField(label="Initial Image", value=stable_animation_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    init_sizing = Dropdown(label="Init Sizing", hint_text="", width=200, options=[dropdown.Option("Cover"), dropdown.Option("Stretch"), dropdown.Option("Resize-Canvas")], value=stable_animation_prefs['init_sizing'], autofocus=False, on_change=lambda e:changed(e, 'init_sizing'))
    mask_image = TextField(label="Mask Image", value=stable_animation_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_mask))
    mask_invert = Checkbox(label="Invert Mask", tooltip="White in mask marks areas to change by default.", value=stable_animation_prefs['mask_invert'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'mask_invert'))
    video_mix_in_curve  = TextField(label="Video Mixin Curve", hint_text="", value=stable_animation_prefs['video_mix_in_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'video_mix_in_curve'))
    animation_mode = Dropdown(label="Animation Mode", hint_text="", width=200, options=[dropdown.Option("2D"), dropdown.Option("3D warp"), dropdown.Option("3D render"), dropdown.Option("Video Input")], value=stable_animation_prefs['animation_mode'], autofocus=False, on_change=lambda e:changed(e, 'animation_mode'))
    init_video = TextField(label="Init Video File", value=stable_animation_prefs['video_init_path'], on_change=lambda e:changed(e,'video_init_path'), expand=True, height=60, suffix=IconButton(icon=icons.VIDEO_CALL, on_click=pick_video))
    video_mix_in_curve  = TextField(label="Video Mixin Curve", hint_text="", value=stable_animation_prefs['video_mix_in_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'video_mix_in_curve'))
    video_flow_warp = Checkbox(label="Video Flow Warp", tooltip="Whether or not to transfer the optical flow from the video to the generated animation as a warp effect.", value=stable_animation_prefs['video_flow_warp'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'video_flow_warp'))
    extract_nth_frame  = TextField(label="Extract Nth Frame", width=200, hint_text="Only use every Nth frame of the video", value=stable_animation_prefs['extract_nth_frame'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'extract_nth_frame', ptype='int'))
    video_init_fps  = TextField(label="Init Video FPS", width=200, hint_text="", value=stable_animation_prefs['video_init_fps'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'video_init_fps', ptype='int'))
    output_fps  = TextField(label="Output FPS", width=200, hint_text="Frame rate to use when generating video output.", value=stable_animation_prefs['output_fps'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'output_fps', ptype='int'))
    frame_interpolation_mode = Dropdown(label="Frame Interpolation Mode", hint_text="", width=200, options=[dropdown.Option("Rife"), dropdown.Option("Film"), dropdown.Option("None")], value=stable_animation_prefs['frame_interpolation_mode'], autofocus=False, on_change=lambda e:changed(e, 'frame_interpolation_mode'))
    frame_interpolation_factor = Dropdown(label="Frame Interpolation Factor", hint_text="", width=200, options=[dropdown.Option("2"), dropdown.Option("4"), dropdown.Option("8")], value=stable_animation_prefs['frame_interpolation_factor'], autofocus=False, on_change=lambda e:changed(e, 'frame_interpolation_factor'))
    width_slider = SliderRow(label="Width", min=256, max=1024, divisions=6, multiple=64, suffix="px", pref=stable_animation_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1024, divisions=6, multiple=64, suffix="px", pref=stable_animation_prefs, key='height')
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=stable_animation_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    #lower_memory = Tooltip(message="Enable CPU offloading, VAE Tiling & Stitching", content=Switcher(label="Lower Memory Mode", value=stable_animation_prefs['lower_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'lower_memory')))
    seed = TextField(label="Seed", width=90, value=str(stable_animation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    resume = Switcher(label="Resume Previous Animation", value=stable_animation_prefs['resume'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_resume)
    resume_from = TextField(label="Resume from Frame", width=150, value=str(stable_animation_prefs['resume_from']), visible=stable_animation_prefs['resume'], keyboard_type=KeyboardType.NUMBER, tooltip="-1 resumes from last frame", on_change=lambda e:changed(e,'seed', ptype='int'))
    batch_folder_name = TextField(label="Animation Folder Name", value=stable_animation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(stable_animation_prefs)
    page.upscalers.append(upscaler)
    page.stable_animation_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.stable_animation_output.controls) > 0
    animation_prompts = Column([], spacing=0)
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🥏  Stable Animation SDK", "Use Stability.ai API Credits for Advanced Video Generation, similar to Deforum & Disco Diffusion.", actions=[copy_preset_button, paste_preset_button, default_preset_button, IconButton(icon=icons.HELP, tooltip="Help with Stable Animation Settings", on_click=stable_animation_help)]),
        #ResponsiveRow([animation_prompt, negative_prompt], vertical_alignment=CrossAxisAlignment.START),
        Row([frame, prompt, add_prompt_keyframe]),
        animation_prompts,
        Divider(thickness=2, height=4),
        Row([negative_prompt, negative_prompt_weight]),
        Row([max_frames, fps]),
        num_inference_row,
        guidance,
        Row([model_checkpoint, generation_sampler, style_preset, clip_guidance], wrap=True),
        ResponsiveRow([steps_strength_adj, interpolate_prompts, locked_seed]),
        ResponsiveRow([noise_add_curve, noise_scale_curve, strength_curve, diffusion_cadence_curve]),
        Row([cadence_interp, cadence_spans]),
        Row([border, inpaint_border, use_inpainting_model]),
        Row([mask_min_value, mask_binarization_thr]),
        ResponsiveRow([color_coherence, brightness_curve, contrast_curve, hue_curve, saturation_curve, lightness_curve]),
        Row([color_match_animate, depth_model_weight]),
        ResponsiveRow([fov_curve, depth_blur_curve, depth_warp_curve]),
        ResponsiveRow([translation_x, translation_y, translation_z, zoom]),
        ResponsiveRow([rotation_x, rotation_y, rotation_z, angle]),
        Row([camera_type, render_mode, mask_power]),
        Row([init_image, init_sizing]),
        Row([mask_image, mask_invert]),
        Row([init_video, animation_mode]),
        Row([video_flow_warp, video_mix_in_curve]),
        Row([extract_nth_frame, video_init_fps, output_fps, Row([resume, resume_from])], wrap=True),
        Row([export_to_video, frame_interpolation_mode, frame_interpolation_factor]),
        width_slider, height_slider,
        upscaler,
        Row([seed, batch_folder_name]),
        Row([
            ElevatedButton(content=Text("🧌  Run Stable Animation", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_stable_animation(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_stable_animation(page, from_list=True))
        ]),
        page.stable_animation_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

svd_prefs = {
    'prompt': '',
    'negative_prompt': '',
    'init_image': '',
    'svd_model': 'SVD-img2vid-XT',
    'num_inference_steps': 25,
    'min_guidance_scale': 1.0,
    'max_guidance_scale': 3.0,
    'fps': 8,
    'target_fps': 30,
    'motion_bucket_id': 127,#180
    'num_frames': 25,
    'decode_chunk_size': 12 if prefs['higher_vram_mode'] else 8,
    'noise_aug_strength': 0.02,
    'export_to_gif': True,
    'export_to_video': True,
    "interpolate_video": True,
    'seed': 0,
    'max_size': 768 if prefs['higher_vram_mode'] else 512,
    'width': 1024,
    'height': 576,
    'cpu_offload': not prefs['higher_vram_mode'],
    'num_videos': 1,
    'resume_frame': False,
    'continue_times': 1,
    'file_prefix': 'svd-',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildSVD(page):
    global svd_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          svd_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def svd_help(e):
      def close_svd_dlg(e):
        nonlocal svd_help_dlg
        svd_help_dlg.open = False
        page.update()
      svd_help_dlg = AlertDialog(title=Text("💁   Help with SVD Image-To-Video"), content=Column([
          Text("Stable Video Diffusion (SVD) Image-to-Video is a diffusion model that takes in a still image as a conditioning frame, and generates a video from it. The SVD checkpoint is trained to generate 14 frames and the SVD-XT checkpoint is further finetuned to generate 25 frames. (SVD) Image-to-Video is a latent diffusion model trained to generate short video clips from an image conditioning. This model was trained to generate 14 frames at resolution 576x1024 given a context frame of the same size. We also finetune the widely used f8-decoder for temporal consistency. Developed and funded by Stability AI."),
          Markdown("If you want to use the latest SDV XT 1.1 model, at this time you have to accept the model card, signing up to use it early and accepting the terms at [StabilityAI SVD-XT-1-1](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1).", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("The generated videos are rather short (<= 4sec), and the model does not achieve perfect photorealism. The model may generate videos without motion, or very slow camera pans. The model cannot be controlled through text. The model cannot render legible text. Faces and people in general may not be generated properly. The autoencoding part of the model is lossy."),
          Markdown("[Huggingface Model](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid) | [GitHub repository](https://github.com/Stability-AI/generative-models) | [Paper](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🆒  Extra cool... ", on_click=close_svd_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(svd_help_dlg)
      svd_help_dlg.open = True
      page.update()
    def toggle_resume(e):
        svd_prefs['resume_frame'] = e.control.value
        resume_container.visible = svd_prefs['resume_frame']
        resume_container.update()
    #prompt = TextField(label="Animation Prompt Text", value=svd_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    #negative_prompt  = TextField(label="Negative Prompt Text", value=svd_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = FileInput(label="Init Image", pref=svd_prefs, key='init_image', filled=True, page=page)
    svd_model = Dropdown(label="SVD Model", width=200, options=[dropdown.Option("SVD-img2vid-XT"), dropdown.Option("SVD-img2vid"), dropdown.Option("SVD-img2vid-XT 1.1")], value=svd_prefs['svd_model'], on_change=lambda e: changed(e, 'svd_model'))
    #num_frames = SliderRow(label="Number of Frames", min=1, max=300, divisions=299, pref=svd_prefs, key='num_frames', tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=svd_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    min_guidance = SliderRow(label="Min Guidance Scale", min=0, max=10, divisions=20, round=1, pref=svd_prefs, key='min_guidance_scale', col={'sm':6}, tooltip="Used for the classifier free guidance with first frame.")
    max_guidance = SliderRow(label="Max Guidance Scale", min=0, max=10, divisions=20, round=1, pref=svd_prefs, key='max_guidance_scale', col={'sm':6}, tooltip="Used for the classifier free guidance with last frame.")
    motion_bucket_id = SliderRow(label="Motion Bucket ID", min=1, max=300, divisions=299, pref=svd_prefs, key='motion_bucket_id', tooltip="Increasing the motion bucket id will increase the motion of the generated video.")
    decode_chunk_size = SliderRow(label="Decode Chunk Size", min=1, max=25, divisions=24, pref=svd_prefs, key='decode_chunk_size', tooltip="The number of frames to decode at a time. The higher the chunk size, the higher the temporal consistency between frames, but also the higher the memory consumption. By default, the decoder will decode all frames at once for maximal quality. Reduce `decode_chunk_size` to reduce memory usage.")
    noise_aug_strength = SliderRow(label="Noise Augmented Strength", min=0.0, max=0.1, divisions=20, round=3, pref=svd_prefs, key='noise_aug_strength', tooltip="The amount of noise added to the init image, the higher it is the less the video will look like the init image. Increase it for more motion.")
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=svd_prefs, key='fps', tooltip="The rate at which the generated images shall be exported to a video after generation. Note that Stable Diffusion Video's UNet was micro-conditioned on fps-1 during training.")
    #width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=svd_prefs, key='width')
    #height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=svd_prefs, key='height')
    export_to_gif = Tooltip(message="Save animated gif file along with Image Sequence", content=Switcher(label="Export to GIF", value=svd_prefs['export_to_gif'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_gif')))
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=svd_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    max_size = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=16, multiple=64, suffix="px", pref=svd_prefs, key='max_size')
    width_slider = SliderRow(label="Width", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=svd_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=svd_prefs, key='height')
    interpolate_video = Switcher(label="Interpolate Video", value=svd_prefs['interpolate_video'], tooltip="Use Google FiLM Interpolation to transition between frames.", on_change=lambda e:changed(e,'interpolate_video'))
    cpu_offload = Switcher(label="CPU Offload", value=svd_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    num_videos = NumberPicker(label="Number of Videos: ", min=1, max=8, value=svd_prefs['num_videos'], on_change=lambda e: changed(e, 'num_videos'))
    file_prefix = TextField(label="Filename Prefix", value=svd_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    batch_folder_name = TextField(label="Video Folder Name", value=svd_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    resume_frame = Switcher(label="Resume from Last Frame", value=svd_prefs['resume_frame'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_resume, tooltip="Continues generating frames in chunks for videos longer than 14 or 25 frame limit.")
    resume_container = Container(content=NumberPicker(label="Times to Continue: ", min=1, max=10, value=svd_prefs['continue_times'], on_change=lambda e: changed(e, 'continue_times'), tooltip="Resumes 14 or 25 more frames in iterations. May degrade over time."), visible=svd_prefs['resume_frame'])
    seed = TextField(label="Seed", width=90, value=str(svd_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(svd_prefs)
    page.upscalers.append(upscaler)
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🍃  Stable Video Diffusion Image-To-Video (uses a lot of VRAM)", "Generate high resolution (576x1024) 2-4 second videos conditioned on the input image...", actions=[save_default(svd_prefs, ['init_image']), IconButton(icon=icons.HELP, tooltip="Help with SVD Settings", on_click=svd_help)]),
        #ResponsiveRow([prompt, negative_prompt]),
        init_image,
        decode_chunk_size,
        motion_bucket_id,
        noise_aug_strength,
        #num_frames,
        fps,
        Row([resume_frame, resume_container]),
        num_inference_row,
        ResponsiveRow([min_guidance, max_guidance]),
        max_size,
        #width_slider, height_slider,
        upscaler,
        Row([svd_model, export_to_gif, interpolate_video, cpu_offload]),
        Row([seed, batch_folder_name, file_prefix]), #num_videos, 
        Row([
            ElevatedButton(content=Text("🪭  Run SVD", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_svd(page)),
        ]),
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

roop_prefs = {
    'source_image': '',
    'target_image': '',
    'frame_processor': 'face_swapper',
    'keep_fps': True,
    'keep_audio': True,
    'keep_frames': True,
    'many_faces': False,
    'video_encoder': 'libx264',
    'video_quality': 18,
    'output_name': '',
    'max_size': 768,
    'num_images': 1,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}
def buildROOP(page):
    global roop_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          roop_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_roop_output(o):
      page.roop_output.controls.append(o)
      page.roop_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.roop_output.controls = []
      page.roop_output.update()
      clear_button.visible = False
      clear_button.update()
    def roop_help(e):
      def close_roop_dlg(e):
        nonlocal roop_help_dlg
        roop_help_dlg.open = False
        page.update()
      roop_help_dlg = AlertDialog(title=Text("💁   Help with ROOP Face Swap"), content=Column([
          Text("Choose a face (image with desired face) and the target image/video (image/video in which you want to replace the face) and click on Start. Open file explorer and navigate to the directory you select your output to be in. You will find a directory named <video_title> where you can see the frames being swapped in realtime. Once the processing is done, it will create the output file. That's it."),
          Text("Roop is such a powerful tool that can be used for many purposes, so it’s crucial to know the potential risks of using Roop. Roop Deepfake is an experimental project that aims to make deep fake technology more accessible and easy to use. It uses a library called insightface and some models to detect and replace faces. You can also use GPU acceleration to speed up the process."),
          Text("Disclaimer: This software is meant to be a productive contribution to the rapidly growing AI-generated media industry. It will help artists with tasks such as animating a custom character or using the character as a model for clothing etc."),
          Markdown(" [GitHub Page](https://github.com/s0md3v/roop) | [HuggingFace Space](https://huggingface.co/spaces/zhsso/roop) | [Colab](https://colab.research.google.com/drive/1uX5k33KNXprOeu_P9iov1byLOd4XGo1i#scrollTo=aN1XeEX_tsra)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("👶  Promise not to abuse... ", on_click=close_roop_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(roop_help_dlg)
      roop_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      nonlocal pick_type
      if e.progress == 1:
        save_file(e.file_name)
    def save_file(file_name):
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
          roop_prefs['file_name'] = file_name.rpartition('.')[0]
        else:
          fname = file_name
          roop_prefs['file_name'] = file_name.rpartition(slash)[2].rpartition('.')[0]
        if pick_type == "source":
          source_image.value = fname
          source_image.update()
          roop_prefs['source_image'] = fname
        elif pick_type == "target":
          target_image.value = fname
          target_image.update()
          roop_prefs['target_image'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    #page.overlay.append(pick_files_dialog)
    def pick_source(e):
        nonlocal pick_type
        pick_type = "source"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Source Image File")
    def pick_target(e):
        nonlocal pick_type
        pick_type = "target"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["mp4", "avi", "png", "PNG", "jpg", "jpeg"], dialog_title="Pick Target Video or Image")
    source_image = TextField(label="Source Image of Face", value=roop_prefs['source_image'], on_change=lambda e:changed(e,'source_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_source))
    target_image = TextField(label="Target Video or Image", value=roop_prefs['target_image'], on_change=lambda e:changed(e,'target_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_target))
    keep_fps = Switcher(label="Keep FPS", value=roop_prefs['keep_fps'], on_change=lambda e:changed(e,'keep_fps'))
    keep_audio = Switcher(label="Keep Audio", value=roop_prefs['keep_audio'], on_change=lambda e:changed(e,'keep_audio'))
    keep_frames = Switcher(label="Keep Frames", value=roop_prefs['keep_frames'], on_change=lambda e:changed(e,'keep_frames'))
    many_faces = Switcher(label="Many Faces", value=roop_prefs['many_faces'], on_change=lambda e:changed(e,'many_faces'))
    frame_processor = Dropdown(label="Frame Processor", width=160, options=[dropdown.Option("face_swapper"), dropdown.Option("face_enhancer")], value=roop_prefs['frame_processor'], on_change=lambda e: changed(e, 'frame_processor'))
    video_encoder = Dropdown(label="Video Encoder", width=160, options=[dropdown.Option("libx264"), dropdown.Option("libx265"), dropdown.Option("libvpx-vp9")], value=roop_prefs['video_encoder'], on_change=lambda e: changed(e, 'video_encoder'))
    video_quality = SliderRow(label="Video Quality", min=0, max=50, divisions=49, pref=roop_prefs, key='video_quality')
    max_row = SliderRow(label="Max Image Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=roop_prefs, key='max_size')
    output_name = TextField(label="Output File Name", value=roop_prefs['output_name'], on_change=lambda e:changed(e,'output_name'))
    batch_folder_name = TextField(label="Batch Folder Name", value=roop_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    num_images = NumberPicker(label="Number of Images: ", min=1, max=8, value=roop_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))
    upscaler = UpscaleBlock(roop_prefs)
    page.upscalers.append(upscaler)
    page.roop_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.roop_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎭  ROOP Face Swapper", "Take a Video or Image and Replace the Face in it with a face of your choice, no dataset, no training needed...", actions=[save_default(roop_prefs, ['source_image', 'target_image']), IconButton(icon=icons.HELP, tooltip="Help with ROOP", on_click=roop_help)]),
        #ResponsiveRow([Row([source_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        source_image,
        target_image,
        ResponsiveRow([Column([keep_fps, keep_frames, frame_processor], col={'md':6, 'lg':4, 'xl':3}), Column([keep_audio, many_faces, video_encoder], col={'md':6, 'lg':4, 'xl':3})]),
        video_quality,
        max_row,
        Row([output_name, batch_folder_name]),
        upscaler,
        #Row([jump_length, jump_n_sample, seed]),
        ElevatedButton(content=Text("😷  Run ROOP Swap", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_roop(page)),
        page.roop_output,
        #clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

hallo_prefs = {
    'input_audio': '',
    'target_image': '',
    'num_inference_steps': 40,
    'guidance_scale': 3.5,
    'face_expand_ratio': 1.2,
    'pose_weight': 1.0,
    'face_weight': 1.0,
    'lip_weight': 1.0,
    'n_motion_frames': 2,
    'n_sample_frames': 16,
    'exp_img': 'neutral',
    'exp_image': '',
    'up_face': 'original',
    'fps': 25,
    'img_size': 512,
    'nosmooth': False,
    'output_name': '',
    'batch_folder_name': '',
}
def buildHallo(page):
    global hallo_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          hallo_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def hallo_help(e):
      def close_hallo_dlg(e):
        nonlocal hallo_help_dlg
        hallo_help_dlg.open = False
        page.update()
      hallo_help_dlg = AlertDialog(title=Text("💁   Help with Hallo"), content=Column([
          Text("The field of portrait image animation, driven by speech audio input, has experienced significant advancements in the generation of realistic and dynamic portraits. This research delves into the complexities of synchronizing facial movements and creating visually appealing, temporally consistent animations within the framework of diffusion-based methodologies. Moving away from traditional paradigms that rely on parametric models for intermediate facial representations, our innovative approach embraces the end-to-end diffusion paradigm and introduces a hierarchical audio-driven visual synthesis module to enhance the precision of alignment between audio inputs and visual outputs, encompassing lip, expression, and pose motion. Our proposed network architecture seamlessly integrates diffusion-based generative models, a UNet-based denoiser, temporal alignment techniques, and a reference network. The proposed hierarchical audio-driven visual synthesis offers adaptive control over expression and pose diversity, enabling more effective personalization tailored to different identities. Through a comprehensive evaluation that incorporates both qualitative and quantitative analyses, our approach demonstrates obvious enhancements in image and video quality, lip synchronization precision, and motion diversity."),
          Text("Credit goes to Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei Zhang, Ce Liu, Jingdong Wang, Yao Yao, Siyu Zhu1, Fudan University, Baidu Inc, ETH Zurich, Nanjing University"),
          Markdown("[Paper](https://arxiv.org/pdf/2406.08801) | [GitHub Page](https://github.com/fudan-generative-vision/hallo) | [Project Page](https://fudan-generative-vision.github.io/hallo/#/) | [HuggingFace Model](https://huggingface.co/fudan-generative-ai/hallo) | [HF Space](https://huggingface.co/spaces/fffiloni/tts-hallo-talking-portrait)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🧏  Start Talkin... ", on_click=close_hallo_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(hallo_help_dlg)
      hallo_help_dlg.open = True
      page.update()
    target_image = FileInput(label="Target Image of Face (square)", pref=hallo_prefs, key='target_image', ftype="image", page=page)
    input_audio = FileInput(label="Input Audio with Dialog", pref=hallo_prefs, key='input_audio', ftype="audio", page=page)
    output_name = TextField(label="Output File Name", value=hallo_prefs['output_name'], on_change=lambda e:changed(e,'output_name'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=hallo_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=hallo_prefs, key='guidance_scale')
    face_expand_ratio = SliderRow(label="Face Expand Ratio", min=0, max=2, divisions=20, round=1, pref=hallo_prefs, key='face_expand_ratio', col={'lg':6})
    pose_weight = SliderRow(label="Pose Weight", min=0, max=2, divisions=20, round=1, pref=hallo_prefs, key='pose_weight', col={'lg':6})
    face_weight = SliderRow(label="Face Weight", min=0, max=2, divisions=20, round=1, pref=hallo_prefs, key='face_weight', col={'lg':6})
    lip_weight = SliderRow(label="Lip Weight", min=0, max=2, divisions=20, round=1, pref=hallo_prefs, key='lip_weight', col={'lg':6})
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=hallo_prefs, key='fps', tooltip="The FPS to save target video clip.", col={'lg':6})
    img_size = SliderRow(label="Max Image Size", min=256, max=1024, divisions=48, multiple=16, suffix="px", pref=hallo_prefs, key='img_size', col={'lg':6})
    batch_folder_name = TextField(label="Batch Folder Name", value=hallo_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("👋  Hallo", "Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation...", actions=[save_default(hallo_prefs, ['target_image', 'input_audio']), IconButton(icon=icons.HELP, tooltip="Help with Hallo", on_click=hallo_help)]),
        #ResponsiveRow([Row([input_audio, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        target_image,
        input_audio,
        num_inference_row,
        guidance,
        ResponsiveRow([face_expand_ratio, pose_weight]),
        ResponsiveRow([face_weight, lip_weight]),
        ResponsiveRow([fps, img_size]),
        Row([output_name, batch_folder_name]),
        ElevatedButton(content=Text("👩  Run Hallo", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_hallo(page)),
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c


video_retalking_prefs = {
    'input_audio': '',
    'target_video': '',
    'exp_img': 'neutral',
    'exp_image': '',
    'up_face': 'original',
    'fps': 25,
    'img_size': 384,
    'nosmooth': False,
    'output_name': '',
    'batch_folder_name': '',
}
def buildVideoReTalking(page):
    global video_retalking_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          video_retalking_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_video_retalking_output(o):
      page.video_retalking_output.controls.append(o)
      page.video_retalking_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.video_retalking_output.controls = []
      page.video_retalking_output.update()
      clear_button.visible = False
      clear_button.update()
    def toggle_expression(e):
      video_retalking_prefs['exp_img'] = e.control.value
      exprerssion.visible = video_retalking_prefs['exp_img'] == "Image"
      exprerssion.update()
    def video_retalking_help(e):
      def close_video_retalking_dlg(e):
        nonlocal video_retalking_help_dlg
        video_retalking_help_dlg.open = False
        page.update()
      video_retalking_help_dlg = AlertDialog(title=Text("💁   Help with Video ReTalking"), content=Column([
          Text("We present VideoReTalking, a new system to edit the faces of a real-world talking head video according to input audio, producing a high-quality and lip-syncing output video even with a different emotion. Our system disentangles this objective into three sequential tasks: (1) face video generation with a canonical expression; (2) audio-driven lip-sync; and (3) face enhancement for improving photo-realism. Given a talking-head video, we first modify the expression of each frame according to the same expression template using the expression editing network, resulting in a video with the canonical expression. This video, together with the given audio, is then fed into the lip-sync network to generate a lip-syncing video. Finally, we improve the photo-realism of the synthesized faces through an identity-aware face enhancement network and post-processing. We use learning-based approaches for all three steps and all our modules can be tackled in a sequential pipeline without any user intervention."),
          Text("Credit goes to Kun Cheng, Xiaodong Cun, Yong Zhang, Menghan Xia, Fei Yin, Mingrui Zhu, Xuan Wang, Jue Wang, Nannan Wang and Xidian University, Tencent AI Lab, Tsinghua University"),
          Markdown("[Paper](https://arxiv.org/abs/2211.14758) | [GitHub Page](https://github.com/OpenTalker/video-retalking) | [Project Page](https://opentalker.github.io/video-retalking/) | [Colab](https://colab.research.google.com/github/vinthony/video-retalking/blob/main/quick_demo.ipynb)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("💬  Let's Lip Sync... ", on_click=close_video_retalking_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(video_retalking_help_dlg)
      video_retalking_help_dlg.open = True
      page.update()
    target_video = FileInput(label="Target Video with Face", pref=video_retalking_prefs, key='target_video', ftype="video", page=page)
    input_audio = FileInput(label="Input Audio with Dialog", pref=video_retalking_prefs, key='input_audio', ftype="audio", page=page)
    output_name = TextField(label="Output File Name", value=video_retalking_prefs['output_name'], on_change=lambda e:changed(e,'output_name'))
    up_face = Dropdown(label="Up Face", hint_text="", width=200, options=[dropdown.Option("original"), dropdown.Option("surprise"), dropdown.Option("angry")], value=video_retalking_prefs['up_face'], autofocus=False, on_change=lambda e:changed(e, 'up_face'))
    exp_img = Dropdown(label="Expression", hint_text="", width=200, options=[dropdown.Option("neutral"), dropdown.Option("smile"), dropdown.Option("Image")], value=video_retalking_prefs['exp_img'], autofocus=False, on_change=toggle_expression)
    exp_image = FileInput(label="Facial Expression Image", pref=video_retalking_prefs, key='exp_image', ftype="image", expand=False, page=page)
    exprerssion = Container(content=exp_image, expand=True, visible=False)
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=video_retalking_prefs, key='fps', tooltip="The FPS to save target video clip.", col={'lg':6})
    img_size = SliderRow(label="Max Image Size", min=256, max=1024, divisions=48, multiple=16, suffix="px", pref=video_retalking_prefs, key='img_size', col={'lg':6})
    batch_folder_name = TextField(label="Batch Folder Name", value=video_retalking_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    page.video_retalking_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.video_retalking_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("👄  Video ReTalking", "Audio-based Lip Synchronization for Talking Head Video Editing in the Wild...", actions=[save_default(video_retalking_prefs, ['target_video', 'input_audio']), IconButton(icon=icons.HELP, tooltip="Help with VideoReTalking", on_click=video_retalking_help)]),
        #ResponsiveRow([Row([input_audio, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        target_video,
        input_audio,
        Row([up_face, exp_img, exprerssion]),
        ResponsiveRow([fps, img_size]),
        Row([output_name, batch_folder_name]),
        ElevatedButton(content=Text("🗣  Run Video-ReTalking", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_video_retalking(page)),
        page.video_retalking_output,
        #clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

live_portrait_prefs = {
    'input_image': '',
    'target_video': '',
    'fps': 25,
    'img_size': 512,
    'disable_pasteback': False,
    'disable_relative': False,
    'disable_lip_zero': False,
    'eye_retargeting': False,
    'lip_retargeting': False,
    #'prepare_video': False,
    'keep_audio': True,
    'output_name': '',
    'batch_folder_name': '',
}
def buildLivePortrait(page):
    global live_portrait_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          live_portrait_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def live_portrait_help(e):
      def close_live_portrait_dlg(e):
        nonlocal live_portrait_help_dlg
        live_portrait_help_dlg.open = False
        page.update()
      live_portrait_help_dlg = AlertDialog(title=Text("💁   Help with LivePortrait"), content=Column([
          Text("Record yourself with little shoulder movement, in a video editing app change the resolution to 512x512, then zoom in on the face so it takes up 60-90% of the screen. It should output a 512x512 of just your face, recommended 25fps. Portrait animation aims to synthesize a lifelike video from a single source image, using it as an appearance reference, with motion (i.e., facial expressions and head pose) derived from a driving video, audio, text, or generation. Instead of following mainstream diffusion-based methods, we explore and extend the potential of the implicit-keypoint-based framework, which effectively balances computational efficiency and controllability. Building upon this, we develop a videodriven portrait animation framework named LivePortrait with a focus on better generalization, controllability, and efficiency for practical usage. To enhance the generation quality and generalization ability, we scale up the training data to about 69 million high-quality frames, adopt a mixed image-video training strategy, upgrade the network architecture, and design better motion transformation and optimization objectives. Additionally, we discover that compact implicit keypoints can effectively represent a kind of blendshapes and meticulously propose a stitching and two retargeting modules, which utilize a small MLP with negligible computational overhead, to enhance the controllability. Experimental results demonstrate the efficacy of our framework even compared to diffusion-based methods. The generation speed remarkably reaches 12.8ms on an RTX 4090 GPU with PyTorch."),
          Text("Credit goes to Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, Di Zhang, Kuaishou Technology, University of Science and Technology of China, Fudan University"),
          Markdown("[Paper](https://arxiv.org/pdf/2407.03168) | [GitHub Page](https://github.com/KwaiVGI/LivePortrait) | [Project Page](https://liveportrait.github.io) | [HF Space](https://huggingface.co/spaces/KwaiVGI/liveportrait)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("👱  Funny face time... ", on_click=close_live_portrait_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(live_portrait_help_dlg)
      live_portrait_help_dlg.open = True
      page.update()
    target_video = FileInput(label="Driving Video with Face (cropped 512x512)", pref=live_portrait_prefs, key='target_video', ftype="video", page=page)
    input_image = FileInput(label="Source Portrait Image to Animate", pref=live_portrait_prefs, key='input_image', ftype="image", page=page)
    output_name = TextField(label="Output File Name", value=live_portrait_prefs['output_name'], on_change=lambda e:changed(e,'output_name'))
    #fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=live_portrait_prefs, key='fps', tooltip="The FPS to save target video clip.", col={'lg':6})
    disable_pasteback = Switcher(label="Disable Paste-Back", value=live_portrait_prefs['disable_pasteback'], on_change=lambda e:changed(e,'disable_pasteback'), tooltip="Whether to paste-back/stitch the animated face cropping from the face-cropping space to the original image space.")
    disable_relative = Switcher(label="Disable Relative Motion", value=live_portrait_prefs['disable_relative'], on_change=lambda e:changed(e,'disable_relative'), tooltip="")
    disable_lip_zero = Switcher(label="Disable Lip Zero", value=live_portrait_prefs['disable_lip_zero'], on_change=lambda e:changed(e,'disable_lip_zero'), tooltip="Whether to set lips to close state before animation. Only takes effect when eye_retargeting and lip_retargeting is False.")
    eye_retargeting = Switcher(label="Eye Retargeting", value=live_portrait_prefs['eye_retargeting'], on_change=lambda e:changed(e,'eye_retargeting'), tooltip="")
    lip_retargeting = Switcher(label="Lip Retargeting", value=live_portrait_prefs['lip_retargeting'], on_change=lambda e:changed(e,'lip_retargeting'), tooltip="")
    #prepare_video = Switcher(label="Prepare Driving Video", value=live_portrait_prefs['prepare_video'], on_change=lambda e:changed(e,'prepare_video'), tooltip="")
    keep_audio = Switcher(label="Keep Audio", value=live_portrait_prefs['keep_audio'], on_change=lambda e:changed(e,'keep_audio'), tooltip="Transfers the audio layer from driving video and apply it to output animation.")
    #img_size = SliderRow(label="Max Image Size", min=256, max=1024, divisions=48, multiple=16, suffix="px", pref=live_portrait_prefs, key='img_size', col={'lg':6})
    batch_folder_name = TextField(label="Batch Folder Name", value=live_portrait_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("👧  LivePortrait (under construction, but may work)", "Portrait Animation with Stitching and Retargeting Control... Transfers Face Movements to Source Image.", actions=[save_default(live_portrait_prefs, ['target_video', 'input_image']), IconButton(icon=icons.HELP, tooltip="Help with LivePortrait", on_click=live_portrait_help)]),
        target_video,
        input_image,
        #ResponsiveRow([fps]),
        Row([disable_pasteback, disable_relative, disable_lip_zero]),
        Row([eye_retargeting, lip_retargeting, keep_audio]),
        Row([output_name, batch_folder_name]),
        ElevatedButton(content=Text("😏  Run LivePortrait", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_live_portrait(page)),
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

style_crafter_prefs = {
    'init_video': '',
    'init_image': '',
    'style_images': [],
    'prompt': '',
    'negative_prompt': '',
    'num_inference_steps': 50,
    'guidance_scale': 7.0,
    'eta': 1.0,
    'style_strength': 1.0,
    'selected_mode': 'video',
    'export_to_video': True,
    'save_frames': False,
    "output_video": True,
    'seed': 0,
    'width': 512,
    'height': 320,
    'max_size': 1024,
    'n_samples': 1,
    'batch_size': 1,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildStyleCrafter(page):
    global style_crafter_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          style_crafter_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def style_crafter_help(e):
      def close_style_crafter_dlg(e):
        nonlocal style_crafter_help_dlg
        style_crafter_help_dlg.open = False
        page.update()
      style_crafter_help_dlg = AlertDialog(title=Text("💁   Help with Style Crafter"), content=Column([
          Text("We present StyleCrafter, a generic method that enhances pre-trained T2V models with style control, supporting Style-Guided Text-to-Image Generation and Style-Guided Text-to-Video Generation. Text-to-video (T2V) models have shown remarkable capabilities in generating diverse videos. However, they struggle to produce user-desired stylized videos due to (i) text's inherent clumsiness in expressing specific styles and (ii) the generally degraded style fidelity. To address these challenges, we introduce StyleCrafter, a generic method that enhances pre-trained T2V models with a style control adapter, enabling video generation in any style by providing a reference image. Considering the scarcity of stylized video datasets, we propose to first train a style control adapter using style-rich image datasets, then transfer the learned stylization ability to video generation through a tailor-made finetuning paradigm. To promote content-style disentanglement, we remove style descriptions from the text prompt and extract style information solely from the reference image using a decoupling learning strategy. Additionally, we design a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features, which helps generalization across various text and style combinations. styleCrafter efficiently generates high-quality stylized videos that align with the content of the texts and resemble the style of the reference images. Experiments demonstrate that our approach is more flexible and efficient than existing competitors."),
          Text("We propose a method to equip pre-trained Text-to-Video (T2V) models with a style adapter, allowing for the generation of stylized videos based on both a text prompt and a style reference image. The overview diagram is illustrated as the following figure. In this framework, the textual description dictates the video content, while the style image governs the visual style, ensuring a disentangled control over the video generation process. Given the limited availability of stylized videos, we employ a two-stage training strategy. Initially, we utilize an image dataset abundant in artistic styles to learn reference-based style modulation. Subsequently, adaptation finetuning on a mixed dataset of style images and realistic videos is conducted to improve the temporal quality of the generated videos."),
          Markdown("Credits go to [GongyeLiu](https://github.com/GongyeLiu), [Menghan Xia](https://menghanxia.github.io/), [Yong Zhang](https://yzhang2016.github.io), [Haoxin Chen](https://scholar.google.com/citations?user=6UPJSvwAAAAJ&hl=zh-CN&oi=ao), [Jinbo Xing](https://doubiiu.github.io/), [Xintao Wang](https://xinntao.github.io/), [Yujiu Yang](https://scholar.google.com/citations?user=4gH3sxsAAAAJ&hl=zh-CN&oi=ao), [Ying Shan](https://scholar.google.com/citations?hl=en&user=4oXBp9UAAAAJ&view_op=list_works&sortby=pubdate)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Markdown("[Project Page](https://gongyeliu.github.io/StyleCrafter.github.io/) | [Paper](https://arxiv.org/abs/2312.00330) | [GitHub Code](https://github.com/GongyeLiu/StyleCrafter) | [HuggingFace Space](https://huggingface.co/spaces/liuhuohuo/StyleCrafter)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("👸  So Pretty... ", on_click=close_style_crafter_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(style_crafter_help_dlg)
      style_crafter_help_dlg.open = True
      page.update()
    def delete_image(e):
        f = e.control.data
        if os.path.isfile(f):
          os.remove(f)
          for i, fl in enumerate(page.style_file_list.controls):
            if fl.title.value == f:
              del page.style_file_list.controls[i]
              page.style_file_list.update()
              continue
    def delete_all_images(e):
        for fl in page.style_file_list.controls:
          f = fl.title.value
          if os.path.isfile(f):
            os.remove(f)
        page.style_file_list.controls.clear()
        page.style_file_list.update()
    def image_details(e):
        img = e.control.data
        #TODO: Get file size & resolution
        alert_msg(e.page, "Image Details", content=Column([Text(img), Img(src=asset_dir(img), gapless_playback=True)]), sound=False)
    def add_file(fpath, update=True):
        page.style_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[#TODO: View Image
              PopupMenuItem(icon=icons.INFO, text="Image Details", on_click=image_details, data=fpath),
              PopupMenuItem(icon=icons.DELETE, text="Delete Image", on_click=delete_image, data=fpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All", on_click=delete_all_images, data=fpath),
          ]), data=fpath, on_click=image_details))
        if update: page.style_file_list.update()
    def add_image(e):
        save_dir = uploads_dir
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        if init_image.value.startswith('http'):
          import requests
          from io import BytesIO
          response = requests.get(init_image.value)
          fpath = os.path.join(save_dir, init_image.value.rpartition(slash)[2])
          model_image = PILImage.open(BytesIO(response.content)).convert("RGB")
          width, height = model_image.size
          width, height = scale_dimensions(width, height, style_crafter_prefs['resolution'])
          model_image = model_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          model_image.save(fpath)
          add_file(fpath)
        elif os.path.isfile(init_image.value):
          fpath = os.path.join(save_dir, init_image.value.rpartition(slash)[2])
          original_img = PILImage.open(init_image.value)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, style_crafter_prefs['max_size'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          #shutil.copy(init_image.value, fpath)
          add_file(fpath)
        elif os.path.isdir(init_image.value):
          for f in os.listdir(init_image.value):
            file_path = os.path.join(init_image.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              fpath = os.path.join(save_dir, f)
              original_img = PILImage.open(file_path)
              width, height = original_img.size
              width, height = scale_dimensions(width, height, style_crafter_prefs['max_size'])
              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              original_img.save(fpath)
              #shutil.copy(file_path, fpath)
              add_file(fpath)
        else:
          if bool(init_image.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          #else:
          #  pick_path(e)
          return
        init_image.value = ""
        init_image.update()
        
    prompt = TextField(label="Stylized Video Prompt Text", value=style_crafter_prefs['prompt'], col={'md': 9}, filled=True, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=style_crafter_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    #init_video = FileInput(label="Init Video Clip", pref=style_crafter_prefs, key='init_video', ftype="video", page=page)
    init_image = FileInput(label="Input Style Image", pref=style_crafter_prefs, key='init_image', expand=1, ftype="image", page=page)
    add_image_button = ft.FilledButton(content=Text("➕  Add Image"), on_click=add_image)
    page.style_file_list = Column([], tight=True, spacing=0)

    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=style_crafter_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=style_crafter_prefs, key='guidance_scale')
    #strength = SliderRow(label="Init Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=style_crafter_prefs, key='strength', tooltip="Conceptually, indicates how much to transform the Reference Image over the Vid Generation. Higher value give less influence.")
    #max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=style_crafter_prefs, key='max_size')
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=32, multiple=32, suffix="px", pref=style_crafter_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=32, multiple=32, suffix="px", pref=style_crafter_prefs, key='height')
    style_strength = SliderRow(label="Style Strength", min=0.0, max=1.0, divisions=10, round=1, pref=style_crafter_prefs, key='style_strength', col={'lg':6}, tooltip="")
    output_video = Tooltip(message="Otherwise will Save Image with style", content=Switcher(label="Output Animated Video", value=style_crafter_prefs['output_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'output_video')))
    interpolate_vid = Switcher(label="Interpolate Video", value=style_crafter_prefs['export_to_video'], tooltip="Use Google FiLM Interpolation to transition between frames.", on_change=lambda e:changed(e,'export_to_video'))
    save_frames = Tooltip(message="Save Frames", content=Switcher(label="Save Frames", value=style_crafter_prefs['save_frames'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'save_frames')))
    def change_mode(e):
        style_crafter_prefs['selected_mode'] = e.data
        mode = e.data.split('"')[1].title()
        prompt.label = f"Stylized {mode} Prompt Text"
        prompt.update()
        interpolate_vid.visible = mode == "Video"
        save_frames.visible = mode == "Video"
        interpolate_vid.update()
        save_frames.update()
    selected_mode = ft.SegmentedButton(on_change=change_mode, selected={style_crafter_prefs['selected_mode']}, allow_multiple_selection=False,
        segments=[
            ft.Segment(value="video", label=ft.Text("Video"), icon=ft.Icon(ft.icons.VIDEO_CAMERA_BACK)),
            ft.Segment(value="image", label=ft.Text("Image"), icon=ft.Icon(ft.icons.IMAGE)),
        ],
    )
    #model = Dropdown(label="Video Model", hint_text="", expand=True, options=[dropdown.Option("damo-vilab/text-to-video-ms-1.7b"), dropdown.Option("modelscope-damo-text2video-synthesis"), dropdown.Option("modelscope-damo-text2video-pruned-weights"), dropdown.Option("cerspense/zeroscope_v2_XL"), dropdown.Option("cerspense/zeroscope_v2_576w")], value=style_crafter_prefs['model'], autofocus=False, on_change=lambda e:changed(e, 'model'))
    batch_folder_name = TextField(label="Batch Folder Name", value=style_crafter_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    seed = TextField(label="Seed", width=90, value=str(style_crafter_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("👗  StyleCrafter Text-to-Video-or-Image", "Enhancing Stylized Video or Image Generation with Style Adapter...", actions=[save_default(style_crafter_prefs, ['init_video', 'init_image', 'style_images']), IconButton(icon=icons.HELP, tooltip="Help with StyleCrafter Settings", on_click=style_crafter_help)]),
        #ResponsiveRow([prompt, negative_prompt]),
        prompt,
        #init_video,
        Row([init_image, add_image_button]),
        page.style_file_list,
        num_inference_row,
        guidance,
        style_strength,
        #max_row,
        width_slider, height_slider,
        Row([selected_mode, interpolate_vid, save_frames]),
        Row([seed, batch_folder_name]),
        Row([ElevatedButton(content=Text("🚤   Run StyleCrafter", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_style_crafter(page)),]),
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

rave_prefs = {
    'init_video': '',
    'prompt': '',
    'negative_prompt': '',
    'control_task': 'SoftEdge HED',
    'control_tasks': [],
    'conditioning_scale': 1.8,
    'control_guidance_start': 0.0,
    'control_guidance_end': 1.0,
    'controlnet_strength': 0.9,
    'num_inference_steps': 50,
    'num_inversion_steps': 50,
    'max_size': 512,
    'give_control_inversion': True,
    'is_ddim_inversion': False,
    'is_shuffle': False,
    'save_frames': True,
    'seed': 0,
    'batch_size': 1,
    'batch_size_vae': 1,
    'file_prefix': 'rave-',
    'output_name': '',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildRAVE(page):
    global rave_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          rave_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def rave_help(e):
      def close_rave_dlg(e):
        nonlocal rave_help_dlg
        rave_help_dlg.open = False
        page.update()
      rave_help_dlg = AlertDialog(title=Text("💁   Help with RAVE"), content=Column([
          Text("RAVE is a zero-shot, lightweight, and fast framework for text-guided video editing, supporting videos of any length utilizing text-to-image pretrained diffusion models."),
          Text('Recent advancements in diffusion-based models have demonstrated significant success in generating images from text. However, video editing models have not yet reached the same level of visual quality and user control. To address this, we introduce RAVE, a zero-shot video editing method that leverages pre-trained text-to-image diffusion models without additional training. RAVE takes an input video and a text prompt to produce high-quality videos while preserving the original motion and semantic structure. It employs a novel noise shuffling strategy, leveraging spatio-temporal interactions between frames, to produce temporally consistent videos faster than existing methods. It is also efficient in terms of memory requirements, allowing it to handle longer videos. RAVE is capable of a wide range of edits, from local attribute modifications to shape transformations. In order to demonstrate the versatility of RAVE, we create a comprehensive video evaluation dataset ranging from object-focused scenes to complex human activities like dancing and typing, and dynamic scenes featuring swimming fish and boats. Our qualitative and quantitative experiments highlight the effectiveness of RAVE in diverse video editing scenarios compared to existing methods.'),
          Markdown("Credits go to [Ozgur Kara](https://karaozgur.com/), [Bariscan Kurtkaya](https://bariscankurtkaya.github.io/), [Hidir Yesiltepe](https://sites.google.com/view/hidir-yesiltepe), [James M. Rehg](https://scholar.google.com/citations?hl=en&user=8kA3eDwAAAAJ), [Pinar Yanardag](https://scholar.google.com/citations?user=qzczdd8AAAAJ&hl=en), Georgia Institute of Technology, KUIS AI Center, University of Illinois Urbana-Champaign, Virginia Tech", on_tap_link=lambda e: e.page.launch_url(e.data)),
          #Text(""),
          Text("LineArt - An image with line art, usually black lines on a white background."),
          Text("Canny Map Edge - A monochrome image with white edges on a black background."),
          Text("Depth - A grayscale image with black representing deep areas and white representing shallow areas."),
          Text("SoftEdge HED - A monochrome image with white soft edges on a black background."),
          Markdown("[GitHub Code](https://github.com/rehg-lab/RAVE) | [arXiv](https://arxiv.org/abs/2312.04524) | [Paper](https://rave-video.github.io/static/pdfs/RAVE.pdf) | [Project Page](https://rave-video.github.io/)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("💺  Real Smooth... ", on_click=close_rave_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(rave_help_dlg)
      rave_help_dlg.open = True
      page.update()
    prompt = TextField(label="Prompt Text", value=rave_prefs['prompt'], filled=True, col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=rave_prefs['negative_prompt'], filled=True, col={'md':4}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    #a_prompt = TextField(label="Additional Prompt Text", value=rave_prefs['a_prompt'], multiline=True, filled=True, col={'md':4}, on_change=lambda e:changed(e,'a_prompt'))
    #'aesthetic', 'lineart21', 'hed', 'hed21', 'canny', 'canny21', 'openpose', 'openpose21', 'depth', 'depth21', 'normal', 'mlsd'
    control_task = Dropdown(label="ControlNet Task", width=220, options=[dropdown.Option(t) for t in ['LineArt Realistic', 'LineArt Coarse', "LineArt Standard", "LineArt Anime", "LineArt Anime Denoise", "SoftEdge HED", "SoftEdge HEDsafe", "SoftEdge PIDInet", "SoftEdge PIDsafe", "Canny", "Depth Leres", "Depth Leres++", "Depth Midas", "Depth Zoe"]], value=rave_prefs['control_task'], on_change=lambda e:changed(e,'control_task'))
    #conditioning_scale = SliderRow(label="Conditioning Scale", min=0, max=2, divisions=20, round=1, pref=rave_prefs, key='conditioning_scale', tooltip="The outputs of the controlnet are multiplied by `rave_conditioning_scale` before they are added to the residual in the original unet.")
    conditioning_scale = SliderRow(label="Conditioning Scale", min=0.0, max=5.0, divisions=50, round=1, pref=rave_prefs, key='conditioning_scale', tooltip="Strength of the ControlNet Mask.")
    control_guidance_start = SliderRow(label="Control Guidance Start", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=rave_prefs, key='control_guidance_start', tooltip="The percentage of total steps at which the controlnet starts applying.")
    control_guidance_end = SliderRow(label="Control Guidance End", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=rave_prefs, key='control_guidance_end', tooltip="The percentage of total steps at which the controlnet stops applying.")
    controlnet_strength = SliderRow(label="ControlNet Strength", min=0.0, max=1.0, divisions=20, round=2, pref=rave_prefs, key='controlnet_strength', tooltip="How much influence the controlnet annotator's output is used to guide the denoising process.")
    init_video = FileInput(label="Init Video Clip", pref=rave_prefs, key='init_video', ftype="video", expand=True, page=page)
    #fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=rave_prefs, key='fps', tooltip="The FPS to extract from the init video clip.")
    num_inference_steps = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=rave_prefs, key='num_inference_steps', tooltip="Steps during the sampling process.")
    num_inversion_steps = SliderRow(label="Number of Inversion Steps", min=1, max=100, divisions=99, pref=rave_prefs, key='num_inversion_steps', tooltip="Steps during the inversion process.")
    #prompt_strength = SliderRow(label="Prompt Strength", min=0, max=30, divisions=60, round=1, pref=rave_prefs, key='prompt_strength', tooltip="How much influence the prompt has on the output. Guidance Scale.")
    max_size = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=48, multiple=16, suffix="px", pref=rave_prefs, key='max_size')
    give_control_inversion = Switcher(label="Smooth Boundary", value=rave_prefs['give_control_inversion'], tooltip="Give control to the inversion.", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'give_control_inversion'))
    is_ddim_inversion = Switcher(label="DDIM Inversion", value=rave_prefs['is_ddim_inversion'], tooltip="Use ddim for inversion", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'is_ddim_inversion'))
    is_shuffle = Switcher(label="Shuffling Between Grids", value=rave_prefs['is_shuffle'], tooltip="Apply shuffling between the grids.", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'is_shuffle'))
    save_frames = Switcher(label="Save Frames", value=rave_prefs['save_frames'], tooltip="Save the dumped frames to images_out batch folder. Otherwise only saves final video, keeping pngs in temp folder.", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'save_frames'))
    seed = TextField(label="Seed", value=rave_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width=120, on_change=lambda e:changed(e,'seed', ptype="int"))
    file_prefix = TextField(label="Filename Prefix",  value=rave_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))
    output_name = TextField(label="Output Name", value=rave_prefs['output_name'], on_change=lambda e:changed(e,'output_name'))
    batch_size = NumberPicker(label="Batch Size: ", min=1, max=4, value=rave_prefs['batch_size'], tooltip="Batch size of grids (e.g. 4 grids run in parallel)", on_change=lambda e: changed(e, 'batch_size'))
    batch_size_vae = NumberPicker(label="VAE Batch Size: ", min=1, max=4, value=rave_prefs['batch_size_vae'], tooltip="Batch size for the VAE (e.g. 1 grid runs in parallel for the VAE)", on_change=lambda e: changed(e, 'batch_size_vae'))
    batch_folder_name = TextField(label="Batch Folder Name", value=rave_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(rave_prefs)
    page.upscalers.append(upscaler)
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🐦‍⬛  RAVE Video-to-Video", "Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models...", actions=[save_default(rave_prefs, ['init_video', 'control_tasks']), IconButton(icon=icons.HELP, tooltip="Help with RAVE Vid2Vid Settings", on_click=rave_help)]),
        ResponsiveRow([prompt, negative_prompt]),
        Row([control_task, init_video]),
        conditioning_scale,
        Row([control_guidance_start, control_guidance_end]),
        controlnet_strength,
        num_inference_steps,
        num_inversion_steps,
        max_size,
        #ResponsiveRow([motion_alpha, motion_sigma]),
        #ResponsiveRow([max_dimension, min_dimension]),
        Row([batch_size, batch_size_vae, give_control_inversion, is_ddim_inversion, is_shuffle]),
        Row([seed, output_name, batch_folder_name, file_prefix]),
        upscaler,
        Row([ElevatedButton(content=Text("💀  Run RAVE on Video", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_rave(page))]),
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

tokenflow_prefs = {
    'init_video': '',
    'prompt': '',
    'negative_prompt': 'ugly, blurry, low res, unrealistic, unaesthetic',
    'inversion_prompt': '',
    'num_inference_steps': 50,
    'num_inversion_steps': 500,
    'guidance_scale': 7.5,
    'fps': 30,
    'sd_version': '2.1',
    'num_frames': 40,
    'export_to_video': False,
    'seed': 0,
    'max_size': 672,
    'width': 1024,
    'height': 576,
    'batch_size': 8,
    'selected_mode': 'pnp',
    'pnp_attn_t': 0.5,
    'pnp_f_t': 0.8,
    'start': 0.9,
    'use_ddim_noise': True,
    'batch_folder_name': '',
}

def buildTokenFlow(page):
    global tokenflow_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          tokenflow_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def tokenflow_help(e):
      def close_tokenflow_dlg(e):
        nonlocal tokenflow_help_dlg
        tokenflow_help_dlg.open = False
        page.update()
      tokenflow_help_dlg = AlertDialog(title=Text("💁   Help with TokenFlow Video-To-Video"), content=Column([
          Text("The generative AI revolution has been recently expanded to videos. Nevertheless, current state-of-the-art video models are still lagging behind image models in terms of visual quality and user control over the generated content. In this work, we present a framework that harnesses the power of a text-to-image diffusion model for the task of text-driven video editing. Specifically, given a source video and a target text-prompt, our method generates a high-quality video that adheres to the target text, while preserving the spatial layout and dynamics of the input video. Our method is based on our key observation that consistency in the edited video can be obtained by enforcing consistency in the diffusion feature space. We achieve this by explicitly propagating diffusion features based on inter-frame correspondences, readily available in the model. Thus, our framework does not require any training or fine-tuning, and can work in conjunction with any off-the-shelf text-to-image editing method. We demonstrate state-of-the-art editing results on a variety of real-world videos."),
          Text("We observe that the level of temporal consistency of a video is tightly related to the temporal consistency of its feature representation, as can be seen in the feature visualization below. The features of a natural video have a shared, temporally consistent representation. When editing the video per frame, this consistency breaks. Our method ensures the same level of feature consistency as in the original video features."),
          Markdown("[Project Page](https://diffusion-tokenflow.github.io) | [GitHub repository](https://github.com/omerbt/TokenFlow) | [Paper](https://diffusion-tokenflow.github.io/TokenFlow_Arxiv.pdf)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Credits go to Geyer, Michal and Bar-Tal, Omer Bar-Tal, Bagon, Shai and Dekel, Tali, Harry Chen and HuggingFace")
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🌟  Extra Bonus Points... ", on_click=close_tokenflow_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(tokenflow_help_dlg)
      tokenflow_help_dlg.open = True
      page.update()
    def change_mode(e):
        mode = e.data.split('"')[1]
        tokenflow_prefs['selected_mode'] = mode
        pnp_container.visible = mode == "pnp"
        sdedit_container.visible = mode == "sdedit"
        pnp_container.update()
        sdedit_container.update()
        num_inversion_steps.set_value(500 if mode == "pnp" else 100)
    prompt = TextField(label="Animation Prompt Text", value=tokenflow_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=tokenflow_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    inversion_prompt = TextField(label="Inversion Prompt Describing Video", value=tokenflow_prefs['inversion_prompt'], multiline=True, on_change=lambda e:changed(e,'inversion_prompt'))
    init_video = FileInput(label="Initial Video Clip", pref=tokenflow_prefs, key='init_video', ftype="video", page=page)
    num_frames = SliderRow(label="Number of Frames", min=1, max=300, divisions=299, pref=tokenflow_prefs, key='num_frames', tooltip="The number of video frames that are generated from init video.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=tokenflow_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    num_inversion_steps = SliderRow(label="Number of Inversion Steps", min=1, max=600, divisions=599, pref=tokenflow_prefs, key='num_inversion_steps', tooltip="Steps during the inversion process.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=tokenflow_prefs, key='guidance_scale')
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=tokenflow_prefs, key='fps')
    batch_size = SliderRow(label="Batch Size", min=1, max=60, divisions=59, pref=tokenflow_prefs, key='batch_size')
    #eta_slider = SliderRow(label="ETA", min=0, max=1.0, divisions=20, round=1, pref=tokenflow_prefs, key='eta', tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.")
    #width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=tokenflow_prefs, key='width')
    #height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=tokenflow_prefs, key='height')
    #export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=tokenflow_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    max_size = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=32, multiple=32, suffix="px", pref=tokenflow_prefs, key='max_size')
    width_slider = SliderRow(label="Width", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=tokenflow_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=tokenflow_prefs, key='height')
    selected_mode = ft.SegmentedButton(on_change=change_mode, selected={tokenflow_prefs['selected_mode']}, allow_multiple_selection=False,
        segments=[
            ft.Segment(value="pnp", label=ft.Text("Plug-and-Play"), icon=ft.Icon(ft.icons.POWER)),
            ft.Segment(value="sdedit", label=ft.Text("SDEdit"), icon=ft.Icon(ft.icons.AUTO_FIX_NORMAL)),
        ],
    )
    pnp_attn_t = SliderRow(label="Attention Token", min=0, max=1.0, divisions=20, round=1, pref=tokenflow_prefs, key='pnp_attn_t', col={'sm':6}, tooltip="Attention")
    pnp_f_t = SliderRow(label="Frame Token", min=0, max=1.0, divisions=20, round=1, pref=tokenflow_prefs, key='pnp_f_t', col={'sm':6}, tooltip="Frame")
    pnp_container = Container(ResponsiveRow([pnp_attn_t, pnp_f_t]), visible = tokenflow_prefs['selected_mode'] == "pnp")
    start = SliderRow(label="Start Sampling", min=0, max=1.0, divisions=20, round=1, pref=tokenflow_prefs, key='start', expand=True, tooltip="Start sampling from t = start * 1000")
    use_ddim_noise = Switcher(label="Use DDIM Noise", value=tokenflow_prefs['use_ddim_noise'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_ddim_noise'), tooltip="Use DDIM noise to noise the images. Better structure preservation.")
    sdedit_container = Container(Row([start, use_ddim_noise]), visible = tokenflow_prefs['selected_mode'] == "sdedit")
    batch_folder_name = TextField(label="Video Folder Name", value=tokenflow_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    seed = TextField(label="Seed", width=90, value=str(tokenflow_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    sd_version = Dropdown(label="Stable Diffision Version", width=170, options=[dropdown.Option(t) for t in ['1.5', '2.0', '2.1', 'ControlNet', 'depth', 'XL']], value=tokenflow_prefs['sd_version'], on_change=lambda e:changed(e,'sd_version'))
    #page.tokenflow_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)
    #clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    #clear_button.visible = len(page.tokenflow_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🌞  TokenFlow Video-To-Video (under construction)", "Consistent Diffusion Features for Consistent Video Editing...", actions=[save_default(tokenflow_prefs, ['init_video']), IconButton(icon=icons.HELP, tooltip="Help with TokenFlow Settings", on_click=tokenflow_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        init_video,
        inversion_prompt,
        ResponsiveRow([prompt, negative_prompt]),
        #Row([export_to_video, lower_memory]),
        num_frames,
        fps,
        num_inversion_steps,
        num_inference_row,
        guidance,
        batch_size,
        Row([Text("Diffusion Mode: "), selected_mode]),
        pnp_container,
        sdedit_container,
        #eta_slider,
        #width_slider, height_slider,
        max_size,
        #page.ESRGAN_block_tokenflow,
        Row([sd_version, seed, batch_folder_name]),
        Row([
            ElevatedButton(content=Text("🌈  Run TokenFlow", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_tokenflow(page)),
        ]),
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c


animate_diff_prefs = {
    'prompt': '',
    'negative_prompt': '',
    'head_prompt': '',
    'tail_prompt': '',
    'use_prompt_map': False,
    'frame': '0',
    'steps': 25,
    'guidance_scale': 7.5,
    'editing_prompts': [],
    'animation_prompts': {},
    'prompt_map': {},
    'dreambooth_lora': 'realisticVisionV40_v20Novae',
    'lora_alpha': 0.8,
    'custom_lora': '',
    'lora_layer': 'Add Detail',
    'lora_layer_alpha': 0.8,
    'custom_lora_layer': '',
    'lora_map': [],
    'motion_module': 'mm_sd_v15_v2',
    'scheduler': 'k_dpmpp_2m',
    'seed': 0,
    'video_length': 16,
    'width': 512,
    'height': 512,
    'overlap': 12,
    'stride': 4,
    'context': 16,
    'context_schedule': "Uniform",
    'clip_skip': 1,
    'save_frames': True,
    'save_gif': True,
    'save_video': False,
    'is_loop': False,
    'compile': prefs['enable_torch_compile'],
    'control_task': 'Canny',
    'control_frame': '0',
    'original_image': '',
    'conditioning_scale': 1.0,
    'control_guidance_start': 0.0,
    'control_guidance_end': 1.0,
    'control_scale_list': '0.5,0.4,0.3,0.2,0.1',
    'ref_image': '',
    'controlnet_image': '',
    'controlnet_layers': [],
    'controlnet_tile': False, 'controlnet_ip2p': False, 'controlnet_lineart_anime': False, 'controlnet_openpose': False, 'controlnet_softedge': False, 'controlnet_shuffle': False, 'controlnet_depth': False, 'controlnet_canny': False, 'controlnet_inpaint': False, 'controlnet_lineart': False, 'controlnet_mlsd': False, 'controlnet_normalbae': False, 'controlnet_': False, 'controlnet_scribble': False, 'controlnet_seg': False,
    'upscale_tile': False, 'upscale_ip2p': False, 'upscale_lineart_anime': False, 'upscale_ip2p': False, 'upscale_ref': False,
    'upscale_steps': 20, 'upscale_strength': 0.5, 'upscale_guidance_scale': 10,
    'upscale_amount': 1.5,
    'use_ip_adapter': False,
    'ip_adapter_image': '',
    'ip_adapter_frame': '0',
    'ip_adapter_layers': {},
    'ip_adapter_scale': 0.5,
    'ip_adapter_is_plus': True,
    'ip_adapter_is_full_face': False,
    'ip_adapter_is_plus_face': False,
    'ip_adapter_light': False,
    'use_img2img': False,
    'img2img_image': '',
    'img2img_frame': '0',
    'img2img_layers': {},
    'img2img_strength': 0.7,
    'is_simple_composite': False,
    'motion_loras': [],
    'motion_loras_strength': 0.5,
    'apply_lcm_lora': False,
    'num_images': 1,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

animate_diff_loras = [
    {'name': 'toonyou_beta3', 'file': 'toonyou_beta3.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/toonyou_beta3.safetensors'},
    {'name': 'CounterfeitV30_v30', 'file': 'CounterfeitV30_v30.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/CounterfeitV30_v30.safetensors'},
    {'name': 'epiCRealism-Natural_Sin', 'file': 'epicrealism_naturalSinRC1VAE.safetensors', 'path': 'https://huggingface.co/Justin-Choo/epiCRealism-Natural_Sin_RC1_VAE/blob/main/epicrealism_naturalSinRC1VAE.safetensors'},
    {'name': 'FilmVelvia2', 'file': 'FilmVelvia2.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/FilmVelvia2.safetensors'},
    {'name': 'Pyramid lora_Ghibli_n3', 'file': 'Pyramid%20lora_Ghibli_n3.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/Pyramid%20lora_Ghibli_n3.safetensors'},
    {'name': 'TUSUN', 'file': 'TUSUN.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/TUSUN.safetensors'},
    {'name': 'lyriel_v16', 'file': 'lyriel_v16.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/lyriel_v16.safetensors'},
    {'name': 'majicmixRealistic_v5Preview', 'file': 'majicmixRealistic_v5Preview.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/majicmixRealistic_v5Preview.safetensors'},
    {'name': 'moonfilm_filmGrain10', 'file': 'moonfilm_filmGrain10.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/moonfilm_filmGrain10.safetensors'},
    {'name': 'moonfilm_reality20', 'file': 'moonfilm_reality20.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/moonfilm_reality20.safetensors'},
    {'name': 'rcnzCartoon3d_v10', 'file': 'rcnzCartoon3d_v10.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/rcnzCartoon3d_v10.safetensors'},
    {'name': 'realisticVisionV20_v20', 'file': 'realisticVisionV20_v20.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/blob/main/realisticVisionV20_v20.safetensors'},
    {'name': 'realisticVisionV40_v20Novae', 'file': 'realisticVisionV40_v20Novae.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/realisticVisionV40_v20Novae.safetensors'},
    {'name': 'Mistoon_Anime', 'file': 'mistoonAnime_v20.safetensors', 'path': 'https://huggingface.co/WickedOne/MistoonAnimeV2/blob/main/mistoonAnime_v20.safetensors'},
    {'name': 'ToonYou beta6', 'file': 'toonyou_beta6.safetensors', 'path': 'https://huggingface.co/frankjoshua/toonyou_beta6/blob/main/toonyou_beta6.safetensors'},
    {'name': 'XXMix_9realistic_v4', 'file': 'xxmix9realistic_v40.safetensors', 'path': 'https://huggingface.co/jzli/XXMix_9realistic-v4/resolve/main/xxmix9realistic_v40.safetensors'},
    {'name': 'revAnimated_v122', 'file': 'revAnimated_v122.safetensors', 'path': 'https://huggingface.co/joaov33/revanimated/blob/main/revAnimated_v122.safetensors'},
    #{'name': 'Custom', 'file': '', 'path': ''},
]
animate_diff_lora_layers = [
    {'name': 'Add Detail', 'file': 'add_detail.safetensors', 'path': 'https://civitai.com/api/download/models/118311?type=Model&format=SafeTensor'},
    {'name': 'Muffet v2', 'file': 'muffet_v2.safetensors', 'path': 'https://civitai.com/api/download/models/36809?type=Model&format=SafeTensor'},
    {'name': 'Vector Art', 'file': 'vector_art.safetensors', 'path': 'https://civitai.com/api/download/models/68115?type=Model&format=SafeTensor'},
    {'name': 'Detail Enhancer', 'file': 'detail_enhancer.safetensors', 'path': 'https://civitai.com/api/download/models/87153?type=Model&format=SafeTensor'},
    {'name': 'Studio Ghibli', 'file': 'studio_ghibli.safetensors', 'path': 'https://civitai.com/api/download/models/7657?type=Model&format=SafeTensor&size=full&fp=fp16'},
    {'name': 'Arcane Style', 'file': 'arcane_style.safetensors', 'path': 'https://civitai.com/api/download/models/8339?type=Model&format=SafeTensor&size=full&fp=fp16'},
    {'name': 'DC Comics', 'file': 'DC_Comics.safetensors', 'path': 'https://civitai.com/api/download/models/10580?type=Model&format=SafeTensor&size=full&fp=fp16'},
    {'name': 'SteampunkAI', 'file': 'SteampunkAI.safetensors', 'path': 'https://civitai.com/api/download/models/24794?type=Model&format=SafeTensor&size=full&fp=fp16'},
    {'name': 'Liminal Space', 'file': 'liminal_space.safetensors', 'path': 'https://civitai.com/api/download/models/72282?type=Model&format=SafeTensor'},
    {'name': 'Landscapes Mix', 'file': 'landscapes_mix.safetensors', 'path': 'https://civitai.com/api/download/models/8787?type=Model&format=SafeTensor&size=full&fp=fp16'},
    {'name': 'Elixir Enhancer', 'file': 'elixir_enhancer.safetensors', 'path': 'https://civitai.com/api/download/models/83081?type=Model&format=SafeTensor'},
    {'name': 'Amateur Porn', 'file': 'amateur_porn.safetensors', 'path': 'https://civitai.com/api/download/models/56939?type=Model&format=SafeTensor'},
    {'name': 'HD Porn', 'file': 'hd_porn.safetensors', 'path': 'https://civitai.com/api/download/models/54388?type=Model&format=SafeTensor'},
    #{'name': '', 'file': '.safetensors', 'path': ''},
]
animate_diff_motion_modules = [
    {'name': 'mm_sd_v14', 'file': 'mm_sd_v14.ckpt', 'path': 'https://huggingface.co/guoyww/AnimateDiff/resolve/main/mm_sd_v14.ckpt'},
    {'name': 'mm_sd_v15', 'file': 'mm_sd_v15.ckpt', 'path': 'https://huggingface.co/guoyww/AnimateDiff/resolve/main/mm_sd_v15.ckpt'},
    {'name': 'mm_sd_v15_v2', 'file': 'mm_sd_v15_v2.ckpt', 'path': 'https://huggingface.co/guoyww/AnimateDiff/resolve/main/mm_sd_v15_v2.ckpt'},
    {'name': 'mm_sdxl_v10_beta', 'file': 'mm_sdxl_v10_beta.ckpt', 'path': 'https://huggingface.co/guoyww/AnimateDiff/resolve/main/mm_sdxl_v10_beta.ckpt'},
    {'name': 'v3_sd15_mm', 'file': 'v3_sd15_mm.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v3_sd15_mm.ckpt'},
    {'name': 'Long_mm_16_64_frames', 'file': 'lt_long_mm_16_64_frames.ckpt', 'path': 'https://huggingface.co/Lightricks/LongAnimateDiff/resolve/main/lt_long_mm_16_64_frames.ckpt'},
    {'name': 'Long_mm_32_frames', 'file': 'lt_long_mm_32_frames.ckpt', 'path': 'https://huggingface.co/Lightricks/LongAnimateDiff/resolve/main/lt_long_mm_32_frames.ckpt'},
    {'name': 'improved3DMotion', 'file': 'improved3DMotion_improved3DV1.ckpt', 'path': 'https://civitai.com/api/download/models/178017?type=Model&format=PickleTensor'},
    {'name': 'TemporalDiff', 'file': 'temporaldiffMotion_v10.ckpt', 'path': 'https://civitai.com/api/download/models/160418?type=Model&format=PickleTensor'},
    {'name': 'YoinkoorLab NSFW', 'file': 'yoinkoorlabsNSFWMotion_godmodev20.ckpt', 'path': 'https://civitai.com/api/download/models/177016?type=Model&format=PickleTensor'},
    {'name': 'Improved Humans', 'file': 'improvedHumansMotion_refinedHumanMovement.ckpt', 'path': 'https://civitai.com/api/download/models/174464?type=Model&format=PickleTensor'},
    {'name': 'ZlikwidDiff', 'file': 'zlikwiddiffV1_v10.ckpt', 'path': 'https://civitai.com/api/download/models/178745?type=Model&format=PickleTensor'},
    {'name': 'Viddle-Pix2Pix', 'file': 'viddle-pix2pix-animatediff-v1.ckpt', 'path': 'https://huggingface.co/viddle/viddle-pix2pix-animatediff/resolve/main/viddle-pix2pix-animatediff-v1.ckpt'},
]
animate_diff_motion_loras = [
    {'name': 'Zoom-In', 'file': 'v2_lora_ZoomOut.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v2_lora_ZoomOut.ckpt'},
    {'name': 'Zoom-Out', 'file': 'v2_lora_ZoomIn.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v2_lora_ZoomIn.ckpt'},
    {'name': 'Pan-Left', 'file': 'v2_lora_PanLeft.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v2_lora_PanLeft.ckpt'},
    {'name': 'Pan-Right', 'file': 'v2_lora_PanRight.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v2_lora_PanRight.ckpt'},
    {'name': 'Tilt-Up', 'file': 'v2_lora_TiltUp.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v2_lora_TiltUp.ckpt'},
    {'name': 'Tilt-Down', 'file': 'v2_lora_TiltDown.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v2_lora_TiltDown.ckpt'},
    {'name': 'Clockwise', 'file': 'v2_lora_RollingClockwise.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v2_lora_RollingClockwise.ckpt'},
    {'name': 'Anti-clockwise', 'file': 'v2_lora_RollingAnticlockwise.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v2_lora_RollingAnticlockwise.ckpt'},
    {'name': 'v3-SD1.5-Adapter', 'file': 'v3_sd15_adapter.ckpt', 'path': 'https://huggingface.co/guoyww/animatediff/resolve/main/v3_sd15_adapter.ckpt'},
]

def buildAnimateDiff(page):
    global animate_diff_prefs, prefs, pipe_animate_diff, editing_prompt
    editing_prompt = {'prompt':'', 'negative_prompt':'', 'seed':0}
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          animate_diff_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_animate_diff_output(o):
      page.animate_diff_output.controls.append(o)
      page.animate_diff_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.animate_diff_output.controls = []
      page.animate_diff_output.update()
      clear_button.visible = False
      clear_button.update()
    def animate_diff_help(e):
      def close_animate_diff_dlg(e):
        nonlocal animate_diff_help_dlg
        animate_diff_help_dlg.open = False
        page.update()
      animate_diff_help_dlg = AlertDialog(title=Text("💁   Help with AnimateDiff"), content=Column([
          Text("With the advance of text-to-image models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. Subsequently, there is a great demand for image animation techniques to further combine generated static images with motion dynamics. In this report, we propose a practical framework to animate most of the existing personalized text-to-image models once and for all, saving efforts in model-specific tuning. At the core of the proposed framework is to insert a newly initialized motion modeling module into the frozen text-to-image model and train it on video clips to distill reasonable motion priors. Once trained, by simply injecting this motion modeling module, all personalized versions derived from the same base T2I readily become text-driven models that produce diverse and personalized animated images. We conduct our evaluation on several public representative personalized text-to-image models across anime pictures and realistic photographs, and demonstrate that our proposed framework helps these models generate temporally smooth animation clips while preserving the domain and diversity of their outputs."),
          Text("Credits: Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, Bo Dai. Also Neggles for refactoring, Camenduru and UI by Alan Bedian"),
          Markdown("[Neggles GitHub Code](https://github.com/neggles/animatediff-cli) - [S9roll7 Prompt Travel](https://github.com/s9roll7/animatediff-cli-prompt-travel) - [Original GitHub Code](https://github.com/guoyww/animatediff/) - [Project Page](https://animatediff.github.io/) - [Arxiv Paper](https://arxiv.org/abs/2307.04725)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🧞  Make a Wish... ", on_click=close_animate_diff_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(animate_diff_help_dlg)
      animate_diff_help_dlg.open = True
      page.update()
    def changed_lora(e):
      animate_diff_prefs['dreambooth_lora'] = e.control.value
      custom_lora.visible = e.control.value == "Custom"
      custom_lora.update()
    def changed_lora_layer(e):
      animate_diff_prefs['lora_layer'] = e.control.value
      custom_lora_layer.visible = e.control.value == "Custom"
      custom_lora_layer.update()
    def add_lora(e):
      lora = animate_diff_prefs['lora_layer']
      lora_scale = animate_diff_prefs['lora_layer_alpha']
      lora_layer = {}
      if lora == "Custom":
        lora_layer = {'name': 'Custom', 'file':'', 'path':animate_diff_prefs['custom_lora_layer'], 'scale': lora_scale}
      else:
        for l in animate_diff_lora_layers:
          if l['name'] == lora:
            lora_layer = l.copy()
            lora_layer['scale'] = lora_scale
        for l in animate_diff_prefs['lora_map']:
          if l['name'] == lora:
            return
      animate_diff_prefs['lora_map'].append(lora_layer)
      title = Markdown(f"**{lora_layer['name']}** - Alpha Scale: [{lora_layer['scale']}] - {lora_layer['path']}")
      lora_layer_map.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
        items=[
            PopupMenuItem(icon=icons.DELETE, text="Delete LoRA Layer", on_click=delete_lora_layer, data=lora_layer),
            PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_lora_layers, data=lora_layer),
        ]), data=lora_layer))
      lora_layer_map.update()
    def delete_lora_layer(e):
        for l in animate_diff_prefs['lora_map']:
          if l['name'] == e.control.data['name']:
            animate_diff_prefs['lora_map'].remove(l)
          #del l #animate_diff_prefs['lora_map'][]
        for c in lora_layer_map.controls:
          if c.data['name'] == e.control.data['name']:
             lora_layer_map.controls.remove(c)
             break
        lora_layer_map.update()
    def delete_all_lora_layers(e):
        animate_diff_prefs['lora_map'].clear()
        lora_layer_map.controls.clear()
        lora_layer_map.update()
    def animate_diff_tile(animate_prompt):
        params = []
        for k, v in animate_prompt.items():
            if k == 'prompt': continue
            params.append(f'{to_title(k)}: {v}')
        sub = ', '.join(params)
        return ListTile(title=Text(animate_prompt['prompt'], max_lines=6, theme_style=TextThemeStyle.BODY_LARGE), subtitle=Text(sub), dense=True, data=animate_prompt, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[PopupMenuItem(icon=icons.EDIT, text="Edit Animation Prompt", on_click=lambda e: edit_animate_diff(animate_prompt), data=animate_prompt),
                 PopupMenuItem(icon=icons.DELETE, text="Delete Animation Prompt", on_click=lambda e: del_animate_diff(animate_prompt), data=animate_prompt)]), on_click=lambda e: edit_animate_diff(animate_prompt))
    def edit_animate_diff(edit=None):
        animate_diff_prompt = edit if bool(edit) else editing_prompt.copy()
        edit_prompt = edit['prompt'] if bool(edit) else animate_diff_prefs['prompt']
        #negative_prompt = edit['negative_prompt'] if bool(edit) else animate_diff_prefs['negative_prompt']
        if not bool(edit):
            animate_diff_prompt['prompt'] = animate_diff_prefs['prompt']
            animate_diff_prompt['negative_prompt'] = animate_diff_prefs['negative_prompt']
            animate_diff_prompt['seed'] = animate_diff_prefs['seed']
        def close_dlg(e):
            dlg_edit.open = False
            page.update()
        def changed_p(e, pref=None):
            if pref is not None:
                animate_diff_prompt[pref] = e.control.value
        def save_animate_diff_prompt(e):
            if edit == None:
                animate_diff_prefs['editing_prompts'].append(animate_diff_prompt)
                page.animate_diff_prompts.controls.append(animate_diff_tile(animate_diff_prompt))
                page.animate_diff_prompts.update()
            else:
                for s in animate_diff_prefs['editing_prompts']:
                    if s['prompt'] == edit_prompt:
                        s = animate_diff_prompt
                        break
                for t in page.animate_diff_prompts.controls:
                    #print(f"{t.data['prompt']} == {edit_prompt} - {t.title.value}")
                    if t.title.value == edit_prompt: #t.data['prompt']
                        params = []
                        for k, v in animate_diff_prompt.items():
                            if k == 'prompt': continue
                            params.append(f'{to_title(k)}: {v}')
                        sub = ', '.join(params)
                        t.title = Text(animate_diff_prompt['prompt'], max_lines=6, theme_style=TextThemeStyle.BODY_LARGE)
                        t.subtitle = Text(sub)
                        t.data = animate_diff_prompt
                        t.update()
                        break
                dlg_edit.open = False
                e.control.update()
                page.update()
        animate_diff_editing_prompt = TextField(label="AnimateDiff Prompt Modifier", value=animate_diff_prompt['prompt'], autofocus=True, on_change=lambda e:changed_p(e,'prompt'))
        animate_diff_negative_prompt = TextField(label="Negative Prompt", value=animate_diff_prompt['negative_prompt'], autofocus=True, on_change=lambda e:changed_p(e,'negative_prompt'))
        animate_diff_seed = TextField(label="Seed", width=90, value=str(animate_diff_prompt['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed_p(e,'seed'), col={'md':1})
        #reverse_editing_direction = Checkbox(label="Reverse Editing Direction", value=animate_diff_prompt['reverse_editing_direction'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed_p(e,'reverse_editing_direction'), tooltip="Whether the corresponding prompt in `editing_prompt` should be increased or decreased.")
        if edit != None:
            dlg_edit = AlertDialog(modal=False, title=Text(f"♟️ {'Edit' if bool(edit) else 'Add'} Animated Prompt"), content=Container(Column([
                animate_diff_editing_prompt,
                animate_diff_negative_prompt, animate_diff_seed,
            ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO), width=(page.width if page.web else page.window.width) - 180), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Prompt ", size=19, weight=FontWeight.BOLD), on_click=save_animate_diff_prompt)], actions_alignment=MainAxisAlignment.END)
            page.overlay.append(dlg_edit)
            dlg_edit.open = True
            page.update()
        else:
            save_animate_diff_prompt(None)
    def del_animate_diff(edit=None):
        for s in animate_diff_prefs['editing_prompts']:
            if s['prompt'] == edit['prompt']:
                animate_diff_prefs['editing_prompts'].remove(s)
                break
        for t in page.animate_diff_prompts.controls:
            if t.data['prompt'] == edit['prompt']:
                page.animate_diff_prompts.controls.remove(t)
                break
        page.animate_diff_prompts.update()
        play_snd(Snd.DELETE, page)
    def clear_animate_diff_prompts(e):
        animate_diff_prefs['editing_prompts'].clear()
        page.animate_diff_prompts.controls.clear()
        page.animate_diff_prompts.update()
        play_snd(Snd.DELETE, page)
    def edit_prompt(e):
      nonlocal animation_prompts
      f = int(e.control.data)
      edit_prompt = animate_diff_prefs['animation_prompts'][str(f)]
      def close_dlg(e):
        dlg_edit.open = False
        page.update()
      def save_prompt(e):
        fr = int(editing_frame.value)
        pline = f'{fr}: "{editing_prompt.value}"'
        for p in animation_prompts.controls:
          if p.data == f:
            p.title.value = pline
            p.data = fr
            for button in p.trailing.items:
              button.data = fr
            break
        animate_diff_prefs['animation_prompts'][str(editing_frame.value)] = editing_prompt.value.strip()
        if f != int(editing_frame.value):
          del animate_diff_prefs['animation_prompts'][str(f)]
          sorted_dict = {}
          for key in sorted(animate_diff_prefs['animation_prompts'].keys()):
              sorted_dict[key] = animate_diff_prefs['animation_prompts'][key]
          animate_diff_prefs['animation_prompts'] = sorted_dict
          animation_prompts.controls = sorted(animation_prompts.controls, key=lambda tile: tile.data)
        animation_prompts.update()
        close_dlg(e)
      editing_frame = TextField(label="Frame", width=90, value=str(f), keyboard_type=KeyboardType.NUMBER, tooltip="")
      editing_prompt = TextField(label="Keyframe Prompt Animation", expand=True, multiline=True, value=edit_prompt, autofocus=True)
      dlg_edit = AlertDialog(modal=False, title=Text(f"♟️ Edit Prompt Keyframe"), content=Container(Column([
          Row([editing_frame, editing_prompt])
      ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO), width=(page.width if page.web else page.window.width) - 180), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Prompt ", size=19, weight=FontWeight.BOLD), on_click=save_prompt)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(dlg_edit)
      dlg_edit.open = True
      page.update()
    def del_prompt(e):
      f = e.control.data
      for i, p in enumerate(animation_prompts.controls):
        if p.data == f:
          del animation_prompts.controls[i]
          break
      animation_prompts.update()
      del animate_diff_prefs['animation_prompts'][str(f)]
      play_snd(Snd.DELETE, page)
    def clear_prompts(e):
      animation_prompts.controls.clear()
      animation_prompts.update()
      animate_diff_prefs['animation_prompts'] = {}
      clear_prompt(e)
      play_snd(Snd.DELETE, page)
    def clear_prompt(e):
      prompt.value = ""
      prompt.update()
    def copy_prompt(e):
      p = animate_diff_prefs['animation_prompts'][str(e.control.data)]
      page.set_clipboard(p)
      toast_msg(page, f"📋  Prompt Text copied to clipboard...")
    def add_prompt(e, f=None, p=None, sound=True):
      if (not bool(prompt.value) or not bool(frame.value)) and f == None: return
      if f == None: f = int(frame.value)
      if p == None: p = prompt.value.strip()
      pline = f'{f}: "{p}"'
      if str(f) in animate_diff_prefs['animation_prompts']:
        for i, pr in enumerate(animation_prompts.controls):
          if pr.data == f:
            pr.title.value = pline
      else:
        animation_prompts.controls.append(ListTile(title=Text(pline, size=14), data=f, trailing=PopupMenuButton(icon=icons.MORE_VERT,
            items=[PopupMenuItem(icon=icons.EDIT, text="Edit Animation Prompt", on_click=edit_prompt, data=f),
                  PopupMenuItem(icon=icons.COPY, text="Copy Prompt Text", on_click=copy_prompt, data=f),
                  PopupMenuItem(icon=icons.DELETE, text="Delete Animation Prompt", on_click=del_prompt, data=f), PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Prompts", on_click=clear_prompts)]), dense=True, on_click=edit_prompt))
      animate_diff_prefs['animation_prompts'][str(f)] = p
      #animate_diff_prefs['animation_prompts'] = {int(k):v for k,v in animate_diff_prefs['animation_prompts'].items()}
      #animate_diff_prefs['animation_prompts'] = sorted(animate_diff_prefs['animation_prompts'].keys())
      #animate_diff_prefs['animation_prompts'] = {i: animate_diff_prefs['animation_prompts'][i] for i in list(animate_diff_prefs['animation_prompts'].keys()).sort()}
      sorted_dict = {}
      for key in sorted(animate_diff_prefs['animation_prompts'].keys()):
          sorted_dict[key] = animate_diff_prefs['animation_prompts'][key]
      animate_diff_prefs['animation_prompts'] = sorted_dict
      animation_prompts.controls = sorted(animation_prompts.controls, key=lambda tile: tile.data)
      animation_prompts.update()
      if prefs['enable_sounds'] and sound: page.snd_drop.play()
    def add_layer(e):
        control_images = {str(int(animate_diff_prefs['control_frame'])): animate_diff_prefs['original_image']}
        updating = False
        for l in animate_diff_prefs['controlnet_layers']:
            if l['control_task'] == animate_diff_prefs['control_task']:
                control_images = merge_dict(l['control_images'], control_images)
                updating = True
        layer = {'control_task': animate_diff_prefs['control_task'], 'control_images': control_images, 'control_frame': animate_diff_prefs['control_frame'], 'original_image': animate_diff_prefs['original_image'], 'control_scale_list': animate_diff_prefs['control_scale_list'], 'conditioning_scale': animate_diff_prefs['conditioning_scale'], 'control_guidance_start': animate_diff_prefs['control_guidance_start'], 'control_guidance_end': animate_diff_prefs['control_guidance_end'], 'use_init_video': False}
        images = ""
        for f, img in control_images.items():
            images += f" - {f}: {img}"
        if updating:
            for i, cn in enumerate(animate_diff_prefs['controlnet_layers']):
                if cn['control_task'] == animate_diff_prefs['control_task']:
                    animate_diff_prefs['controlnet_layers'][i] = layer
            for l in multi_layers.controls:
                if l.data['control_task'] == animate_diff_prefs['control_task']:
                    l.title = Markdown(f"**{layer['control_task']}** - Scale List: [{animate_diff_prefs['control_scale_list']}] - Conditioning Scale: {layer['conditioning_scale']} - Start: {layer['control_guidance_start']}, End: {layer['control_guidance_end']}{images}")
                    #l.title = Row([Text(layer['control_task'] + " - ", weight=FontWeight.BOLD), Text(f"{images}Scale List: [{animate_diff_prefs['control_scale_list']}] - Conditioning Scale: {layer['conditioning_scale']} - Start: {layer['control_guidance_start']}, End: {layer['control_guidance_end']}")])
                    l.update()
                    l.data = layer
        else:
            animate_diff_prefs['controlnet_layers'].append(layer)
            title = Markdown(f"**{layer['control_task']}** - Scale List: [{animate_diff_prefs['control_scale_list']}] - Conditioning Scale: {layer['conditioning_scale']} - Start: {layer['control_guidance_start']}, End: {layer['control_guidance_end']}{images}")
            #multi_layers.controls.append(ListTile(title=Row([Text(layer['control_task'] + " - ", weight=FontWeight.BOLD), Text(f"{images}Scale List: [{animate_diff_prefs['control_scale_list']}] - Conditioning Scale: {layer['conditioning_scale']} - Start: {layer['control_guidance_start']}, End: {layer['control_guidance_end']}")]), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
            multi_layers.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
              items=[
                  PopupMenuItem(icon=icons.DELETE, text="Delete Control Layer", on_click=delete_layer, data=layer),
                  PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
              ]), data=layer))
        multi_layers.update()
        #animate_diff_prefs['original_image'] = ""
        #original_image.value = ""
        #original_image.update()
    def delete_layer(e):
        animate_diff_prefs['controlnet_layers'].remove(e.control.data)
        for c in multi_layers.controls:
          if c.data['original_image'] == e.control.data['original_image']:
             multi_layers.controls.remove(c)
             break
        multi_layers.update()
    def delete_all_layers(e):
        animate_diff_prefs['controlnet_layers'].clear()
        multi_layers.controls.clear()
        multi_layers.update()
    def toggle_ip_adapter(e):
      animate_diff_prefs['use_ip_adapter'] = e.control.value
      ip_adapter_container.height=None if animate_diff_prefs['use_ip_adapter'] else 0
      ip_adapter_container.update()
    def add_ip_layer(e):
        ip_adapter_image = {str(int(animate_diff_prefs['ip_adapter_frame'])): animate_diff_prefs['ip_adapter_image']}
        updating = False
        if str(int(animate_diff_prefs['ip_adapter_frame'])) in animate_diff_prefs['ip_adapter_layers']:
            updating = True
        animate_diff_prefs['ip_adapter_layers'].update(ip_adapter_image)
        title = Markdown(f"**{str(int(animate_diff_prefs['ip_adapter_frame']))}:** {animate_diff_prefs['ip_adapter_image']}")
        if updating:
            animate_diff_prefs['ip_adapter_layers'][str(int(animate_diff_prefs['ip_adapter_frame']))] = animate_diff_prefs['ip_adapter_image']
            for l in ip_adapter_layers.controls:
                if next(iter(l.data.items()))[0] == str(int(animate_diff_prefs['ip_adapter_frame'])):
                    l.title = title
                    l.update()
                    l.data = ip_adapter_image
        else:
            ip_adapter_layers.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
              items=[
                  PopupMenuItem(icon=icons.DELETE, text="Delete IP Frame", on_click=delete_ip_layer, data=ip_adapter_image),
                  PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Frames", on_click=delete_all_ip_layers, data=ip_adapter_image),
              ]), data=ip_adapter_image))
        ip_adapter_layers.update()
        #animate_diff_prefs['ip_adapter_image'] = ""
        #ip_adapter_image.value = ""
        #ip_adapter_image.update()
    def delete_ip_layer(e):
        del animate_diff_prefs['ip_adapter_layers'][next(iter(e.control.data.items()))[0]]
        for c in ip_adapter_layers.controls:
          if next(iter(c.data.items()))[0] ==  next(iter(e.control.data.items()))[0]:
             ip_adapter_layers.controls.remove(c)
             break
        ip_adapter_layers.update()
    def delete_all_ip_layers(e):
        animate_diff_prefs['ip_adapter_layers'].clear()
        ip_adapter_layers.controls.clear()
        ip_adapter_layers.update()
    
    def toggle_img2img(e):
        animate_diff_prefs['use_img2img'] = e.control.value
        img2img_strength.show = animate_diff_prefs['use_img2img']
        img2img_container.height=None if animate_diff_prefs['use_img2img'] else 0
        img2img_container.update()
        img2img_strength.update()
    def add_img2img_layer(e):
        img2img_image = {str(int(animate_diff_prefs['img2img_frame'])): animate_diff_prefs['img2img_image']}
        updating = False
        if str(int(animate_diff_prefs['img2img_frame'])) in animate_diff_prefs['img2img_layers']:
            updating = True
        animate_diff_prefs['img2img_layers'].update(img2img_image)
        title = Markdown(f"**{str(int(animate_diff_prefs['img2img_frame']))}:** {animate_diff_prefs['img2img_image']}")
        if updating:
            animate_diff_prefs['img2img_layers'][str(int(animate_diff_prefs['img2img_frame']))] = animate_diff_prefs['img2img_image']
            for l in img2img_layers.controls:
                if next(iter(l.data.items()))[0] == str(int(animate_diff_prefs['img2img_frame'])):
                    l.title = title
                    l.update()
                    l.data = img2img_image
        else:
            img2img_layers.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
              items=[
                  PopupMenuItem(icon=icons.DELETE, text="Delete Img2Img Frame", on_click=delete_img2img_layer, data=img2img_image),
                  PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Frames", on_click=delete_all_img2img_layers, data=img2img_image),
              ]), data=img2img_image))
        img2img_layers.update()
    def delete_img2img_layer(e):
        del animate_diff_prefs['img2img_layers'][next(iter(e.control.data.items()))[0]]
        for c in img2img_layers.controls:
          if next(iter(c.data.items()))[0] ==  next(iter(e.control.data.items()))[0]:
             img2img_layers.controls.remove(c)
             break
        img2img_layers.update()
    def delete_all_img2img_layers(e):
        animate_diff_prefs['img2img_layers'].clear()
        img2img_layers.controls.clear()
        img2img_layers.update()
    def changed_motion_lora(e):
        on = e.control.value
        if e.control.data in animate_diff_prefs['motion_loras']:
            animate_diff_prefs['motion_loras'].remove(e.control.data)
        else:
            animate_diff_prefs['motion_loras'].append(e.control.data)
    head_prompt = TextField(label="Head Prompt Text", value=animate_diff_prefs['head_prompt'], filled=False, multiline=True, on_change=lambda e:changed(e,'head_prompt'), col={'md':6})
    tail_prompt = TextField(label="Tail Prompt Text", value=animate_diff_prefs['tail_prompt'], filled=False, multiline=True, on_change=lambda e:changed(e,'tail_prompt'), col={'md':6})
    prompt = TextField(label="Animation Prompt Text", value=animate_diff_prefs['prompt'], filled=True, expand=True, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=animate_diff_prefs['negative_prompt'], expand=True, col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    seed = TextField(label="Seed", width=76, value=str(animate_diff_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'), col={'md':1})
    frame = TextField(label="Frame", width=76, value="0", filled=True, keyboard_type=KeyboardType.NUMBER, tooltip="")
    add_prompt_keyframe = ft.FilledButton("➕  Add Keyframe", on_click=add_prompt)
    video_length = SliderRow(label="Video Length", min=1, max=500, divisions=499, pref=animate_diff_prefs, key='video_length', tooltip="The number of frames to animate.")
    num_inference_row = SliderRow(label="Inference Steps", min=1, max=150, divisions=149, pref=animate_diff_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=animate_diff_prefs, key='guidance_scale')
    context = SliderRow(label="Context Frames to Condition", min=1, max=24, divisions=23, pref=animate_diff_prefs, key='context', expand=True, col={'md': 6}, tooltip="Number of frames to condition on. Drop to 8 on cards with less than 8GB VRAM, can raise it to 20-24 on cards with more. (default: max of <length> or 24)")
    stride = SliderRow(label="Max Motion Stride", min=1, max=8, divisions=7, pref=animate_diff_prefs, key='stride', expand=True, col={'md': 6}, tooltip="Max motion stride as a power of 2 (default: 4)")
    clip_skip = SliderRow(label="Clip Skip", min=0, max=4, divisions=4, pref=animate_diff_prefs, key='clip_skip', expand=True, col={'md': 6}, tooltip="Skips part of the image generation process, leading to slightly different results from the CLIP model.")
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=animate_diff_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=animate_diff_prefs, key='height')
    scheduler = Dropdown(label="Scheduler", options=[dropdown.Option("ddim"), dropdown.Option("pndm"), dropdown.Option("lms"), dropdown.Option("euler"), dropdown.Option("euler_a"), dropdown.Option("dpm_2"), dropdown.Option("k_dpm_2"), dropdown.Option("dpm_2_a"), dropdown.Option("k_dpm_2_a"), dropdown.Option("dpmpp_2m"), dropdown.Option("k_dpmpp_2m"), dropdown.Option("unipc"), dropdown.Option("dpmpp_sde"), dropdown.Option("k_dpmpp_sde"), dropdown.Option("dpmpp_2m_sde"), dropdown.Option("k_dpmpp_2m_sde")], width=176, value=animate_diff_prefs['scheduler'], on_change=lambda e: changed(e, 'scheduler'))
    #motion_module = Dropdown(label="Motion Module", options=[dropdown.Option("improved3DMotion"), dropdown.Option("mm_sd_v15_v2"), dropdown.Option("mm_sd_v15"), dropdown.Option("mm_sd_v14")], width=176, value=animate_diff_prefs['motion_module'], on_change=lambda e: changed(e, 'motion_module'))
    motion_module = Dropdown(label="Motion Module", options=[], width=220, value=animate_diff_prefs['motion_module'], on_change=lambda e: changed(e, 'motion_module'))
    for mm in animate_diff_motion_modules:
        motion_module.options.append(dropdown.Option(mm['name']))
    dreambooth_lora = Dropdown(label="DreamBooth LoRA", options=[dropdown.Option("Custom")], value=animate_diff_prefs['dreambooth_lora'], on_change=changed_lora)
    custom_lora = TextField(label="Custom LoRA Safetensor (URL or Path)", value=animate_diff_prefs['custom_lora'], expand=True, visible=animate_diff_prefs['dreambooth_lora']=="Custom", on_change=lambda e:changed(e,'custom_lora'))
    for lora in animate_diff_loras:
        dreambooth_lora.options.insert(1, dropdown.Option(lora['name']))
    lora_alpha = SliderRow(label="LoRA Alpha", min=0, max=1, divisions=10, round=1, expand=True, pref=animate_diff_prefs, key='lora_alpha', tooltip="The Weight of the custom LoRA Model to influence diffusion.")
    lora_layer = Dropdown(label="LoRA Layer Map", options=[dropdown.Option("Custom")], value=animate_diff_prefs['lora_layer'], on_change=changed_lora_layer)
    custom_lora_layer = TextField(label="Custom LoRA Safetensor (URL or Path)", value=animate_diff_prefs['custom_lora_layer'], expand=True, visible=animate_diff_prefs['lora_layer']=="Custom", on_change=lambda e:changed(e,'custom_lora_layer'))
    for lora in animate_diff_lora_layers:
        lora_layer.options.insert(1, dropdown.Option(lora['name']))
    lora_layer_alpha = SliderRow(label="LoRA Alpha", min=0, max=1, divisions=10, round=1, expand=True, pref=animate_diff_prefs, key='lora_layer_alpha', tooltip="The Weight of the custom LoRA Model to influence diffusion.")
    add_lora_layer = ft.FilledButton("➕  Add LoRA", on_click=add_lora)
    lora_layer_map = Column([], spacing=0)
    motion_loras_checkboxes = ResponsiveRow(controls=[Text("Motion Module LoRAs:", col={'xs':12, 'sm':6, 'md':3, 'lg':2, 'xl': 1.5})], run_spacing=0, vertical_alignment=CrossAxisAlignment.CENTER)
    for m in animate_diff_motion_loras:
        motion_loras_checkboxes.controls.append(Checkbox(label=m['name'], data=m['name'], value=m['name'] in animate_diff_prefs['motion_loras'], on_change=changed_motion_lora, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':2, 'xl': 1}))
    motion_loras_strength = SliderRow(label="Motion Module LoRA Strength", min=0, max=1, divisions=10, round=1, pref=animate_diff_prefs, key='motion_loras_strength', tooltip="The Weight of the custom Motion LoRA Module to influence camera.")
    save_frames = Switcher(label="Save Frames", value=animate_diff_prefs['save_frames'], on_change=lambda e:changed(e,'save_frames'))
    save_gif = Switcher(label="Save Animated GIF", value=animate_diff_prefs['save_gif'], on_change=lambda e:changed(e,'save_gif'))
    save_video = Switcher(label="Save Video", value=animate_diff_prefs['save_video'], on_change=lambda e:changed(e,'save_video'))
    is_loop = Switcher(label="Loop", value=animate_diff_prefs['is_loop'], on_change=lambda e:changed(e,'is_loop'))
    is_simple_composite = Switcher(label="Simple Composite", value=animate_diff_prefs['is_simple_composite'], on_change=lambda e:changed(e,'is_simple_composite'))
    apply_lcm_lora = Switcher(label="Apply LCM LoRA", value=animate_diff_prefs['apply_lcm_lora'], on_change=lambda e:changed(e,'apply_lcm_lora'))
    control_task = Dropdown(label="ControlNet Task", width=215, options=[dropdown.Option(t) for t in ['Canny', 'OpenPose', "SoftEdge", "Shuffle", "Depth", "Inpaint", "LineArt", "MLSD", "NormalBAE", "IP2P", "Scribble", "Seg", "LineArt", "LineArt_Anime", "Tile", "QR_Code_Monster_v1", "QR_Code_Monster_v2", "Mediapipe_Face", "AnimateDiff ControlNet"]], value=animate_diff_prefs['control_task'], on_change=lambda e:changed(e,'control_task'))
    original_image = FileInput(label="Original Image or Video Clip", pref=animate_diff_prefs, key='original_image', ftype="picture", expand=True, page=page)
    control_frame = TextField(label="Frame", width=76, value="0", keyboard_type=KeyboardType.NUMBER, tooltip="", on_change=lambda e:changed(e,'control_frame', ptype='int'))
    #, dropdown.Option("Scribble"), dropdown.Option("HED"), dropdown.Option("M-LSD"), dropdown.Option("Normal Map"), dropdown.Option("Shuffle"), dropdown.Option("Instruct Pix2Pix"), dropdown.Option("Brightness"), dropdown.Option("Video Canny Edge"), dropdown.Option("Video OpenPose")
    conditioning_scale = SliderRow(label="Conditioning Scale", min=0, max=2, divisions=20, round=1, expand=True, pref=animate_diff_prefs, key='conditioning_scale', tooltip="The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added to the residual in the original unet.")
    control_guidance_start = SliderRow(label="Control Guidance Start", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=animate_diff_prefs, key='control_guidance_start', tooltip="The percentage of total steps at which the controlnet starts applying.")
    control_guidance_end = SliderRow(label="Control Guidance End", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=animate_diff_prefs, key='control_guidance_end', tooltip="The percentage of total steps at which the controlnet stops applying.")
    ref_image = FileInput(label="Reference Image (optional)", pref=animate_diff_prefs, key='ref_image', page=page)
    control_scale_list  = TextField(label="Control Scale List", width=250, value=animate_diff_prefs['control_scale_list'], on_change=lambda e:changed(e,'control_scale_list'))
    #add_layer_btn = IconButton(icons.ADD, tooltip="Add Multi-ControlNetXL Layer", on_click=add_layer)
    add_layer_btn = ft.FilledButton("➕ Add Layer", width=140, on_click=add_layer)
    multi_layers = Column([], spacing=0)
    ip_adapter_layers = Column([], spacing=0)
    use_ip_adapter = Switcher(label="Use IP-Adapter Layers", value=animate_diff_prefs['use_ip_adapter'], on_change=toggle_ip_adapter)
    ip_adapter_frame = TextField(label="Frame", width=76, value="0", keyboard_type=KeyboardType.NUMBER, tooltip="", on_change=lambda e:changed(e,'ip_adapter_frame'))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=animate_diff_prefs, key='ip_adapter_image', expand=True, page=page)
    add_frame_btn = ft.FilledButton("➕ Add Frame", width=144, on_click=add_ip_layer)
    ip_adapter_scale = SliderRow(label="IP-Adapter Scale", min=0.0, max=1.0, divisions=20, round=2, expand=True, pref=animate_diff_prefs, key='ip_adapter_scale', tooltip="")
    ip_adapter_is_plus = Checkbox(label="IP-Adapter Plus", value=animate_diff_prefs['ip_adapter_is_plus'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'ip_adapter_is_plus'))
    ip_adapter_is_full_face = Checkbox(label="IP-Adapter Full Face", value=animate_diff_prefs['ip_adapter_is_full_face'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'ip_adapter_is_full_face'))
    ip_adapter_is_plus_face = Checkbox(label="IP-Adapter Plus Face", value=animate_diff_prefs['ip_adapter_is_plus_face'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'ip_adapter_is_plus_face'))
    ip_adapter_light = Checkbox(label="IP-Adapter Light", value=animate_diff_prefs['ip_adapter_light'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'ip_adapter_light'))
    ip_adapter_container = Container(animate_size=animation.Animation(700, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, alignment = alignment.top_left, height = None if animate_diff_prefs['use_ip_adapter'] else 0, padding=padding.only(top=4), content=Column([
      Row([ip_adapter_frame, ip_adapter_image, add_frame_btn]),
      ip_adapter_layers,
      Row([ip_adapter_scale, ip_adapter_is_plus, ip_adapter_is_full_face, ip_adapter_is_plus_face, ip_adapter_light]),
      Divider(thickness=2, height=4),
    ]))
    img2img_layers = Column([], spacing=0)
    use_img2img = Switcher(label="Use Image2Image Layers", value=animate_diff_prefs['use_img2img'], on_change=toggle_img2img)
    img2img_frame = TextField(label="Frame", width=76, value="0", keyboard_type=KeyboardType.NUMBER, tooltip="", on_change=lambda e:changed(e,'img2img_frame'))
    img2img_image = FileInput(label="Init Img2Img Image", pref=animate_diff_prefs, key='img2img_image', expand=True, page=page)
    add_img2img_frame_btn = ft.FilledButton("➕ Add Frame", width=144, on_click=add_img2img_layer)
    img2img_strength = SliderRow(label="Denoising Strength", min=0.0, max=1.0, divisions=20, round=2, expand=True, visible=False, pref=animate_diff_prefs, key='img2img_strength', tooltip="The smaller the value, the closer the result will be to the initial image.")
    img2img_container = Container(animate_size=animation.Animation(700, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, alignment = alignment.top_left, height = None if animate_diff_prefs['use_img2img'] else 0, padding=padding.only(top=4), content=Column([
      Row([img2img_frame, img2img_image, add_img2img_frame_btn]),
      img2img_layers,
      Divider(thickness=2, height=4),
    ]))

    upscale_tile = Switcher(label="Upscale Tile", value=animate_diff_prefs['upscale_tile'], on_change=lambda e:changed(e,'upscale_tile'))
    upscale_ip2p = Switcher(label="Upscale IP2P", value=animate_diff_prefs['upscale_ip2p'], on_change=lambda e:changed(e,'upscale_ip2p'))
    upscale_lineart_anime = Switcher(label="Upscale LineArt Anime", value=animate_diff_prefs['upscale_lineart_anime'], on_change=lambda e:changed(e,'upscale_lineart_anime'))
    upscale_steps = SliderRow(label="Upscale Steps", min=1, max=50, divisions=49, pref=animate_diff_prefs, key='upscale_steps', col={'md': 6}, tooltip="")
    upscale_strength = SliderRow(label="Upscale Strength", min=0, max=1, divisions=10, round=1, expand=True, pref=animate_diff_prefs, key='upscale_strength', col={'md': 6}, tooltip="")
    upscale_guidance_scale = SliderRow(label="Upscale Guidance", min=0, max=20, divisions=40, round=1, pref=animate_diff_prefs, col={'md': 6}, key='upscale_guidance_scale')
    upscale_slider = SliderRow(label="Upscale Amount", min=1, max=4, divisions=12, round=2, suffix="x", pref=animate_diff_prefs, col={'md': 6}, key='upscale_amount')
    context_schedule = Dropdown(label="Context Schedule", width=136, options=[dropdown.Option(t) for t in ['Uniform', 'Shuffle', "Composite"]], value=animate_diff_prefs['context_schedule'], on_change=lambda e:changed(e,'context_schedule'))
    batch_folder_name = TextField(label="Batch Folder Name (required)", value=animate_diff_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    num_videos = NumberPicker(label="Number of Videos: ", min=1, max=8, value=animate_diff_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))
    upscaler = UpscaleBlock(animate_diff_prefs)
    page.upscalers.append(upscaler)
    page.animate_diff_prompts = Column([], spacing=0)
    page.animate_diff_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.animate_diff_output.controls) > 0
    animation_prompts = Column([], spacing=0)
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("👫  AnimateDiff Enhanced Text-to-Video", "Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning...", actions=[save_default(animate_diff_prefs, ['editing_prompts', 'animation_prompts', 'prompt_map', 'lora_map', 'ip_adapter_image', 'ip_adapter_layers', 'original_image', 'controlnet_image', 'controlnet_layers', 'img2img_image', 'img2img_layers']), IconButton(icon=icons.HELP, tooltip="Help with AnimateDiff Settings", on_click=animate_diff_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        #ResponsiveRow([prompt, negative_prompt, seed]),
        #Row([Text("AnimateDiff Prompts", style=TextThemeStyle.TITLE_LARGE, weight=FontWeight.BOLD),
        #            Row([ft.FilledTonalButton("Clear Prompts", on_click=clear_animate_diff_prompts), ft.FilledButton("Add Diff Prompt", on_click=lambda e: edit_animate_diff(None))])], alignment=MainAxisAlignment.SPACE_BETWEEN),
        #page.animate_diff_prompts,
        ResponsiveRow([head_prompt, tail_prompt]),
        Row([frame, prompt, add_prompt_keyframe]),
        animation_prompts,
        Divider(thickness=2, height=4),
        Row([seed, negative_prompt]),
        num_inference_row,
        guidance,
        width_slider, height_slider,
        video_length,
        ResponsiveRow([context, stride, clip_skip]),
        Row([dreambooth_lora, custom_lora]),#, lora_alpha
        Row([lora_layer, custom_lora_layer, lora_layer_alpha, add_lora_layer]),
        lora_layer_map,
        Divider(thickness=4, height=4),
        Row([control_task, control_frame, original_image, add_layer_btn]),
        Row([control_scale_list,
        conditioning_scale]),
        Row([control_guidance_start, control_guidance_end]),
        multi_layers,
        Divider(thickness=2, height=4),
        use_ip_adapter, 
        ip_adapter_container,
        Row([use_img2img, img2img_strength]),
        img2img_container,
        ref_image,
        Row([upscale_tile, upscale_ip2p, upscale_lineart_anime, apply_lcm_lora]),
        ResponsiveRow([upscale_steps, upscale_guidance_scale]),
        ResponsiveRow([upscale_strength, upscale_slider]),
        #Row([Text("Enable Motion Module LoRAs:"), motion_loras_strength]),
        #ResponsiveRow([Container(Text("Motion Module LoRAs:"), col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}), motion_loras_checkboxes]),
        motion_loras_checkboxes,
        motion_loras_strength,
        Row([motion_module, scheduler, context_schedule, batch_folder_name]),
        Row([is_loop, save_frames, save_gif, save_video, is_simple_composite]),
        upscaler,
        #Row([jump_length, jump_n_sample, seed]),
        Row([
            ElevatedButton(content=Text("💚  Run AnimateDiff", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_animate_diff(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_animate_diff(page, from_list=True))
        ]),
        page.animate_diff_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

animatediff_img2video_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "animatediff-",
    "num_images": 1,
    "width": 1024 if prefs['higher_vram_mode'] else 512,
    "height":1024 if prefs['higher_vram_mode'] else 512,
    "guidance_scale": 7.5,
    'num_inference_steps': 50,
    "seed": 0,
    'init_image': '',
    'init_image_strength': 0.8,
    'video_length': 16,
    'fps': 8,
    'target_fps': 25,
    'latent_interpolation_method': "Slerp",
    'clip_skip': 1,
    'lora_alpha': 0.8,
    'custom_lora': '',
    'lora_layer': 'Analog.Redmond',
    'lora_layer_alpha': 0.8,
    'custom_lora_layer': '',
    'lora_map': [],
    'motion_loras': [],
    'motion_loras_strength': 0.5,
    'motion_module': 'animatediff-motion-adapter-v1-5-2',
    "animatediff_img2video_model": "Realistic_Vision_V5.1_noVAE",
    "custom_model": "",
    'use_ip_adapter': False,
    'ip_adapter_image': '',
    'ip_adapter_model': 'SD v1.5',
    'ip_adapter_strength': 0.8,
    'export_to_video': True,
    "interpolate_video": True,
    "cpu_offload": False,
    "free_init": False,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}
animatediff_motion_loras = [
    {'name': 'Zoom-In', 'path': 'guoyww/animatediff-motion-lora-zoom-in'},
    {'name': 'Zoom-Out', 'path': 'guoyww/animatediff-motion-lora-zoom-out'},
    {'name': 'Pan-Left', 'path': 'guoyww/animatediff-motion-lora-pan-left'},
    {'name': 'Pan-Right', 'path': 'guoyww/animatediff-motion-lora-pan-right'},
    {'name': 'Tilt-Up', 'path': 'guoyww/animatediff-motion-lora-tilt-up'},
    {'name': 'Tilt-Down', 'path': 'guoyww/animatediff-motion-lora-tilt-down'},
    {'name': 'Rolling-Clockwise', 'path': 'guoyww/animatediff-motion-lora-rolling-clockwise'},
    {'name': 'Rolling-Anticlockwise', 'path': 'guoyww/animatediff-motion-lora-rolling-anticlockwise'},
    #{'name': 'Custom', 'path': ''},
]

def buildAnimateDiffImage2Video(page):
    global prefs, animatediff_img2video_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          animatediff_img2video_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def animatediff_img2video_help(e):
      def close_animatediff_img2video_dlg(e):
        nonlocal animatediff_img2video_help_dlg
        animatediff_img2video_help_dlg.open = False
        page.update()
      animatediff_img2video_help_dlg = AlertDialog(title=Text("🙅   Help with AnimateDiff Image2Video Pipeline"), content=Column([
          Text("Experimental Image-To-Video support for AnimateDiff (open to improvements)."),
          Text("AnimateDiff can also be used to generate visually similar videos or enable style/character/background or other edits starting from an initial video, allowing you to seamlessly explore creative possibilities."),
          Markdown("[Diffusers Project](https://github.com/huggingface/diffusers/pull/6328) | [Colab](https://drive.google.com/file/d/1TvzCDPHhfFtdcJZe4RLloAwyoLKuttWK/view?usp=sharing) | [Aryan V S](https://github.com/a-r-r-o-w)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          #Markdown("The pipelines were contributed by [luosiallen](https://luosiallen.github.io/), [nagolinc](https://github.com/nagolinc), and [dg845](https://github.com/dg845).", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😈  It's Alive!", on_click=close_animatediff_img2video_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(animatediff_img2video_help_dlg)
      animatediff_img2video_help_dlg.open = True
      page.update()
    def changed_lora_layer(e):
      animatediff_img2video_prefs['lora_layer'] = e.control.value
      custom_lora_layer.visible = e.control.value == "Custom"
      custom_lora_layer.update()
    def add_lora(e):
      lora = animatediff_img2video_prefs['lora_layer']
      lora_scale = animatediff_img2video_prefs['lora_layer_alpha']
      lora_layer = {}
      if lora == "Custom":
        lora_layer = {'name': 'Custom', 'file':'', 'path':animatediff_img2video_prefs['custom_lora_layer'], 'scale': lora_scale}
      else:
        for l in LoRA_models:
          if l['name'] == lora:
            lora_layer = l.copy()
            lora_layer['scale'] = lora_scale
        for l in animatediff_img2video_prefs['lora_map']:
          if l['name'] == lora:
            return
      animatediff_img2video_prefs['lora_map'].append(lora_layer)
      title = Markdown(f"**{lora_layer['name']}** - Alpha Scale: [{lora_layer['scale']}] - {lora_layer['path']}")
      lora_layer_map.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
        items=[
            PopupMenuItem(icon=icons.DELETE, text="Delete LoRA Layer", on_click=delete_lora_layer, data=lora_layer),
            PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_lora_layers, data=lora_layer),
        ]), data=lora_layer))
      lora_layer_map.update()
    def delete_lora_layer(e):
        for l in animatediff_img2video_prefs['lora_map']:
          if l['name'] == e.control.data['name']:
            animatediff_img2video_prefs['lora_map'].remove(l)
          #del l #animatediff_img2video_prefs['lora_map'][]
        for c in lora_layer_map.controls:
          if c.data['name'] == e.control.data['name']:
             lora_layer_map.controls.remove(c)
             break
        lora_layer_map.update()
    def delete_all_lora_layers(e):
        animatediff_img2video_prefs['lora_map'].clear()
        lora_layer_map.controls.clear()
        lora_layer_map.update()
    def toggle_ip_adapter(e):
      animatediff_img2video_prefs['use_ip_adapter'] = e.control.value
      ip_adapter_container.height=None if animatediff_img2video_prefs['use_ip_adapter'] else 0
      ip_adapter_container.update()
    def changed_model(e):
        animatediff_img2video_prefs['animatediff_img2video_model'] = e.control.value
        animatediff_img2video_custom_model.visible = e.control.value == "Custom"
        animatediff_img2video_custom_model.update()
    def changed_motion_lora(e):
        on = e.control.value
        if e.control.data in animatediff_img2video_prefs['motion_loras']:
            animatediff_img2video_prefs['motion_loras'].remove(e.control.data)
        else:
            animatediff_img2video_prefs['motion_loras'].append(e.control.data)
    prompt = TextField(label="Prompt Text", value=animatediff_img2video_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=animatediff_img2video_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = FileInput(label="Init Image or Video", pref=animatediff_img2video_prefs, key='init_image', ftype="picture", page=page, col={'md':6})
    init_image_strength = SliderRow(label="Init-Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=animatediff_img2video_prefs, key='init_image_strength', col={'md':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    video_length = SliderRow(label="Video Length", min=1, max=64, divisions=63, pref=animatediff_img2video_prefs, key='video_length', tooltip="The number of frames to animate.")
    batch_folder_name = TextField(label="Batch Folder Name", value=animatediff_img2video_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=animatediff_img2video_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Videos", min=1, max=9, step=1, value=animatediff_img2video_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=80, divisions=80, pref=animatediff_img2video_prefs, key='num_inference_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=animatediff_img2video_prefs, key='guidance_scale')
    clip_skip = SliderRow(label="Clip Skip", min=0, max=4, divisions=4, pref=animatediff_img2video_prefs, key='clip_skip', expand=True, col={'md': 6}, tooltip="Skips part of the image generation process, leading to slightly different results from the LoRA CLIP model.")
    width_slider = SliderRow(label="Width", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=animatediff_img2video_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=animatediff_img2video_prefs, key='height')
    def toggle_ip_adapter(e):
        animatediff_img2video_prefs['use_ip_adapter'] = e.control.value
        ip_adapter_container.height = None if e.control.value else 0
        ip_adapter_container.update()
        ip_adapter_model.visible = e.control.value
        ip_adapter_model.update()
    use_ip_adapter = Switcher(label="Use IP-Adapter Reference Image", value=animatediff_img2video_prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip="Uses both image and text to condition the image generation process.")
    ip_adapter_model = Dropdown(label="IP-Adapter SD Model", width=220, options=[], value=animatediff_img2video_prefs['ip_adapter_model'], visible=animatediff_img2video_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))
    for m in ip_adapter_models:
        ip_adapter_model.options.append(dropdown.Option(m['name']))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=animatediff_img2video_prefs, key='ip_adapter_image', page=page, col={'lg':6})
    ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=animatediff_img2video_prefs, key='ip_adapter_strength', col={'lg':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    ip_adapter_container = Container(Column([ResponsiveRow([ip_adapter_image, ip_adapter_strength]), Divider(thickness=4, height=4)]), height = None if animatediff_img2video_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    lora_alpha = SliderRow(label="LoRA Alpha", min=0, max=1, divisions=10, round=1, expand=True, pref=animatediff_img2video_prefs, key='lora_alpha', tooltip="The Weight of the custom LoRA Model to influence diffusion.")
    lora_layer = Dropdown(label="LoRA Layer Map", options=[dropdown.Option("Custom")], value=animatediff_img2video_prefs['lora_layer'], on_change=changed_lora_layer)
    custom_lora_layer = TextField(label="Custom LoRA Safetensor (URL or Path)", value=animatediff_img2video_prefs['custom_lora_layer'], expand=True, visible=animatediff_img2video_prefs['lora_layer']=="Custom", on_change=lambda e:changed(e,'custom_lora_layer'))
    if len(prefs['custom_LoRA_models']) > 0:
        for l in prefs['custom_LoRA_models']:
            lora_layer.options.append(dropdown.Option(l['name']))
    for m in LoRA_models:
        lora_layer.options.append(dropdown.Option(m['name']))
    #lora_layer.options.append(dropdown.Option("Custom LoRA Path"))
    #for lora in animatediff_motion_loras:
    #    lora_layer.options.insert(1, dropdown.Option(lora['name']))
    lora_layer_alpha = SliderRow(label="LoRA Alpha", min=0, max=1, divisions=10, round=1, expand=True, pref=animatediff_img2video_prefs, key='lora_layer_alpha', tooltip="The Weight of the custom LoRA Model to influence diffusion.")
    add_lora_layer = ft.FilledButton("➕  Add LoRA", on_click=add_lora)
    lora_layer_map = Column([], spacing=0)
    motion_loras_checkboxes = ResponsiveRow(controls=[Text("Motion Module LoRAs:", col={'xs':12, 'sm':6, 'md':3, 'lg':2, 'xl': 1.5})], run_spacing=0, vertical_alignment=CrossAxisAlignment.CENTER)
    for m in animatediff_motion_loras:
        motion_loras_checkboxes.controls.append(Checkbox(label=m['name'], data=m['name'], value=m['name'] in animatediff_img2video_prefs['motion_loras'], on_change=changed_motion_lora, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':2, 'xl': 1}))
    motion_loras_strength = SliderRow(label="Motion Module LoRA Strength", min=0, max=1, divisions=10, round=1, pref=animatediff_img2video_prefs, key='motion_loras_strength', tooltip="The Weight of the custom Motion LoRA Module to influence camera.")
    animatediff_img2video_model = Dropdown(label="AnimateDiff Model", width=250, options=[dropdown.Option("Custom"), dropdown.Option("Realistic_Vision_V5.1_noVAE"), dropdown.Option("dreamshaper-8"), dropdown.Option("epiCRealism")], value=animatediff_img2video_prefs['animatediff_img2video_model'], on_change=changed_model)
    animatediff_img2video_custom_model = TextField(label="Custom AnimateDiff Model (URL or Path)", value=animatediff_img2video_prefs['custom_model'], expand=True, visible=animatediff_img2video_prefs['animatediff_img2video_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    cpu_offload = Switcher(label="CPU Offload", value=animatediff_img2video_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    free_init = Switcher(label="Free-Init", value=animatediff_img2video_prefs['free_init'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'free_init'), tooltip="Improves temporal consistency and overall quality of videos generated using video-diffusion-models without any addition training.")
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=animatediff_img2video_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    interpolate_video = Switcher(label="Interpolate Video", value=animatediff_img2video_prefs['interpolate_video'], tooltip="Use Google FiLM Interpolation to transition between frames.", on_change=lambda e:changed(e,'interpolate_video'))
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=animatediff_img2video_prefs, key='fps', col={'md': 6}, tooltip="The rate at which the generated images shall be exported to a video after generation. Note that Stable Diffusion Video's UNet was micro-conditioned on fps-1 during training.")
    latent_interpolation_method = Dropdown(label="Latent Interpolation", width=140, options=[dropdown.Option("Slerp"), dropdown.Option("Lerp")], value=animatediff_img2video_prefs['latent_interpolation_method'], on_change=lambda e: changed(e, 'latent_interpolation_method'))
    seed = TextField(label="Seed", width=90, value=str(animatediff_img2video_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(animatediff_img2video_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🐲   Run AnimateDiff", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_animatediff_img2video(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_animatediff_img2video(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_animatediff_img2video(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.animatediff_img2video_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🐉  AnimateDiff Image/Video-to-Video", "Bring an Image or Video Clip to Life! Similar to Stable Video Diffusion, with more control...", actions=[save_default(animatediff_img2video_prefs, ['init_image', 'ip_adapter_image', 'lora_map']), IconButton(icon=icons.HELP, tooltip="Help with AnimateDiff Image2Video Settings", on_click=animatediff_img2video_help)]),
            ResponsiveRow([init_image, init_image_strength]),
            ResponsiveRow([prompt, negative_prompt]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            video_length,
            ResponsiveRow([fps, clip_skip]),
            Row([use_ip_adapter, ip_adapter_model], vertical_alignment=CrossAxisAlignment.START),
            ip_adapter_container,
            Row([lora_layer, custom_lora_layer, lora_layer_alpha, add_lora_layer]),
            lora_layer_map,
            Divider(thickness=4, height=4),
            motion_loras_checkboxes,
            motion_loras_strength,
            Divider(thickness=4, height=4),
            Row([animatediff_img2video_model, animatediff_img2video_custom_model, latent_interpolation_method]),
            Row([cpu_offload, export_to_video, interpolate_video]),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.animatediff_img2video_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

animatediff_sdxl_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "animatediff-",
    "num_images": 1,
    "width": 1024 if prefs['higher_vram_mode'] else 512,
    "height":1024 if prefs['higher_vram_mode'] else 512,
    "guidance_scale": 7.5,
    'num_inference_steps': 50,
    "seed": 0,
    'init_image': '',
    'init_image_strength': 0.8,
    'video_length': 16,
    'fps': 8,
    'target_fps': 25,
    'latent_interpolation_method': "Slerp",
    'clip_skip': 1,
    'lora_alpha': 0.8,
    'custom_lora': '',
    'lora_layer': '3D Redmond',
    'lora_layer_alpha': 0.8,
    'custom_lora_layer': '',
    'lora_map': [],
    'motion_loras': [],
    'motion_loras_strength': 0.5,
    'motion_module': 'animatediff-motion-adapter-sdxl-beta',
    "animatediff_sdxl_model": "Realistic_Vision_V5.1_noVAE",
    "custom_model": "",
    'use_ip_adapter': False,
    'ip_adapter_image': '',
    'ip_adapter_SDXL_model': 'SDXL',
    'ip_adapter_strength': 0.8,
    'export_to_video': True,
    "interpolate_video": True,
    "cpu_offload": False,
    "free_init": True,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildAnimateDiffSDXL(page):
    global prefs, animatediff_sdxl_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          animatediff_sdxl_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def animatediff_sdxl_help(e):
      def close_animatediff_sdxl_dlg(e):
        nonlocal animatediff_sdxl_help_dlg
        animatediff_sdxl_help_dlg.open = False
        page.update()
      animatediff_sdxl_help_dlg = AlertDialog(title=Text("🙅   Help with AnimateDiff SDXL Pipeline"), content=Column([
          Text("Experimental Stable Diffusion XL support for AnimateDiff (open to improvements). This is currently an experimental feature as only a beta release of the motion adapter checkpoint is available."),
          Text("AnimateDiff can also be used to generate visually similar videos or enable style/character/background, allowing you to seamlessly explore creative possibilities."),
          Markdown("[Diffusers Project](https://github.com/huggingface/diffusers/pull/6721) | [Motion Module](https://github.com/guoyww/AnimateDiff/tree/sdxl) | [Colab](https://colab.research.google.com/drive/1076fX0AtMgYNdQyYF-mwug8Bl2SfZ8Be) | [Aryan V S (a-r-r-o-w)](https://github.com/a-r-r-o-w)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          #Markdown("The pipelines were contributed by [luosiallen](https://luosiallen.github.io/), [nagolinc](https://github.com/nagolinc), and [dg845](https://github.com/dg845).", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🤩  Might Wow", on_click=close_animatediff_sdxl_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(animatediff_sdxl_help_dlg)
      animatediff_sdxl_help_dlg.open = True
      page.update()
    def changed_lora_layer(e):
      animatediff_sdxl_prefs['lora_layer'] = e.control.value
      custom_lora_layer.visible = e.control.value == "Custom"
      custom_lora_layer.update()
    def add_lora(e):
      lora = animatediff_sdxl_prefs['lora_layer']
      lora_scale = animatediff_sdxl_prefs['lora_layer_alpha']
      lora_layer = {}
      if lora == "Custom":
        lora_layer = {'name': 'Custom', 'file':'', 'path':animatediff_sdxl_prefs['custom_lora_layer'], 'scale': lora_scale}
      else:
        for l in SDXL_LoRA_models:
          if l['name'] == lora:
            lora_layer = l.copy()
            lora_layer['scale'] = lora_scale
        for l in animatediff_sdxl_prefs['lora_map']:
          if l['name'] == lora:
            return
      animatediff_sdxl_prefs['lora_map'].append(lora_layer)
      title = Markdown(f"**{lora_layer['name']}** - Alpha Scale: [{lora_layer['scale']}] - {lora_layer['path']}")
      lora_layer_map.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
        items=[
            PopupMenuItem(icon=icons.DELETE, text="Delete LoRA Layer", on_click=delete_lora_layer, data=lora_layer),
            PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_lora_layers, data=lora_layer),
        ]), data=lora_layer))
      lora_layer_map.update()
    def delete_lora_layer(e):
        for l in animatediff_sdxl_prefs['lora_map']:
          if l['name'] == e.control.data['name']:
            animatediff_sdxl_prefs['lora_map'].remove(l)
          #del l #animatediff_sdxl_prefs['lora_map'][]
        for c in lora_layer_map.controls:
          if c.data['name'] == e.control.data['name']:
             lora_layer_map.controls.remove(c)
             break
        lora_layer_map.update()
    def delete_all_lora_layers(e):
        animatediff_sdxl_prefs['lora_map'].clear()
        lora_layer_map.controls.clear()
        lora_layer_map.update()
    def toggle_ip_adapter(e):
      animatediff_sdxl_prefs['use_ip_adapter'] = e.control.value
      ip_adapter_container.height=None if animatediff_sdxl_prefs['use_ip_adapter'] else 0
      ip_adapter_container.update()
    def changed_model(e):
        animatediff_sdxl_prefs['animatediff_sdxl_model'] = e.control.value
        animatediff_sdxl_custom_model.visible = e.control.value == "Custom"
        animatediff_sdxl_custom_model.update()
    def changed_motion_lora(e):
        on = e.control.value
        if e.control.data in animatediff_sdxl_prefs['motion_loras']:
            animatediff_sdxl_prefs['motion_loras'].remove(e.control.data)
        else:
            animatediff_sdxl_prefs['motion_loras'].append(e.control.data)
    prompt = TextField(label="Prompt Text", value=animatediff_sdxl_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=animatediff_sdxl_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = FileInput(label="Init Image or Video", pref=animatediff_sdxl_prefs, key='init_image', ftype="picture", page=page, col={'md':6})
    init_image_strength = SliderRow(label="Init-Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=animatediff_sdxl_prefs, key='init_image_strength', col={'md':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    video_length = SliderRow(label="Video Length", min=1, max=64, divisions=63, pref=animatediff_sdxl_prefs, key='video_length', tooltip="The number of frames to animate.")
    batch_folder_name = TextField(label="Batch Folder Name", value=animatediff_sdxl_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=animatediff_sdxl_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Videos", min=1, max=9, step=1, value=animatediff_sdxl_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=80, divisions=80, pref=animatediff_sdxl_prefs, key='num_inference_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=animatediff_sdxl_prefs, key='guidance_scale')
    clip_skip = SliderRow(label="Clip Skip", min=0, max=4, divisions=4, pref=animatediff_sdxl_prefs, key='clip_skip', expand=True, col={'md': 6}, tooltip="Skips part of the image generation process, leading to slightly different results from the LoRA CLIP model.")
    width_slider = SliderRow(label="Width", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=animatediff_sdxl_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=animatediff_sdxl_prefs, key='height')
    def toggle_ip_adapter(e):
        animatediff_sdxl_prefs['use_ip_adapter'] = e.control.value
        ip_adapter_container.height = None if e.control.value else 0
        ip_adapter_container.update()
        ip_adapter_SDXL_model.visible = e.control.value
        ip_adapter_SDXL_model.update()
    use_ip_adapter = Switcher(label="Use IP-Adapter Reference Image", value=animatediff_sdxl_prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip="Uses both image and text to condition the image generation process.")
    ip_adapter_SDXL_model = Dropdown(label="IP-Adapter SDXL Model", width=220, options=[], value=animatediff_sdxl_prefs['ip_adapter_SDXL_model'], visible=animatediff_sdxl_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_SDXL_model'))
    for m in ip_adapter_SDXL_models:
        ip_adapter_SDXL_model.options.append(dropdown.Option(m['name']))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=animatediff_sdxl_prefs, key='ip_adapter_image', page=page, col={'lg':6})
    ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=animatediff_sdxl_prefs, key='ip_adapter_strength', col={'lg':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    ip_adapter_container = Container(Column([ResponsiveRow([ip_adapter_image, ip_adapter_strength]), Divider(thickness=4, height=4)]), height = None if animatediff_sdxl_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    lora_alpha = SliderRow(label="LoRA Alpha", min=0, max=1, divisions=10, round=1, expand=True, pref=animatediff_sdxl_prefs, key='lora_alpha', tooltip="The Weight of the custom LoRA Model to influence diffusion.")
    lora_layer = Dropdown(label="LoRA Layer Map", options=[dropdown.Option("Custom")], value=animatediff_sdxl_prefs['lora_layer'], on_change=changed_lora_layer)
    custom_lora_layer = TextField(label="Custom LoRA Safetensor (URL or Path)", value=animatediff_sdxl_prefs['custom_lora_layer'], expand=True, visible=animatediff_sdxl_prefs['lora_layer']=="Custom", on_change=lambda e:changed(e,'custom_lora_layer'))
    if len(prefs['custom_SDXL_LoRA_models']) > 0:
        for l in prefs['custom_SDXL_LoRA_models']:
            lora_layer.options.append(dropdown.Option(l['name']))
    for m in SDXL_LoRA_models:
        lora_layer.options.append(dropdown.Option(m['name']))
    #lora_layer.options.append(dropdown.Option("Custom SDXL LoRA Path"))
    #for lora in animatediff_motion_loras:
    #    lora_layer.options.insert(1, dropdown.Option(lora['name']))
    lora_layer_alpha = SliderRow(label="LoRA Alpha", min=0, max=1, divisions=10, round=1, expand=True, pref=animatediff_sdxl_prefs, key='lora_layer_alpha', tooltip="The Weight of the custom LoRA Model to influence diffusion.")
    add_lora_layer = ft.FilledButton("➕  Add LoRA", on_click=add_lora)
    lora_layer_map = Column([], spacing=0)
    motion_loras_checkboxes = ResponsiveRow(controls=[Text("Motion Module LoRAs:", col={'xs':12, 'sm':6, 'md':3, 'lg':2, 'xl': 1.5})], run_spacing=0, vertical_alignment=CrossAxisAlignment.CENTER)
    for m in animatediff_motion_loras:
        motion_loras_checkboxes.controls.append(Checkbox(label=m['name'], data=m['name'], value=m['name'] in animatediff_sdxl_prefs['motion_loras'], on_change=changed_motion_lora, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':2, 'xl': 1}))
    motion_loras_strength = SliderRow(label="Motion Module LoRA Strength", min=0, max=1, divisions=10, round=1, pref=animatediff_sdxl_prefs, key='motion_loras_strength', tooltip="The Weight of the custom Motion LoRA Module to influence camera.")
    animatediff_sdxl_model = Dropdown(label="AnimateDiff Model", width=250, options=[dropdown.Option("Custom"), dropdown.Option("Realistic_Vision_V5.1_noVAE"), dropdown.Option("dreamshaper-8"), dropdown.Option("epiCRealism")], value=animatediff_sdxl_prefs['animatediff_sdxl_model'], on_change=changed_model)
    animatediff_sdxl_custom_model = TextField(label="Custom AnimateDiff Model (URL or Path)", value=animatediff_sdxl_prefs['custom_model'], expand=True, visible=animatediff_sdxl_prefs['animatediff_sdxl_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    cpu_offload = Switcher(label="CPU Offload", value=animatediff_sdxl_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    free_init = Switcher(label="Free-Init", value=animatediff_sdxl_prefs['free_init'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'free_init'), tooltip="Improves temporal consistency and overall quality of videos generated using video-diffusion-models without any addition training.")
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=animatediff_sdxl_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    interpolate_video = Switcher(label="Interpolate Video", value=animatediff_sdxl_prefs['interpolate_video'], tooltip="Use Google FiLM Interpolation to transition between frames.", on_change=lambda e:changed(e,'interpolate_video'))
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=animatediff_sdxl_prefs, key='fps', col={'md': 6}, tooltip="The rate at which the generated images shall be exported to a video after generation. Note that Stable Diffusion Video's UNet was micro-conditioned on fps-1 during training.")
    latent_interpolation_method = Dropdown(label="Latent Interpolation", width=140, options=[dropdown.Option("Slerp"), dropdown.Option("Lerp")], value=animatediff_sdxl_prefs['latent_interpolation_method'], on_change=lambda e: changed(e, 'latent_interpolation_method'))
    seed = TextField(label="Seed", width=90, value=str(animatediff_sdxl_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(animatediff_sdxl_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🙀   Run AnimateDiff SDXL", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_animatediff_sdxl(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_animatediff_sdxl(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_animatediff_sdxl(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.animatediff_sdxl_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🤯  AnimateDiff SDXL", "Create Video Clips from Text Prompt with SDXL Model and Beta Motion Module...", actions=[save_default(animatediff_sdxl_prefs, ['init_image', 'ip_adapter_image', 'lora_map']), IconButton(icon=icons.HELP, tooltip="Help with AnimateDiff SDXL Settings", on_click=animatediff_sdxl_help)]),
            #ResponsiveRow([init_image, init_image_strength]),
            ResponsiveRow([prompt, negative_prompt]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            video_length,
            ResponsiveRow([fps, clip_skip]),
            Row([use_ip_adapter, ip_adapter_SDXL_model], vertical_alignment=CrossAxisAlignment.START),
            ip_adapter_container,
            Row([lora_layer, custom_lora_layer, lora_layer_alpha, add_lora_layer]),
            lora_layer_map,
            Divider(thickness=4, height=4),
            #motion_loras_checkboxes,
            #motion_loras_strength,
            #Divider(thickness=4, height=4),
            #Row([animatediff_sdxl_model, animatediff_sdxl_custom_model, latent_interpolation_method]),
            Row([free_init, cpu_offload, export_to_video, interpolate_video]),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.animatediff_sdxl_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

diffsynth_prefs = {
    'prompt': '',
    'negative_prompt': '',
    'init_image': '',
    'init_video': '',
    'diffsynth_mode': 'video_rerender',
    'diffsynth_model': 'DiffSynth-img2vid-XT',
    'guidance_scale': 7.5,
    'min_guidance_scale': 1.0,
    'max_guidance_scale': 3.0,
    'num_inference_steps': 50,
    'controlnet_strength': 0.5,
    'fps': 8,
    'target_fps': 30,
    'motion_bucket_id': 127,  # 180
    'stride': 4,
    'batch_size': 8,  # 3-8
    'clip_skip': 1,
    'contrast_enhance_scale': 1.2,
    'num_frames': 64,
    'export_to_gif': True,
    'export_to_video': True,
    "interpolate_video": False,
    'seed': 0,
    'max_size': 768 if prefs['higher_vram_mode'] else 512,
    'width': 1024,
    'height': 576,
    'cpu_offload': not prefs['higher_vram_mode'],
    'num_videos': 1,
    'file_prefix': 'diffsynth-',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}


def buildDiffSynth(page):
    global diffsynth_prefs, prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
            try:
                diffsynth_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
            except Exception:
                alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
                pass
    def diffsynth_help(e):
        def close_diffsynth_dlg(e):
            nonlocal diffsynth_help_dlg
            diffsynth_help_dlg.open = False
            page.update()
        diffsynth_help_dlg = AlertDialog(title=Text("💁   Help with DiffSynth Image-To-Video"), content=Column([
            Text("DiffSynth Studio is a Diffusion engine. We have restructured architectures including Text Encoder, UNet, VAE, among others, maintaining compatibility with models from the open-source community while enhancing computational performance. We provide many interesting features. Enjoy the magic of Diffusion models!"),
            Markdown("[DiffSynth](https://ecnu-cilab.github.io/DiffSynth.github.io/) | [ExVideo](https://ecnu-cilab.github.io/ExVideoProjectPage/) | [Diffutoon](https://ecnu-cilab.github.io/DiffutoonProjectPage/) | [Huggingface Model](https://huggingface.co/ameerazam08/DiffSynth-Studio) | [GitHub repository](https://github.com/modelscope/DiffSynth-Studio) | [Paper](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🆒  Extra cool... ", on_click=close_diffsynth_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(diffsynth_help_dlg)
        diffsynth_help_dlg.open = True
        page.update()
    def change_mode(e):
        mode = e.data.split('"')[1]
        diffsynth_prefs['diffsynth_mode'] = mode
        init_image.show = mode in ["exvideo_svd", "svd_text_to_video", "sd_text_to_video"]
        init_video.show = mode in ["video_rerender", "diffutoon", "exvideo_svd", "sd_toon_shading"]
        text_prompts.visible = mode in ["video_rerender", "exvideo_svd", "sd_text_to_video", "sdxl_text_to_video", "svd_text_to_video", "diffutoon", "sd_toon_shading"]
        text_prompts.update()
        num_inference_row.show = mode in ["video_rerender", "exvideo_svd", "sd_text_to_video", "sdxl_text_to_video", "svd_text_to_video", "diffutoon", "sd_toon_shading"]
        guidance.show = mode in ["video_rerender", "exvideo_svd", "sd_text_to_video", "sdxl_text_to_video", "svd_text_to_video", "diffutoon"]
        guidance_row.visible = mode in ["exvideo_svd"]
        guidance_row.update()
        batch_size.show = mode in ["video_rerender", "diffutoon", "sd_toon_shading", "sd_text_to_video"]
        stride.show = mode in ["video_rerender", "diffutoon", "sd_toon_shading"]
        controlnet_strength.show = mode in ["video_rerender", "diffutoon"]
        clip_skip.show = mode in ["diffutoon", "sd_toon_shading"]
        motion_bucket_id.show = mode in ["exvideo_svd", "svd_text_to_video"]
        num_frames.show = mode in ["exvideo_svd", "sd_text_to_video", "sdxl_text_to_video", "svd_text_to_video", "diffutoon"]
    selected_mode = ft.SegmentedButton(on_change=change_mode, selected={diffsynth_prefs['diffsynth_mode']}, allow_multiple_selection=False,
                                       segments=[
        ft.Segment(value="video_rerender", label=ft.Text("Video ReRender"), icon=ft.Icon(ft.icons.ONDEMAND_VIDEO), tooltip="Video stylization without video models."),
        ft.Segment(value="exvideo_svd", label=ft.Text("ExVideo SVD"), icon=ft.Icon(ft.icons.ONDEMAND_VIDEO), tooltip="Post-tuning technique aimed at enhancing the capability of video generation models. Up to 128 frames."),
        ft.Segment(value="sd_text_to_video", label=ft.Text("SD Text-to-Video"), icon=ft.Icon(ft.icons.ONDEMAND_VIDEO)),
        ft.Segment(value="sdxl_text_to_video", label=ft.Text("SDXL Text-to-Video"), icon=ft.Icon(ft.icons.ONDEMAND_VIDEO)),
        ft.Segment(value="svd_text_to_video", label=ft.Text("SVD Text-to-Video"), icon=ft.Icon(ft.icons.ONDEMAND_VIDEO)),
        ft.Segment(value="diffutoon", label=ft.Text("Diffutoon Shading"), icon=ft.Icon(ft.icons.ONDEMAND_VIDEO), tooltip="Render realistic videos in a flatten style and enable video editing features."),
        ft.Segment(value="sd_toon_shading", label=ft.Text("SD Toon Shading"), icon=ft.Icon(ft.icons.ONDEMAND_VIDEO), tooltip="Render realistic videos in a flatten style and enable video editing features."),
    ],
    )
    prompt = TextField(label="Animation Prompt Text", value=diffsynth_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e: changed(e, 'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=diffsynth_prefs['negative_prompt'], filled=True, col={'md': 3}, on_change=lambda e: changed(e, 'negative_prompt'))
    text_prompts = ResponsiveRow([prompt, negative_prompt], visible=diffsynth_prefs['diffsynth_mode'] in ["video_rerender", "exvideo_svd", "sd_text_to_video", "sdxl_text_to_video", "svd_text_to_video", "diffutoon"])
    init_image = FileInput(label="Initial Image (optional)", pref=diffsynth_prefs, key='init_image', filled=True, page=page, visible=diffsynth_prefs['diffsynth_mode'] == "exvideo_svd")
    init_video = FileInput(label="Initial Video", pref=diffsynth_prefs, key='init_video', ftype="video", page=page, visible=diffsynth_prefs['diffsynth_mode'] in ["video_rerender", "diffutoon", "exvideo_svd"])
    # diffsynth_model = Dropdown(label="DiffSynth Model", width=200, options=[dropdown.Option("DiffSynth-img2vid-XT"), dropdown.Option("DiffSynth-img2vid"), dropdown.Option("DiffSynth-img2vid-XT 1.1")], value=diffsynth_prefs['diffsynth_model'], on_change=lambda e: changed(e, 'diffsynth_model'))
    num_frames = SliderRow(label="Number of Frames", min=1, max=300, divisions=299, pref=diffsynth_prefs, key='num_frames', visible=diffsynth_prefs['diffsynth_mode'] in ["exvideo_svd", "sd_text_to_video", "sdxl_text_to_video", "svd_text_to_video", "diffutoon"], tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=diffsynth_prefs, key='num_inference_steps', visible=diffsynth_prefs['diffsynth_mode'] in ["video_rerender", "exvideo_svd", "sd_text_to_video", "sdxl_text_to_video", "svd_text_to_video", "diffutoon"], tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=diffsynth_prefs, key='guidance_scale', visible=diffsynth_prefs['diffsynth_mode'] in ["video_rerender", "exvideo_svd", "sd_text_to_video", "sdxl_text_to_video", "svd_text_to_video", "diffutoon"])
    min_guidance = SliderRow(label="Min Guidance Scale", min=0, max=10, divisions=20, round=1, pref=diffsynth_prefs, key='min_guidance_scale', col={'sm': 6}, tooltip="Used for the classifier free guidance with first frame.")
    max_guidance = SliderRow(label="Max Guidance Scale", min=0, max=10, divisions=20, round=1, pref=diffsynth_prefs, key='max_guidance_scale', col={'sm': 6}, tooltip="Used for the classifier free guidance with last frame.")
    guidance_row = ResponsiveRow([min_guidance, max_guidance], visible=diffsynth_prefs['diffsynth_mode'] == "exvideo_svd")
    controlnet_strength = SliderRow(label="ControlNet Strength", min=0.0, max=1.0, divisions=20, round=2, pref=diffsynth_prefs, key='controlnet_strength', visible=diffsynth_prefs['diffsynth_mode'] in ["video_rerender", "diffutoon"], tooltip="How much influence the controlnet annotator's output is used to guide the denoising process.")
    clip_skip = SliderRow(label="Clip Skip", min=0, max=4, divisions=4, pref=diffsynth_prefs, key='clip_skip', expand=True, col={'md': 6}, tooltip="Recommeded to leave at 1 for this Pipeline. Skips part of the image generation process, leading to slightly different results from the LoRA CLIP model.")
    motion_bucket_id = SliderRow(label="Motion Bucket ID", min=1, max=300, divisions=299, pref=diffsynth_prefs, key='motion_bucket_id', visible=diffsynth_prefs['diffsynth_mode'] == "exvideo_svd", tooltip="Increasing the motion bucket id will increase the motion of the generated video.")
    stride = SliderRow(label="Max Motion Stride", min=1, max=8, divisions=7, pref=diffsynth_prefs, key='stride', expand=True, col={'md': 6}, visible=diffsynth_prefs['diffsynth_mode'] in ["video_rerender", "diffutoon"], tooltip="Max motion stride as a power of 2 (default: 4)")
    batch_size = SliderRow(label="Batch Size", min=1, max=60, divisions=59, pref=diffsynth_prefs, key='batch_size', visible=diffsynth_prefs['diffsynth_mode'] in ["video_rerender", "diffutoon"], tooltip="Affects coherancy. To avoid out-of-memory, use small batch size.")
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=diffsynth_prefs, key='fps', col={'sm': 6}, tooltip="The rate at which the generated images shall be exported to a video after generation. Note that Stable Diffusion Video's UNet was micro-conditioned on fps-1 during training.")
    target_fps = SliderRow(label="Target FPS", min=0, max=60, suffix="fps", divisions=60, expand=1, pref=diffsynth_prefs, key='target_fps', col={'sm': 6})
    # width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=diffsynth_prefs, key='width')
    # height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=diffsynth_prefs, key='height')
    export_to_gif = Tooltip(message="Save animated gif file along with Image Sequence", content=Switcher(label="Export to GIF", value=diffsynth_prefs['export_to_gif'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e: changed(e, 'export_to_gif')))
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=diffsynth_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e: changed(e, 'export_to_video')))
    max_size = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=16, multiple=64, suffix="px", pref=diffsynth_prefs, key='max_size')
    width_slider = SliderRow(label="Width", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=diffsynth_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=diffsynth_prefs, key='height')
    interpolate_video = Switcher(label="Interpolate Video", value=diffsynth_prefs['interpolate_video'], tooltip="Use Google FiLM Interpolation to transition between frames.", on_change=lambda e: changed(e, 'interpolate_video'))
    # cpu_offload = Switcher(label="CPU Offload", value=diffsynth_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    num_videos = NumberPicker(label="Number of Videos: ", min=1, max=8, value=diffsynth_prefs['num_videos'], on_change=lambda e: changed(e, 'num_videos'))
    file_prefix = TextField(label="Filename Prefix", value=diffsynth_prefs['file_prefix'], width=120, on_change=lambda e: changed(e, 'file_prefix'))
    batch_folder_name = TextField(label="Video Folder Name", value=diffsynth_prefs['batch_folder_name'], on_change=lambda e: changed(e, 'batch_folder_name'))
    # resume_frame = Switcher(label="Resume from Last Frame", value=diffsynth_prefs['resume_frame'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_resume, tooltip="Continues generating frames in chunks for videos longer than 14 or 25 frame limit.")
    # resume_container = Container(content=NumberPicker(label="Times to Continue: ", min=1, max=10, value=diffsynth_prefs['continue_times'], on_change=lambda e: changed(e, 'continue_times'), tooltip="Resumes 14 or 25 more frames in iterations. May degrade over time."), visible=diffsynth_prefs['resume_frame'])
    seed = TextField(label="Seed", width=90, value=str(diffsynth_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e: changed(e, 'seed', ptype='int'))
    #upscaler = UpscaleBlock(diffsynth_prefs)
    #page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text("🪭  Run DiffSynth", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_diffsynth(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_diffsynth(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_diffsynth(page, from_list=True, with_params=True))
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10),
        content=Column([
            Header("🔥  DiffSynth Studio (under construction, but may work)", "Diffusion engine with multiple optimized modes, restructured architectures including Text Encoder, UNet, VAE, among others...", actions=[save_default(diffsynth_prefs, ['init_image']), IconButton(icon=icons.HELP, tooltip="Help with DiffSynth Settings", on_click=diffsynth_help)]),
            # ResponsiveRow([prompt, negative_prompt]),
            Row([Text("Mode:"), selected_mode]),
            text_prompts,
            init_image,
            init_video,
            motion_bucket_id,
            # Row([resume_frame, resume_container]),
            ResponsiveRow([guidance]),
            guidance_row,
            ResponsiveRow([batch_size, controlnet_strength]),
            ResponsiveRow([stride, clip_skip]),
            width_slider, height_slider,
            #max_size,
            num_frames,
            num_inference_row,
            ResponsiveRow([fps, target_fps]),
            # width_slider, height_slider,
            #upscaler,
            Row([export_to_gif, interpolate_video]),
            Row([seed, batch_folder_name, file_prefix]),  # num_videos,
            Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True),
        ]
        ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c


pia_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "pia-",
    "num_images": 1,
    "width": 1024 if prefs['higher_vram_mode'] else 512,
    "height":1024 if prefs['higher_vram_mode'] else 512,
    "guidance_scale": 7.5,
    'num_inference_steps': 50,
    "seed": 0,
    'init_image': '',
    'init_image_strength': 0.8,
    "motion_scale": 0,
    'video_length': 16,
    'fps': 8,
    'target_fps': 25,
    'clip_skip': 1,
    "pia_model": "Realistic_Vision_V6.0_B1_noVAE",
    "custom_model": "",
    'lora_alpha': 0.8,
    'custom_lora': '',
    'lora_layer': 'Analog.Redmond',
    'lora_layer_alpha': 0.8,
    'custom_lora_layer': '',
    'lora_map': [],
    'use_ip_adapter': False,
    'ip_adapter_image': '',
    'ip_adapter_model': 'SD v1.5',
    'ip_adapter_strength': 0.8,
    'export_to_video': True,
    "interpolate_video": True,
    "cpu_offload": False,
    "free_init": False,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildPIA(page):
    global prefs, pia_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          pia_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def pia_help(e):
      def close_pia_dlg(e):
        nonlocal pia_help_dlg
        pia_help_dlg.open = False
        page.update()
      pia_help_dlg = AlertDialog(title=Text("🙅   Help with Personalized Image Animator Pipeline"), content=Column([
          Text("PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models"),
          Text("Recent advancements in personalized text-to-image (T2I) models have revolutionized content creation, empowering non-experts to generate stunning images with unique styles. While promising, adding realistic motions into these personalized images by text poses significant challenges in preserving distinct styles, high-fidelity details, and achieving motion controllability by text. In this paper, we present PIA, a Personalized Image Animator that excels in aligning with condition images, achieving motion controllability by text, and the compatibility with various personalized T2I models without specific tuning. To achieve these goals, PIA builds upon a base T2I model with well-trained temporal alignment layers, allowing for the seamless transformation of any personalized T2I model into an image animation model. A key component of PIA is the introduction of the condition module, which utilizes the condition frame and inter-frame affinity as input to transfer appearance information guided by the affinity hint for individual frame synthesis in the latent space. This design mitigates the challenges of appearance-related image alignment within and allows for a stronger focus on aligning with motion-related guidance."),
          Markdown("[Project](https://pi-animator.github.io/) | [Paper](https://arxiv.org/abs/2312.13964) | [PIAPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/pia/pipeline_pia.py) | [Model Checkpoint](https://huggingface.co/openmmlab/PIA-condition-adapter)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Credits go to Tianxing Wu, Chenyang Si, Yuming Jiang, Ziqi Huang, Ziwei Liu and HuggingFace.")
          #Markdown("The pipelines were contributed by [luosiallen](https://luosiallen.github.io/), [nagolinc](https://github.com/nagolinc), and [dg845](https://github.com/dg845).", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🦘  Jump Right In...", on_click=close_pia_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(pia_help_dlg)
      pia_help_dlg.open = True
      page.update()
    def toggle_ip_adapter(e):
      pia_prefs['use_ip_adapter'] = e.control.value
      ip_adapter_container.height=None if pia_prefs['use_ip_adapter'] else 0
      ip_adapter_container.update()
    def changed_model(e):
        pia_prefs['pia_model'] = e.control.value
        pia_custom_model.visible = e.control.value == "Custom"
        pia_custom_model.update()
    def changed_lora_layer(e):
      pia_prefs['lora_layer'] = e.control.value
      custom_lora_layer.visible = e.control.value == "Custom"
      custom_lora_layer.update()
    def add_lora(e):
      lora = pia_prefs['lora_layer']
      lora_scale = pia_prefs['lora_layer_alpha']
      lora_layer = {}
      if lora == "Custom":
        lora_layer = {'name': 'Custom', 'file':'', 'path':pia_prefs['custom_lora_layer'], 'scale': lora_scale}
      else:
        for l in LoRA_models:
          if l['name'] == lora:
            lora_layer = l.copy()
            lora_layer['scale'] = lora_scale
        for l in pia_prefs['lora_map']:
          if l['name'] == lora:
            return
      pia_prefs['lora_map'].append(lora_layer)
      title = Markdown(f"**{lora_layer['name']}** - Alpha Scale: [{lora_layer['scale']}] - {lora_layer['path']}")
      lora_layer_map.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
        items=[
            PopupMenuItem(icon=icons.DELETE, text="Delete LoRA Layer", on_click=delete_lora_layer, data=lora_layer),
            PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_lora_layers, data=lora_layer),
        ]), data=lora_layer))
      lora_layer_map.update()
    def delete_lora_layer(e):
        for l in pia_prefs['lora_map']:
          if l['name'] == e.control.data['name']:
            pia_prefs['lora_map'].remove(l)
          #del l #pia_prefs['lora_map'][]
        for c in lora_layer_map.controls:
          if c.data['name'] == e.control.data['name']:
             lora_layer_map.controls.remove(c)
             break
        lora_layer_map.update()
    def delete_all_lora_layers(e):
        pia_prefs['lora_map'].clear()
        lora_layer_map.controls.clear()
        lora_layer_map.update()
    prompt = TextField(label="Prompt Text", value=pia_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=pia_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = FileInput(label="Init Image", pref=pia_prefs, key='init_image', ftype="image", page=page, col={'md':6})
    init_image_strength = SliderRow(label="Init-Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=pia_prefs, key='init_image_strength', col={'md':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    video_length = SliderRow(label="Video Length", min=1, max=64, divisions=63, pref=pia_prefs, key='video_length', tooltip="The number of frames to animate.")
    batch_folder_name = TextField(label="Batch Folder Name", value=pia_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=pia_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Videos", min=1, max=9, step=1, value=pia_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=80, divisions=80, pref=pia_prefs, key='num_inference_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=pia_prefs, key='guidance_scale')
    clip_skip = SliderRow(label="Clip Skip", min=0, max=4, divisions=4, pref=pia_prefs, key='clip_skip', expand=True, col={'md': 6}, tooltip="Skips part of the image generation process, leading to slightly different results from the LoRA CLIP model.")
    motion_scale = SliderRow(label="Motion Scale", min=0, max=8, divisions=8, pref=pia_prefs, key='motion_scale', tooltip="Amount and type of motion that is added to the image. Increasing the value increases the amount of motion. 0-2 increase the amount of motion; 3-5 create looping motion; 6-8 perform motion with image style transfer")
    width_slider = SliderRow(label="Width", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=pia_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=2048, divisions=15, multiple=128, suffix="px", pref=pia_prefs, key='height')
    def toggle_ip_adapter(e):
        pia_prefs['use_ip_adapter'] = e.control.value
        ip_adapter_container.height = None if e.control.value else 0
        ip_adapter_container.update()
        ip_adapter_model.visible = e.control.value
        ip_adapter_model.update()
    use_ip_adapter = Switcher(label="Use IP-Adapter Reference Image", value=pia_prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip="Uses both image and text to condition the image generation process.")
    ip_adapter_model = Dropdown(label="IP-Adapter SD Model", width=220, options=[], value=pia_prefs['ip_adapter_model'], visible=pia_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))
    for m in ip_adapter_models:
        ip_adapter_model.options.append(dropdown.Option(m['name']))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=pia_prefs, key='ip_adapter_image', page=page, col={'lg':6})
    ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=pia_prefs, key='ip_adapter_strength', col={'lg':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    ip_adapter_container = Container(ResponsiveRow([ip_adapter_image, ip_adapter_strength]), height = None if pia_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    pia_model = Dropdown(label="PIA Model", width=280, options=[dropdown.Option("Custom"), dropdown.Option("Realistic_Vision_V6.0_B1_noVAE"), dropdown.Option("Realistic_Vision_V5.1_noVAE"), dropdown.Option("dreamshaper-8")], value=pia_prefs['pia_model'], on_change=changed_model)
    pia_custom_model = TextField(label="Custom PIA Model (URL or Path)", value=pia_prefs['custom_model'], expand=True, visible=pia_prefs['pia_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    lora_layer = Dropdown(label="LoRA Layer Map", options=[dropdown.Option("Custom")], value=pia_prefs['lora_layer'], on_change=changed_lora_layer)
    custom_lora_layer = TextField(label="Custom LoRA Safetensor (URL or Path)", value=pia_prefs['custom_lora_layer'], expand=True, visible=pia_prefs['lora_layer']=="Custom", on_change=lambda e:changed(e,'custom_lora_layer'))
    if len(prefs['custom_LoRA_models']) > 0:
        for l in prefs['custom_LoRA_models']:
            lora_layer.options.append(dropdown.Option(l['name']))
    for m in LoRA_models:
        lora_layer.options.append(dropdown.Option(m['name']))
    #lora_layer.options.append(dropdown.Option("Custom LoRA Path"))
    #for lora in animatediff_motion_loras:
    #    lora_layer.options.insert(1, dropdown.Option(lora['name']))
    lora_layer_alpha = SliderRow(label="LoRA Alpha", min=0, max=1, divisions=10, round=1, expand=True, pref=pia_prefs, key='lora_layer_alpha', tooltip="The Weight of the custom LoRA Model to influence diffusion.")
    add_lora_layer = ft.FilledButton("➕  Add LoRA", on_click=add_lora)
    lora_layer_map = Column([], spacing=0)
    cpu_offload = Switcher(label="CPU Offload", value=pia_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    free_init = Switcher(label="Free-Init", value=pia_prefs['free_init'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'free_init'), tooltip="Improves temporal consistency and overall quality of videos generated using video-diffusion-models without any addition training.")
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=pia_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    interpolate_video = Switcher(label="Interpolate Video", value=pia_prefs['interpolate_video'], tooltip="Use Google FiLM Interpolation to transition between frames.", on_change=lambda e:changed(e,'interpolate_video'))
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=pia_prefs, key='fps', col={'md': 6}, tooltip="The rate at which the generated images shall be exported to a video after generation. Note that Stable Diffusion Video's UNet was micro-conditioned on fps-1 during training.")
    seed = TextField(label="Seed", width=90, value=str(pia_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    #upscaler = UpscaleBlock(pia_prefs)
    #page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🪖   Run PIA", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_pia(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_pia(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_pia(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True)
    page.pia_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🎬  Personalized Image Animator Image-to-Video", "Image-to-Video Generation with PIA via Plug-and-Play Modules in Text-to-Image Models...", actions=[save_default(pia_prefs, ['init_image', 'ip_adapter_image', 'lora_map']), IconButton(icon=icons.HELP, tooltip="Help with Personalized Image Animator Settings", on_click=pia_help)]),
            ResponsiveRow([init_image, init_image_strength]),
            ResponsiveRow([prompt, negative_prompt]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            motion_scale,
            video_length,
            ResponsiveRow([fps, clip_skip]),
            Row([use_ip_adapter, ip_adapter_model], vertical_alignment=CrossAxisAlignment.START),
            ip_adapter_container,
            Divider(thickness=4, height=4),
            Row([lora_layer, custom_lora_layer, lora_layer_alpha, add_lora_layer]),
            lora_layer_map,
            Divider(thickness=4, height=4),
            Row([pia_model, pia_custom_model]),
            Row([cpu_offload, free_init, export_to_video, interpolate_video]),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            #upscaler,
            parameters_row,
            page.pia_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

easyanimate_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "easyanimate-",
    "num_images": 1,
    "width": 1008,
    "height":576,
    "guidance_scale": 7.0,
    'num_inference_steps': 30,
    "seed": 0,
    'init_image': '',
    'end_image': '',
    'video_length': 48,
    'fps': 24,
    'target_fps': 30,
    'lora_alpha': 0.8,
    'custom_lora': '',
    'lora_layer': '3D Redmond',
    'lora_layer_alpha': 0.8,
    'custom_lora_layer': '',
    'lora_map': [],
    "easyanimate_model": "EasyAnimateV3-XL-2-InP-512x512",
    "custom_model": "",
    "sampler": "DPM++",#"Euler" "Euler A" "DPM++" "PNDM" and "DDIM"
    'generate_image': False,
    'export_to_video': True,
    "interpolate_video": True,
    "cpu_offload": True,
}

def buildEasyAnimate(page):
    global prefs, easyanimate_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          easyanimate_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def easyanimate_help(e):
      def close_easyanimate_dlg(e):
        nonlocal easyanimate_help_dlg
        easyanimate_help_dlg.open = False
        page.update()
      easyanimate_help_dlg = AlertDialog(title=Text("🙅   Help with EasyAnimate Pipeline"), content=Column([
          Text("EasyAnimate is an end-to-end solution for generating high-resolution and long videos. We can train transformer based diffusion generators, train VAEs for processing long videos, and preprocess metadata. Based on Sora like structure and DIT, we use transformer as a diffuser for video generation. We built easyanimate based on motion module, u-vit and slice-vae. In the future, we will try more training programs to improve the effect."),
          Text("EasyAnimate is a pipeline based on the transformer architecture that can be used to generate AI photos and videos, train baseline models and Lora models for the Diffusion Transformer. We support making predictions directly from the pre-trained EasyAnimate model to generate videos of about different resolutions, 6 seconds with 24 fps (1 ~ 144 frames, in the future, we will support longer videos). Updated v3 supports up to 720p 144 frames (960x960, 6s, 24fps) video generation, and supports text and image generated video models."),
          Markdown("[Project](https://easyanimate.github.io/) | [GitHub](https://github.com/aigc-apps/EasyAnimate) | [Paper](https://arxiv.org/abs/2405.18991) | [Model Checkpoint](https://huggingface.co/alibaba-pai/EasyAnimateV2-XL-2-512x512) | [HF Space](https://huggingface.co/spaces/alibaba-pai/EasyAnimate)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Credits go to Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, Jun Huang, Platform of AI (PAI), and Alibaba Group"),
          #Markdown("The pipelines were contributed by [luosiallen](https://luosiallen.github.io/), [nagolinc](https://github.com/nagolinc), and [dg845](https://github.com/dg845).", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🛞  Surprisingly cool...", on_click=close_easyanimate_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(easyanimate_help_dlg)
      easyanimate_help_dlg.open = True
      page.update()
    def changed_model(e):
        easyanimate_prefs['easyanimate_model'] = e.control.value
        easyanimate_custom_model.visible = e.control.value == "Custom"
        easyanimate_custom_model.update()
    def changed_lora_layer(e):
      easyanimate_prefs['lora_layer'] = e.control.value
      custom_lora_layer.visible = e.control.value == "Custom"
      custom_lora_layer.update()
    def toggle_image(e):
      easyanimate_prefs['generate_image'] = e.control.value
      video_length.visible = not e.control.value
      video_length.update()
    def add_lora(e):
      lora = easyanimate_prefs['lora_layer']
      lora_scale = easyanimate_prefs['lora_layer_alpha']
      lora_layer = {}
      if lora == "Custom":
        lora_layer = {'name': 'Custom', 'file':'', 'path':easyanimate_prefs['custom_lora_layer'], 'scale': lora_scale}
      else:
        for l in SDXL_LoRA_models:
          if l['name'] == lora:
            lora_layer = l.copy()
            lora_layer['scale'] = lora_scale
        for l in easyanimate_prefs['lora_map']:
          if l['name'] == lora:
            return
      easyanimate_prefs['lora_map'].append(lora_layer)
      title = Markdown(f"**{lora_layer['name']}** - Alpha Scale: [{lora_layer['scale']}] - {lora_layer['path']}")
      lora_layer_map.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
        items=[
            PopupMenuItem(icon=icons.DELETE, text="Delete LoRA Layer", on_click=delete_lora_layer, data=lora_layer),
            PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_lora_layers, data=lora_layer),
        ]), data=lora_layer))
      lora_layer_map.update()
    def delete_lora_layer(e):
        for l in easyanimate_prefs['lora_map']:
          if l['name'] == e.control.data['name']:
            easyanimate_prefs['lora_map'].remove(l)
          #del l #easyanimate_prefs['lora_map'][]
        for c in lora_layer_map.controls:
          if c.data['name'] == e.control.data['name']:
             lora_layer_map.controls.remove(c)
             break
        lora_layer_map.update()
    def delete_all_lora_layers(e):
        easyanimate_prefs['lora_map'].clear()
        lora_layer_map.controls.clear()
        lora_layer_map.update()
    prompt = TextField(label="Prompt Text", value=easyanimate_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=easyanimate_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = FileInput(label="Init Image (optional)", pref=easyanimate_prefs, key='init_image', ftype="image", page=page, col={'md':6})
    end_image = FileInput(label="End Image (optional)", pref=easyanimate_prefs, key='end_image', ftype="image", page=page, col={'md':6})
    generate_image = Switcher(label="Generate Still Image", value=easyanimate_prefs['generate_image'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_image, tooltip="Create a single image instead of an animated sequence. Good for testing..")
    video_length = SliderRow(label="Video Length", min=1, max=144, divisions=143, pref=easyanimate_prefs, key='video_length', tooltip="The number of frames to animate.")
    batch_folder_name = TextField(label="Batch Folder Name", value=easyanimate_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=easyanimate_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Videos", min=1, max=9, step=1, value=easyanimate_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=80, divisions=80, pref=easyanimate_prefs, key='num_inference_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=20, divisions=40, round=1, pref=easyanimate_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=easyanimate_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=easyanimate_prefs, key='height')
    easyanimate_model = Dropdown(label="EasyAnimate Model", width=305, options=[dropdown.Option("Custom"), dropdown.Option("EasyAnimateV3-XL-2-InP-512x512"), dropdown.Option("EasyAnimateV3-XL-2-InP-768x768"), dropdown.Option("EasyAnimateV3-XL-2-InP-960x960")], value=easyanimate_prefs['easyanimate_model'], on_change=changed_model)
    easyanimate_custom_model = TextField(label="Custom EasyAnimate Model (URL or Path)", value=easyanimate_prefs['custom_model'], expand=True, visible=easyanimate_prefs['easyanimate_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    sampler = Dropdown(label="Sampler", width=150, options=[dropdown.Option(s) for s in ["Euler", "Euler A", "DPM++", "PNDM", "DDIM"]], value=easyanimate_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})
    #lora_alpha = SliderRow(label="LoRA Alpha", min=0, max=1, divisions=10, round=1, expand=True, pref=easyanimate_prefs, key='lora_alpha', tooltip="The Weight of the custom LoRA Model to influence diffusion.")
    lora_layer = Dropdown(label="SDXL LoRA Layer Map", options=[dropdown.Option("Custom")], value=easyanimate_prefs['lora_layer'], on_change=changed_lora_layer)
    custom_lora_layer = TextField(label="Custom LoRA Safetensor (URL or Path)", value=easyanimate_prefs['custom_lora_layer'], expand=True, visible=easyanimate_prefs['lora_layer']=="Custom", on_change=lambda e:changed(e,'custom_lora_layer'))
    if len(prefs['custom_SDXL_LoRA_models']) > 0:
        for l in prefs['custom_SDXL_LoRA_models']:
            lora_layer.options.append(dropdown.Option(l['name']))
    for m in SDXL_LoRA_models:
        lora_layer.options.append(dropdown.Option(m['name']))
    #lora_layer.options.append(dropdown.Option("Custom SDXL LoRA Path"))
    #for lora in animatediff_motion_loras:
    #    lora_layer.options.insert(1, dropdown.Option(lora['name']))
    lora_layer_alpha = SliderRow(label="LoRA Alpha", min=0, max=1, divisions=10, round=1, expand=True, pref=easyanimate_prefs, key='lora_layer_alpha', tooltip="The Weight of the custom LoRA Model to influence diffusion.")
    add_lora_layer = ft.FilledButton("➕  Add LoRA", on_click=add_lora)
    lora_layer_map = Column([], spacing=0)
    cpu_offload = Switcher(label="CPU Offload", value=easyanimate_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=easyanimate_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    interpolate_video = Switcher(label="Interpolate Video", value=easyanimate_prefs['interpolate_video'], tooltip="Use Google FiLM Interpolation to transition between frames.", on_change=lambda e:changed(e,'interpolate_video'))
    fps = SliderRow(label="Target FPS", min=1, max=30, divisions=29, suffix='fps', pref=easyanimate_prefs, key='fps', col={'md': 6}, tooltip="The rate at which the generated images shall be exported to a video after generation. This is also used as a 'micro-condition' while generation.")
    seed = TextField(label="Seed", width=90, value=str(easyanimate_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    parameters_button = ElevatedButton(content=Text(value="😊   Run EasyAnimate", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_easyanimate(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_easyanimate(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_easyanimate(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.easyanimate_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("📷  EasyAnimate v3 Text/Image-to-Video", "End-to-End Solution for High-Resolution and Long Video Generation, by Alibaba...", actions=[save_default(easyanimate_prefs, ['init_image', 'lora_map']), IconButton(icon=icons.HELP, tooltip="Help with EasyAnimate Settings", on_click=easyanimate_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            ResponsiveRow([init_image, end_image]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            generate_image,
            video_length,
            fps,
            #Row([lora_layer, custom_lora_layer, lora_layer_alpha, add_lora_layer]),
            #lora_layer_map,
            #Divider(thickness=4, height=4),
            Row([easyanimate_model, easyanimate_custom_model]),
            Row([sampler, cpu_offload]),#, export_to_video, interpolate_video
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            parameters_row,
            page.easyanimate_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

i2vgen_xl_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "i2vgen_xl-",
    "num_images": 1,
    "width": 1280,
    "height":704,
    "guidance_scale": 8.5,
    'num_inference_steps': 50,
    "seed": 0,
    'init_image': '',
    'video_length': 16,
    'fps': 16,
    'target_fps': 16,
    'clip_skip': 1,
    'lora_alpha': 0.8,
    'custom_lora': '',
    'lora_layer': '3D Redmond',
    'lora_layer_alpha': 0.8,
    'custom_lora_layer': '',
    'lora_map': [],
    "i2vgen_xl_model": "I2VGen-XL",
    "custom_model": "",
    'export_to_video': True,
    "interpolate_video": True,
    "cpu_offload": False,
    "free_init": False,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildI2VGenXL(page):
    global prefs, i2vgen_xl_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          i2vgen_xl_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def i2vgen_xl_help(e):
      def close_i2vgen_xl_dlg(e):
        nonlocal i2vgen_xl_help_dlg
        i2vgen_xl_help_dlg.open = False
        page.update()
      i2vgen_xl_help_dlg = AlertDialog(title=Text("🙅   Help with I2VGen-XL Pipeline"), content=Column([
          Text("Video synthesis has recently made remarkable strides benefiting from the rapid development of diffusion models. However, it still encounters challenges in terms of semantic accuracy, clarity and spatio-temporal continuity. They primarily arise from the scarcity of well-aligned text-video data and the complex inherent structure of videos, making it difficult for the model to simultaneously ensure semantic and qualitative excellence. In this report, we propose a cascaded I2VGen-XL approach that enhances model performance by decoupling these two factors and ensures the alignment of the input data by utilizing static images as a form of crucial guidance. I2VGen-XL consists of two stages: i) the base stage guarantees coherent semantics and preserves content from input images by using two hierarchical encoders, and ii) the refinement stage enhances the video's details by incorporating an additional brief text and improves the resolution to 1280×720. To improve the diversity, we collect around 35 million single-shot text-video pairs and 6 billion text-image pairs to optimize the model. By this means, I2VGen-XL can simultaneously enhance the semantic accuracy, continuity of details and clarity of generated videos. Through extensive experiments, we have investigated the underlying principles of I2VGen-XL and compared it with current top methods, which can demonstrate its effectiveness on diverse data."),
          Markdown("[Project](https://i2vgen-xl.github.io/) | [GitHub](https://github.com/ali-vilab/i2vgen-xl/) | [Paper](https://hf.co/papers/2311.04145.pdf) | [Model Checkpoint](https://huggingface.co/ali-vilab/)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Credits go to Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou."),
          #Markdown("The pipelines were contributed by [luosiallen](https://luosiallen.github.io/), [nagolinc](https://github.com/nagolinc), and [dg845](https://github.com/dg845).", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🛞  Roll with it...", on_click=close_i2vgen_xl_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(i2vgen_xl_help_dlg)
      i2vgen_xl_help_dlg.open = True
      page.update()
    def changed_model(e):
        i2vgen_xl_prefs['i2vgen_xl_model'] = e.control.value
        i2vgen_xl_custom_model.visible = e.control.value == "Custom"
        i2vgen_xl_custom_model.update()
    def changed_lora_layer(e):
      i2vgen_xl_prefs['lora_layer'] = e.control.value
      custom_lora_layer.visible = e.control.value == "Custom"
      custom_lora_layer.update()
    def add_lora(e):
      lora = i2vgen_xl_prefs['lora_layer']
      lora_scale = i2vgen_xl_prefs['lora_layer_alpha']
      lora_layer = {}
      if lora == "Custom":
        lora_layer = {'name': 'Custom', 'file':'', 'path':i2vgen_xl_prefs['custom_lora_layer'], 'scale': lora_scale}
      else:
        for l in SDXL_LoRA_models:
          if l['name'] == lora:
            lora_layer = l.copy()
            lora_layer['scale'] = lora_scale
        for l in i2vgen_xl_prefs['lora_map']:
          if l['name'] == lora:
            return
      i2vgen_xl_prefs['lora_map'].append(lora_layer)
      title = Markdown(f"**{lora_layer['name']}** - Alpha Scale: [{lora_layer['scale']}] - {lora_layer['path']}")
      lora_layer_map.controls.append(ListTile(title=title, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
        items=[
            PopupMenuItem(icon=icons.DELETE, text="Delete LoRA Layer", on_click=delete_lora_layer, data=lora_layer),
            PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_lora_layers, data=lora_layer),
        ]), data=lora_layer))
      lora_layer_map.update()
    def delete_lora_layer(e):
        for l in i2vgen_xl_prefs['lora_map']:
          if l['name'] == e.control.data['name']:
            i2vgen_xl_prefs['lora_map'].remove(l)
          #del l #i2vgen_xl_prefs['lora_map'][]
        for c in lora_layer_map.controls:
          if c.data['name'] == e.control.data['name']:
             lora_layer_map.controls.remove(c)
             break
        lora_layer_map.update()
    def delete_all_lora_layers(e):
        i2vgen_xl_prefs['lora_map'].clear()
        lora_layer_map.controls.clear()
        lora_layer_map.update()
    prompt = TextField(label="Prompt Text", value=i2vgen_xl_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=i2vgen_xl_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = FileInput(label="Init Image", pref=i2vgen_xl_prefs, key='init_image', ftype="image", page=page, col={'md':6})
    video_length = SliderRow(label="Video Length", min=1, max=64, divisions=63, pref=i2vgen_xl_prefs, key='video_length', tooltip="The number of frames to animate.")
    batch_folder_name = TextField(label="Batch Folder Name", value=i2vgen_xl_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=i2vgen_xl_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_images = NumberPicker(label="Number of Videos", min=1, max=9, step=1, value=i2vgen_xl_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=80, divisions=80, pref=i2vgen_xl_prefs, key='num_inference_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=i2vgen_xl_prefs, key='guidance_scale')
    clip_skip = SliderRow(label="Clip Skip", min=0, max=4, divisions=4, pref=i2vgen_xl_prefs, key='clip_skip', expand=True, col={'md': 6}, tooltip="Recommeded to leave at 1 for this Pipeline. Skips part of the image generation process, leading to slightly different results from the LoRA CLIP model.")
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=i2vgen_xl_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=i2vgen_xl_prefs, key='height')
    i2vgen_xl_model = Dropdown(label="I2VGen-XL Model", width=250, options=[dropdown.Option("Custom"), dropdown.Option("I2VGen-XL")], value=i2vgen_xl_prefs['i2vgen_xl_model'], on_change=changed_model)
    i2vgen_xl_custom_model = TextField(label="Custom I2VGen-XL Model (URL or Path)", value=i2vgen_xl_prefs['custom_model'], expand=True, visible=i2vgen_xl_prefs['i2vgen_xl_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    #lora_alpha = SliderRow(label="LoRA Alpha", min=0, max=1, divisions=10, round=1, expand=True, pref=i2vgen_xl_prefs, key='lora_alpha', tooltip="The Weight of the custom LoRA Model to influence diffusion.")
    lora_layer = Dropdown(label="SDXL LoRA Layer Map", options=[dropdown.Option("Custom")], value=i2vgen_xl_prefs['lora_layer'], on_change=changed_lora_layer)
    custom_lora_layer = TextField(label="Custom LoRA Safetensor (URL or Path)", value=i2vgen_xl_prefs['custom_lora_layer'], expand=True, visible=i2vgen_xl_prefs['lora_layer']=="Custom", on_change=lambda e:changed(e,'custom_lora_layer'))
    if len(prefs['custom_SDXL_LoRA_models']) > 0:
        for l in prefs['custom_SDXL_LoRA_models']:
            lora_layer.options.append(dropdown.Option(l['name']))
    for m in SDXL_LoRA_models:
        lora_layer.options.append(dropdown.Option(m['name']))
    #lora_layer.options.append(dropdown.Option("Custom SDXL LoRA Path"))
    #for lora in animatediff_motion_loras:
    #    lora_layer.options.insert(1, dropdown.Option(lora['name']))
    lora_layer_alpha = SliderRow(label="LoRA Alpha", min=0, max=1, divisions=10, round=1, expand=True, pref=i2vgen_xl_prefs, key='lora_layer_alpha', tooltip="The Weight of the custom LoRA Model to influence diffusion.")
    add_lora_layer = ft.FilledButton("➕  Add LoRA", on_click=add_lora)
    lora_layer_map = Column([], spacing=0)
    cpu_offload = Switcher(label="CPU Offload", value=i2vgen_xl_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    free_init = Switcher(label="Free-Init", value=i2vgen_xl_prefs['free_init'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'free_init'), tooltip="Improves temporal consistency and overall quality of videos generated using video-diffusion-models without any addition training.")
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=i2vgen_xl_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    interpolate_video = Switcher(label="Interpolate Video", value=i2vgen_xl_prefs['interpolate_video'], tooltip="Use Google FiLM Interpolation to transition between frames.", on_change=lambda e:changed(e,'interpolate_video'))
    fps = SliderRow(label="Target FPS", min=1, max=30, divisions=29, suffix='fps', pref=i2vgen_xl_prefs, key='fps', col={'md': 6}, tooltip="The rate at which the generated images shall be exported to a video after generation. This is also used as a 'micro-condition' while generation.")
    seed = TextField(label="Seed", width=90, value=str(i2vgen_xl_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(i2vgen_xl_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="♊   Run I2VGen-XL", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_i2vgen_xl(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_i2vgen_xl(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_i2vgen_xl(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.i2vgen_xl_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🧬  I2VGen-XL Image-to-Video", "High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models...", actions=[save_default(i2vgen_xl_prefs, ['init_image', 'lora_map']), IconButton(icon=icons.HELP, tooltip="Help with I2VGen-XL Settings", on_click=i2vgen_xl_help)]),
            init_image,
            #ResponsiveRow([init_image]),
            ResponsiveRow([prompt, negative_prompt]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            video_length,
            ResponsiveRow([fps, clip_skip]),
            #Row([lora_layer, custom_lora_layer, lora_layer_alpha, add_lora_layer]),
            #lora_layer_map,
            #Divider(thickness=4, height=4),
            Row([i2vgen_xl_model, i2vgen_xl_custom_model]),
            Row([cpu_offload, export_to_video, interpolate_video]),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.i2vgen_xl_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c


hotshot_xl_prefs = {
    'prompt': '',
    'negative_prompt': 'text, watermark, copyright, blurry, low resolution, blur, low quality',
    'num_inference_steps': 25,
    'guidance_scale': 23.0,
    'fps': 24,
    'video_length': 16,
    'video_duration': 1000,
    'export_to_video': False,
    'seed': 0,
    'width': 608,
    'height': 416,
    'scheduler': 'EulerAncestralDiscreteScheduler',
    'gif': '',
    'controlnet_type': 'Canny',
    'conditioning_scale': 0.7,
    'control_guidance_start': 0.0,
    'control_guidance_end': 1.0,
    'lora_layer': 'None',
    'custom_lora_layer': '',
    'init_video': '',
    'init_weight': 0.5,
    'num_images': 1,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": False,
    "enlarge_scale": 2,
    "display_upscaled_image": False,
}

def buildHotshotXL(page):
    global hotshot_xl_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          hotshot_xl_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.hotshot_xl_output.controls = []
      page.hotshot_xl_output.update()
      clear_button.visible = False
      clear_button.update()
    def hotshot_xl_help(e):
      def close_hotshot_xl_dlg(e):
        nonlocal hotshot_xl_help_dlg
        hotshot_xl_help_dlg.open = False
        page.update()
      hotshot_xl_help_dlg = AlertDialog(title=Text("💁   Help with Hotshot-XL Text-To-GIF"), content=Column([
          Text("You’ll be able to make GIFs with any existing or newly fine-tuned SDXL model you may want to use. If you'd like to make GIFs of personalized subjects, you can load your own SDXL based LORAs, and not have to worry about fine-tuning Hotshot-XL. This is awesome because it’s usually much easier to find suitable images for training data than it is to find videos. It also hopefully fits into everyone's existing LORA usage/workflows. Hotshot-XL is compatible with SDXL ControlNet to make GIFs in the composition/layout you’d like. Hotshot-XL was trained to generate 1 second GIFs at 8 FPS. Hotshot-XL was trained on various aspect ratios. For best results with the base Hotshot-XL model, we recommend using it with an SDXL model that has been fine-tuned with 512x512 images."),
          Markdown("[Project Website](https://www.hotshot.co/) | [GitHub repository](https://github.com/hotshotco/Hotshot-XL)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("♨️  Hot Stuff... ", on_click=close_hotshot_xl_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(hotshot_xl_help_dlg)
      hotshot_xl_help_dlg.open = True
      page.update()
    def changed_lora_layer(e):
        hotshot_xl_prefs['lora_layer'] = e.control.value
        custom_lora_layer.visible = e.control.value == "Custom"
        custom_lora_layer.update()
    prompt = TextField(label="Animation Prompt Text", value=hotshot_xl_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=hotshot_xl_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    video_length = SliderRow(label="Number of Frames", min=1, max=300, divisions=299, pref=hotshot_xl_prefs, key='video_length', tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.")
    video_duration = SliderRow(label="Video Duration", min=1, max=6000, divisions=5999, suffix="ms", pref=hotshot_xl_prefs, key='video_duration', tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=hotshot_xl_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=hotshot_xl_prefs, key='guidance_scale')
    #fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=hotshot_xl_prefs, key='fps')
    export_to_video = Tooltip(message="Save mp4 video file instead of Animated GIF", content=Switcher(label="Export to Video", value=hotshot_xl_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    width_slider = SliderRow(label="Width", min=256, max=1024, divisions=24, multiple=16, suffix="px", pref=hotshot_xl_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1024, divisions=24, multiple=16, suffix="px", pref=hotshot_xl_prefs, key='height')
    scheduler = Dropdown(label="Scheduler", options=[dropdown.Option("EulerAncestralDiscreteScheduler"), dropdown.Option("EulerDiscreteScheduler")], width=300, value=hotshot_xl_prefs['scheduler'], on_change=lambda e: changed(e, 'scheduler'))
    gif = FileInput(label="Init Animated GIF (optional)", pref=hotshot_xl_prefs, expand=True, key='gif', ftype="gif", page=page)
    controlnet_type = Dropdown(label="ControlNet Image Layer", width=177, options=[dropdown.Option("Canny"), dropdown.Option("Depth")], value=hotshot_xl_prefs['controlnet_type'], on_change=lambda e: changed(e, 'controlnet_type'))
    conditioning_scale = SliderRow(label="Conditioning Scale", min=0.0, max=1.0, divisions=10, round=1, pref=hotshot_xl_prefs, col={'md': 6, 'lg': 4}, key='conditioning_scale', tooltip="Strength of the ControlNet Mask.")
    control_guidance_start = SliderRow(label="Control Guidance Start", min=0.0, max=1.0, divisions=10, round=1, pref=hotshot_xl_prefs, col={'md': 6, 'lg': 4}, key='control_guidance_start', tooltip="The percentage of total steps at which the controlnet starts applying.")
    control_guidance_end = SliderRow(label="Control Guidance End", min=0.0, max=1.0, divisions=10, round=1, pref=hotshot_xl_prefs, col={'md': 6, 'lg': 4}, key='control_guidance_end', tooltip="The percentage of total steps at which the controlnet stops applying.")
    lora_layer = Dropdown(label="SDXL LoRA Layer", options=[dropdown.Option("None"), dropdown.Option("Custom")], value=hotshot_xl_prefs['lora_layer'], on_change=changed_lora_layer)
    custom_lora_layer = TextField(label="Custom LoRA Safetensor (URL or Path)", value=hotshot_xl_prefs['custom_lora_layer'], expand=True, visible=hotshot_xl_prefs['lora_layer']=="Custom", on_change=lambda e:changed(e,'custom_lora_layer'))
    for lora in SDXL_LoRA_models:
        lora_layer.options.insert(2, dropdown.Option(lora['name']))
    num_images = NumberPicker(label="Number of Animations: ", min=1, max=8, value=hotshot_xl_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))
    batch_folder_name = TextField(label="Video Folder Name", value=hotshot_xl_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    seed = TextField(label="Seed", width=90, value=str(hotshot_xl_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    #upscaler = UpscaleBlock(hotshot_xl_prefs)
    #page.upscalers.append(upscaler)
    page.hotshot_xl_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.hotshot_xl_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🔥  Hotshot-XL Text-To-GIF with SDXL", "Generate Animated GIFs with any fine-tuned SDXL model... (Work in Progress)", actions=[save_default(hotshot_xl_prefs, ['gif', 'init_video']), IconButton(icon=icons.HELP, tooltip="Help with Hotshot-XL Settings", on_click=hotshot_xl_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        ResponsiveRow([prompt, negative_prompt]),
        #Row([export_to_video, lower_memory]),
        video_length,
        video_duration,
        num_inference_row,
        #guidance,
        width_slider, height_slider,
        Row([controlnet_type, gif]),
        ResponsiveRow([control_guidance_start, control_guidance_end, conditioning_scale]),
        Divider(height=4),
        Row([lora_layer, custom_lora_layer]),
        Row([scheduler, export_to_video]),
        #upscaler,
        Row([num_images, seed, batch_folder_name]),
        Row([
            ElevatedButton(content=Text("🫠  Run Hotshot-XL", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_hotshot_xl(page)),
        ]),
        page.hotshot_xl_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

rerender_a_video_prefs = {
    'init_video': '',
    'prompt': '',
    'negative_prompt': '',
    'a_prompt': '',
    'control_task': 'Canny',
    'controlnet_strength': 0.8,
    'x0_strength': 0.95,
    'mask_strength': 0.5,
    'inner_strength': 0.9,
    "dreambooth_lora": "realisticVisionV20_v20",
    'custom_lora': '',
    'frame_count': 102,
    'max_size': 512,
    'low_threshold': 50, #1-255 canny
    'high_threshold': 100, #1-255
    'crop': {'top': 0, 'left':0, 'right':0, 'bottom':0},
    'enable_freeu': True,
    'freeu_args': {'b1': 1.1, 'b2':1.2, 's1':1.0, 's2':0.2},
    'interval': 10,
    'style_update_freq': 10,
    'prompt_strength': 7.5,
    'start_time': 0.0,
    'end_time': 0.0,
    'duration': 0.0,
    'smooth_boundary': False,
    'color_preserve': False,
    'loose_cfattn': False,
    'first_frame': False,
    'show_console': True,
    'save_frames': True,
    'seed': 0,
    'file_prefix': 'rerender-',
    'output_name': '',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildRerender_a_video(page):
    global rerender_a_video_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          rerender_a_video_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.rerender_a_video_output.controls = []
      page.rerender_a_video_output.update()
      clear_button.visible = False
      clear_button.update()
    def rerender_a_video_help(e):
      def close_rerender_a_video_dlg(e):
        nonlocal rerender_a_video_help_dlg
        rerender_a_video_help_dlg.open = False
        page.update()
      rerender_a_video_help_dlg = AlertDialog(title=Text("💁   Help with Rerender-a-Video"), content=Column([
          Text("The framework includes two parts: key frame translation and full video translation. The first part uses an adapted diffusion model to generate key frames, with hierarchical cross-frame constraints applied to enforce coherence in shapes, textures and colors. The second part propagates the key frames to other frames with temporal-aware patch matching and frame blending. Our framework achieves global style and local texture temporal consistency at a low cost (without re-training or optimization). The adaptation is compatible with existing image diffusion techniques, allowing our framework to take advantage of them, such as customizing a specific subject with LoRA, and introducing extra spatial guidance with ControlNet. Extensive experimental results demonstrate the effectiveness of our proposed framework over existing methods in rendering high-quality and temporally-coherent videos."),
          #Text(""),
          Markdown("""Features:
* Temporal consistency: cross-frame constraints for low-level temporal consistency.
* Zero-shot: no training or fine-tuning required.
* Flexibility: compatible with off-the-shelf models (e.g., ControlNet, LoRA) for customized translation.

[GitHub Page](https://github.com/williamyang1991/Rerender_A_Video) | [HuggingFace Space](https://huggingface.co/spaces/Anonymous-sub/Rerender) | [Project Page](https://www.mmlab-ntu.com/project/rerender/)""", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text('ControlNet is a neural network structure to control diffusion models by adding extra conditions. It copys the weights of neural network blocks into a "locked" copy and a "trainable" copy. The "trainable" one learns your condition. The "locked" one preserves your model. Thanks to this, training with small dataset of image pairs will not destroy the production-ready diffusion models. The "zero convolution" is 1×1 convolution with both weight and bias initialized as zeros. Before training, all zero convolutions output zeros, and ControlNet will not cause any distortion.  No layer is trained from scratch. You are still fine-tuning. Your original model is safe.  This allows training on small-scale or even personal devices. This is also friendly to merge/replacement/offsetting of models/weights/blocks/layers.'),
          Text("Canny Map Edge - A monochrome image with white edges on a black background."),
          Text("Depth - A grayscale image with black representing deep areas and white representing shallow areas."),
          Text("HED - A monochrome image with white soft edges on a black background."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("📺  Change Reality... ", on_click=close_rerender_a_video_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(rerender_a_video_help_dlg)
      rerender_a_video_help_dlg.open = True
      page.update()
    def change_task(e):
        changed(e,'control_task')
        canny_threshold.height = None if rerender_a_video_prefs['control_task'] == "Canny" or rerender_a_video_prefs['control_task'] == "Canny21" else 0
        canny_threshold.update()
    def changed_lora(e):
      rerender_a_video_prefs['dreambooth_lora'] = e.control.value
      custom_lora.visible = e.control.value == "Custom"
      custom_lora.update()
    def toggle_freeu(e):
        changed(e,'enable_freeu')
        freeu_args.height = None if rerender_a_video_prefs['enable_freeu'] else 0
        freeu_args.update()
    prompt = TextField(label="Prompt Text", value=rerender_a_video_prefs['prompt'], filled=True, col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=rerender_a_video_prefs['negative_prompt'], multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    a_prompt = TextField(label="Additional Prompt Text", value=rerender_a_video_prefs['a_prompt'], multiline=True, filled=True, col={'md':4}, on_change=lambda e:changed(e,'a_prompt'))
    #'aesthetic', 'lineart21', 'hed', 'hed21', 'canny', 'canny21', 'openpose', 'openpose21', 'depth', 'depth21', 'normal', 'mlsd'
    control_task = Dropdown(label="ControlNet Task", width=150, options=[dropdown.Option("HED"), dropdown.Option("Canny"), dropdown.Option("Depth")], value=rerender_a_video_prefs['control_task'], on_change=change_task)
    #conditioning_scale = SliderRow(label="Conditioning Scale", min=0, max=2, divisions=20, round=1, pref=rerender_a_video_prefs, key='conditioning_scale', tooltip="The outputs of the controlnet are multiplied by `rerender_a_video_conditioning_scale` before they are added to the residual in the original unet.")
    controlnet_strength = SliderRow(label="ControlNet Strength", min=0.0, max=1.0, divisions=20, round=2, pref=rerender_a_video_prefs, key='controlnet_strength', tooltip="How much influence the controlnet annotator's output is used to guide the denoising process.")
    x0_strength = SliderRow(label="Denoise Strength", min=0.0, max=1.05, divisions=21, round=2, pref=rerender_a_video_prefs, key='x0_strength', tooltip="Repaint degree, low to make output look more like init video. 0: fully recover the input.1.05: fully rerender the input.")
    mask_strength = SliderRow(label="Mask Strength", min=0.0, max=1.0, divisions=20, round=2, pref=rerender_a_video_prefs, key='mask_strength', tooltip="")
    inner_strength = SliderRow(label="Inner Strength", min=0.0, max=1.0, divisions=20, round=2, pref=rerender_a_video_prefs, key='inner_strength', tooltip="")
    init_video = FileInput(label="Init Video Clip", pref=rerender_a_video_prefs, key='init_video', ftype="video", expand=True, page=page)
    #init_video = TextField(label="Init Video Clip", value=rerender_a_video_prefs['init_video'], expand=True, on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))
    #fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=rerender_a_video_prefs, key='fps', tooltip="The FPS to extract from the init video clip.")
    start_time = TextField(label="Start Time (s)", value=rerender_a_video_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype="float"))
    end_time = TextField(label="End Time (0 for all)", value=rerender_a_video_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype="float"))
    duration = TextField(label="Duration (0 for all)", value=rerender_a_video_prefs['duration'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'duration', ptype="float"))
    vid_params = Container(content=Column([Row([start_time, end_time, duration])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(top=5))
    interval = SliderRow(label="Number of Intervals", min=1, max=100, divisions=99, pref=rerender_a_video_prefs, key='interval', tooltip="Uniformly sample the key frame every K frames. Small value for large or fast motions.")
    frame_count = SliderRow(label="Frame Count", min=1, max=300, divisions=299, pref=rerender_a_video_prefs, key='frame_count', tooltip="The final output video will have K*M+1 frames with M+1 key frames.")
    style_update_freq = SliderRow(label="Style Update Frequency", min=1, max=100, divisions=99, pref=rerender_a_video_prefs, key='style_update_freq', tooltip="")
    prompt_strength = SliderRow(label="Prompt Strength", min=0, max=30, divisions=60, round=1, pref=rerender_a_video_prefs, key='prompt_strength', tooltip="How much influence the prompt has on the output. Guidance Scale.")
    low_threshold_row = SliderRow(label="Canny Low Threshold", min=1, max=255, divisions=254, pref=rerender_a_video_prefs, key='low_threshold', expand=True, col={'lg':6}, tooltip="Lower increases sensitivity to weaker edges, higher gives fewer but more reliable edge detections.")
    high_threshold_row = SliderRow(label="Canny High Threshold", min=1, max=255, divisions=254, pref=rerender_a_video_prefs, key='high_threshold', expand=True, col={'lg':6}, tooltip="Higher value decreases the amount of noise but could result in missing some true edges.")
    canny_threshold = Container(ResponsiveRow([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(bottom=8))
    canny_threshold.height = None if rerender_a_video_prefs['control_task'] == "Canny" else 0
    crop_left = SliderRow(label="Crop Left", min=0, max=512, divisions=512, suffix="px", pref=rerender_a_video_prefs['crop'], key='left', expand=True, col={'lg':6}, tooltip="")
    crop_right = SliderRow(label="Crop Right", min=0, max=512, divisions=512, suffix="px", pref=rerender_a_video_prefs['crop'], key='right', expand=True, col={'lg':6})
    crop_top = SliderRow(label="Crop Top", min=0, max=512, divisions=512, suffix="px", pref=rerender_a_video_prefs['crop'], key='top', expand=True, col={'lg':6})
    crop_bottom = SliderRow(label="Crop Bottom", min=0, max=512, divisions=512, suffix="px", pref=rerender_a_video_prefs['crop'], key='bottom', expand=True, col={'lg':6})

    b1 = SliderRow(label="b1", min=1.0, max=1.2, divisions=4, round=2, pref=rerender_a_video_prefs['freeu_args'], key='b1', expand=True, col={'md':6}, tooltip="Backbone factor of the first stage block of decoder.")
    b2 = SliderRow(label="b2", min=1.2, max=1.6, divisions=8, round=2, pref=rerender_a_video_prefs['freeu_args'], key='b2', expand=True, col={'md':6}, tooltip="Backbone factor of the second stage block of decoder.")
    s1 = SliderRow(label="s1", min=0, max=1, divisions=20, round=1, pref=rerender_a_video_prefs['freeu_args'], key='s1', expand=True, col={'md':6}, tooltip="Skip factor of the first stage block of decoder.")
    s2 = SliderRow(label="s2", min=0, max=1, divisions=20, round=1, pref=rerender_a_video_prefs['freeu_args'], key='s2', expand=True, col={'md':6}, tooltip="Skip factor of the second stage block of decoder.")
    freeu_args = Container(content=Column([ResponsiveRow([b1, b2, s1, s2])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(top=0, left=13), height = None if rerender_a_video_prefs['enable_freeu'] else 0)
    enable_freeu = Switcher(label="Enable FreeU: Free Lunch UNET", value=rerender_a_video_prefs['enable_freeu'], tooltip="Improves diffusion model sample quality at no costs. Results will have higher contrast and saturation, richer details, and more vivid colors.", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_freeu)
    #color_fix = Dropdown(label="Color Fix", width=150, options=[dropdown.Option("None"), dropdown.Option("RGB"), dropdown.Option("HSV"), dropdown.Option("Lab")], value=rerender_a_video_prefs['color_fix'], on_change=lambda e:changed(e,'color_fix'), tooltip="Prevent color from drifting due to feedback and model bias by fixing the histogram to the first frame. Specify colorspace for histogram matching")
    #color_amount = SliderRow(label="Color Amount", min=0.0, max=1.0, divisions=10, round=1, pref=rerender_a_video_prefs, key='color_amount', expand=True, tooltip="Blend between the original color and the color matched version.")
    #max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=rerender_a_video_prefs, key='max_size')
    dreambooth_lora = Dropdown(label="DreamBooth LoRA", options=[dropdown.Option("Custom")], value=rerender_a_video_prefs['dreambooth_lora'], on_change=changed_lora)
    for lora in animate_diff_loras:
      dreambooth_lora.options.insert(1, dropdown.Option(lora['name']))
    custom_lora = TextField(label="Custom LoRA Safetensor (URL or Path)", value=rerender_a_video_prefs['custom_lora'], expand=True, visible=rerender_a_video_prefs['dreambooth_lora']=="Custom", on_change=lambda e:changed(e,'custom_lora'))
    max_size = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=48, multiple=16, suffix="px", pref=rerender_a_video_prefs, key='max_size')
    smooth_boundary = Switcher(label="Smooth Boundary", value=rerender_a_video_prefs['smooth_boundary'], tooltip="Prevents artifacts at fusion boundaries.", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'smooth_boundary'))
    color_preserve = Switcher(label="Color Preserve", value=rerender_a_video_prefs['color_preserve'], tooltip="Keep the color of the input video", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'color_preserve'))
    loose_cfattn = Switcher(label="Loose Cross-Frame Attention", value=rerender_a_video_prefs['loose_cfattn'], tooltip="Results will better match the input video, thus reducing ghosting artifacts caused by inconsistencies.", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'loose_cfattn'))
    show_console = Switcher(label="Show Console Output", value=rerender_a_video_prefs['show_console'], tooltip="Outputs the progress run log in the console window. Gets messy, but useful.", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'show_console'))
    first_frame = Switcher(label="First Frame Only", value=rerender_a_video_prefs['first_frame'], tooltip="Rerender only the first key frame as test.", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'first_frame'))
    seed = TextField(label="Seed", value=rerender_a_video_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width=120, on_change=lambda e:changed(e,'seed', ptype="int"))
    file_prefix = TextField(label="Filename Prefix",  value=rerender_a_video_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))
    output_name = TextField(label="Output Name", value=rerender_a_video_prefs['output_name'], on_change=lambda e:changed(e,'output_name'))
    batch_folder_name = TextField(label="Batch Folder Name", value=rerender_a_video_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    #upscaler = UpscaleBlock(rerender_a_video_prefs)
    #page.upscalers.append(upscaler)
    page.rerender_a_video_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.rerender_a_video_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("📹  Rerender-a-Video", "Zero-Shot Text-Guided Video-to-Video Translation... (Note: May need 24GB VRAM to run)", actions=[save_default(rerender_a_video_prefs, ['init_video']), IconButton(icon=icons.HELP, tooltip="Help with Rerender Vid2Vid Settings", on_click=rerender_a_video_help)]),
        ResponsiveRow([prompt, a_prompt]),
        negative_prompt,
        Row([control_task, init_video]),
        canny_threshold,
        #vid_params,
        controlnet_strength,
        x0_strength,
        mask_strength,
        inner_strength,
        #prompt_strength,
        frame_count,
        interval,
        style_update_freq,
        Row([dreambooth_lora, custom_lora]),
        #Row([color_fix, color_amount]),
        max_size,
        ResponsiveRow([crop_left, crop_right]),
        ResponsiveRow([crop_top, crop_bottom]),
        #ResponsiveRow([motion_alpha, motion_sigma]),
        #ResponsiveRow([max_dimension, min_dimension]),
        enable_freeu,
        freeu_args,
        Row([smooth_boundary, color_preserve, loose_cfattn, first_frame]),
        Row([seed, output_name, batch_folder_name]),
        #upscaler,
        Row([ElevatedButton(content=Text("📸  Run Rerender on Video", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_rerender_a_video(page))]),
        page.rerender_a_video_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

fresco_prefs = {
    'init_video': '',
    'prompt': '',
    'negative_prompt': 'ugly, blurry, low res, unrealistic, unaesthetic',
    'a_prompt': '',
    'sd_model': 'Realistic_Vision_V2.0',
    'custom_model': '',
    'image_resolution': 512, #256-512 x64
    'control_strength': 1.0, #0-2 x0.01
    'x0_strength': 0.75,
    'control_type': 'HED',
    'low_threshold': 50,
    'high_threshold': 100,
    'num_inference_steps': 20,
    'guidance_scale': 7.5,
    'b1': 1.0, #1-1.6 x0.01
    'b2': 1.0, #1-1.6 x0.01
    's1': 1.0, #0-1 x0.01
    's2': 1.0, #0-1 x0.01
    'mininterv': 5, #1-20
    'maxinterv': 20,#1-50
    'use_constraints': ['Spatial-Guided Attention', 'Cross-Frame Attention', 'Temporal-Guided Attention', 'Spatial-Guided Optimization', 'Temporal-Guided Optimization'],
    'bg_smooth': True,
    'use_poisson': True,
    'max_process': 4, #1-16
    'fps': 30,
    'num_frames': 100, #8-300
    'export_to_video': False,
    'seed': 0,
    'max_size': 672,
    'batch_size': 8, #3-8
    'batch_folder_name': '',
    'project_name': '',
}
fresco_constraints = ['Spatial-Guided Attention', 'Cross-Frame Attention', 'Temporal-Guided Attention', 'Spatial-Guided Optimization', 'Temporal-Guided Optimization']
fresco_models = {
    'Realistic_Vision_V2.0': 'SG161222/Realistic_Vision_V2.0', 
    'stable-diffusion-v1-5': 'runwayml/stable-diffusion-v1-5',
    'rev-animated': 'stablediffusionapi/rev-animated',
    'flat-2d-animerge': 'stablediffusionapi/flat-2d-animerge'
}

def buildFresco(page):
    global fresco_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          fresco_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def fresco_help(e):
      def close_fresco_dlg(e):
        nonlocal fresco_help_dlg
        fresco_help_dlg.open = False
        page.update()
      fresco_help_dlg = AlertDialog(title=Text("💁   Help with Fresco Video-To-Video"), content=Column([
          Text("The remarkable efficacy of text-to-image diffusion models has motivated extensive exploration of their potential application in video domains. Zero-shot methods seek to extend image diffusion models to videos without necessitating model training. Recent methods mainly focus on incorporating inter-frame correspondence into attention mechanisms. However, the soft constraint imposed on determining where to attend to valid features can sometimes be insufficient, resulting in temporal inconsistency. In this paper, we introduce FRESCO, intra-frame correspondence alongside inter-frame correspondence to establish a more robust spatial-temporal constraint. This enhancement ensures a more consistent transformation of semantically similar content across frames. Beyond mere attention guidance, our approach involves an explicit update of features to achieve high spatial-temporal consistency with the input video, significantly improving the visual coherence of the resulting translated videos. Extensive experiments demonstrate the effectiveness of our proposed framework in producing high-quality, coherent videos, marking a notable improvement over existing zero-shot methods."),
          Text("Features: Temporal consistency: use intra-and inter-frame constraint with better consistency and coverage than optical flow alone. Compared with our previous work Rerender-A-Video, FRESCO is more robust to large and quick motion. Zero-shot: no training or fine-tuning required. Flexibility: compatible with off-the-shelf models (e.g., ControlNet, LoRA) for customized translation."),
          Markdown("[Project Page](https://www.mmlab-ntu.com/project/fresco/) | [GitHub repository](https://github.com/williamyang1991/FRESCO) | [Paper](https://arxiv.org/abs/2403.12962)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Credits go to Shuai Yang, Yifan Zhou, Ziwei Liu and Chen Change Loy")
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🍑  So Fresh... ", on_click=close_fresco_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(fresco_help_dlg)
      fresco_help_dlg.open = True
      page.update()
    def change_task(e):
        changed(e,'control_type')
        canny_threshold.height = None if fresco_prefs['control_type'] == "Canny" or fresco_prefs['control_type'] == "Canny21" else 0
        canny_threshold.update()
    def changed_model(e):
        fresco_prefs['sd_model'] = e.control.value
        sd_custom_model.visible = e.control.value == "Custom"
        sd_custom_model.update()
    def switch_version(e):
        page.Fresco = buildFrescoV2V(page)
        for t in page.VideoAIs.tabs:
          if t.text == "FRESCO":
            t.content = page.Fresco
            #t.icon = icons.PARK
            break
        page.VideoAIs.update()
        page.update()
    prompt = TextField(label="Animation Prompt Text", value=fresco_prefs['prompt'], filled=True, col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=fresco_prefs['negative_prompt'], filled=False, col={'md':4}, on_change=lambda e:changed(e,'negative_prompt'))
    a_prompt = TextField(label="Additional Prompt Text", value=fresco_prefs['a_prompt'], multiline=True, filled=True, col={'md':4}, on_change=lambda e:changed(e,'a_prompt'))
    control_type = Dropdown(label="ControlNet Task", width=150, options=[dropdown.Option("HED"), dropdown.Option("Canny"), dropdown.Option("Depth")], value=fresco_prefs['control_type'], on_change=change_task)
    control_strength = SliderRow(label="ControlNet Strength", min=0.0, max=2.0, divisions=40, round=2, pref=fresco_prefs, key='control_strength', tooltip="how well the output matches the input control edges.")
    x0_strength = SliderRow(label="Denoise Strength", min=0.0, max=1.05, divisions=21, round=2, pref=fresco_prefs, key='x0_strength', tooltip="Repaint degree, low to make output look more like init video. 0: fully recover the input.1.05: fully rerender the input.")
    low_threshold_row = SliderRow(label="Canny Low Threshold", min=1, max=255, divisions=254, pref=fresco_prefs, key='low_threshold', expand=True, col={'lg':6}, tooltip="Lower increases sensitivity to weaker edges, higher gives fewer but more reliable edge detections.")
    high_threshold_row = SliderRow(label="Canny High Threshold", min=1, max=255, divisions=254, pref=fresco_prefs, key='high_threshold', expand=True, col={'lg':6}, tooltip="Higher value decreases the amount of noise but could result in missing some true edges.")
    canny_threshold = Container(ResponsiveRow([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE, height = None if fresco_prefs['control_type'] == "Canny" else 0, padding=padding.only(bottom=8))
    b1 = SliderRow(label="b1", min=1.0, max=1.2, divisions=4, round=2, pref=fresco_prefs, key='b1', expand=True, col={'md':6}, tooltip="FreeU Backbone factor of the first stage block of decoder.")
    b2 = SliderRow(label="b2", min=1.2, max=1.6, divisions=8, round=2, pref=fresco_prefs, key='b2', expand=True, col={'md':6}, tooltip="FreeU Backbone factor of the second stage block of decoder.")
    s1 = SliderRow(label="s1", min=0, max=1, divisions=20, round=1, pref=fresco_prefs, key='s1', expand=True, col={'md':6}, tooltip="FreeU Skip factor of the first stage block of decoder.")
    s2 = SliderRow(label="s2", min=0, max=1, divisions=20, round=1, pref=fresco_prefs, key='s2', expand=True, col={'md':6}, tooltip="FreeU Skip factor of the second stage block of decoder.")
    freeu_args = Container(content=Column([ResponsiveRow([b1, b2, s1, s2])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(top=0, left=13))
    mininterv = SliderRow(label="Min Keyframe Interval", min=1, max=20, divisions=19, pref=fresco_prefs, key='mininterv', expand=True, col={'md':6}, tooltip="The keyframes will be detected at least every s_min frames.")
    maxinterv = SliderRow(label="Max Keyframe Interval", min=1, max=20, divisions=19, pref=fresco_prefs, key='maxinterv', expand=True, col={'md':6}, tooltip="The keyframes will be detected at most every s_max frames")
    max_process = SliderRow(label="Parallel Processes", min=1, max=16, divisions=15, pref=fresco_prefs, key='max_process', col={'md':6}, tooltip="Multiprocessing to speed up the process. Large value (4) is recommended.")
    init_video = FileInput(label="Initial Video Clip", pref=fresco_prefs, expand=True, key='init_video', ftype="video", page=page)
    num_frames = SliderRow(label="Number of Frames", min=1, max=300, divisions=299, pref=fresco_prefs, key='num_frames', tooltip="The number of video frames that are generated from init video.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=fresco_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=fresco_prefs, key='guidance_scale')
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=fresco_prefs, key='fps')
    batch_size = SliderRow(label="Batch Size", min=1, max=60, divisions=59, pref=fresco_prefs, key='batch_size', tooltip="Affects coherancy. To avoid out-of-memory, use small batch size.")
    sd_model = Dropdown(label="SD Model", width=280, options=[dropdown.Option(m) for m in fresco_models.keys()], value=fresco_prefs['sd_model'], on_change=changed_model)
    sd_model.options.append(dropdown.Option("Custom"))
    sd_custom_model = TextField(label="Custom SD Model (URL or Path)", value=fresco_prefs['custom_model'], expand=True, visible=fresco_prefs['sd_model']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    #eta_slider = SliderRow(label="ETA", min=0, max=1.0, divisions=20, round=1, pref=fresco_prefs, key='eta', tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.")
    #width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=fresco_prefs, key='width')
    #height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=fresco_prefs, key='height')
    #export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=fresco_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    max_size = SliderRow(label="Frame Resolution Size", min=256, max=512, divisions=4, multiple=64, suffix="px", pref=fresco_prefs, key='max_size', tooltip="Resize the Short side of the video.")
    bg_smooth = Switcher(label="Smooth Background", value=fresco_prefs['bg_smooth'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'bg_smooth'), tooltip="Best for static backgrounds.")
    use_poisson = Switcher(label="Gradient Blending", value=fresco_prefs['use_poisson'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_poisson'), tooltip="Poisson Blending in Gradient, to reduce ghosting artifacts (but may increase flickers)")
    #bg_smooth = Checkbox(label="Smooth Background", value=fresco_prefs['bg_smooth'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'bg_smooth'))
    #use_poisson = Checkbox(label="Gradient Blending", value=fresco_prefs['use_poisson'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'use_poisson'), tooltip="Blend the output video in gradient, to reduce ghosting artifacts (but may increase flickers)")
    batch_folder_name = TextField(label="Video Folder Name", value=fresco_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    project_name = TextField(label="Project Output Name", value=fresco_prefs['project_name'], on_change=lambda e:changed(e,'project_name'))
    seed = TextField(label="Seed", width=90, value=str(fresco_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    def changed_constraints(e):
        on = e.control.value
        if e.control.data in fresco_prefs['use_constraints']:
            fresco_prefs['use_constraints'].remove(e.control.data)
        else:
            fresco_prefs['use_constraints'].append(e.control.data)
    constraints = ResponsiveRow(controls=[Text("Use Constraints:", col={'xs':12, 'sm':6, 'md':4, 'lg':3, 'xl': 2})], run_spacing=0, vertical_alignment=CrossAxisAlignment.CENTER)
    for m in fresco_constraints:
        constraints.controls.append(Checkbox(label=m, data=m, value=m in fresco_prefs['use_constraints'], on_change=changed_constraints, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':4, 'lg':3, 'xl': 2}))
    #page.fresco_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)
    #clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    #clear_button.visible = len(page.fresco_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🌿  FRESCO Video-To-Video (under construction)", "Spatial-Temporal Correspondence for Zero-Shot Video Translation...", actions=[ft.OutlinedButton(content=Text("Switch to Diffusers FRESCO", size=18), on_click=switch_version), save_default(fresco_prefs, ['init_video']), IconButton(icon=icons.HELP, tooltip="Help with Fresco Settings", on_click=fresco_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        #init_video,
        ResponsiveRow([prompt, a_prompt]),
        negative_prompt,
        Row([control_type, init_video]),
        canny_threshold,
        control_strength,
        x0_strength,
        #Row([export_to_video, lower_memory]),
        num_inference_row,
        guidance,
        num_frames,
        #fps,
        freeu_args,
        ResponsiveRow([mininterv, maxinterv]),
        batch_size,
        max_process,
        max_size,
        #Text("Use Constraints:", col={'xs':12, 'sm':6, 'md':4, 'lg':3, 'xl': 2}),
        constraints,
        Row([bg_smooth, use_poisson]),
        Row([sd_model, sd_custom_model]),
        #Row([Text("Diffusion Mode: "), selected_mode]),
        #pnp_container,
        #sdedit_container,
        #eta_slider,
        #width_slider, height_slider,
        #page.ESRGAN_block_fresco,
        Row([seed, batch_folder_name]),
        Row([
            ElevatedButton(content=Text("🌱  Run Fresco", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_fresco(page)),
        ]),
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

fresco_v2v_prefs = {
    'init_video': '',
    'prompt': '',
    'negative_prompt': 'ugly, blurry, low res, unrealistic, unaesthetic',
    'a_prompt': '',
    'image_resolution': 512, #256-512 x64
    'control_strength': 1.0, #0-2 x0.01
    'x0_strength': 0.75,
    'control_type': 'HED',
    'low_threshold': 50,
    'high_threshold': 100,
    'num_inference_steps': 50,
    'guidance_scale': 7.5,
    'guess_mode': False,
    'control_guidance_start': 0.0,
    'control_guidance_end': 1.0,
    'end_opt_step': 15,
    'num_intraattn_steps':1,
    'step_interattn_end':350,
    #'use_constraints': ['Spatial-Guided Attention', 'Cross-Frame Attention', 'Temporal-Guided Attention', 'Spatial-Guided Optimization', 'Temporal-Guided Optimization'],
    #'max_process': 4, #1-16
    'use_ip_adapter': False,
    'ip_adapter_image': '',
    'ip_adapter_model': 'SD v1.5',
    'ip_adapter_strength': 0.8,
    'fps': 30,
    'num_frames': 100, #8-300
    'save_frames': False,
    'export_to_video': False,
    "interpolate_video": False,
    "source_fps": 8,
    "target_fps": 24,
    'seed': 0,
    'max_size': 672,
    'batch_size': 8, #3-8
    'batch_folder_name': '',
    'project_name': '',
}
def buildFrescoV2V(page):
    global fresco_v2v_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          fresco_v2v_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def fresco_v2v_help(e):
      def close_fresco_v2v_dlg(e):
        nonlocal fresco_v2v_help_dlg
        fresco_v2v_help_dlg.open = False
        page.update()
      fresco_v2v_help_dlg = AlertDialog(title=Text("💁   Help with Fresco Video-To-Video"), content=Column([
          Text("The remarkable efficacy of text-to-image diffusion models has motivated extensive exploration of their potential application in video domains. Zero-shot methods seek to extend image diffusion models to videos without necessitating model training. Recent methods mainly focus on incorporating inter-frame correspondence into attention mechanisms. However, the soft constraint imposed on determining where to attend to valid features can sometimes be insufficient, resulting in temporal inconsistency. In this paper, we introduce FRESCO, intra-frame correspondence alongside inter-frame correspondence to establish a more robust spatial-temporal constraint. This enhancement ensures a more consistent transformation of semantically similar content across frames. Beyond mere attention guidance, our approach involves an explicit update of features to achieve high spatial-temporal consistency with the input video, significantly improving the visual coherence of the resulting translated videos. Extensive experiments demonstrate the effectiveness of our proposed framework in producing high-quality, coherent videos, marking a notable improvement over existing zero-shot methods."),
          Text("Features: Temporal consistency: use intra-and inter-frame constraint with better consistency and coverage than optical flow alone. Compared with our previous work Rerender-A-Video, FRESCO is more robust to large and quick motion. Zero-shot: no training or fine-tuning required. Flexibility: compatible with off-the-shelf models (e.g., ControlNet, LoRA) for customized translation."),
          Markdown("[Project Page](https://www.mmlab-ntu.com/project/fresco_v2v/) | [GitHub repository](https://github.com/williamyang1991/FRESCO) | [Paper](https://arxiv.org/abs/2403.12962)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Credits go to Shuai Yang, Yifan Zhou, Ziwei Liu and Chen Change Loy")
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🍑  So Fresh... ", on_click=close_fresco_v2v_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(fresco_v2v_help_dlg)
      fresco_v2v_help_dlg.open = True
      page.update()
    def change_task(e):
        changed(e,'control_type')
        canny_threshold.height = None if fresco_v2v_prefs['control_type'] == "Canny" or fresco_v2v_prefs['control_type'] == "Canny21" else 0
        canny_threshold.update()
    def switch_version(e):
        page.Fresco = buildFresco(page)
        for t in page.VideoAIs.tabs:
          if t.text == "FRESCO":
            t.content = page.Fresco
            #t.icon = icons.PARK
            break
        page.VideoAIs.update()
        page.update()
    prompt = TextField(label="Animation Prompt Text", value=fresco_v2v_prefs['prompt'], filled=True, col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=fresco_v2v_prefs['negative_prompt'], filled=False, col={'md':4}, on_change=lambda e:changed(e,'negative_prompt'))
    a_prompt = TextField(label="Additional Prompt Text", value=fresco_v2v_prefs['a_prompt'], multiline=True, filled=True, col={'md':4}, on_change=lambda e:changed(e,'a_prompt'))
    control_type = Dropdown(label="ControlNet Task", width=150, options=[dropdown.Option("HED"), dropdown.Option("Canny"), dropdown.Option("Depth")], value=fresco_v2v_prefs['control_type'], on_change=change_task)
    control_strength = SliderRow(label="ControlNet Strength", min=0.0, max=2.0, divisions=40, round=2, pref=fresco_v2v_prefs, key='control_strength', tooltip="how well the output matches the input control edges.")
    x0_strength = SliderRow(label="Denoise Strength", min=0.0, max=1.05, divisions=21, round=2, pref=fresco_v2v_prefs, key='x0_strength', tooltip="Repaint degree, low to make output look more like init video. 0: fully recover the input.1.05: fully rerender the input.")
    low_threshold_row = SliderRow(label="Canny Low Threshold", min=1, max=255, divisions=254, pref=fresco_v2v_prefs, key='low_threshold', expand=True, col={'lg':6}, tooltip="Lower increases sensitivity to weaker edges, higher gives fewer but more reliable edge detections.")
    high_threshold_row = SliderRow(label="Canny High Threshold", min=1, max=255, divisions=254, pref=fresco_v2v_prefs, key='high_threshold', expand=True, col={'lg':6}, tooltip="Higher value decreases the amount of noise but could result in missing some true edges.")
    canny_threshold = Container(ResponsiveRow([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE, height = None if fresco_v2v_prefs['control_type'] == "Canny" else 0, padding=padding.only(bottom=8))
    control_guidance_start = SliderRow(label="Control Guidance Start", min=0, max=1.0, divisions=20, round=2, pref=fresco_v2v_prefs, key='control_guidance_start', expand=True, col={'md':6}, tooltip="The percentage of total steps at which the ControlNet starts applying.")
    control_guidance_end = SliderRow(label="Control Guidance End", min=0, max=1.0, divisions=20, round=2, pref=fresco_v2v_prefs, key='control_guidance_end', expand=True, col={'md':6}, tooltip="The percentage of total steps at which the ControlNet stops applying.")
    end_opt_step = SliderRow(label="End Optimization Step", min=1, max=60, divisions=59, pref=fresco_v2v_prefs, key='end_opt_step', tooltip="The feature optimization is activated from strength * num_inference_step to end_opt_step.")
    num_intraattn_steps = SliderRow(label="Attention Steps", min=1, max=60, divisions=59, pref=fresco_v2v_prefs, key='num_intraattn_steps', tooltip="Apply num_interattn_steps steps of spatial-guided attention.")
    step_interattn_end = SliderRow(label="Attention End Step", min=1, max=1000, divisions=999, pref=fresco_v2v_prefs, key='step_interattn_end', tooltip="Apply temporal-guided attention in [step_interattn_end, 1000] steps.")
    #max_process = SliderRow(label="Parallel Processes", min=1, max=16, divisions=15, pref=fresco_v2v_prefs, key='max_process', col={'md':6}, tooltip="Multiprocessing to speed up the process. Large value (4) is recommended.")
    init_video = FileInput(label="Initial Video Clip", pref=fresco_v2v_prefs, expand=True, key='init_video', ftype="video", page=page)
    #num_frames = SliderRow(label="Number of Frames", min=1, max=300, divisions=299, pref=fresco_v2v_prefs, key='num_frames', tooltip="The number of video frames that are generated from init video.")
    source_fps = SliderRow(label="Source FPS", min=1, max=30, suffix="fps", divisions=29, expand=1, pref=fresco_v2v_prefs, col={'md':6}, key='source_fps')
    target_fps = SliderRow(label="Target FPS", min=1, max=30, suffix="fps", divisions=29, expand=1, pref=fresco_v2v_prefs, col={'md':6}, key='target_fps')
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=fresco_v2v_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=fresco_v2v_prefs, key='guidance_scale')
    def toggle_ip_adapter(e):
        fresco_v2v_prefs['use_ip_adapter'] = e.control.value
        ip_adapter_container.height = None if e.control.value else 0
        ip_adapter_container.update()
        ip_adapter_model.visible = e.control.value
        ip_adapter_model.update()
    use_ip_adapter = Switcher(label="Use IP-Adapter Reference Image", value=fresco_v2v_prefs['use_ip_adapter'], on_change=toggle_ip_adapter, tooltip="Uses both image and text to condition the image generation process.")
    ip_adapter_model = Dropdown(label="IP-Adapter SD Model", width=220, options=[], value=fresco_v2v_prefs['ip_adapter_model'], visible=fresco_v2v_prefs['use_ip_adapter'], on_change=lambda e:changed(e,'ip_adapter_model'))
    for m in ip_adapter_models:
        ip_adapter_model.options.append(dropdown.Option(m['name']))
    ip_adapter_image = FileInput(label="IP-Adapter Image", pref=fresco_v2v_prefs, key='ip_adapter_image', page=page, col={'lg':6})
    ip_adapter_strength = SliderRow(label="IP-Adapter Strength", min=0.0, max=1.0, divisions=20, round=2, pref=fresco_v2v_prefs, key='ip_adapter_strength', col={'lg':6}, tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    ip_adapter_container = Container(Column([ResponsiveRow([ip_adapter_image, ip_adapter_strength]), Divider(thickness=4, height=4)]), height = None if fresco_v2v_prefs['use_ip_adapter'] else 0, padding=padding.only(top=3, left=12), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=fresco_v2v_prefs, key='fps')
    batch_size = SliderRow(label="Batch Size", min=1, max=60, divisions=59, pref=fresco_v2v_prefs, key='batch_size', tooltip="Affects coherancy. To avoid out-of-memory, use small batch size.")
    #eta_slider = SliderRow(label="ETA", min=0, max=1.0, divisions=20, round=1, pref=fresco_v2v_prefs, key='eta', tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.")
    #width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=fresco_v2v_prefs, key='width')
    #height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=fresco_v2v_prefs, key='height')
    #export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=fresco_v2v_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    max_size = SliderRow(label="Frame Resolution Size", min=256, max=512, divisions=4, multiple=64, suffix="px", pref=fresco_v2v_prefs, key='max_size', tooltip="Resize the Short side of the video.")
    save_frames = Switcher(label="Save Frames", value=fresco_v2v_prefs['save_frames'], tooltip="Save the dumped frames to images_out batch folder. Otherwise only saves final video, keeping pngs in temp folder.", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'save_frames'))
    interpolate_vid = Switcher(label="Interpolate", value=fresco_v2v_prefs['interpolate_video'], tooltip="Use Google FiLM Interpolation to transition between frames.", on_change=lambda e:changed(e,'interpolate_video'))
    guess_mode = Switcher(label="Guess Mode", value=fresco_v2v_prefs['guess_mode'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'guess_mode'), tooltip="The ControlNet encoder tries to recognize the content of the input image even if you remove all prompts. A `guidance_scale` value between 3.0 and 5.0 is recommended.")
    batch_folder_name = TextField(label="Video Folder Name", value=fresco_v2v_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    project_name = TextField(label="Project Output Name", value=fresco_v2v_prefs['project_name'], on_change=lambda e:changed(e,'project_name'))
    seed = TextField(label="Seed", width=90, value=str(fresco_v2v_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    #page.fresco_v2v_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)
    #clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    #clear_button.visible = len(page.fresco_v2v_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🌿  FRESCO Video-To-Video", "Spatial-Temporal Correspondence for Zero-Shot Video Translation...", actions=[ft.OutlinedButton(content=Text("Switch to Original FRESCO", size=18), on_click=switch_version), save_default(fresco_v2v_prefs, ['init_video']), IconButton(icon=icons.HELP, tooltip="Help with Fresco Settings", on_click=fresco_v2v_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        ResponsiveRow([prompt, a_prompt]),
        negative_prompt,
        Row([control_type, init_video]),
        canny_threshold,
        control_strength,
        x0_strength,
        num_inference_row,
        guidance,
        #num_frames,
        #fps,
        #freeu_args,
        ResponsiveRow([control_guidance_start, control_guidance_end]),
        end_opt_step,
        num_intraattn_steps,
        step_interattn_end,
        #batch_size,
        #max_process,
        Row([use_ip_adapter, ip_adapter_model], vertical_alignment=CrossAxisAlignment.START),
        ip_adapter_container,
        ResponsiveRow([source_fps, target_fps]),
        #max_size,
        Row([guess_mode, save_frames, interpolate_vid]),
        #eta_slider,
        #width_slider, height_slider,
        #page.ESRGAN_block_fresco_v2v,
        Row([seed, batch_folder_name]),
        Row([
            ElevatedButton(content=Text("🌱  Run Fresco", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_fresco_v2v(page)),
        ]),
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

latte_prefs = {
    'prompt': '',
    'negative_prompt': '',
    'num_inference_steps': 50,
    'guidance_scale': 7.5,
    'fps': 24,
    'num_frames': 16,
    'export_to_video': True,
    'seed': 0,
    'width': 512,
    'height': 512,
    'clean_caption': True,
    'cpu_offload': True,
    'num_images': 1,
    'batch_folder_name': '',
}

def buildLatte(page):
    global latte_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          latte_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.latte_output.controls = []
      page.latte_output.update()
      clear_button.visible = False
      clear_button.update()
    def latte_help(e):
      def close_latte_dlg(e):
        nonlocal latte_help_dlg
        latte_help_dlg.open = False
        page.update()
      latte_help_dlg = AlertDialog(title=Text("💁   Help with Latte Text-To-Video"), content=Column([
          Markdown("Latte is a latent diffusion transformer proposed as a backbone for modeling different modalities (trained for text-to-video generation here). It achieves state-of-the-art performance across four standard video benchmarks - [FaceForensics](https://arxiv.org/abs/1803.09179), [SkyTimelapse](https://arxiv.org/abs/1709.07592), [UCF101](https://arxiv.org/abs/1212.0402) and [Taichi-HD](https://arxiv.org/abs/2003.00196). To prepare and download the datasets for evaluation, please refer to [this https URL](https://github.com/Vchitect/Latte/blob/main/docs/datasets_evaluation.md).", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("We propose a novel Latent Diffusion Transformer, namely Latte, for video generation. Latte first extracts spatio-temporal tokens from input videos and then adopts a series of Transformer blocks to model video distribution in the latent space. In order to model a substantial number of tokens extracted from videos, four efficient variants are introduced from the perspective of decomposing the spatial and temporal dimensions of input videos. To improve the quality of generated videos, we determine the best practices of Latte through rigorous experimental analysis, including video clip patch embedding, model variants, timestep-class information injection, temporal positional embedding, and learning strategies. Our comprehensive evaluation demonstrates that Latte achieves state-of-the-art performance across four standard video generation datasets, i.e., FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In addition, we extend Latte to text-to-video generation (T2V) task, where Latte achieves comparable results compared to recent T2V models. We strongly believe that Latte provides valuable insights for future research on incorporating Transformers into diffusion models for video generation."),
          Markdown("[Project Page](https://maxin-cn.github.io/latte_project/) | [GitHub](https://github.com/Vchitect/Latte) | [Paper](https://arxiv.org/abs/2401.03048) | [Model](https://huggingface.co/maxin-cn/Latte-1)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🫖  Strong Cup... ", on_click=close_latte_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(latte_help_dlg)
      latte_help_dlg.open = True
      page.update()
    prompt = TextField(label="Animation Prompt Text", value=latte_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=latte_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    num_frames = SliderRow(label="Number of Frames", min=1, max=16, divisions=15, pref=latte_prefs, key='num_frames', tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=latte_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=latte_prefs, key='guidance_scale')
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=latte_prefs, key='fps')
    clean_caption = Switcher(label="Clean Caption", value=latte_prefs['clean_caption'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'clean_caption'), tooltip="Whether or not to clean the caption before creating embeddings.")
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=latte_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    width_slider = SliderRow(label="Width", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=latte_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=latte_prefs, key='height')
    num_images = NumberPicker(label="Number of Animations: ", min=1, max=12, value=latte_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))
    cpu_offload = Switcher(label="CPU Offload", value=latte_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    batch_folder_name = TextField(label="Video Folder Name", value=latte_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    seed = TextField(label="Seed", width=90, value=str(latte_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    page.latte_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.latte_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("☕️  Latte-1 Text-To-Video Synthesis", "Latent Diffusion Transformer for Video Generation...", actions=[save_default(latte_prefs), IconButton(icon=icons.HELP, tooltip="Help with Latte Settings", on_click=latte_help)]),
        ResponsiveRow([prompt, negative_prompt]),
        num_frames,
        fps,
        num_inference_row,
        guidance,
        width_slider, height_slider,
        Row([clean_caption, cpu_offload]),
        Row([num_images, seed, batch_folder_name]),
        Row([
            ElevatedButton(content=Text("🧋  Run Latte", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_latte(page)),
        ]),
        page.latte_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

open_sora_plan_prefs = {
    'prompt': '',
    'negative_prompt': '',
    'num_inference_steps': 40,
    'guidance_scale': 6.5,
    'fps': 24,
    'num_frames': 93,
    'export_to_video': True,
    'seed': 0,
    'width': 1280,
    'height': 720,
    'scheduler': 'EulerA-Solver',
    'generate_image': False,
    'speed_up_T5': True,
    'cpu_offload': True,
    'export_to_gif': False,
    'num_images': 1,
    'batch_size': 1,
    'batch_folder_name': '',
}

def buildOpenSoraPlan(page):
    global open_sora_plan_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          open_sora_plan_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def open_sora_plan_help(e):
      def close_open_sora_plan_dlg(e):
        nonlocal open_sora_plan_help_dlg
        open_sora_plan_help_dlg.open = False
        page.update()
      open_sora_plan_help_dlg = AlertDialog(title=Text("💁   Help with Open-Sora-Plan Text-To-Video"), content=Column([
          Text("We are thrilled to present Open-Sora-Plan v1.2.0, which significantly enhances video generation quality and text control capabilities. Thanks to HUAWEI Ascend Team for supporting us. The latest v1.2.0 utilizes a 3D full attention architecture instead of 2+1D. We released a true 3D video diffusion model trained on 4s 720p. In the second stage, we used Huawei Ascend computing power for training. This stage's training and inference were fully supported by Huawei. Models trained on Huawei Ascend can also be loaded into GPUs and generate videos of the same quality. Open-Sora Plan significantly improves video quality and length, and is fully open source! This project aims to create a simple and scalable repo, to reproduce Sora (OpenAI, but we prefer to call it 'ClosedAI' ). We wish the open-source community can contribute to this project."),
          Markdown("[GitHub](https://github.com/PKU-YuanGroup/Open-Sora-Plan) | [HF Space](https://huggingface.co/spaces/LanguageBind/Open-Sora-Plan-v1.1.0) | [Model](https://huggingface.co/LanguageBind/Open-Sora-Plan-v1.2.0) | [v1.2 Report](https://github.com/PKU-YuanGroup/Open-Sora-Plan/blob/main/docs/Report-v1.2.0.md)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🪽  Let's Soara... ", on_click=close_open_sora_plan_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(open_sora_plan_help_dlg)
      open_sora_plan_help_dlg.open = True
      page.update()
    prompt = TextField(label="Animation Prompt Text", value=open_sora_plan_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=open_sora_plan_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    #generate_image = Switcher(label="Generate Still Image", value=open_sora_plan_prefs['generate_image'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_image, tooltip="Create a single image instead of an animated sequence. Good for testing..")
    num_frames = SliderRow(label="Number of Frames", min=1, max=100, divisions=99, pref=open_sora_plan_prefs, key='num_frames', tooltip="The number of video frames that are generated. Defaults to 93 frames which at 8 frames per seconds amounts to 2 seconds of video.")
    #num_frames = Dropdown(label="Number of Frames", options=[dropdown.Option("1"), dropdown.Option("65"), dropdown.Option("221")], width=130, value=open_sora_plan_prefs['num_frames'], on_change=lambda e: changed(e, 'num_frames'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=open_sora_plan_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=open_sora_plan_prefs, key='guidance_scale')
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=open_sora_plan_prefs, key='fps')
    speed_up_T5 = Switcher(label="Speed up Text Encoder", value=open_sora_plan_prefs['speed_up_T5'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'speed_up_T5'), tooltip="Further optimize the T5 Encoder with Better Transformer.")
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=open_sora_plan_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=open_sora_plan_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=open_sora_plan_prefs, key='height')
    scheduler = Dropdown(label="De-noise Scheduler", width=220, options=[dropdown.Option(s) for s in ["PNDM-Solver", "EulerA-Solver", "DPM-Solver", "SA-Solver", "DDIM-Solver", "Euler-Solver", "DDPM-Solver", "DEISM-Solver"]], value=open_sora_plan_prefs['scheduler'], on_change=lambda e: changed(e, 'scheduler'))
    num_images = NumberPicker(label="Number of Animations: ", min=1, max=12, value=open_sora_plan_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))
    cpu_offload = Switcher(label="CPU Offload", value=open_sora_plan_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    batch_folder_name = TextField(label="Video Folder Name", value=open_sora_plan_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    seed = TextField(label="Seed", width=90, value=str(open_sora_plan_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🌴  Open-Sora-Plan v1.2 Text-To-Video Synthesis", "Transformer-based Text-to-Video Diffusion trained at 720p 93 Frames on Text Embeddings from mT5-xxl... (uses A LOT of VRAM & drive space)", actions=[save_default(open_sora_plan_prefs), IconButton(icon=icons.HELP, tooltip="Help with Open-Sora-Plan Settings", on_click=open_sora_plan_help)]),
        ResponsiveRow([prompt, negative_prompt]),
        num_inference_row,
        guidance,
        width_slider, height_slider,
        #generate_image,
        #Row([num_frames, fps]),
        num_frames, fps,
        Row([scheduler, speed_up_T5, cpu_offload]),
        Row([num_images, seed, batch_folder_name]),
        Row([
            ElevatedButton(content=Text("👒  Run Open-Sora-Plan", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_open_sora_plan(page)),
        ]),
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

video_infinity_prefs = {
    'prompt': '',
    'prompts': [],
    #'negative_prompt': '',
    'num_inference_steps': 30,
    'guidance_scale': 7.5,
    'fps': 8,
    'target_fps': 30,
    'num_frames': 24,
    'export_to_video': True,
    'seed': 0,
    'width': 512,
    'height': 320,
    'num_images': 1,
    'batch_folder_name': '',
}

def buildVideoInfinity(page):
    global video_infinity_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          video_infinity_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def video_infinity_help(e):
      def close_video_infinity_dlg(e):
        nonlocal video_infinity_help_dlg
        video_infinity_help_dlg.open = False
        page.update()
      video_infinity_help_dlg = AlertDialog(title=Text("💁   Help with VideoInfinity Text-To-Video"), content=Column([
          #Markdown("", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Distributed Long Video Generation Capable of generating videos with 2,300 frames in 5 minutes. 100 times faster than the prior methods. Diffusion models have recently achieved remarkable results for video generation. Despite the encouraging performances, the generated videos are typically constrained to a small number of frames, resulting in clips lasting merely a few seconds. The primary challenges in producing longer videos include the substantial memory requirements and the extended processing time required on a single GPU. A straightforward solution would be to split the workload across multiple GPUs, which, however, leads to two issues: (1) ensuring all GPUs communicate effectively to share timing and context information, and (2) modifying existing video diffusion models, which are usually trained on short sequences, to create longer videos without additional training. To tackle these, in this paper we introduce Video-Infinity, a distributed inference pipeline that enables parallel processing across multiple GPUs for long-form video generation. Specifically, we propose two coherent mechanisms: Clip parallelism and Dual-scope attention. Clip parallelism optimizes the gathering and sharing of context information across GPUs which minimizes communication overhead, while Dual-scope attention modulates the temporal self-attention to balance local and global contexts efficiently across the devices. Together, the two mechanisms join forces to distribute the workload and enable the fast generation of long videos. Under an 8 x Nvidia 6000 Ada GPU (48G) setup, our method generates videos up to 2,300 frames in approximately 5 minutes, enabling long video generation at a speed 100 times faster than the prior methods."),
          Markdown("[Project Page](https://video-infinity.tanzhenxiong.com/) | [GitHub](https://github.com/Yuanshi9815/Video-Infinity) | [Paper](https://arxiv.org/abs/2406.16260) | [Model](https://huggingface.co/adamdad/videocrafterv2_diffusers) | [VideoCrafter2](https://ailab-cvc.github.io/videocrafter2)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Credits go to Zhenxiong Tan, Xingyi Yang, Songhua Liu, and Xinchao Wang Learning and Vision Lab, National University of Singapore"),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🪘  Longer is Better... ", on_click=close_video_infinity_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(video_infinity_help_dlg)
      video_infinity_help_dlg.open = True
      page.update()
    def add_prompt(e):
        if not bool(video_infinity_prefs['prompt']): return
        layer = {'prompt': video_infinity_prefs['prompt']}
        video_infinity_prefs['prompts'].append(layer)
        prompt_layers.controls.append(ListTile(title=Row([Text(layer['prompt'], weight=FontWeight.BOLD)], alignment=MainAxisAlignment.SPACE_BETWEEN), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.EDIT, text="Edit Text Layer", on_click=edit_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE, text="Delete Text Layer", on_click=delete_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
              PopupMenuItem(icon=icons.ARROW_UPWARD, text="Move Up", on_click=move_up, data=layer),
              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text="Move Down", on_click=move_down, data=layer),
          ]), data=layer, on_click=edit_layer))
        prompt_layers.update()
        video_infinity_prefs['prompt'] = ""
        prompt.value = ""
        prompt.update()
    def delete_layer(e):
        video_infinity_prefs['prompts'].remove(e.control.data)
        for c in prompt_layers.controls:
          if c.data['prompt'] == e.control.data['prompt']:
              prompt_layers.controls.remove(c)
              break
        prompt_layers.update()
    def delete_all_layers(e):
        video_infinity_prefs['prompts'].clear()
        prompt_layers.controls.clear()
        prompt_layers.update()
    def move_down(e):
        idx = video_infinity_prefs['prompts'].index(e.control.data)
        if idx < (len(video_infinity_prefs['prompts']) - 1):
          d = video_infinity_prefs['prompts'].pop(idx)
          video_infinity_prefs['prompts'].insert(idx+1, d)
          dr = prompt_layers.controls.pop(idx)
          prompt_layers.controls.insert(idx+1, dr)
          prompt_layers.update()
    def move_up(e):
        idx = video_infinity_prefs['prompts'].index(e.control.data)
        if idx > 0:
          d = video_infinity_prefs['prompts'].pop(idx)
          video_infinity_prefs['prompts'].insert(idx-1, d)
          dr = prompt_layers.controls.pop(idx)
          prompt_layers.controls.insert(idx-1, dr)
          prompt_layers.update()
    def edit_layer(e):
        data = e.control.data
        prompt_value = data["prompt"]
        image_value = ""
        def close_dlg(e):
            dlg_edit.open = False
            page.update()
        def save_layer(e):
            layer = None
            for l in video_infinity_prefs['prompts']:
                if data["prompt"] == l["prompt"]:
                    layer = l
                    layer['prompt'] = prompt_text.value
                    break
            for c in prompt_layers.controls:
                if 'prompt' not in data: continue
                if c.data['prompt'] == data['prompt']:
                    c.title.controls[0].value = layer['prompt']
                    c.update()
                    break
            layer['prompt'] = prompt_text.value
            dlg_edit.open = False
            e.control.update()
            page.update()
        prompt_text = TextField(label="Interpolation Prompt Text", value=prompt_value, multiline=True)
        dlg_edit = AlertDialog(modal=False, title=Text(f"🧳 Edit Interpolation Prompt"), content=Container(Column([prompt_text], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO, width=(page.width if page.web else page.window.width) - 100)), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Layer ", size=19, weight=FontWeight.BOLD), on_click=save_layer)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(dlg_edit)
        dlg_edit.open = True
        page.update()
    def toggle_video(e):
        video_infinity_prefs['save_video'] = e.control.value
        video_container.visible = video_infinity_prefs['save_video']
        video_container.update()
    add_prompt_btn = ft.FilledButton("➕ Add Prompt", width=150, on_click=add_prompt)
    prompt = TextField(label="Animation Prompt Text", value=video_infinity_prefs['prompt'], filled=True, expand=True, multiline=True, on_submit=add_prompt, on_change=lambda e:changed(e,'prompt'))
    prompt_row = Row([prompt, add_prompt_btn])
    prompt_layers = Column([], spacing=0)
    #prompt = TextField(label="Animation Prompt Text", value=video_infinity_prefs['prompt'], filled=True, col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    #negative_prompt  = TextField(label="Negative Prompt Text", value=video_infinity_prefs['negative_prompt'], filled=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    num_frames = SliderRow(label="Number of Frames", min=1, max=240, divisions=239, pref=video_infinity_prefs, key='num_frames', tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=video_infinity_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=video_infinity_prefs, key='guidance_scale')
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=video_infinity_prefs, key='fps', col={'sm': 6})
    target_fps = SliderRow(label="Target FPS", min=0, max=60, suffix="fps", divisions=60, expand=1, pref=video_infinity_prefs, key='target_fps', col={'sm': 6})
    #export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=video_infinity_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    width_slider = SliderRow(label="Width", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=video_infinity_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=video_infinity_prefs, key='height')
    num_images = NumberPicker(label="Number of Animations: ", min=1, max=12, value=video_infinity_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))
    #cpu_offload = Switcher(label="CPU Offload", value=video_infinity_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 24GB VRAM. Otherwise can run out of memory.")
    batch_folder_name = TextField(label="Video Folder Name", value=video_infinity_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    seed = TextField(label="Seed", width=90, value=str(video_infinity_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("♾️  Video-Infinity VideoCrafter v2 Text-To-Video Synthesis", "Generates Long Videos Quickly using Multiple GPUs...", actions=[save_default(video_infinity_prefs), IconButton(icon=icons.HELP, tooltip="Help with VideoInfinity Settings", on_click=video_infinity_help)]),
        #ResponsiveRow([prompt, negative_prompt]),
        #prompt, #TODO: Prompts List
        prompt_row,
        prompt_layers,
        Divider(height=9, thickness=2),
        num_frames,
        #fps,
        num_inference_row,
        guidance,
        width_slider, height_slider,
        ResponsiveRow([fps, target_fps]),
        #Row([clean_caption, cpu_offload]),
        Row([num_images, seed, batch_folder_name]),
        Row([
            ElevatedButton(content=Text("🌌  Run Video-Infinity", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_video_infinity(page)),
        ]),
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c


materialdiffusion_prefs = {
    "material_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "material-",
    "num_outputs": 1,
    "steps":50,
    "eta":0.4,
    "width": 512,
    "height":512,
    "guidance_scale":7.5,
    "seed":0,
    "init_image": '',
    "prompt_strength": 0.5,
    "mask_image": '',
    "invert_mask": False,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    #"face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildMaterialDiffusion(page):
    global prefs, materialdiffusion_prefs, status

    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          materialdiffusion_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def changed_pref(e, pref=None):
      if pref is not None:
        prefs[pref] = e.control.value
        status['changed_prefs'] = True
    def materialdiffusion_help(e):
      alert_msg(page, "💁   Help with Material Diffusion", content=[
          Text("A fork of the Stable Diffusion Cog model that outputs tileable images for use in 3D applications such as Monaverse."),
          Markdown("[GitHub Project](https://github.com/TomMoore515/material_stable_diffusion) | [Replicate](https://replicate.com/tommoore515/material_stable_diffusion) | [Monaverse](https://monaverse.com/)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], okay="😖  Get Tiling... ", sound=False)
    def pick_files_result(e: FilePickerResultEvent):
        if e.files:
            img = e.files
            uf = []
            fname = img[0]
            print(", ".join(map(lambda f: f.name, e.files)))
            src_path = page.get_upload_url(fname.name, 600)
            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))
            pick_files_dialog.upload(uf)
            print(str(src_path))
            #src_path = ''.join(src_path)
            print(str(uf[0]))
            dst_path = os.path.join(root_dir, fname.name)
            print(f'Copy {src_path} to {dst_path}')
            #shutil.copy(src_path, dst_path)
            # TODO: is init or mask?
            init_image.value = dst_path
    pick_files_dialog = FilePicker(on_result=pick_files_result)
    page.overlay.append(pick_files_dialog)
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
            upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
        nonlocal pick_type
        if e.progress == 1:
            save_file(e.file_name)
    def save_file(file_name):
        if not slash in e.file_name:
          fname = os.path.join(root_dir, file_name)
        else:
          fname = file_name
        if pick_type == "init":
            init_image.value = fname
            init_image.update()
            materialdiffusion_prefs['init_image'] = fname
        elif pick_type == "mask":
            mask_image.value = fname
            mask_image.update()
            materialdiffusion_prefs['mask_image'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    #page.overlay.append(pick_files_dialog)
    def pick_init(e):
        nonlocal pick_type
        pick_type = "init"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Init Image File")
    def pick_mask(e):
        nonlocal pick_type
        pick_type = "mask"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Black & White Mask Image")
    def change_strength(e):
        strength_value.value = f" {int(e.control.value * 100)}%"
        strength_value.update()
        guidance.update()
        changed(e, 'prompt_strength', ptype="float")
    def switch_version(e):
        page.MaterialDiffusion = buildMaterialDiffusion_SDXL(page)
        for t in page.ImageAIs.tabs:
          if t.text == "Material Diffusion":
            t.content = page.MaterialDiffusion
            break
        page.ImageAIs.update()
        page.update()
    material_prompt = TextField(label="Material Prompt", value=materialdiffusion_prefs['material_prompt'], filled=True, multiline=True, on_change=lambda e:changed(e,'material_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=materialdiffusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=materialdiffusion_prefs['file_prefix'], width=150, on_change=lambda e:changed(e,'file_prefix'))
    #num_outputs = NumberPicker(label="Num of Outputs", min=1, max=4, step=4, value=materialdiffusion_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #num_outputs = TextField(label="num_outputs", value=materialdiffusion_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #n_iterations = TextField(label="Number of Iterations", value=materialdiffusion_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype="int"))
    steps = SliderRow(label="Inference Steps", min=0, max=200, divisions=200, pref=materialdiffusion_prefs, key='steps')
    eta = SliderRow(label="DDIM ETA", min=0, max=1, divisions=20, round=2, pref=materialdiffusion_prefs, key='eta')
    seed = TextField(label="Seed", value=materialdiffusion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width=120, on_change=lambda e:changed(e,'seed', ptype="int"))
    param_rows = ResponsiveRow([Column([batch_folder_name, file_prefix, NumberPicker(label="Output Images", min=1, max=8, step=1, value=materialdiffusion_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype="int"))], col={'xs':12, 'md':6}),
                      Column([steps, eta, seed], col={'xs':12, 'lg':6})], vertical_alignment=CrossAxisAlignment.START)
    batch_row = Row([batch_folder_name, file_prefix], col={'xs':12, 'lg':6})
    number_row = Row([NumberPicker(label="Output Images", min=1, max=4, step=3, value=materialdiffusion_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype="int")), seed], col={'xs':12, 'md':6})
    param_rows = ResponsiveRow([number_row, batch_row], vertical_alignment=CrossAxisAlignment.START)
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=materialdiffusion_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=materialdiffusion_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=materialdiffusion_prefs, key='height')
    init_image = FileInput(label="Init Image", pref=materialdiffusion_prefs, key='init_image', page=page, expand=True, col={'xs':12, 'md':6})
    mask_image = FileInput(label="Mask Image", pref=materialdiffusion_prefs, key='mask_image', page=page, expand=True, col={'xs':10, 'md':5})
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=materialdiffusion_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})
    image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    prompt_strength = Slider(min=0.1, max=0.9, divisions=16, label="{value}", round=2, value=materialdiffusion_prefs['prompt_strength'], on_change=change_strength, expand=True)
    strength_value = Text(f" {int(materialdiffusion_prefs['prompt_strength'] * 100)}%", weight=FontWeight.BOLD)
    strength_slider = Row([Text("Prompt Strength: "), strength_value, prompt_strength])
    #strength_slider = SliderRow(label="Prompt Strength", min=0.1, max=0.9, divisions=16, suffix="%", pref=materialdiffusion_prefs, key='prompt_strength')
    img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    api_instructions = Markdown("Get **Replicate API Token** from [https://replicate.com/account](https://replicate.com/account)", on_tap_link=lambda e: e.page.launch_url(e.data))
    Replicate_api = TextField(label="Replicate API Key", value=prefs['Replicate_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed_pref(e, 'Replicate_api_key'))
    upscaler = UpscaleBlock(materialdiffusion_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="💨   Run Material Diffusion", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_materialdiffusion(page))
    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)
    page.materialdiffusion_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🧱  Replicate Material Diffusion", "Create Seamless Tiled Textures with your Prompt. Requires account at Replicate.com and your Key.", actions=[ft.OutlinedButton(content=Text("Switch to SDXL", size=18), on_click=switch_version), save_default(materialdiffusion_prefs, ['init_image', 'mask_image']), IconButton(icon=icons.HELP, tooltip="Help with Material Diffusion Settings", on_click=materialdiffusion_help)]),
            material_prompt,
            steps,
            guidance,
            eta,
            width_slider, height_slider, #Divider(height=9, thickness=2),
            img_block, 
            upscaler,
            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
            api_instructions,
            Replicate_api,
            param_rows,
            #batch_row,
            #number_row,
            parameters_row,
            page.materialdiffusion_output
        ],
    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, eta, seed,
    return c

materialdiffusion_sdxl_prefs = {
    "material_prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "material-",
    "num_outputs": 1,
    "steps":50,
    "refine_steps":20,
    "scheduler": "K_EULER_ANCESTRAL",
    "width": "640",
    "height": "640",
    "guidance_scale":7.5,
    "refine": True,
    "high_noise_frac": 0.8,
    "seed":0,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    #"face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildMaterialDiffusion_SDXL(page):
    global prefs, materialdiffusion_sdxl_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          materialdiffusion_sdxl_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def changed_pref(e, pref=None):
      if pref is not None:
        prefs[pref] = e.control.value
        status['changed_prefs'] = True
    def materialdiffusion_sdxl_help(e):
      alert_msg(page, "💁   Help with Material Diffusion SDXL", content=[
          Text("A fork of the Stable Diffusion Cog model that outputs tileable images for use in 3D applications such as Monaverse."),
          Markdown("[GitHub Project](https://github.com/Pwntus/material-diffusion-sdxl) | [Replicate](https://replicate.com/pwntus/material-diffusion-sdxl) | [Monaverse](https://monaverse.com/)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], okay="😖  Get Tiling... ", sound=False)
    def switch_version(e):
        page.MaterialDiffusion = buildMaterialDiffusion(page)
        for t in page.ImageAIs.tabs:
          if t.text == "Material Diffusion":
            t.content = page.MaterialDiffusion
            break
        page.ImageAIs.update()
        page.update()
    material_prompt = TextField(label="Material Prompt", value=materialdiffusion_sdxl_prefs['material_prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'material_prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=materialdiffusion_sdxl_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=materialdiffusion_sdxl_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=materialdiffusion_sdxl_prefs['file_prefix'], width=150, on_change=lambda e:changed(e,'file_prefix'))
    steps = SliderRow(label="Inference Steps", min=0, max=100, divisions=100, pref=materialdiffusion_sdxl_prefs, key='steps')
    refine_steps = SliderRow(label="Refiner Steps", min=0, max=60, divisions=60, expand=True, pref=materialdiffusion_sdxl_prefs, key='refine_steps')
    refine = Switcher(label="Use Refiner ", value=materialdiffusion_sdxl_prefs['refine'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'refine'), tooltip="Uses Expert Ensemble Refiner to clean-up after generation.")
    seed = TextField(label="Seed", value=materialdiffusion_sdxl_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width=120, on_change=lambda e:changed(e,'seed', ptype="int"))
    batch_row = Row([batch_folder_name, file_prefix], col={'xs':12, 'lg':6})
    number_row = Row([NumberPicker(label="Output Images", min=1, max=4, step=1, value=materialdiffusion_sdxl_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype="int")), seed], col={'xs':12, 'md':6})
    param_rows = ResponsiveRow([number_row, batch_row], vertical_alignment=CrossAxisAlignment.START)
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=materialdiffusion_sdxl_prefs, key='guidance_scale')
    #width_slider = SliderRow(label="Width", min=128, max=1536, divisions=11, multiple=128, suffix="px", pref=materialdiffusion_sdxl_prefs, key='width')
    #height_slider = SliderRow(label="Height", min=128, max=1536, divisions=11, multiple=128, suffix="px", pref=materialdiffusion_sdxl_prefs, key='height')
    width = Dropdown(label="Width", width=90, options=[dropdown.Option(s) for s in ['128', '256', '384', '448', '512', '576', '640', '704', '768', '832', '896', '960', '1024', '1088', '1152', '1216', '1280', '1344', '1408', '1472', '1536', '1600']], value=materialdiffusion_sdxl_prefs['width'], on_change=lambda e: changed(e, 'width'))
    height = Dropdown(label="Height", width=90, options=[dropdown.Option(s) for s in ['128', '256', '384', '448', '512', '576', '640', '704', '768', '832', '896', '960', '1024', '1088', '1152', '1216', '1280', '1344', '1408', '1472', '1536', '1600']], value=materialdiffusion_sdxl_prefs['height'], on_change=lambda e: changed(e, 'height'))
    SDXL_high_noise_frac = SliderRow(label="SDXL High Noise Fraction", min=0, max=1, divisions=20, round=2, pref=materialdiffusion_sdxl_prefs, key='high_noise_frac', tooltip="Percentage of Steps to use Base model, then Refiner model. Known as an Ensemble of Expert Denoisers. Value of 1 skips Refine steps.", on_change=lambda e:changed(e,'high_noise_frac'))
    scheduler = Dropdown(label="De-noise Scheduler", width=220, options=[dropdown.Option(s) for s in ['DDIM', 'DPMSolverMultistep', 'HeunDiscrete', 'KarrasDPM', 'K_EULER_ANCESTRAL', 'K_EULER', 'PNDM']], value=materialdiffusion_sdxl_prefs['scheduler'], on_change=lambda e: changed(e, 'scheduler'))
    api_instructions = Markdown("Get **Replicate API Token** from [https://replicate.com/account](https://replicate.com/account)", on_tap_link=lambda e: e.page.launch_url(e.data))
    Replicate_api = TextField(label="Replicate API Key", value=prefs['Replicate_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed_pref(e, 'Replicate_api_key'))
    upscaler = UpscaleBlock(materialdiffusion_sdxl_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="💨   Run Material Diffusion SDXL", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_materialdiffusion_sdxl(page))
    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)
    page.materialdiffusion_sdxl_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🧱  Replicate Material Diffusion SDXL", "Create Seamless Tiled Textures with your Prompt. Requires account at Replicate.com and your Key.", actions=[ft.OutlinedButton(content=Text("Switch to SD1.5", size=18), on_click=switch_version), save_default(materialdiffusion_sdxl_prefs, ['init_image', 'mask_image']), IconButton(icon=icons.HELP, tooltip="Help with Material Diffusion SDXL Settings", on_click=materialdiffusion_sdxl_help)]),
            ResponsiveRow([material_prompt, negative_prompt]),
            steps,
            Row([refine, refine_steps]),
            SDXL_high_noise_frac,
            guidance,
            #width_slider, height_slider, #Divider(height=9, thickness=2),
            Row([width, Text("x"), height, Text("  "), scheduler]),
            upscaler,
            api_instructions,
            Replicate_api,
            param_rows,
            parameters_row,
            page.materialdiffusion_sdxl_output
        ],
    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, eta, seed,
    return c

ImageNet_classes = {'ATM': 480, 'Acinonyx jubatus': 293, 'Aepyceros melampus': 352, 'Afghan': 160, 'Afghan hound': 160, 'African chameleon': 47, 'African crocodile': 49, 'African elephant': 386, 'African gray': 87, 'African grey': 87, 'African hunting dog': 275, 'Ailuropoda melanoleuca': 388, 'Ailurus fulgens': 387, 'Airedale': 191, 'Airedale terrier': 191, 'Alaska crab': 121, 'Alaska king crab': 121, 'Alaskan king crab': 121, 'Alaskan malamute': 249, 'Alligator mississipiensis': 50, 'Alopex lagopus': 279, 'Ambystoma maculatum': 28, 'Ambystoma mexicanum': 29, 'American Staffordshire terrier': 180, 'American alligator': 50, 'American black bear': 295, 'American chameleon': 40, 'American coot': 137, 'American eagle': 22, 'American egret': 132, 'American lobster': 122, 'American pit bull terrier': 180, 'American robin': 15, 'Angora': 332, 'Angora rabbit': 332, 'Anolis carolinensis': 40, 'Appenzeller': 240, 'Aptenodytes patagonica': 145, 'Arabian camel': 354, 'Aramus pictus': 135, 'Aranea diademata': 74, 'Araneus cavaticus': 73, 'Arctic fox': 279, 'Arctic wolf': 270, 'Arenaria interpres': 139, 'Argiope aurantia': 72, 'Ascaphus trui': 32, 'Asiatic buffalo': 346, 'Ateles geoffroyi': 381, 'Australian terrier': 193, 'Band Aid': 419, 'Bedlington terrier': 181, 'Bernese mountain dog': 239, 'Biro': 418, 'Blenheim spaniel': 156, 'Bonasa umbellus': 82, 'Border collie': 232, 'Border terrier': 182, 'Boston bull': 195, 'Boston terrier': 195, 'Bouvier des Flandres': 233, 'Bouviers des Flandres': 233, 'Brabancon griffon': 262, 'Bradypus tridactylus': 364, 'Brittany spaniel': 215, 'Bubalus bubalis': 346, 'CD player': 485, 'CRO': 688, 'CRT screen': 782, 'Cacatua galerita': 89, 'Camelus dromedarius': 354, 'Cancer irroratus': 119, 'Cancer magister': 118, 'Canis dingo': 273, 'Canis latrans': 272, 'Canis lupus': 269, 'Canis lupus tundrarum': 270, 'Canis niger': 271, 'Canis rufus': 271, 'Cape hunting dog': 275, 'Capra ibex': 350, 'Carassius auratus': 1, 'Carcharodon carcharias': 2, 'Cardigan': 264, 'Cardigan Welsh corgi': 264, 'Carduelis carduelis': 11, 'Caretta caretta': 33, 'Carphophis amoenus': 52, 'Carpodacus mexicanus': 12, 'Cavia cobaya': 338, 'Cebus capucinus': 378, 'Cerastes cornutus': 66, 'Chamaeleo chamaeleon': 47, 'Chesapeake Bay retriever': 209, 'Chihuahua': 151, 'Chlamydosaurus kingi': 43, 'Christmas stocking': 496, 'Ciconia ciconia': 127, 'Ciconia nigra': 128, 'Constrictor constrictor': 61, 'Crock Pot': 521, 'Crocodylus niloticus': 49, 'Crotalus adamanteus': 67, 'Crotalus cerastes': 68, 'Cuon alpinus': 274, 'Cygnus atratus': 100, 'Cypripedium calceolus': 986, 'Cypripedium parviflorum': 986, 'Danaus plexippus': 323, 'Dandie Dinmont': 194, 'Dandie Dinmont terrier': 194, 'Dermochelys coriacea': 34, 'Doberman': 236, 'Doberman pinscher': 236, 'Dugong dugon': 149, 'Dungeness crab': 118, 'Dutch oven': 544, 'Egretta albus': 132, 'Egretta caerulea': 131, 'Egyptian cat': 285, 'Elephas maximus': 385, 'English cocker spaniel': 219, 'English foxhound': 167, 'English setter': 212, 'English springer': 217, 'English springer spaniel': 217, 'EntleBucher': 241, 'Erolia alpina': 140, 'Erythrocebus patas': 371, 'Eschrichtius gibbosus': 147, 'Eschrichtius robustus': 147, 'Eskimo dog': 248, 'Euarctos americanus': 295, 'European fire salamander': 25, 'European gallinule': 136, 'Felis concolor': 286, 'Felis onca': 290, 'French bulldog': 245, 'French horn': 566, 'French loaf': 930, 'Fringilla montifringilla': 10, 'Fulica americana': 137, 'Galeocerdo cuvieri': 3, 'German police dog': 235, 'German shepherd': 235, 'German shepherd dog': 235, 'German short-haired pointer': 210, 'Gila monster': 45, 'Gordon setter': 214, 'Gorilla gorilla': 366, 'Granny Smith': 948, 'Great Dane': 246, 'Great Pyrenees': 257, 'Greater Swiss Mountain dog': 238, 'Grifola frondosa': 996, 'Haliaeetus leucocephalus': 22, 'Heloderma suspectum': 45, 'Hippopotamus amphibius': 344, 'Holocanthus tricolor': 392, 'Homarus americanus': 122, 'Hungarian pointer': 211, 'Hylobates lar': 368, 'Hylobates syndactylus': 369, 'Hypsiglena torquata': 60, 'Ibizan Podenco': 173, 'Ibizan hound': 173, 'Iguana iguana': 39, 'Indian cobra': 63, 'Indian elephant': 385, 'Indri brevicaudatus': 384, 'Indri indri': 384, 'Irish setter': 213, 'Irish terrier': 184, 'Irish water spaniel': 221, 'Irish wolfhound': 170, 'Italian greyhound': 171, 'Japanese spaniel': 152, 'Kakatoe galerita': 89, 'Kerry blue terrier': 183, 'Komodo dragon': 48, 'Komodo lizard': 48, 'Labrador retriever': 208, 'Lacerta viridis': 46, 'Lakeland terrier': 189, 'Latrodectus mactans': 75, 'Lemur catta': 383, 'Leonberg': 255, 'Lepisosteus osseus': 395, 'Lhasa': 204, 'Lhasa apso': 204, 'Loafer': 630, 'Loxodonta africana': 386, 'Lycaon pictus': 275, 'Madagascar cat': 383, 'Maine lobster': 122, 'Maltese': 153, 'Maltese dog': 153, 'Maltese terrier': 153, 'Melursus ursinus': 297, 'Mergus serrator': 98, 'Mexican hairless': 268, 'Model T': 661, 'Mustela nigripes': 359, 'Mustela putorius': 358, 'Naja naja': 63, 'Nasalis larvatus': 376, 'Newfoundland': 256, 'Newfoundland dog': 256, 'Nile crocodile': 49, 'Norfolk terrier': 185, 'Northern lobster': 122, 'Norwegian elkhound': 174, 'Norwich terrier': 186, 'Old English sheepdog': 229, 'Oncorhynchus kisutch': 391, 'Orcinus orca': 148, 'Ornithorhynchus anatinus': 103, 'Ovis canadensis': 349, 'Pan troglodytes': 367, 'Panthera leo': 291, 'Panthera onca': 290, 'Panthera pardus': 288, 'Panthera tigris': 292, 'Panthera uncia': 289, 'Paralithodes camtschatica': 121, 'Passerina cyanea': 14, 'Peke': 154, 'Pekinese': 154, 'Pekingese': 154, 'Pembroke': 263, 'Pembroke Welsh corgi': 263, 'Persian cat': 283, 'Petri dish': 712, 'Phalangium opilio': 70, 'Phascolarctos cinereus': 105, 'Polaroid Land camera': 732, 'Polaroid camera': 732, 'Polyporus frondosus': 996, 'Pomeranian': 259, 'Pongo pygmaeus': 365, 'Porphyrio porphyrio': 136, 'Psittacus erithacus': 87, 'Python sebae': 62, 'R.V.': 757, 'RV': 757, 'Rana catesbeiana': 30, 'Rhodesian ridgeback': 159, 'Rocky Mountain bighorn': 349, 'Rocky Mountain sheep': 349, 'Rottweiler': 234, 'Russian wolfhound': 169, 'Saimiri sciureus': 382, 'Saint Bernard': 247, 'Salamandra salamandra': 25, 'Saluki': 176, 'Samoyed': 258, 'Samoyede': 258, 'Sciurus niger': 335, 'Scotch terrier': 199, 'Scottie': 199, 'Scottish deerhound': 177, 'Scottish terrier': 199, 'Sealyham': 190, 'Sealyham terrier': 190, 'Shetland': 230, 'Shetland sheep dog': 230, 'Shetland sheepdog': 230, 'Shih-Tzu': 155, 'Siamese': 284, 'Siamese cat': 284, 'Siberian husky': 250, 'St Bernard': 247, 'Staffordshire bull terrier': 179, 'Staffordshire bullterrier': 179, 'Staffordshire terrier': 180, 'Strix nebulosa': 24, 'Struthio camelus': 9, 'Sus scrofa': 342, 'Sussex spaniel': 220, 'Sydney silky': 201, 'Symphalangus syndactylus': 369, 'T-shirt': 610, 'Thalarctos maritimus': 296, 'Tibetan mastiff': 244, 'Tibetan terrier': 200, 'Tinca tinca': 0, 'Tringa totanus': 141, 'Triturus vulgaris': 26, 'Turdus migratorius': 15, 'U-boat': 833, 'Urocyon cinereoargenteus': 280, 'Ursus Maritimus': 296, 'Ursus americanus': 295, 'Ursus arctos': 294, 'Ursus ursinus': 297, 'Varanus komodoensis': 48, 'Virginia fence': 912, 'Vulpes macrotis': 278, 'Vulpes vulpes': 277, 'Walker foxhound': 166, 'Walker hound': 166, 'Weimaraner': 178, 'Welsh springer spaniel': 218, 'West Highland white terrier': 203, 'Windsor tie': 906, 'Yorkshire terrier': 187, 'abacus': 398, 'abaya': 399, 'academic gown': 400, 'academic robe': 400, 'accordion': 401, 'acorn': 988, 'acorn squash': 941, 'acoustic guitar': 402, 'admiral': 321, 'aegis': 461, 'affenpinscher': 252, 'agama': 42, 'agaric': 992, 'ai': 364, 'aircraft carrier': 403, 'airliner': 404, 'airship': 405, 'albatross': 146, 'all-terrain bike': 671, 'alligator lizard': 44, 'alp': 970, 'alsatian': 235, 'altar': 406, 'ambulance': 407, 'amphibian': 408, 'amphibious vehicle': 408, 'analog clock': 409, 'ananas': 953, 'anemone': 108, 'anemone fish': 393, 'anole': 40, 'ant': 310, 'anteater': 102, 'apiary': 410, 'apron': 411, 'armadillo': 363, 'armored combat vehicle': 847, 'armoured combat vehicle': 847, 'army tank': 847, 'artichoke': 944, 'articulated lorry': 867, 'ash bin': 412, 'ash-bin': 412, 'ashbin': 412, 'ashcan': 412, 'assault gun': 413, 'assault rifle': 413, 'attack aircraft carrier': 403, 'automated teller': 480, 'automated teller machine': 480, 'automatic teller': 480, 'automatic teller machine': 480, 'automatic washer': 897, 'axolotl': 29, 'baboon': 372, 'back pack': 414, 'backpack': 414, 'badger': 362, 'bagel': 931, 'bakehouse': 415, 'bakery': 415, 'bakeshop': 415, 'balance beam': 416, 'bald eagle': 22, 'balloon': 417, 'ballpen': 418, 'ballplayer': 981, 'ballpoint': 418, 'ballpoint pen': 418, 'balusters': 421, 'balustrade': 421, 'banana': 954, 'bandeau': 459, 'banded gecko': 38, 'banister': 421, 'banjo': 420, 'bannister': 421, 'barbell': 422, 'barber chair': 423, 'barbershop': 424, 'barn': 425, 'barn spider': 73, 'barometer': 426, 'barracouta': 389, 'barrel': 427, 'barrow': 428, 'bars': 702, 'baseball': 429, 'baseball player': 981, 'basenji': 253, 'basketball': 430, 'basset': 161, 'basset hound': 161, 'bassinet': 431, 'bassoon': 432, 'bath': 435, 'bath towel': 434, 'bathing cap': 433, 'bathing trunks': 842, 'bathing tub': 435, 'bathroom tissue': 999, 'bathtub': 435, 'beach waggon': 436, 'beach wagon': 436, 'beacon': 437, 'beacon light': 437, 'beagle': 162, 'beaker': 438, 'beam': 416, 'bear cat': 387, 'bearskin': 439, 'beaver': 337, 'bee': 309, 'bee eater': 92, 'bee house': 410, 'beer bottle': 440, 'beer glass': 441, 'beigel': 931, 'bell': 494, 'bell cot': 442, 'bell cote': 442, 'bell pepper': 945, 'bell toad': 32, 'bib': 443, 'bicycle-built-for-two': 444, 'bighorn': 349, 'bighorn sheep': 349, 'bikini': 445, 'billfish': 395, 'billfold': 893, 'billiard table': 736, 'binder': 446, 'binoculars': 447, 'birdhouse': 448, 'bison': 347, 'bittern': 133, 'black Maria': 734, 'black and gold garden spider': 72, 'black bear': 295, 'black grouse': 80, 'black stork': 128, 'black swan': 100, 'black widow': 75, 'black-and-tan coonhound': 165, 'black-footed ferret': 359, 'bloodhound': 163, 'blow drier': 589, 'blow dryer': 589, 'blower': 545, 'blowfish': 397, 'blue jack': 391, 'blue jean': 608, 'bluetick': 164, 'boa': 552, 'boa constrictor': 61, 'boar': 342, 'board': 532, 'boat paddle': 693, 'boathouse': 449, 'bob': 450, 'bobsled': 450, 'bobsleigh': 450, 'bobtail': 229, 'bola': 451, 'bola tie': 451, 'bolete': 997, 'bolo': 451, 'bolo tie': 451, 'bonnet': 452, 'book jacket': 921, 'bookcase': 453, 'bookshop': 454, 'bookstall': 454, 'bookstore': 454, 'borzoi': 169, 'bottle screw': 512, 'bottlecap': 455, 'bow': 456, 'bow tie': 457, 'bow-tie': 457, 'bowtie': 457, 'box tortoise': 37, 'box turtle': 37, 'boxer': 242, 'bra': 459, 'brain coral': 109, 'brambling': 10, 'brass': 458, 'brassiere': 459, 'breakwater': 460, 'breastplate': 461, 'briard': 226, 'bridegroom': 982, 'broccoli': 937, 'broom': 462, 'brown bear': 294, 'bruin': 294, 'brush kangaroo': 104, 'brush wolf': 272, 'bubble': 971, 'bucket': 463, 'buckeye': 990, 'buckle': 464, 'buckler': 787, 'bulbul': 16, 'bull mastiff': 243, 'bullet': 466, 'bullet train': 466, 'bulletproof vest': 465, 'bullfrog': 30, 'bulwark': 460, 'burrito': 965, 'busby': 439, 'bustard': 138, 'butcher shop': 467, 'butternut squash': 942, 'cab': 468, 'cabbage butterfly': 324, 'cairn': 192, 'cairn terrier': 192, 'caldron': 469, 'can opener': 473, 'candle': 470, 'candy store': 509, 'cannon': 471, 'canoe': 472, 'capitulum': 998, 'capuchin': 378, 'car mirror': 475, 'car wheel': 479, 'carabid beetle': 302, 'carbonara': 959, 'cardigan': 474, 'cardoon': 946, 'carousel': 476, "carpenter's kit": 477, "carpenter's plane": 726, 'carriage': 705, 'carriage dog': 251, 'carrier': 403, 'carrion fungus': 994, 'carrousel': 476, 'carton': 478, 'cash dispenser': 480, 'cash machine': 480, 'cask': 427, 'cassette': 481, 'cassette player': 482, 'castle': 483, 'cat bear': 387, 'catamaran': 484, 'catamount': 287, 'cathode-ray oscilloscope': 688, 'cauldron': 469, 'cauliflower': 938, 'cell': 487, 'cello': 486, 'cellphone': 487, 'cellular phone': 487, 'cellular telephone': 487, 'centipede': 79, 'cerastes': 66, 'chain': 488, 'chain armor': 490, 'chain armour': 490, 'chain mail': 490, 'chain saw': 491, 'chainlink fence': 489, 'chainsaw': 491, 'chambered nautilus': 117, 'cheeseburger': 933, 'cheetah': 293, 'chest': 492, 'chetah': 293, 'chickadee': 19, 'chiffonier': 493, 'chime': 494, 'chimp': 367, 'chimpanzee': 367, 'china cabinet': 495, 'china closet': 495, 'chiton': 116, 'chocolate sauce': 960, 'chocolate syrup': 960, 'chopper': 499, 'chow': 260, 'chow chow': 260, 'chrysanthemum dog': 200, 'chrysomelid': 304, 'church': 497, 'church building': 497, 'chute': 701, 'cicada': 316, 'cicala': 316, 'cimarron': 349, 'cinema': 498, 'claw': 600, 'cleaver': 499, 'cliff': 972, 'cliff dwelling': 500, 'cloak': 501, 'clog': 502, 'closet': 894, 'clumber': 216, 'clumber spaniel': 216, 'coach': 705, 'coach dog': 251, 'coast': 978, 'coat-of-mail shell': 116, 'cock': 7, 'cocker': 219, 'cocker spaniel': 219, 'cockroach': 314, 'cocktail shaker': 503, 'coffee mug': 504, 'coffeepot': 505, 'coho': 391, 'coho salmon': 391, 'cohoe': 391, 'coil': 506, 'collie': 231, 'colobus': 375, 'colobus monkey': 375, 'combination lock': 507, 'comfort': 750, 'comforter': 750, 'comic book': 917, 'commode': 493, 'common iguana': 39, 'common newt': 26, 'computer keyboard': 508, 'computer mouse': 673, 'conch': 112, 'confectionary': 509, 'confectionery': 509, 'conker': 990, 'consomme': 925, 'container ship': 510, 'container vessel': 510, 'containership': 510, 'convertible': 511, 'coon bear': 388, 'coral fungus': 991, 'coral reef': 973, 'corkscrew': 512, 'corn': 987, 'cornet': 513, 'cot': 520, 'cottontail': 330, 'cottontail rabbit': 330, 'coucal': 91, 'cougar': 286, 'courgette': 939, 'cowboy boot': 514, 'cowboy hat': 515, 'coyote': 272, 'cradle': 516, 'crampfish': 5, 'crane': 517, 'crash helmet': 518, 'crate': 519, 'crawdad': 124, 'crawdaddy': 124, 'crawfish': 124, 'crayfish': 124, 'crib': 520, 'cricket': 312, 'crinoline': 601, 'croquet ball': 522, 'crossword': 918, 'crossword puzzle': 918, 'crutch': 523, 'cucumber': 943, 'cuirass': 524, 'cuke': 943, 'cup': 968, 'curly-coated retriever': 206, 'custard apple': 956, 'daddy longlegs': 70, 'daisy': 985, 'dalmatian': 251, 'dam': 525, 'damselfly': 320, 'dark glasses': 837, 'darning needle': 319, 'day bed': 831, 'deerhound': 177, 'denim': 608, 'desk': 526, 'desktop computer': 527, "devil's darning needle": 319, 'devilfish': 147, 'dhole': 274, 'dial phone': 528, 'dial telephone': 528, 'diamondback': 67, 'diamondback rattlesnake': 67, 'diaper': 529, 'digital clock': 530, 'digital watch': 531, 'dike': 525, 'dingo': 273, 'dining table': 532, 'dipper': 20, 'dirigible': 405, 'disc brake': 535, 'dish washer': 534, 'dishcloth': 533, 'dishrag': 533, 'dishwasher': 534, 'dishwashing machine': 534, 'disk brake': 535, 'dock': 536, 'dockage': 536, 'docking facility': 536, 'dog sled': 537, 'dog sleigh': 537, 'dogsled': 537, 'dome': 538, 'doormat': 539, 'dough': 961, 'dowitcher': 142, 'dragon lizard': 48, 'dragonfly': 319, 'drake': 97, 'drilling platform': 540, 'dromedary': 354, 'drop': 972, 'drop-off': 972, 'drum': 541, 'drumstick': 542, 'duck-billed platypus': 103, 'duckbill': 103, 'duckbilled platypus': 103, 'dugong': 149, 'dumbbell': 543, 'dung beetle': 305, 'dunlin': 140, 'dust cover': 921, 'dust jacket': 921, 'dust wrapper': 921, 'dustbin': 412, 'dustcart': 569, 'dyke': 525, 'ear': 998, 'earthstar': 995, 'eastern fox squirrel': 335, 'eatery': 762, 'eating house': 762, 'eating place': 762, 'echidna': 102, 'eel': 390, 'eft': 27, 'eggnog': 969, 'egis': 461, 'electric fan': 545, 'electric guitar': 546, 'electric locomotive': 547, 'electric ray': 5, 'electric switch': 844, 'electrical switch': 844, 'elkhound': 174, 'emmet': 310, 'entertainment center': 548, 'envelope': 549, 'espresso': 967, 'espresso maker': 550, 'essence': 711, 'estate car': 436, 'ewer': 725, 'face powder': 551, 'feather boa': 552, 'ferret': 359, 'fiddle': 889, 'fiddler crab': 120, 'field glasses': 447, 'fig': 952, 'file': 553, 'file cabinet': 553, 'filing cabinet': 553, 'fire engine': 555, 'fire screen': 556, 'fire truck': 555, 'fireboat': 554, 'fireguard': 556, 'fitch': 358, 'fixed disk': 592, 'flagpole': 557, 'flagstaff': 557, 'flamingo': 130, 'flat-coated retriever': 205, 'flattop': 403, 'flatworm': 110, 'flowerpot': 738, 'flute': 558, 'fly': 308, 'folding chair': 559, 'food market': 582, 'football helmet': 560, 'footstall': 708, 'foreland': 976, 'forklift': 561, 'foulmart': 358, 'foumart': 358, 'fountain': 562, 'fountain pen': 563, 'four-poster': 564, 'fox squirrel': 335, 'freight car': 565, 'frilled lizard': 43, 'frying pan': 567, 'frypan': 567, 'fur coat': 568, 'gar': 395, 'garbage can': 412, 'garbage truck': 569, 'garden cart': 428, 'garden spider': 74, 'garfish': 395, 'garpike': 395, 'garter snake': 57, 'gas helmet': 570, 'gas pump': 571, 'gasmask': 570, 'gasoline pump': 571, 'gazelle': 353, 'gazelle hound': 176, 'geta': 502, 'geyser': 974, 'giant lizard': 48, 'giant panda': 388, 'giant schnauzer': 197, 'gibbon': 368, 'glasshouse': 580, 'globe artichoke': 944, 'globefish': 397, 'go-kart': 573, 'goblet': 572, 'golden retriever': 207, 'goldfinch': 11, 'goldfish': 1, 'golf ball': 574, 'golf cart': 575, 'golfcart': 575, 'gondola': 576, 'gong': 577, 'goose': 99, 'gorilla': 366, 'gown': 578, 'grampus': 148, 'grand': 579, 'grand piano': 579, 'grass snake': 57, 'grasshopper': 311, 'gray fox': 280, 'gray whale': 147, 'gray wolf': 269, 'great gray owl': 24, 'great grey owl': 24, 'great white heron': 132, 'great white shark': 2, 'green lizard': 46, 'green mamba': 64, 'green snake': 55, 'greenhouse': 580, 'grey fox': 280, 'grey whale': 147, 'grey wolf': 269, 'grille': 581, 'grocery': 582, 'grocery store': 582, 'groenendael': 224, 'groin': 460, 'groom': 982, 'ground beetle': 302, 'groyne': 460, 'grunter': 341, 'guacamole': 924, 'guenon': 370, 'guenon monkey': 370, 'guillotine': 583, 'guinea pig': 338, 'gyromitra': 993, 'hack': 468, 'hair drier': 589, 'hair dryer': 589, 'hair slide': 584, 'hair spray': 585, 'half track': 586, 'hammer': 587, 'hammerhead': 4, 'hammerhead shark': 4, 'hamper': 588, 'hamster': 333, 'hand blower': 589, 'hand-held computer': 590, 'hand-held microcomputer': 590, 'handbasin': 896, 'handkerchief': 591, 'handrail': 421, 'hankey': 591, 'hankie': 591, 'hanky': 591, 'hard disc': 592, 'hard disk': 592, 'hare': 331, 'harmonica': 593, 'harp': 594, 'hartebeest': 351, 'harvester': 595, 'harvestman': 70, 'hatchet': 596, 'hautbois': 683, 'hautboy': 683, 'haversack': 414, 'hay': 958, 'head': 976, 'head cabbage': 936, 'headland': 976, 'hedgehog': 334, 'helix': 506, 'hen': 8, 'hen of the woods': 996, 'hen-of-the-woods': 996, 'hermit crab': 125, 'high bar': 602, 'hip': 989, 'hippo': 344, 'hippopotamus': 344, 'hockey puck': 746, 'hodometer': 685, 'hog': 341, 'hognose snake': 54, 'holothurian': 329, 'holster': 597, 'home theater': 598, 'home theatre': 598, 'honeycomb': 599, 'hook': 600, 'hoopskirt': 601, 'hopper': 311, 'horizontal bar': 602, 'horn': 566, 'hornbill': 93, 'horned asp': 66, 'horned rattlesnake': 68, 'horned viper': 66, 'horse cart': 603, 'horse chestnut': 990, 'horse-cart': 603, 'hot dog': 934, 'hot pot': 926, 'hotdog': 934, 'hotpot': 926, 'hourglass': 604, 'house finch': 12, 'howler': 379, 'howler monkey': 379, 'hummingbird': 94, 'hunting spider': 77, 'husky': 248, 'hussar monkey': 371, 'hyaena': 276, 'hyena': 276, 'hyena dog': 275, 'iPod': 605, 'ibex': 350, 'ice bear': 296, 'ice cream': 928, 'ice lolly': 929, 'icebox': 760, 'icecream': 928, 'igniter': 626, 'ignitor': 626, 'iguana': 39, 'impala': 352, 'indigo bird': 14, 'indigo bunting': 14, 'indigo finch': 14, 'indri': 384, 'indris': 384, 'internet site': 916, 'iron': 606, 'island dispenser': 571, 'isopod': 126, 'jacamar': 95, 'jack': 955, "jack-o'-lantern": 607, 'jackfruit': 955, 'jaguar': 290, 'jak': 955, 'jammies': 697, 'jay': 17, 'jean': 608, 'jeep': 609, 'jellyfish': 107, 'jersey': 610, 'jetty': 460, "jeweler's loupe": 633, 'jigsaw puzzle': 611, 'jinrikisha': 612, 'joystick': 613, "judge's robe": 400, 'junco': 13, 'kangaroo bear': 105, 'keeshond': 261, 'kelpie': 227, 'keypad': 508, 'killer': 148, 'killer whale': 148, 'kimono': 614, 'king crab': 121, 'king of beasts': 291, 'king penguin': 145, 'king snake': 56, 'kingsnake': 56, 'kit fox': 278, 'kite': 21, 'knapsack': 414, 'knee pad': 615, 'knot': 616, 'koala': 105, 'koala bear': 105, 'komondor': 228, 'kuvasz': 222, 'lab coat': 617, 'laboratory coat': 617, 'labyrinth': 646, 'lacewing': 318, 'lacewing fly': 318, 'ladle': 618, 'lady beetle': 301, 'ladybeetle': 301, 'ladybird': 301, 'ladybird beetle': 301, 'ladybug': 301, 'lakeshore': 975, 'lakeside': 975, 'lamp shade': 619, 'lampshade': 619, 'landrover': 609, 'langouste': 123, 'langur': 374, 'laptop': 620, 'laptop computer': 620, 'lavabo': 896, 'lawn cart': 428, 'lawn mower': 621, 'leaf beetle': 304, 'leafhopper': 317, 'leatherback': 34, 'leatherback turtle': 34, 'leathery turtle': 34, 'lemon': 951, 'lens cap': 622, 'lens cover': 622, 'leopard': 288, 'lesser panda': 387, 'letter box': 637, 'letter opener': 623, 'library': 624, 'lifeboat': 625, 'light': 626, 'lighter': 626, 'lighthouse': 437, 'limo': 627, 'limousine': 627, 'limpkin': 135, 'liner': 628, 'linnet': 12, 'lion': 291, 'lionfish': 396, 'lip rouge': 629, 'lipstick': 629, 'little blue heron': 131, 'llama': 355, 'loggerhead': 33, 'loggerhead turtle': 33, 'lollipop': 929, 'lolly': 929, 'long-horned beetle': 303, 'longicorn': 303, 'longicorn beetle': 303, 'lorikeet': 90, 'lotion': 631, 'loudspeaker': 632, 'loudspeaker system': 632, 'loupe': 633, 'lumbermill': 634, 'lycaenid': 326, 'lycaenid butterfly': 326, 'lynx': 287, 'macaque': 373, 'macaw': 88, 'magnetic compass': 635, 'magpie': 18, 'mail': 490, 'mailbag': 636, 'mailbox': 637, 'maillot': 639, 'malamute': 249, 'malemute': 249, 'malinois': 225, 'man-eater': 2, 'man-eating shark': 2, 'maned wolf': 271, 'manhole cover': 640, 'mantid': 315, 'mantis': 315, 'manufactured home': 660, 'maraca': 641, 'marimba': 642, 'market': 582, 'marmoset': 377, 'marmot': 336, 'marsh hen': 137, 'mashed potato': 935, 'mask': 643, 'matchstick': 644, 'maypole': 645, 'maze': 646, 'measuring cup': 647, 'meat cleaver': 499, 'meat loaf': 962, 'meat market': 467, 'meatloaf': 962, 'medicine cabinet': 648, 'medicine chest': 648, 'meerkat': 299, 'megalith': 649, 'megalithic structure': 649, 'membranophone': 541, 'memorial tablet': 458, 'menu': 922, 'merry-go-round': 476, 'microphone': 650, 'microwave': 651, 'microwave oven': 651, 'mierkat': 299, 'mike': 650, 'mileometer': 685, 'military plane': 895, 'military uniform': 652, 'milk can': 653, 'milkweed butterfly': 323, 'milometer': 685, 'mini': 655, 'miniature pinscher': 237, 'miniature poodle': 266, 'miniature schnauzer': 196, 'minibus': 654, 'miniskirt': 655, 'minivan': 656, 'mink': 357, 'missile': 744, 'mitten': 658, 'mixing bowl': 659, 'mobile home': 660, 'mobile phone': 487, 'modem': 662, 'mole': 460, 'mollymawk': 146, 'monarch': 323, 'monarch butterfly': 323, 'monastery': 663, 'mongoose': 298, 'monitor': 664, 'monkey dog': 252, 'monkey pinscher': 252, 'monocycle': 880, 'mop': 840, 'moped': 665, 'mortar': 666, 'mortarboard': 667, 'mosque': 668, 'mosquito hawk': 319, 'mosquito net': 669, 'motor scooter': 670, 'mountain bike': 671, 'mountain lion': 286, 'mountain tent': 672, 'mouse': 673, 'mousetrap': 674, 'mouth harp': 593, 'mouth organ': 593, 'movie house': 498, 'movie theater': 498, 'movie theatre': 498, 'moving van': 675, 'mower': 621, 'mud hen': 137, 'mud puppy': 29, 'mud turtle': 35, 'mushroom': 947, 'muzzle': 676, 'nail': 677, 'napkin': 529, 'nappy': 529, 'native bear': 105, 'nautilus': 117, 'neck brace': 678, 'necklace': 679, 'nematode': 111, 'nematode worm': 111, 'night snake': 60, 'nipple': 680, 'notebook': 681, 'notebook computer': 681, 'notecase': 893, 'nudibranch': 115, 'numbfish': 5, 'nursery': 580, 'obelisk': 682, 'oboe': 683, 'ocarina': 684, 'ocean liner': 628, 'odometer': 685, 'off-roader': 671, 'offshore rig': 540, 'oil filter': 686, 'one-armed bandit': 800, 'opera glasses': 447, 'orang': 365, 'orange': 950, 'orangutan': 365, 'orangutang': 365, 'orca': 148, 'organ': 687, 'oscilloscope': 688, 'ostrich': 9, 'otter': 360, 'otter hound': 175, 'otterhound': 175, 'ounce': 289, 'overskirt': 689, 'ox': 345, 'oxcart': 690, 'oxygen mask': 691, 'oyster catcher': 143, 'oystercatcher': 143, 'packet': 692, 'packsack': 414, 'paddle': 693, 'paddle wheel': 694, 'paddlewheel': 694, 'paddy wagon': 734, 'padlock': 695, 'pail': 463, 'paintbrush': 696, 'painter': 286, 'pajama': 697, 'palace': 698, 'paling': 716, 'panda': 388, 'panda bear': 388, 'pandean pipe': 699, 'panpipe': 699, 'panther': 290, 'paper knife': 623, 'paper towel': 700, 'paperknife': 623, 'papillon': 157, 'parachute': 701, 'parallel bars': 702, 'park bench': 703, 'parking meter': 704, 'partridge': 86, 'passenger car': 705, 'patas': 371, 'patio': 706, 'patrol wagon': 734, 'patten': 502, 'pay-phone': 707, 'pay-station': 707, 'peacock': 84, 'pearly nautilus': 117, 'pedestal': 708, 'pelican': 144, 'pencil box': 709, 'pencil case': 709, 'pencil eraser': 767, 'pencil sharpener': 710, 'penny bank': 719, 'perfume': 711, 'petrol pump': 571, 'pharos': 437, 'photocopier': 713, 'piano accordion': 401, 'pick': 714, 'pickelhaube': 715, 'picket fence': 716, 'pickup': 717, 'pickup truck': 717, 'picture palace': 498, 'pier': 718, 'pig': 341, 'pigboat': 833, 'piggy bank': 719, 'pill bottle': 720, 'pillow': 721, 'pineapple': 953, 'ping-pong ball': 722, 'pinwheel': 723, 'pipe organ': 687, 'pirate': 724, 'pirate ship': 724, 'pismire': 310, 'pit bull terrier': 180, 'pitcher': 725, 'pizza': 963, 'pizza pie': 963, "pj's": 697, 'plane': 726, 'planetarium': 727, 'plaque': 458, 'plastic bag': 728, 'plate': 923, 'plate rack': 729, 'platyhelminth': 110, 'platypus': 103, 'plectron': 714, 'plectrum': 714, 'plinth': 708, 'plough': 730, 'plow': 730, "plumber's helper": 731, 'plunger': 731, 'pocketbook': 893, 'poke bonnet': 452, 'polar bear': 296, 'pole': 733, 'polecat': 361, 'police van': 734, 'police wagon': 734, 'polyplacophore': 116, 'pomegranate': 957, 'poncho': 735, 'pool table': 736, 'pop bottle': 737, 'popsicle': 929, 'porcupine': 334, 'postbag': 636, 'pot': 738, 'potpie': 964, "potter's wheel": 739, 'power drill': 740, 'prairie chicken': 83, 'prairie fowl': 83, 'prairie grouse': 83, 'prairie wolf': 272, 'prayer mat': 741, 'prayer rug': 741, 'press': 894, 'pretzel': 932, 'printer': 742, 'prison': 743, 'prison house': 743, 'proboscis monkey': 376, 'projectile': 744, 'projector': 745, 'promontory': 976, 'ptarmigan': 81, 'puck': 746, 'puff': 750, 'puff adder': 54, 'puffer': 397, 'pufferfish': 397, 'pug': 254, 'pug-dog': 254, 'puma': 286, 'punch bag': 747, 'punchball': 747, 'punching bag': 747, 'punching ball': 747, 'purse': 748, 'pyjama': 697, 'quail': 85, 'quill': 749, 'quill pen': 749, 'quilt': 750, 'race car': 751, 'racer': 751, 'racing car': 751, 'racket': 752, 'racquet': 752, 'radiator': 753, 'radiator grille': 581, 'radio': 754, 'radio reflector': 755, 'radio telescope': 755, 'rain barrel': 756, 'ram': 348, 'rapeseed': 984, 'reaper': 595, 'recreational vehicle': 757, 'red fox': 277, 'red hot': 934, 'red panda': 387, 'red setter': 213, 'red wine': 966, 'red wolf': 271, 'red-backed sandpiper': 140, 'red-breasted merganser': 98, 'redbone': 168, 'redshank': 141, 'reel': 758, 'reflex camera': 759, 'refrigerator': 760, 'remote': 761, 'remote control': 761, 'respirator': 570, 'restaurant': 762, 'revolver': 763, 'rhinoceros beetle': 306, 'ribbed toad': 32, 'ricksha': 612, 'rickshaw': 612, 'rifle': 764, 'rig': 867, 'ring armor': 490, 'ring armour': 490, 'ring mail': 490, 'ring snake': 53, 'ring-binder': 446, 'ring-necked snake': 53, 'ring-tailed lemur': 383, 'ringlet': 322, 'ringlet butterfly': 322, 'ringneck snake': 53, 'ringtail': 378, 'river horse': 344, 'roach': 314, 'robin': 15, 'rock beauty': 392, 'rock crab': 119, 'rock lobster': 123, 'rock python': 62, 'rock snake': 62, 'rocker': 765, 'rocking chair': 765, 'rose hip': 989, 'rosehip': 989, 'rotisserie': 766, 'roundabout': 476, 'roundworm': 111, 'rubber': 767, 'rubber eraser': 767, 'rucksack': 414, 'ruddy turnstone': 139, 'ruffed grouse': 82, 'rugby ball': 768, 'rule': 769, 'ruler': 769, 'running shoe': 770, 'sabot': 502, 'safe': 771, 'safety pin': 772, 'salt shaker': 773, 'saltshaker': 773, 'sand bar': 977, 'sand viper': 66, 'sandal': 774, 'sandbar': 977, 'sarong': 775, 'sawmill': 634, 'sax': 776, 'saxophone': 776, 'scabbard': 777, 'scale': 778, 'schipperke': 223, 'school bus': 779, 'schooner': 780, 'scooter': 670, 'scope': 688, 'scoreboard': 781, 'scorpion': 71, 'screen': 782, 'screw': 783, 'screwdriver': 784, 'scuba diver': 983, 'sea anemone': 108, 'sea cradle': 116, 'sea crawfish': 123, 'sea cucumber': 329, 'sea lion': 150, 'sea slug': 115, 'sea snake': 65, 'sea star': 327, 'sea urchin': 328, 'sea wolf': 148, 'sea-coast': 978, 'seacoast': 978, 'seashore': 978, 'seat belt': 785, 'seatbelt': 785, 'seawall': 460, 'semi': 867, 'sewing machine': 786, 'sewing needle': 319, 'shades': 837, 'shako': 439, 'shield': 787, 'shoe shop': 788, 'shoe store': 788, 'shoe-shop': 788, 'shoji': 789, 'shopping basket': 790, 'shopping cart': 791, 'shovel': 792, 'shower cap': 793, 'shower curtain': 794, 'siamang': 369, 'sidewinder': 68, 'silky terrier': 201, 'silver salmon': 391, 'site': 916, 'six-gun': 763, 'six-shooter': 763, 'skeeter hawk': 319, 'ski': 795, 'ski mask': 796, 'skillet': 567, 'skunk': 361, 'sleeping bag': 797, 'sleuthhound': 163, 'slide rule': 798, 'sliding door': 799, 'slipstick': 798, 'slot': 800, 'sloth bear': 297, 'slug': 114, 'smoothing iron': 606, 'snail': 113, 'snake doctor': 319, 'snake feeder': 319, 'snake fence': 912, 'snake-rail fence': 912, 'snoek': 389, 'snooker table': 736, 'snorkel': 801, 'snow leopard': 289, 'snowbird': 13, 'snowmobile': 802, 'snowplough': 803, 'snowplow': 803, 'soap dispenser': 804, 'soccer ball': 805, 'sock': 806, 'soda bottle': 737, 'soft-coated wheaten terrier': 202, 'solar collector': 807, 'solar dish': 807, 'solar furnace': 807, 'sombrero': 808, 'sorrel': 339, 'soup bowl': 809, 'space bar': 810, 'space heater': 811, 'space shuttle': 812, 'spaghetti squash': 940, 'spatula': 813, 'speaker': 632, 'speaker system': 632, 'speaker unit': 632, 'speedboat': 814, 'spider monkey': 381, 'spider web': 815, "spider's web": 815, 'spike': 998, 'spindle': 816, 'spiny anteater': 102, 'spiny lobster': 123, 'spiral': 506, 'spoonbill': 129, 'sport car': 817, 'sports car': 817, 'spot': 818, 'spotlight': 818, 'spotted salamander': 28, 'squealer': 341, 'squeeze box': 401, 'squirrel monkey': 382, 'stage': 819, 'standard poodle': 267, 'standard schnauzer': 198, 'starfish': 327, 'station waggon': 436, 'station wagon': 436, 'steam locomotive': 820, 'steel arch bridge': 821, 'steel drum': 822, 'stethoscope': 823, 'stick insect': 313, 'stingray': 6, 'stinkhorn': 994, 'stole': 824, 'stone wall': 825, 'stop watch': 826, 'stoplight': 920, 'stopwatch': 826, 'stove': 827, 'strainer': 828, 'strawberry': 949, 'street sign': 919, 'streetcar': 829, 'stretcher': 830, 'studio couch': 831, 'stupa': 832, 'sturgeon': 394, 'sub': 833, 'submarine': 833, 'suit': 834, 'suit of clothes': 834, 'sulfur butterfly': 325, 'sulphur butterfly': 325, 'sulphur-crested cockatoo': 89, 'sun blocker': 838, 'sunblock': 838, 'sundial': 835, 'sunglass': 836, 'sunglasses': 837, 'sunscreen': 838, 'suspension bridge': 839, 'swab': 840, 'sweatshirt': 841, 'sweet potato': 684, 'swimming cap': 433, 'swimming trunks': 842, 'swing': 843, 'switch': 844, 'swob': 840, 'syringe': 845, 'syrinx': 699, 'tabby': 281, 'tabby cat': 281, 'table lamp': 846, 'tailed frog': 32, 'tailed toad': 32, 'tam-tam': 577, 'tandem': 444, 'tandem bicycle': 444, 'tank': 847, 'tank suit': 639, 'tape player': 848, 'taper': 470, 'tarantula': 76, 'taxi': 468, 'taxicab': 468, 'teapot': 849, 'teddy': 850, 'teddy bear': 850, 'tee shirt': 610, 'television': 851, 'television system': 851, 'ten-gallon hat': 515, 'tench': 0, 'tennis ball': 852, 'terrace': 706, 'terrapin': 36, 'thatch': 853, 'thatched roof': 853, 'theater curtain': 854, 'theatre curtain': 854, 'thimble': 855, 'thrasher': 856, 'three-toed sloth': 364, 'thresher': 856, 'threshing machine': 856, 'throne': 857, 'thunder snake': 52, 'tick': 78, 'tiger': 292, 'tiger beetle': 300, 'tiger cat': 282, 'tiger shark': 3, 'tile roof': 858, 'timber wolf': 269, 'tin opener': 473, 'titi': 380, 'titi monkey': 380, 'toaster': 859, 'tobacco shop': 860, 'tobacconist': 860, 'tobacconist shop': 860, 'toilet paper': 999, 'toilet seat': 861, 'toilet tissue': 999, 'tool kit': 477, 'tope': 832, 'torch': 862, 'torpedo': 5, 'totem pole': 863, 'toucan': 96, 'tow car': 864, 'tow truck': 864, 'toy poodle': 265, 'toy terrier': 158, 'toyshop': 865, 'trackless trolley': 874, 'tractor': 866, 'tractor trailer': 867, 'traffic light': 920, 'traffic signal': 920, 'trailer truck': 867, 'tram': 829, 'tramcar': 829, 'transverse flute': 558, 'trash barrel': 412, 'trash bin': 412, 'trash can': 412, 'tray': 868, 'tree frog': 31, 'tree-frog': 31, 'trench coat': 869, 'triceratops': 51, 'tricycle': 870, 'trifle': 927, 'trike': 870, 'trilobite': 69, 'trimaran': 871, 'tripod': 872, 'triumphal arch': 873, 'trolley': 829, 'trolley car': 829, 'trolley coach': 874, 'trolleybus': 874, 'trombone': 875, 'trucking rig': 867, 'trump': 513, 'trumpet': 513, 'tub': 876, 'tup': 348, 'turnstile': 877, 'tusker': 101, 'two-piece': 445, 'tympan': 541, 'typewriter keyboard': 878, 'umbrella': 879, 'unicycle': 880, 'upright': 881, 'upright piano': 881, 'vacuum': 882, 'vacuum cleaner': 882, 'vale': 979, 'valley': 979, 'vase': 883, 'vat': 876, 'vault': 884, 'velocipede': 870, 'velvet': 885, 'vending machine': 886, 'vestment': 887, 'viaduct': 888, 'vine snake': 59, 'violin': 889, 'violoncello': 486, 'vizsla': 211, 'volcano': 980, 'volleyball': 890, 'volute': 506, 'vulture': 23, 'waffle iron': 891, 'waggon': 436, 'wagon': 734, 'walking stick': 313, 'walkingstick': 313, 'wall clock': 892, 'wallaby': 104, 'wallet': 893, 'wardrobe': 894, 'warplane': 895, 'warragal': 273, 'warrigal': 273, 'warthog': 343, 'wash-hand basin': 896, 'washbasin': 896, 'washbowl': 896, 'washer': 897, 'washing machine': 897, 'wastebin': 412, 'water bottle': 898, 'water buffalo': 346, 'water hen': 137, 'water jug': 899, 'water ouzel': 20, 'water ox': 346, 'water snake': 58, 'water tower': 900, 'wax light': 470, 'weasel': 356, 'web site': 916, 'website': 916, 'weevil': 307, 'weighing machine': 778, 'welcome mat': 539, 'wheelbarrow': 428, 'whippet': 172, 'whiptail': 41, 'whiptail lizard': 41, 'whirligig': 476, 'whiskey jug': 901, 'whistle': 902, 'white fox': 279, 'white shark': 2, 'white stork': 127, 'white wolf': 270, 'whorl': 506, 'wig': 903, 'wild boar': 342, 'window screen': 904, 'window shade': 905, 'wine bottle': 907, 'wing': 908, 'wire-haired fox terrier': 188, 'wireless': 754, 'wok': 909, 'wolf spider': 77, 'wombat': 106, 'wood pussy': 361, 'wood rabbit': 330, 'wooden spoon': 910, 'woodworking plane': 726, 'wool': 911, 'woolen': 911, 'woollen': 911, 'worm fence': 912, 'worm snake': 52, 'wreck': 913, 'wrecker': 864, 'xylophone': 642, 'yawl': 914, "yellow lady's slipper": 986, 'yellow lady-slipper': 986, 'yurt': 915, 'zebra': 340, 'zucchini': 939}
DiT_prefs = {
    'prompt': '',
    'batch_folder_name': '',
    'guidance_scale': 4.0,
    'num_inference_steps': 50,
    'seed': 0,
    'num_images': 1,
    #'variance_type': 'learned_range',#fixed_small_log
    #'num_train_timesteps': 1000,
    #'prediction_type': 'epsilon',#sample
    #'clip_sample': True,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": True,
}
def buildDiT(page):
    global DiT_prefs, prefs, pipe_DiT
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          DiT_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_DiT_output(o):
      page.DiT_output.controls.append(o)
      page.DiT_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_DiT_output = add_to_DiT_output
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.DiT_output.controls = []
      page.DiT_output.update()
      clear_button.visible = False
      clear_button.update()
    def DiT_help(e):
      def close_DiT_dlg(e):
        nonlocal DiT_help_dlg
        DiT_help_dlg.open = False
        page.update()
      DiT_help_dlg = AlertDialog(title=Text("🙅   Help with DiT Pipeline"), content=Column([
          Text("Provide a comma separated list of general ImageNet Classes to create images. Press Class List to see availble classes, click to copy a token to clipboard then paste in textfield."),
          Text("We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter."),
          Markdown("The DiT model in diffusers comes from  can be found here: [Scalable Diffusion Models with Transformers](https://www.wpeebles.com/DiT) (DiT) and [facebookresearch/dit](https://github.com/facebookresearch/dit)..", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😕  Interesting... ", on_click=close_DiT_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(DiT_help_dlg)
      DiT_help_dlg.open = True
      page.update()
    def copy_class(e):
      page.set_clipboard(e.control.text)
      toast_msg(page, f"📋   Class {e.control.text} copied to clipboard...")
    def show_classes(e):
      classes = []
      for c in ImageNet_classes.keys():
        #TODO Copy to clipboard on click
        classes.append(TextButton(c, col={'sm':4, 'md':3, 'lg':2,}, on_click=copy_class))
      alert_msg(page, "ImageNET Class List", content=Container(Column([ResponsiveRow(
        controls=classes,
        expand=True,
      )], spacing=0), width=(page.width if page.web else page.window.width) - 150), okay="😲  That's a lot...", sound=False)
    guidance_scale = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=DiT_prefs, key='guidance_scale')
    prompt = TextField(label="ImageNet Class Names (separated by commas)", value=DiT_prefs['prompt'], filled=True, on_change=lambda e:changed(e,'prompt'))
    seed = TextField(label="Seed", width=90, value=str(DiT_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=DiT_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    batch_folder_name = TextField(label="Batch Folder Name", value=DiT_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    #eta = TextField(label="ETA", value=str(DiT_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(DiT_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta_row = Row([Text("DDIM ETA: "), eta])
    #max_size = Slider(min=256, max=1280, divisions=64, label="{value}px", value=int(DiT_prefs['max_size']), expand=True, on_change=lambda e:changed(e,'max_size', ptype='int'))
    #max_row = Row([Text("Max Resolution Size: "), max_size])
    upscaler = UpscaleBlock(DiT_prefs)
    page.upscalers.append(upscaler)
    page.DiT_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.DiT_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("⚧️   DiT Models with Transformers Class-to-Image Generator", "Scalable Diffusion Models with Transformers...", actions=[ft.OutlinedButton("Class List", on_click=show_classes), IconButton(icon=icons.HELP, tooltip="Help with DiT Settings", on_click=DiT_help)]),
        prompt,
        #Row([prompt, mask_image, invert_mask]),
        num_inference_row,
        guidance_scale,
        Row([NumberPicker(label="Number of Images: ", min=1, max=20, value=DiT_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        upscaler,
        Row([ElevatedButton(content=Text("🔀   Get DiT Generation", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_DiT(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_DiT(page, from_list=True))
             ]),
      ]
    )), page.DiT_output,
        clear_button,
    ], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

dall_e_2_prefs = {
    'prompt': '',
    'size': '512x512',
    'num_images': 1,
    'init_image': '',
    'mask_image': '',
    'variation': False,
    "invert_mask": False,
    'file_prefix': 'dalle2-',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
    "batch_folder_name": '',
}

def buildDallE2(page):
    global dall_e_2_prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          dall_e_2_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def switch_version(e):
        page.DallE = buildDallE3(page)
        for t in page.ImageAIs.tabs:
          if t.text == "DALL•E":
            t.content = page.DallE
            t.icon = icons.BLUR_ON
            break
        page.ImageAIs.update()
        page.update()
    prompt = TextField(label="Prompt Text", value=dall_e_2_prefs['prompt'], filled=True, multiline=True, on_change=lambda e:changed(e,'prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=dall_e_2_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=dall_e_2_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #num_images = NumberPicker(label="Num of Images", min=1, max=10, step=9, value=dall_e_2_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    #num_images = TextField(label="num_images", value=dall_e_2_prefs['num_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_images', ptype="int"))
    #n_iterations = TextField(label="Number of Iterations", value=dall_e_2_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype="int"))
    #steps = TextField(label="Inference Steps", value=dall_e_2_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype="int"))
    #eta = TextField(label="DDIM ETA", value=dall_e_2_prefs['eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'eta', ptype="float"))
    #seed = TextField(label="Seed", value=dall_e_2_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'seed', ptype="int"))
    size = Dropdown(label="Image Size", width=120, options=[dropdown.Option("256x256"), dropdown.Option("512x512"), dropdown.Option("1024x1024")], value=dall_e_2_prefs['size'], on_change=lambda e:changed(e,'size'))
    param_rows = ResponsiveRow([Row([batch_folder_name, file_prefix], col={'lg':6}), Row([size, NumberPicker(label=" Number of Images", min=1, max=10, step=1, value=dall_e_2_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))], col={'lg':6})])
    #width = Slider(min=128, max=1024, divisions=6, label="{value}px", value=dall_e_2_prefs['width'], on_change=change_width, expand=True)
    #width_value = Text(f" {int(dall_e_2_prefs['width'])}px", weight=FontWeight.BOLD)
    #width_slider = Row([Text(f"Width: "), width_value, width])
    #height = Slider(min=128, max=1024, divisions=6, label="{value}px", value=dall_e_2_prefs['height'], on_change=change_height, expand=True)
    #height_value = Text(f" {int(dall_e_2_prefs['height'])}px", weight=FontWeight.BOLD)
    #height_slider = Row([Text(f"Height: "), height_value, height])
    init_image = FileInput(label="Init Image (optional)", pref=dall_e_2_prefs, key='init_image', page=page, expand=True, col={"*":1, "md":3})
    mask_image = FileInput(label="Mask Image (optional)", pref=dall_e_2_prefs, key='mask_image', page=page, expand=True, col={"*":1, "md":3})
    variation = Checkbox(label="Variation   ", tooltip="Creates Variation of Init Image. Disregards the Prompt and Mask.", value=dall_e_2_prefs['variation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'variation'))
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=dall_e_2_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
    image_pickers = Container(content=ResponsiveRow([Row([init_image, variation], col={"md":6}), Row([mask_image, invert_mask], col={"md":6})], run_spacing=2), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    #prompt_strength = Slider(min=0.1, max=0.9, divisions=16, label="{value}%", value=dall_e_2_prefs['prompt_strength'], on_change=change_strength, expand=True)
    #strength_value = Text(f" {int(dall_e_2_prefs['prompt_strength'] * 100)}%", weight=FontWeight.BOLD)
    #strength_slider = Row([Text("Prompt Strength: "), strength_value, prompt_strength])
    img_block = Container(Column([image_pickers, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    upscaler = UpscaleBlock(dall_e_2_prefs)
    page.upscalers.append(upscaler)
    list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dall_e(page, from_list=True))
    parameters_button = ElevatedButton(content=Text(value="🖼️   Run DALL•E 2", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dall_e(page))
    parameters_row = Row([parameters_button, list_button], spacing=22)#, alignment=MainAxisAlignment.SPACE_BETWEEN)
    page.dall_e_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("👺  OpenAI DALL•E 2", "Generates Images using your OpenAI API Key. Note: Uses same credits as official website.", actions=[ft.OutlinedButton(content=Text("Switch to DALL•E 3", size=18), on_click=switch_version), save_default(dall_e_2_prefs, ['init_image', 'mask_image'])]),
            prompt,
            img_block, 
            upscaler,
            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
            param_rows,
            parameters_row,
            page.dall_e_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

dall_e_3_prefs = {
    'prompt': '',
    'size': '1024x1024',
    'num_images': 1,
    'init_image': '',
    'mask_image': '',
    'variation': False,
    "invert_mask": False,
    'hd_quality': False,
    'natural_style': False,
    'file_prefix': 'dalle3-',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
    "batch_folder_name": '',
}

def buildDallE3(page):
    global dall_e_3_prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          dall_e_3_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def switch_version(e):
        page.DallE = buildDallE2(page)
        for t in page.ImageAIs.tabs:
          if t.text == "DALL•E":
            t.content = page.DallE
            t.icon = icons.BLUR_CIRCULAR
            break
        page.ImageAIs.update()
        page.update()
    prompt = TextField(label="Prompt Text", value=dall_e_3_prefs['prompt'], filled=True, multiline=True, on_change=lambda e:changed(e,'prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=dall_e_3_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=dall_e_3_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #num_images = TextField(label="num_images", value=dall_e_3_prefs['num_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_images', ptype="int"))
    #num_images = NumberPicker(label="Num of Images", min=1, max=10, step=9, value=dall_e_3_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    size = Dropdown(label="Image Size", width=130, options=[dropdown.Option("1024x1024"), dropdown.Option("1024x1792"), dropdown.Option("1792x1024")], value=dall_e_3_prefs['size'], on_change=lambda e:changed(e,'size'))
    param_rows = ResponsiveRow([Row([batch_folder_name, file_prefix], col={'lg':6}), Row([size], col={'lg':6})])
    init_image = FileInput(label="Init Image", pref=dall_e_3_prefs, key='init_image', page=page, expand=True, col={"*":1, "md":3})
    mask_image = FileInput(label="Mask Image", pref=dall_e_3_prefs, key='mask_image', page=page, expand=True, col={"*":1, "md":3})
    variation = Checkbox(label="Variation   ", tooltip="Creates Variation of Init Image. Disregards the Prompt and Mask.", value=dall_e_3_prefs['variation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'variation'))
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=dall_e_3_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
    image_pickers = Container(content=ResponsiveRow([Row([init_image, variation], col={"md":6}), Row([mask_image, invert_mask], col={"md":6})], run_spacing=2), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    hd_quality = Switcher(label="HD Quality", value=dall_e_3_prefs['hd_quality'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'hd_quality'), tooltip="Creates images with finer details and greater consistency across the image")
    natural_style = Switcher(label="Natural Style", value=dall_e_3_prefs['natural_style'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'natural_style'), tooltip="Vivid is default, leaning towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images.")
    img_block = Container(Column([image_pickers, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    upscaler = UpscaleBlock(dall_e_3_prefs)
    page.upscalers.append(upscaler)
    list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dall_e_3(page, from_list=True))
    parameters_button = ElevatedButton(content=Text(value="🙌   Run DALL•E 3", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dall_e_3(page))

    parameters_row = Row([parameters_button, list_button], spacing=22)#, alignment=MainAxisAlignment.SPACE_BETWEEN)
    dall_e_3_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🏇  OpenAI DALL•E 3", "Generates Images using your OpenAI API Key. Note: Uses same credits as official website.", actions=[ft.OutlinedButton(content=Text("Switch to DALL•E 2", size=18), on_click=switch_version), save_default(dall_e_3_prefs, ['init_image', 'mask_image'])]),
            prompt,
            Row([hd_quality, natural_style]),
            #img_block,
            param_rows,
            upscaler,
            parameters_row,
            dall_e_3_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c


kandinsky_3_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "kandinsky-",
    "num_images": 1,
    "steps":25,
    "width": 1024,
    "height":1024,
    "guidance_scale":4,
    #'prior_guidance_scale': 4.0,
    #'prior_steps': 25,
    "init_image": '',
    "strength": 0.3,
    "mask_image": '',
    "invert_mask": False,
    "cpu_offload": False,
    "seed": 0,
    "kandinsky_model": "Kandinsky 3",
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildKandinsky3(page):
    global prefs, kandinsky_3_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          kandinsky_3_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def kandinsky_3_help(e):
      def close_kandinsky_3_dlg(e):
        nonlocal kandinsky_3_help_dlg
        kandinsky_3_help_dlg.open = False
        page.update()
      kandinsky_3_help_dlg = AlertDialog(title=Text("🙅   Help with Kandinsky Pipeline"), content=Column([
          Markdown("Kandinsky 3.0 is an open-source text-to-image diffusion model built upon the Kandinsky2-x model family. In comparison to its predecessors, enhancements have been made to the text understanding and visual quality of the model, achieved by increasing the size of the text encoder and Diffusion U-Net models, respectively. Its architecture includes 3 main components: 1) FLAN-UL2, which is an encoder decoder model based on the T5 architecture. 2) New U-Net architecture featuring BigGAN-deep blocks doubles depth while maintaining the same number of parameters. 3) Sber-MoVQGAN is a decoder proven to have superior results in image restoration.  Kandinsky inherits best practices from [DALL-E 2](https://arxiv.org/abs/2204.06125) and [Latent Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/latent_diffusion), while introducing some new ideas.\nIt uses [CLIP](https://huggingface.co/docs/transformers/model_doc/clip) for encoding images and text, and a diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach enhances the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov) and the original codebase can be found [here](https://github.com/ai-forever/Kandinsky-2)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!"),
          Text("The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1."),
          Markdown("Kandinsky 3 is created by [Vladimir Arkhipkin](https://github.com/oriBetelgeuse),[Anastasia Maltseva](https://github.com/NastyaMittseva),[Igor Pavlov](https://github.com/boomb0om),[Andrei Filatov](https://github.com/anvilarth),[Arseniy Shakhmatov](https://github.com/cene555),[Andrey Kuznetsov](https://github.com/kuznetsoffandrey),[Denis Dimitrov](https://github.com/denndimitrov), [Zein Shaheen](https://github.com/zeinsh). Check out the [Kandinsky Community](https://huggingface.co/kandinsky-community) organization on the Hub for the official model checkpoints for tasks like text-to-image, image-to-image, and inpainting.", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🤤  Quality... ", on_click=close_kandinsky_3_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(kandinsky_3_help_dlg)
      kandinsky_3_help_dlg.open = True
      page.update()
    def change_version(e):
        status['kandinsky_version'] = e.control.value
        if '2.1' in status['kandinsky_version']:
            page.Kandinsky = buildKandinsky21(page)
        elif '2.2' in status['kandinsky_version']:
            page.Kandinsky = buildKandinsky(page)
        elif '3.0' in status['kandinsky_version']:
            page.Kandinsky = buildKandinsky3(page)
        for t in page.ImageAIs.tabs:
          if t.text == "Kandinsky":
            t.content = page.Kandinsky
            break
        page.ImageAIs.update()
        page.update()
    prompt = TextField(label="Prompt Text", value=kandinsky_3_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=kandinsky_3_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=kandinsky_3_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=kandinsky_3_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    steps = TextField(label="Number of Steps", value=kandinsky_3_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype="int"))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=kandinsky_3_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=kandinsky_3_prefs, key='steps')
    #prior_guidance_scale = SliderRow(label="Prior Guidance Scale", min=0, max=10, divisions=20, round=1, expand=True, pref=kandinsky_3_prefs, key='prior_guidance_scale', col={'xs':12, 'md':6})
    #prior_steps = SliderRow(label="Prior Steps", min=0, max=50, divisions=50, expand=True, pref=kandinsky_3_prefs, key='prior_steps', col={'xs':12, 'md':6})
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=kandinsky_3_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=kandinsky_3_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=kandinsky_3_prefs, key='height')
    init_image = FileInput(label="Init Image (optional)", pref=kandinsky_3_prefs, key='init_image', expand=True, page=page)
    mask_image = FileInput(label="Mask Image", pref=kandinsky_3_prefs, key='mask_image', expand=True, page=page)
    #, mask_image, invert_mask
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=kandinsky_3_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})
    image_pickers = Container(content=ResponsiveRow([init_image]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    strength_slider = SliderRow(label="Init Image Strength", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky_3_prefs, key='strength')
    img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    cpu_offload = Switcher(label="CPU Offload", value=kandinsky_3_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 16GB VRAM. Otherwise can run out of memory.")
    seed = TextField(label="Seed", width=90, value=str(kandinsky_3_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(kandinsky_3_prefs)
    page.upscalers.append(upscaler)
    kandinsky_version = Dropdown(width=155, options=[dropdown.Option("Kandinsky 3.0"), dropdown.Option("Kandinsky 2.2"), dropdown.Option("Kandinsky 2.1")], value=status['kandinsky_version'], on_change=change_version)
    parameters_button = ElevatedButton(content=Text(value="✨   Run Kandinsky 3", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky3(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky3(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky3(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.Kandinsky_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([#ft.OutlinedButton(content=Text("Switch to 2.1", size=18), on_click=switch_version)
            Header("🎎  Kandinsky 3.0", "A Latent Diffusion model with two Multilingual text encoders, supports 100+ languages...", actions=[kandinsky_version, save_default(kandinsky_3_prefs, ['init_image', 'mask_image']), IconButton(icon=icons.HELP, tooltip="Help with Kandinsky Settings", on_click=kandinsky_3_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            #ResponsiveRow([prior_steps, prior_guidance_scale]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            img_block,
            upscaler,
            ResponsiveRow([Row([n_images, seed, cpu_offload], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            parameters_row,
            page.Kandinsky_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

kandinsky_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "kandinsky-",
    "num_images": 1,
    "steps":25,
    #"ddim_eta":0.05,
    "width": 512,
    "height":512,
    "guidance_scale":4,
    'prior_guidance_scale': 4.0,
    'prior_steps': 25,
    "init_image": '',
    "strength": 0.3,
    "mask_image": '',
    "invert_mask": False,
    "seed": 0,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildKandinsky(page):
    global prefs, kandinsky_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          kandinsky_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def kandinsky_help(e):
      def close_kandinsky_dlg(e):
        nonlocal kandinsky_help_dlg
        kandinsky_help_dlg.open = False
        page.update()
      kandinsky_help_dlg = AlertDialog(title=Text("🙅   Help with Kandinsky Pipeline"), content=Column([
          #Text("NOTE: Right now, installing this may be incompatible with Diffusers packages, so it may not work if you first installed HuggingFace & Stable Diffusion. It's recommended to run this on a fresh runtime, only installing ESRGAN to upscale. We hope to fix this soon, but works great."),
          #Text("Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas."),
          Markdown("Kandinsky 2.2 inherits best practices from [DALL-E 2](https://arxiv.org/abs/2204.06125) and [Latent Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/latent_diffusion), while introducing some new ideas.\nIt uses [CLIP](https://huggingface.co/docs/transformers/model_doc/clip) for encoding images and text, and a diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach enhances the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov) and the original codebase can be found [here](https://github.com/ai-forever/Kandinsky-2)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!"),
          Text("The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🤤  Quality... ", on_click=close_kandinsky_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(kandinsky_help_dlg)
      kandinsky_help_dlg.open = True
      page.update()
    def switch_version(e):
        status['kandinsky_2_2'] = False
        page.Kandinsky = buildKandinsky21(page)
        for t in page.ImageAIs.tabs:
          if t.text == "Kandinsky":
            t.content = page.Kandinsky
            break
        page.ImageAIs.update()
        page.update()
    def change_version(e):
        status['kandinsky_version'] = e.control.value
        if '2.1' in status['kandinsky_version']:
            page.Kandinsky = buildKandinsky21(page)
        elif '2.2' in status['kandinsky_version']:
            page.Kandinsky = buildKandinsky(page)
        elif '3.0' in status['kandinsky_version']:
            page.Kandinsky = buildKandinsky3(page)
        for t in page.ImageAIs.tabs:
          if t.text == "Kandinsky":
            t.content = page.Kandinsky
            break
        page.ImageAIs.update()
        page.update()
    prompt = TextField(label="Prompt Text", value=kandinsky_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=kandinsky_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=kandinsky_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=kandinsky_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #num_outputs = NumberPicker(label="Num of Outputs", min=1, max=4, step=4, value=kandinsky_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #num_outputs = TextField(label="num_outputs", value=kandinsky_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #n_iterations = TextField(label="Number of Iterations", value=kandinsky_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype="int"))
    steps = TextField(label="Number of Steps", value=kandinsky_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype="int"))
    #ddim_eta = TextField(label="DDIM ETA", value=kandinsky_prefs['ddim_eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'ddim_eta', ptype="float"))
    #dynamic_threshold_v = TextField(label="Dynamic Threshold", value=kandinsky_prefs['dynamic_threshold_v'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'dynamic_threshold_v', ptype="float"))
    #sampler = Dropdown(label="Sampler", width=200, options=[dropdown.Option("ddim_sampler"), dropdown.Option("p_sampler")], value=kandinsky_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=kandinsky_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    #param_rows = ResponsiveRow([Column([batch_folder_name, n_images], col={'xs':12, 'md':6}),
                      #Column([file_prefix, sampler], col={'xs':12, 'md':6})
                      #Column([steps, ddim_eta, dynamic_threshold_v], col={'xs':12, 'md':6})
                      #], vertical_alignment=CrossAxisAlignment.START)
    #denoised_type = Dropdown(label="Denoised Type", width=180, options=[dropdown.Option("dynamic_threshold"), dropdown.Option("clip_denoised")], value=kandinsky_prefs['denoised_type'], on_change=lambda e:changed(e,'denoised_type'), col={'xs':12, 'md':6})
    #dropdown_row = ResponsiveRow([sampler])#, denoised_type])
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=kandinsky_prefs, key='steps')
    prior_guidance_scale = SliderRow(label="Prior Guidance Scale", min=0, max=10, divisions=20, round=1, expand=True, pref=kandinsky_prefs, key='prior_guidance_scale', col={'xs':12, 'md':6})
    prior_steps = SliderRow(label="Prior Steps", min=0, max=50, divisions=50, expand=True, pref=kandinsky_prefs, key='prior_steps', col={'xs':12, 'md':6})
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=kandinsky_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky_prefs, key='height')
    init_image = FileInput(label="Init Image (optional)", pref=kandinsky_prefs, key='init_image', expand=True, page=page)
    mask_image = FileInput(label="Mask Image", pref=kandinsky_prefs, key='mask_image', expand=True, page=page)
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=kandinsky_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})
    image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    strength_slider = SliderRow(label="Init Image Strength", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky_prefs, key='strength')
    img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    seed = TextField(label="Seed", width=90, value=str(kandinsky_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(kandinsky_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="✨   Run Kandinsky 2.2", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    #label="Kandinsky Version", 
    kandinsky_version = Dropdown(width=150, options=[dropdown.Option("Kandinsky 3.0"), dropdown.Option("Kandinsky 2.2"), dropdown.Option("Kandinsky 2.1")], value=status['kandinsky_version'], on_change=change_version)
    page.kandinsky_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([#ft.OutlinedButton(content=Text("Switch to 2.1", size=18, on_click=switch_version))
            Header("🎎  Kandinsky 2.2", "A Latent Diffusion model with two Multilingual text encoders, supports 100+ languages...", actions=[kandinsky_version, save_default(kandinsky_prefs, ['init_image', 'mask_image']), IconButton(icon=icons.HELP, tooltip="Help with Kandinsky Settings", on_click=kandinsky_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            #param_rows, #dropdown_row,
            ResponsiveRow([prior_steps, prior_guidance_scale]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            img_block,
            #Row([batch_folder_name, file_prefix]),
            upscaler,
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
            parameters_row,
            page.kandinsky_output
        ],
    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed,
    return c

kandinsky21_prefs = {
    "prompt": '',
    "batch_folder_name": '',
    "file_prefix": "kandinsky-",
    "num_images": 1,
    "steps":100,
    "ddim_eta":0.05,
    "width": 512,
    "height":512,
    "guidance_scale":4,
    'prior_cf_scale': 4,
    'prior_steps': "25",
    "dynamic_threshold_v":99.5,
    "sampler": "ddim_sampler",
    "denoised_type": "dynamic_threshold",
    "init_image": '',
    "strength": 0.5,
    "mask_image": '',
    "invert_mask": False,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildKandinsky21(page):
    global prefs, kandinsky21_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          kandinsky21_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def kandinsky21_help(e):
      def close_kandinsky21_dlg(e):
        nonlocal kandinsky21_help_dlg
        kandinsky21_help_dlg.open = False
        page.update()
      kandinsky21_help_dlg = AlertDialog(title=Text("🙅   Help with Kandinsky Pipeline"), content=Column([
          Text("NOTE: Right now, installing this may be incompatible with Diffusers packages, so it may not work if you first installed HuggingFace & Stable Diffusion. It's recommended to run this on a fresh runtime, only installing ESRGAN to upscale. We hope to fix this soon, but works great."),
          Text("Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas."),
          Text("As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!"),
          Text("The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1."),
          Markdown("[Kandinsky GitHub](https://github.com/ai-forever/Kandinsky-2) | [Kandinsky 2.1 Blog](https://habr.com/ru/companies/sberbank/articles/725282/) | [FusionBrain Demo](https://fusionbrain.ai/diffusion)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🤤  Quality... ", on_click=close_kandinsky21_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(kandinsky21_help_dlg)
      kandinsky21_help_dlg.open = True
      page.update()
    def switch_version(e):
        status['kandinsky_2_2'] = True
        page.Kandinsky = buildKandinsky(page)
        for t in page.ImageAIs.tabs:
          if t.text == "Kandinsky":
            t.content = page.Kandinsky
            break
        page.ImageAIs.update()
        page.update()
    def change_version(e):
        status['kandinsky_version'] = e.control.value
        if '2.1' in status['kandinsky_version']:
            page.Kandinsky = buildKandinsky21(page)
        elif '2.2' in status['kandinsky_version']:
            page.Kandinsky = buildKandinsky(page)
        elif '3.0' in status['kandinsky_version']:
            page.Kandinsky = buildKandinsky3(page)
        for t in page.ImageAIs.tabs:
          if t.text == "Kandinsky":
            t.content = page.Kandinsky
            break
        page.ImageAIs.update()
        page.update()
    prompt = TextField(label="Prompt Text", value=kandinsky21_prefs['prompt'], filled=True, multiline=True, on_change=lambda e:changed(e,'prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=kandinsky21_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=kandinsky21_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #num_outputs = NumberPicker(label="Num of Outputs", min=1, max=4, step=4, value=kandinsky21_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #num_outputs = TextField(label="num_outputs", value=kandinsky21_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #n_iterations = TextField(label="Number of Iterations", value=kandinsky21_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype="int"))
    steps = TextField(label="Number of Steps", value=kandinsky21_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype="int"))
    ddim_eta = TextField(label="DDIM ETA", value=kandinsky21_prefs['ddim_eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'ddim_eta', ptype="float"))
    dynamic_threshold_v = TextField(label="Dynamic Threshold", value=kandinsky21_prefs['dynamic_threshold_v'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'dynamic_threshold_v', ptype="float"))
    sampler = Dropdown(label="Sampler", width=200, options=[dropdown.Option("ddim_sampler"), dropdown.Option("p_sampler")], value=kandinsky21_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=kandinsky21_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    param_rows = ResponsiveRow([Column([batch_folder_name, n_images], col={'xs':12, 'md':6}),
                      Column([file_prefix, sampler], col={'xs':12, 'md':6})
                      #Column([steps, ddim_eta, dynamic_threshold_v], col={'xs':12, 'md':6})
                      ], vertical_alignment=CrossAxisAlignment.START)
    denoised_type = Dropdown(label="Denoised Type", width=180, options=[dropdown.Option("dynamic_threshold"), dropdown.Option("clip_denoised")], value=kandinsky21_prefs['denoised_type'], on_change=lambda e:changed(e,'denoised_type'), col={'xs':12, 'md':6})
    dropdown_row = ResponsiveRow([sampler])#, denoised_type])
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=kandinsky21_prefs, key='steps')
    prior_cf_scale = SliderRow(label="Prior Guidance Scale", min=0, max=10, divisions=10, pref=kandinsky21_prefs, key='prior_cf_scale', col={'xs':12, 'md':6})
    prior_steps = SliderRow(label="Prior Steps", min=0, max=50, divisions=50, pref=kandinsky21_prefs, key='prior_steps', col={'xs':12, 'md':6})
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=kandinsky21_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky21_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky21_prefs, key='height')
    init_image = FileInput(label="Init Image", pref=kandinsky21_prefs, key='init_image', expand=True, page=page)
    mask_image = FileInput(label="Mask Image", pref=kandinsky21_prefs, key='mask_image', expand=True, page=page)
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=kandinsky21_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})
    image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    strength_slider = SliderRow(label="Init Image Strength", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky21_prefs, key='strength')
    img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    upscaler = UpscaleBlock(kandinsky21_prefs)
    page.upscalers.append(upscaler)
    kandinsky_version = Dropdown(width=155, options=[dropdown.Option("Kandinsky 3.0"), dropdown.Option("Kandinsky 2.2"), dropdown.Option("Kandinsky 2.1")], value=status['kandinsky_version'], on_change=change_version)
    parameters_button = ElevatedButton(content=Text(value="✨   Run Kandinsky 2.1", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky(page))
    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)
    page.kandinsky21_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([#ft.OutlinedButton(content=Text("Switch to 2.2", size=18), on_click=switch_version)
            Header("🎎  Kandinsky 2.1", "A Latent Diffusion model with two Multilingual text encoders, supports 100+ languages...", actions=[kandinsky_version, save_default(kandinsky21_prefs, ['init_image', 'mask_image']), IconButton(icon=icons.HELP, tooltip="Help with Kandinsky Settings", on_click=kandinsky21_help)]),
            prompt,
            #param_rows, #dropdown_row,
            ResponsiveRow([prior_steps, prior_cf_scale]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            img_block,
            upscaler,
            ResponsiveRow([Row([n_images, sampler], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
            parameters_row,
            page.kandinsky21_output
        ],
    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed,
    return c

kandinsky_fuse_prefs = {
    "prompt": '',
    "batch_folder_name": '',
    "file_prefix": "kandinsky-",
    "num_images": 1,
    "mixes": [],
    "steps":25,
    "width": 512,
    "height":512,
    "guidance_scale":8,
    #'prior_cf_scale': 4,
    #'prior_steps': "25",
    #"sampler": "ddim_sampler",
    "init_image": '',
    "weight": 0.5,
    #"mask_image": '',
    #"invert_mask": False,
    "seed": 0,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildKandinskyFuse(page):
    global prefs, kandinsky_fuse_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          kandinsky_fuse_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def kandinsky_help(e):
      def close_kandinsky_dlg(e):
        nonlocal kandinsky_help_dlg
        kandinsky_help_dlg.open = False
        page.update()
      kandinsky_help_dlg = AlertDialog(title=Text("🙅   Help with Kandinsky Fuse Pipeline"), content=Column([
          #Text("NOTE: Right now, installing this may be incompatible with Diffusers packages, so it may not work if you first installed HuggingFace & Stable Diffusion. It's recommended to run this on a fresh runtime, only installing ESRGAN to upscale. We hope to fix this soon, but works great."),
          Text("This variation lets you fuse together many images together with multiple text prompts to create a mix. Set the weights of the prompts and images to adjust the amount of influence it has on the generated style. Get experimental"),
          Text("Kandinsky 2.2 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas."),
          Text("As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!"),
          Text("The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1."),
          Markdown("[Kandinsky GitHub](https://github.com/ai-forever/Kandinsky-2) | [Kandinsky 2.1 Blog](https://habr.com/ru/companies/sberbank/articles/725282/) | [FusionBrain Demo](https://fusionbrain.ai/diffusion)"),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🫢  Possibility Overload... ", on_click=close_kandinsky_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(kandinsky_help_dlg)
      kandinsky_help_dlg.open = True
      page.update()
    def switch_version(e):
        status['kandinsky_fuse_2_2'] = False
        page.KandinskyFuse = buildKandinsky21Fuse(page)
        for t in page.ImageAIs.tabs:
          if t.text == "Kandinsky Fuse":
            t.content = page.KandinskyFuse
            break
        page.ImageAIs.update()
        page.update()
    def add_image(e):
        if not bool(kandinsky_fuse_prefs['init_image']): return
        layer = {'init_image': kandinsky_fuse_prefs['init_image'], 'weight': kandinsky_fuse_prefs['weight']}
        kandinsky_fuse_prefs['mixes'].append(layer)
        fuse_layers.controls.append(ListTile(title=Row([Text(layer['init_image'], weight=FontWeight.BOLD), Text(f"Weight: {layer['weight']}")], alignment=MainAxisAlignment.SPACE_BETWEEN), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.EDIT, text="Edit Image Layer", on_click=edit_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE, text="Delete Image Layer", on_click=delete_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
              PopupMenuItem(icon=icons.ARROW_UPWARD, text="Move Up", on_click=move_up, data=layer),
              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text="Move Down", on_click=move_down, data=layer),
          ]), data=layer, on_click=edit_layer))
        fuse_layers.update()
        kandinsky_fuse_prefs['init_image'] = ""
        init_image.value = ""
        init_image.update()
    def add_prompt(e):
        if not bool(kandinsky_fuse_prefs['prompt']): return
        layer = {'prompt': kandinsky_fuse_prefs['prompt'], 'weight': kandinsky_fuse_prefs['weight']}
        kandinsky_fuse_prefs['mixes'].append(layer)
        fuse_layers.controls.append(ListTile(title=Row([Text(layer['prompt'], weight=FontWeight.BOLD), Text(f"Weight: {layer['weight']}")], alignment=MainAxisAlignment.SPACE_BETWEEN), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.EDIT, text="Edit Text Layer", on_click=edit_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE, text="Delete Text Layer", on_click=delete_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
              PopupMenuItem(icon=icons.ARROW_UPWARD, text="Move Up", on_click=move_up, data=layer),
              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text="Move Down", on_click=move_down, data=layer),
          ]), data=layer, on_click=edit_layer))
        fuse_layers.update()
        kandinsky_fuse_prefs['prompt'] = ""
        prompt.value = ""
        prompt.update()
    def delete_layer(e):
        kandinsky_fuse_prefs['mixes'].remove(e.control.data)
        for c in fuse_layers.controls:
            if 'prompt' in c.data:
                if c.data['prompt'] == e.control.data['prompt']:
                    fuse_layers.controls.remove(c)
                    break
            else:
                if c.data['init_image'] == e.control.data['init_image']:
                    fuse_layers.controls.remove(c)
                    break
        fuse_layers.update()
    def delete_all_layers(e):
        kandinsky_fuse_prefs['mixes'].clear()
        fuse_layers.controls.clear()
        fuse_layers.update()
    def move_down(e):
        idx = kandinsky_fuse_prefs['mixes'].index(e.control.data)
        if idx < (len(kandinsky_fuse_prefs['mixes']) - 1):
          d = kandinsky_fuse_prefs['mixes'].pop(idx)
          kandinsky_fuse_prefs['mixes'].insert(idx+1, d)
          dr = fuse_layers.controls.pop(idx)
          fuse_layers.controls.insert(idx+1, dr)
          fuse_layers.update()
    def move_up(e):
        idx = kandinsky_fuse_prefs['mixes'].index(e.control.data)
        if idx > 0:
          d = kandinsky_fuse_prefs['mixes'].pop(idx)
          kandinsky_fuse_prefs['mixes'].insert(idx-1, d)
          dr = fuse_layers.controls.pop(idx)
          fuse_layers.controls.insert(idx-1, dr)
          fuse_layers.update()
    def edit_layer(e):
        data = e.control.data
        layer_type = "prompt" if "prompt" in data else "image"
        def pick_files_result(e: FilePickerResultEvent):
            if e.files:
                img = e.files
                uf = []
                fname = img[0]
                src_path = page.get_upload_url(fname.name, 600)
                uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))
                pick_files_dialog.upload(uf)
                dst_path = os.path.join(root_dir, fname.name)
                image_mix.value = dst_path
        pick_files_dialog = FilePicker(on_result=pick_files_result)
        page.overlay.append(pick_files_dialog)
        def file_picker_result(e: FilePickerResultEvent):
            if e.files != None:
                upload_files(e)
        def on_upload_progress(e: FilePickerUploadEvent):
            if e.progress == 1:
                save_file(e.file_name)
        def save_file(file_name):
            if not slash in file_name:
              fname = os.path.join(root_dir, file_name)
            else:
              fname = file_name
            image_mix.value = fname
            image_mix.update()
            data['init_image'] = fname
            page.update()
        file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
        def upload_files(e):
            uf = []
            if file_picker.result != None and file_picker.result.files != None:
                for f in file_picker.result.files:
                  if page.web:
                    uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
                  else:
                    save_file(f.path)
                file_picker.upload(uf)
        page.overlay.append(file_picker)
        def pick_image(e):
            file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Init Image File")
        #name = e.control.title.controls[0].value
        #path = e.control.title.controls[1].value
        if layer_type == "prompt":
            prompt_value = data["prompt"]
            image_value = ""
        else:
            prompt_value = ""
            image_value = data["init_image"]
        def close_dlg(e):
            dlg_edit.open = False
            page.update()
        def save_layer(e):
            layer = None
            for l in kandinsky_fuse_prefs['mixes']:
                if "prompt" in l:
                  if layer_type == "prompt":
                      if data["prompt"] == l["prompt"]:
                        layer = l
                        layer['prompt'] = prompt_text.value
                        break
                else:
                    if layer_type == "image":
                      if data["init_image"] == l["init_image"]:
                        layer = l
                        layer['init_image'] = image_mix.value
                        break
            for c in fuse_layers.controls:
                if 'prompt' in c.data:
                    if 'prompt' not in data: continue
                    if c.data['prompt'] == data['prompt']:
                        c.title.controls[0].value = layer['prompt']
                        c.title.controls[1].value = f"Weight: {layer['weight']}"
                        c.update()
                        break
                else:
                    if 'init_image' not in data: continue
                    if c.data['init_image'] == data['init_image']:
                        c.title.controls[0].value = layer['init_image']
                        c.title.controls[1].value = f"Weight: {layer['weight']}"
                        c.update()
                        break
            layer['prompt'] = prompt_text.value
            #layer['weight'] = model_path.value
            dlg_edit.open = False
            e.control.update()
            page.update()
        prompt_text = TextField(label="Fuse Prompt Text", value=prompt_value, multiline=True, visible=layer_type == "prompt")
        image_mix = TextField(label="Fuse Image Path", value=image_value, visible=layer_type == "image", height=65, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_image))
        edit_weights = SliderRow(label="Weight/Strength", min=0, max=1, divisions=20, round=1, pref=data, key='weight', tooltip="Indicates how much each individual concept should influence the overall guidance. If no weights are provided all concepts are applied equally.")
        dlg_edit = AlertDialog(modal=False, title=Text(f"🧳 Edit Kandinsky Fuse {layer_type.title()} Mix"), content=Container(Column([prompt_text, image_mix, edit_weights], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO, width=(page.width if page.web else page.window.width) - 100)), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Layer ", size=19, weight=FontWeight.BOLD), on_click=save_layer)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(dlg_edit)
        dlg_edit.open = True
        page.update()
    add_prompt_btn = ft.FilledButton("➕ Add Prompt", width=150, on_click=add_prompt)
    #add_prompt_btn = IconButton(icons.ADD, tooltip="Add Text Prompt", on_click=add_prompt)
    add_image_btn = ft.FilledButton("➕ Add Image", width=150, on_click=add_image)
    #add_image_btn = IconButton(icons.ADD, tooltip="Add Image to Mix", on_click=add_image)
    prompt = TextField(label="Mix Prompt Text", value=kandinsky_fuse_prefs['prompt'], filled=True, expand=True, multiline=True, on_submit=add_prompt, on_change=lambda e:changed(e,'prompt'))
    prompt_row = Row([prompt, add_prompt_btn])
    init_image = FileInput(label="Mixing Image", pref=kandinsky_fuse_prefs, key='init_image', expand=True, page=page)
    image_row = Row([init_image, add_image_btn])
    weight_slider = SliderRow(label="Text or Image Weight", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky_fuse_prefs, key='weight')
    fuse_layers = Column([], spacing=0)
    batch_folder_name = TextField(label="Batch Folder Name", value=kandinsky_fuse_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=kandinsky_fuse_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #num_outputs = NumberPicker(label="Num of Outputs", min=1, max=4, step=4, value=kandinsky_fuse_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #num_outputs = TextField(label="num_outputs", value=kandinsky_fuse_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #n_iterations = TextField(label="Number of Iterations", value=kandinsky_fuse_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype="int"))
    steps = TextField(label="Number of Steps", value=kandinsky_fuse_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype="int"))
    #sampler = Dropdown(label="Sampler", width=200, options=[dropdown.Option("ddim_sampler"), dropdown.Option("p_sampler")], value=kandinsky_fuse_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=kandinsky_fuse_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    #param_rows = ResponsiveRow([Column([batch_folder_name, n_images], col={'xs':12, 'md':6}),
    #                  Column([file_prefix, sampler], col={'xs':12, 'md':6})
    #                  #Column([steps, ddim_eta, dynamic_threshold_v], col={'xs':12, 'md':6})
    #                  ], vertical_alignment=CrossAxisAlignment.START)
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=kandinsky_fuse_prefs, key='steps')
    #prior_cf_scale = SliderRow(label="Prior CF Scale", min=0, max=10, divisions=10, pref=kandinsky_fuse_prefs, key='prior_cf_scale')
    #prior_steps = SliderRow(label="Prior Steps", min=0, max=50, divisions=50, pref=kandinsky_fuse_prefs, key='prior_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=kandinsky_fuse_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky_fuse_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky_fuse_prefs, key='height')
    seed = TextField(label="Seed", width=90, value=str(kandinsky_fuse_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    #mask_image = TextField(label="Mask Image", value=kandinsky_fuse_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})
    #invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=kandinsky_fuse_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})
    #image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    #img_block = Container(Column([image_pickers, weight_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    upscaler = UpscaleBlock(kandinsky_fuse_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="💥   Run Kandinsky 2.2 Fuser", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky_fuse(page))

    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)
    page.kandinsky_fuse_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("💣  Kandinsky 2.2 Fuse", "Mix multiple Images and Prompts together to Interpolate. A Latent Diffusion model with two Multilingual text encoders, supports 100+ languages...", actions=[ft.OutlinedButton(content=Text("Switch to 2.1", size=18), on_click=switch_version), save_default(kandinsky_fuse_prefs, ['init_image', 'mixes']), IconButton(icon=icons.HELP, tooltip="Help with Kandinsky Settings", on_click=kandinsky_help)]),
            prompt_row,
            image_row,
            weight_slider,
            Divider(height=5, thickness=4),
            fuse_layers,
            #Divider(height=2, thickness=2),
            #param_rows, #dropdown_row,
            steps,
            #prior_steps, prior_cf_scale,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            #Row([batch_folder_name, file_prefix]),
            #Row([n_images, sampler]),
            upscaler,
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
            parameters_row,
            page.kandinsky_fuse_output
        ],
    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed,
    return c

kandinsky21_fuse_prefs = {
    "prompt": '',
    "batch_folder_name": '',
    "file_prefix": "kandinsky-",
    "num_images": 1,
    "mixes": [],
    "steps":100,
    "width": 512,
    "height":512,
    "guidance_scale":8,
    'prior_cf_scale': 4,
    'prior_steps': "25",
    "sampler": "ddim_sampler",
    "init_image": '',
    "weight": 0.5,
    "mask_image": '',
    "invert_mask": False,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildKandinsky21Fuse(page):
    global prefs, kandinsky21_fuse_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          kandinsky21_fuse_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def kandinsky21_help(e):
      def close_kandinsky21_dlg(e):
        nonlocal kandinsky21_help_dlg
        kandinsky21_help_dlg.open = False
        page.update()
      kandinsky21_help_dlg = AlertDialog(title=Text("🙅   Help with Kandinsky Fuse Pipeline"), content=Column([
          Text("NOTE: Right now, installing this may be incompatible with Diffusers packages, so it may not work if you first installed HuggingFace & Stable Diffusion. It's recommended to run this on a fresh runtime, only installing ESRGAN to upscale. We hope to fix this soon, but works great."),
          Text("This variation lets you fuse together many images together with multiple text prompts to create a mix. Set the weights of the prompts and images to adjust the amount of influence it has on the generated style. Get experimental"),
          Text("Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas."),
          Text("As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!"),
          Text("The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1."),
          Markdown("[Kandinsky GitHub](https://github.com/ai-forever/Kandinsky-2) | [Kandinsky 2.1 Blog](https://habr.com/ru/companies/sberbank/articles/725282/) | [FusionBrain Demo](https://fusionbrain.ai/diffusion)"),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🫢  Possibility Overload... ", on_click=close_kandinsky21_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(kandinsky21_help_dlg)
      kandinsky21_help_dlg.open = True
      page.update()
    def switch_version(e):
        status['kandinsky_fuse_2_2'] = True
        page.KandinskyFuse = buildKandinskyFuse(page)
        for t in page.ImageAIs.tabs:
          if t.text == "Kandinsky Fuse":
            t.content = page.KandinskyFuse
            break
        page.ImageAIs.update()
        page.update()
    def add_image(e):
        if not bool(kandinsky21_fuse_prefs['init_image']): return
        layer = {'init_image': kandinsky21_fuse_prefs['init_image'], 'weight': kandinsky21_fuse_prefs['weight']}
        kandinsky21_fuse_prefs['mixes'].append(layer)
        fuse_layers.controls.append(ListTile(title=Row([Text(layer['init_image'], weight=FontWeight.BOLD), Text(f"Weight: {layer['weight']}")], alignment=MainAxisAlignment.SPACE_BETWEEN), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.EDIT, text="Edit Image Layer", on_click=edit_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE, text="Delete Image Layer", on_click=delete_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
              PopupMenuItem(icon=icons.ARROW_UPWARD, text="Move Up", on_click=move_up, data=layer),
              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text="Move Down", on_click=move_down, data=layer),
          ]), data=layer, on_click=edit_layer))
        fuse_layers.update()
        kandinsky21_fuse_prefs['init_image'] = ""
        init_image.value = ""
        init_image.update()
    def add_prompt(e):
        if not bool(kandinsky21_fuse_prefs['prompt']): return
        layer = {'prompt': kandinsky21_fuse_prefs['prompt'], 'weight': kandinsky21_fuse_prefs['weight']}
        kandinsky21_fuse_prefs['mixes'].append(layer)
        fuse_layers.controls.append(ListTile(title=Row([Text(layer['prompt'], weight=FontWeight.BOLD), Text(f"Weight: {layer['weight']}")], alignment=MainAxisAlignment.SPACE_BETWEEN), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.EDIT, text="Edit Text Layer", on_click=edit_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE, text="Delete Text Layer", on_click=delete_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
              PopupMenuItem(icon=icons.ARROW_UPWARD, text="Move Up", on_click=move_up, data=layer),
              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text="Move Down", on_click=move_down, data=layer),
          ]), data=layer, on_click=edit_layer))
        fuse_layers.update()
        kandinsky21_fuse_prefs['prompt'] = ""
        prompt.value = ""
        prompt.update()
    def delete_layer(e):
        kandinsky21_fuse_prefs['mixes'].remove(e.control.data)
        for c in fuse_layers.controls:
            if 'prompt' in c.data:
                if c.data['prompt'] == e.control.data['prompt']:
                    fuse_layers.controls.remove(c)
                    break
            else:
                if c.data['init_image'] == e.control.data['init_image']:
                    fuse_layers.controls.remove(c)
                    break
        fuse_layers.update()
    def delete_all_layers(e):
        kandinsky21_fuse_prefs['mixes'].clear()
        fuse_layers.controls.clear()
        fuse_layers.update()
    def move_down(e):
        idx = kandinsky21_fuse_prefs['mixes'].index(e.control.data)
        if idx < (len(kandinsky21_fuse_prefs['mixes']) - 1):
          d = kandinsky21_fuse_prefs['mixes'].pop(idx)
          kandinsky21_fuse_prefs['mixes'].insert(idx+1, d)
          dr = fuse_layers.controls.pop(idx)
          fuse_layers.controls.insert(idx+1, dr)
          fuse_layers.update()
    def move_up(e):
        idx = kandinsky21_fuse_prefs['mixes'].index(e.control.data)
        if idx > 0:
          d = kandinsky21_fuse_prefs['mixes'].pop(idx)
          kandinsky21_fuse_prefs['mixes'].insert(idx-1, d)
          dr = fuse_layers.controls.pop(idx)
          fuse_layers.controls.insert(idx-1, dr)
          fuse_layers.update()
    def edit_layer(e):
        data = e.control.data
        layer_type = "prompt" if "prompt" in data else "image"
        def pick_files_result(e: FilePickerResultEvent):
            if e.files:
                img = e.files
                uf = []
                fname = img[0]
                src_path = page.get_upload_url(fname.name, 600)
                uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))
                pick_files_dialog.upload(uf)
                dst_path = os.path.join(root_dir, fname.name)
                image_mix.value = dst_path
        pick_files_dialog = FilePicker(on_result=pick_files_result)
        page.overlay.append(pick_files_dialog)
        def file_picker_result(e: FilePickerResultEvent):
            if e.files != None:
                upload_files(e)
        def on_upload_progress(e: FilePickerUploadEvent):
            if e.progress == 1:
                save_file(e.file_name)
        def save_file(file_name):
            if not slash in file_name:
              fname = os.path.join(root_dir, file_name)
            else:
              fname = file_name
            image_mix.value = fname
            image_mix.update()
            data['init_image'] = fname
            page.update()
        file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
        def upload_files(e):
            uf = []
            if file_picker.result != None and file_picker.result.files != None:
                for f in file_picker.result.files:
                  if page.web:
                    uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
                  else:
                    save_file(f.path)
                file_picker.upload(uf)
        page.overlay.append(file_picker)
        def pick_image(e):
            file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Init Image File")
        #name = e.control.title.controls[0].value
        #path = e.control.title.controls[1].value
        if layer_type == "prompt":
            prompt_value = data["prompt"]
            image_value = ""
        else:
            prompt_value = ""
            image_value = data["init_image"]
        def close_dlg(e):
            dlg_edit.open = False
            page.update()
        def save_layer(e):
            layer = None
            for l in kandinsky21_fuse_prefs['mixes']:
                if "prompt" in l:
                  if layer_type == "prompt":
                      if data["prompt"] == l["prompt"]:
                        layer = l
                        layer['prompt'] = prompt_text.value
                        break
                else:
                    if layer_type == "image":
                      if data["init_image"] == l["init_image"]:
                        layer = l
                        layer['init_image'] = image_mix.value
                        break
            for c in fuse_layers.controls:
                if 'prompt' in c.data:
                    if 'prompt' not in data: continue
                    if c.data['prompt'] == data['prompt']:
                        c.title.controls[0].value = layer['prompt']
                        c.title.controls[1].value = f"Weight: {layer['weight']}"
                        c.update()
                        break
                else:
                    if 'init_image' not in data: continue
                    if c.data['init_image'] == data['init_image']:
                        c.title.controls[0].value = layer['init_image']
                        c.title.controls[1].value = f"Weight: {layer['weight']}"
                        c.update()
                        break
            layer['prompt'] = prompt_text.value
            #layer['weight'] = model_path.value
            dlg_edit.open = False
            e.control.update()
            page.update()
        prompt_text = TextField(label="Fuse Prompt Text", value=prompt_value, multiline=True, visible=layer_type == "prompt")
        image_mix = TextField(label="Fuse Image Path", value=image_value, visible=layer_type == "image", height=65, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_image))
        edit_weights = SliderRow(label="Weight/Strength", min=0, max=1, divisions=20, round=1, pref=data, key='weight', tooltip="Indicates how much each individual concept should influence the overall guidance. If no weights are provided all concepts are applied equally.")
        dlg_edit = AlertDialog(modal=False, title=Text(f"🧳 Edit Kandinsky Fuse {layer_type.title()} Mix"), content=Container(Column([prompt_text, image_mix, edit_weights], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO, width=(page.width if page.web else page.window.width) - 100)), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Layer ", size=19, weight=FontWeight.BOLD), on_click=save_layer)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(dlg_edit)
        dlg_edit.open = True
        page.update()
    add_prompt_btn = ft.FilledButton("➕ Add Prompt", width=150, on_click=add_prompt)
    #add_prompt_btn = IconButton(icons.ADD, tooltip="Add Text Prompt", on_click=add_prompt)
    add_image_btn = ft.FilledButton("➕ Add Image", width=150, on_click=add_image)
    #add_image_btn = IconButton(icons.ADD, tooltip="Add Image to Mix", on_click=add_image)
    prompt = TextField(label="Mix Prompt Text", value=kandinsky21_fuse_prefs['prompt'], filled=True, expand=True, multiline=True, on_submit=add_prompt, on_change=lambda e:changed(e,'prompt'))
    prompt_row = Row([prompt, add_prompt_btn])
    init_image = FileInput(label="Mixing Image", pref=kandinsky21_fuse_prefs, key='init_image', page=page)
    #init_image = TextField(label="Mixing Image", value=kandinsky21_fuse_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, height=65, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init), col={'xs':12, 'md':6})
    image_row = Row([init_image, add_image_btn])
    weight_slider = SliderRow(label="Text or Image Weight", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky21_fuse_prefs, key='weight')
    fuse_layers = Column([], spacing=0)
    batch_folder_name = TextField(label="Batch Folder Name", value=kandinsky21_fuse_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=kandinsky21_fuse_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #num_outputs = NumberPicker(label="Num of Outputs", min=1, max=4, step=4, value=kandinsky21_fuse_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #num_outputs = TextField(label="num_outputs", value=kandinsky21_fuse_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #n_iterations = TextField(label="Number of Iterations", value=kandinsky21_fuse_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype="int"))
    steps = TextField(label="Number of Steps", value=kandinsky21_fuse_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype="int"))
    sampler = Dropdown(label="Sampler", width=200, options=[dropdown.Option("ddim_sampler"), dropdown.Option("p_sampler")], value=kandinsky21_fuse_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=kandinsky21_fuse_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    param_rows = ResponsiveRow([Column([batch_folder_name, n_images], col={'xs':12, 'md':6}),
                      Column([file_prefix, sampler], col={'xs':12, 'md':6})
                      #Column([steps, ddim_eta, dynamic_threshold_v], col={'xs':12, 'md':6})
                      ], vertical_alignment=CrossAxisAlignment.START)
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=kandinsky21_fuse_prefs, key='steps')
    prior_cf_scale = SliderRow(label="Prior Guidance Scale", min=0, max=10, divisions=10, pref=kandinsky21_fuse_prefs, key='prior_cf_scale', col={'xs':12, 'md':6})
    prior_steps = SliderRow(label="Prior Steps", min=0, max=50, divisions=50, pref=kandinsky21_fuse_prefs, key='prior_steps', col={'xs':12, 'md':6})
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=kandinsky21_fuse_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky21_fuse_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky21_fuse_prefs, key='height')
    #mask_image = TextField(label="Mask Image", value=kandinsky21_fuse_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})
    #invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=kandinsky21_fuse_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})
    #image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    #img_block = Container(Column([image_pickers, weight_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    upscaler = UpscaleBlock(kandinsky21_fuse_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="💥   Run Kandinsky 2.1 Fuser", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky21_fuse(page))
    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)
    page.kandinsky21_fuse_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("💣  Kandinsky 2.1 Fuse", "Mix multiple Images and Prompts together to Interpolate. A Latent Diffusion model with two Multilingual text encoders, supports 100+ languages...", actions=[ft.OutlinedButton(content=Text("Switch to 2.2", size=18), on_click=switch_version), save_default(kandinsky21_fuse_prefs, ['init_image', 'mask_image', 'mixes']), IconButton(icon=icons.HELP, tooltip="Help with Kandinsky Settings", on_click=kandinsky21_help)]),
            prompt_row,
            image_row,
            weight_slider,
            Divider(height=5, thickness=4),
            fuse_layers,
            #Divider(height=2, thickness=2),
            #param_rows, #dropdown_row,
            ResponsiveRow([prior_steps, prior_cf_scale]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            upscaler,
            ResponsiveRow([Row([n_images, sampler], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
            parameters_row,
            page.kandinsky21_fuse_output
        ],
    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed,
    return c

kandinsky_controlnet_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "kandinsky-",
    "num_images": 1,
    "batch_size": 1,
    "steps": 50,
    "prior_steps": 25,
    #"ddim_eta":0.05,
    "width": 768,
    "height":768,
    "guidance_scale":4,
    "init_image": '',
    "strength": 0.5,
    "prior_strength": 0.8,
    #"mask_image": '',
    #"invert_mask": False,
    "seed": 0,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildKandinskyControlNet(page):
    global prefs, kandinsky_controlnet_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          kandinsky_controlnet_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def kandinsky_controlnet_help(e):
      def close_kandinsky_controlnet_dlg(e):
        nonlocal kandinsky_controlnet_help_dlg
        kandinsky_controlnet_help_dlg.open = False
        page.update()
      kandinsky_controlnet_help_dlg = AlertDialog(title=Text("🙅   Help with Kandinsky ControlNet Pipeline"), content=Column([
          #Text("NOTE: Right now, installing this may be incompatible with Diffusers packages, so it may not work if you first installed HuggingFace & Stable Diffusion. It's recommended to run this on a fresh runtime, only installing ESRGAN to upscale. We hope to fix this soon, but works great."),
          Text("Kandinsky 2.2 includes KandinskyV22ControlnetImg2ImgPipeline that will allow you to add control to the image generation process with both the image and its depth map. This pipeline works really well with KandinskyV22PriorEmb2EmbPipeline, which generates image embeddings based on both a text prompt and an image. For our robot cat example, we will pass the prompt and cat image together to the prior pipeline to generate an image embedding. We will then use that image embedding and the depth map of the cat to further control the image generation process."),
          Markdown("Kandinsky 2.2 inherits best practices from [DALL-E 2](https://arxiv.org/abs/2204.06125) and [Latent Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/latent_diffusion), while introducing some new ideas.\nIt uses [CLIP](https://huggingface.co/docs/transformers/model_doc/clip) for encoding images and text, and a diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach enhances the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov) and the original codebase can be found [here](https://github.com/ai-forever/Kandinsky-2)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!"),
          Text("The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🤤  Quality... ", on_click=close_kandinsky_controlnet_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(kandinsky_controlnet_help_dlg)
      kandinsky_controlnet_help_dlg.open = True
      page.update()
    prompt = TextField(label="Prompt Text", value=kandinsky_controlnet_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=kandinsky_controlnet_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=kandinsky_controlnet_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=kandinsky_controlnet_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #n_iterations = TextField(label="Number of Iterations", value=kandinsky_controlnet_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype="int"))
    steps = TextField(label="Number of Steps", value=kandinsky_controlnet_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype="int"))
    #sampler = Dropdown(label="Sampler", width=200, options=[dropdown.Option("ddim_sampler"), dropdown.Option("p_sampler")], value=kandinsky_controlnet_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})
    batch_size = NumberPicker(label="Batch Size", min=1, max=6, step=1, value=kandinsky_controlnet_prefs['batch_size'], on_change=lambda e:changed(e,'batch_size', ptype="int"))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=kandinsky_controlnet_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    #param_rows = ResponsiveRow([Column([batch_folder_name, n_images], col={'xs':12, 'md':6}),
                      #Column([file_prefix, sampler], col={'xs':12, 'md':6})
                      #Column([steps, ddim_eta, dynamic_threshold_v], col={'xs':12, 'md':6})
                      #], vertical_alignment=CrossAxisAlignment.START)
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=kandinsky_controlnet_prefs, key='steps')
    prior_steps = SliderRow(label="Number of Prior Steps", min=0, max=200, divisions=200, pref=kandinsky_controlnet_prefs, key='prior_steps')
    #prior_cf_scale = SliderRow(label="Prior CF Scale", min=0, max=10, divisions=10, pref=kandinsky_controlnet_prefs, key='prior_cf_scale')
    #prior_steps = SliderRow(label="Prior Steps", min=0, max=50, divisions=50, pref=kandinsky_controlnet_prefs, key='prior_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=kandinsky_controlnet_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky_controlnet_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky_controlnet_prefs, key='height')
    init_image = FileInput(label="Init Image", pref=kandinsky_controlnet_prefs, key='init_image', page=page, col={'xs':12, 'md':6})
    #init_image = TextField(label="Init Image", value=kandinsky_controlnet_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init), col={'xs':12, 'md':6})
    #mask_image = TextField(label="Mask Image", value=kandinsky_controlnet_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})
    strength_slider = SliderRow(label="Init Image Strength", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky_controlnet_prefs, key='strength', tooltip="Indicates how much to transform the reference image.")
    prior_strength_slider = SliderRow(label="Prior Image Strength", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky_controlnet_prefs, key='prior_strength', tooltip="Indicates how much to transform the reference text embeddings.")
    #img_block = Container(Column([init_image, strength_slider, prior_strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    seed = TextField(label="Seed", width=90, value=str(kandinsky_controlnet_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(kandinsky_controlnet_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="🌸   Run Kandinsky ControlNet", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky_controlnet(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky_controlnet(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky_controlnet(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.kandinsky_controlnet_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🏩  Kandinsky 2.2 ControlNet Text+Image-to-Image", "Image-to-Image Generation with ControlNet Conditioning and depth-estimation from transformers.", actions=[save_default(kandinsky_controlnet_prefs, ['init_image']), IconButton(icon=icons.HELP, tooltip="Help with Kandinsky Settings", on_click=kandinsky_controlnet_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            init_image, strength_slider, prior_strength_slider,
            #img_block,
            #param_rows, #dropdown_row,
            steps,
            prior_steps,
            #prior_cf_scale,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            #Row([batch_folder_name, file_prefix]),
            upscaler,
            ResponsiveRow([Row([n_images, batch_size, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
            parameters_row,
            page.kandinsky_controlnet_output
        ],
    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed,
    return c


deep_daze_prefs = {
    'prompt': '',
    'num_layers': 32,
    'save_every': 20,
    'max_size': 512,
    'save_progress': False,
    'learning_rate': 1e-5,
    'iterations': 1050,
    'num_images': 1,
    'batch_folder_name': '',
    'file_prefix': 'daze-',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildDeepDaze(page):
    global deep_daze_prefs, prefs, pipe_deep_daze
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          deep_daze_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_deep_daze_output(o):
      page.deep_daze_output.controls.append(o)
      page.deep_daze_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_deep_daze_output = add_to_deep_daze_output
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.deep_daze_output.controls = []
      page.deep_daze_output.update()
      clear_button.visible = False
      clear_button.update()
    def deep_daze_help(e):
      def close_deep_daze_dlg(e):
        nonlocal deep_daze_help_dlg
        deep_daze_help_dlg.open = False
        page.update()
      deep_daze_help_dlg = AlertDialog(title=Text("🙅   Help with DeepDaze Pipeline"), content=Column([
          Text("Text to image generation using OpenAI's CLIP and Siren. Credit goes to Ryan Murdock for the discovery of this technique (and for coming up with the great name)!"),
          Text("Heavily influenced by Alexander Mordvintsev's Deep Dream, this work uses CLIP to match an image learned by a SIREN network with a given textual description."),
          #Markdown(""),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😵‍💫  Why not... ", on_click=close_deep_daze_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(deep_daze_help_dlg)
      deep_daze_help_dlg.open = True
      page.update()
    prompt = TextField(label="Prompt Text", value=deep_daze_prefs['prompt'], filled=True, on_change=lambda e:changed(e,'prompt'))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=deep_daze_prefs, key='max_size')
    #num_layers = TextField(label="Inference Steps", value=str(deep_daze_prefs['num_layers']), keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_layers', ptype='int'))
    learning_rate = TextField(label="Learning Rate", width=130, value=float(deep_daze_prefs['learning_rate']), keyboard_type=KeyboardType.NUMBER, tooltip="The learning rate of the neural net.", on_change=lambda e:changed(e,'learning_rate', ptype='float'))
    num_layers_row = SliderRow(label="Number of Layers", min=1, max=100, divisions=99, pref=deep_daze_prefs, key='num_layers', tooltip="The number of hidden layers to use with Siren neural network")
    save_every_row = SliderRow(label="Save/Show Every x Steps", min=1, max=100, divisions=99, pref=deep_daze_prefs, key='save_every', tooltip="Generate an image every time iterations is a multiple of this number.")
    iterations_row = SliderRow(label="Number of Iterations", min=1, max=2000, divisions=1999, pref=deep_daze_prefs, key='iterations', tooltip="The number of times to calculate and backpropogate loss in a given epoch.")
    batch_folder_name = TextField(label="Batch Folder Name", value=deep_daze_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=deep_daze_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    save_progress = Tooltip(message="Whether or not to save images generated before training Siren is complete.", content=Switcher(label="Save Progress Steps", value=deep_daze_prefs['save_progress'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'save_progress')))
    #eta = TextField(label="ETA", value=str(deep_daze_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(deep_daze_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta_row = Row([Text("DDIM ETA: "), eta])
    #max_size = Slider(min=256, max=1280, divisions=64, label="{value}px", value=int(deep_daze_prefs['max_size']), expand=True, on_change=lambda e:changed(e,'max_size', ptype='int'))
    #max_row = Row([Text("Max Resolution Size: "), max_size])
    upscaler = UpscaleBlock(deep_daze_prefs)
    page.upscalers.append(upscaler)
    page.deep_daze_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.deep_daze_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("👀  DeepDaze Text-to-Image Generator", "An alternative method using OpenAI's CLIP and Siren. Made a few years ago but still fascinating results....", actions=[save_default(deep_daze_prefs), IconButton(icon=icons.HELP, tooltip="Help with DeepDaze Settings", on_click=deep_daze_help)]),
        prompt,
        num_layers_row,
        iterations_row,
        Row([learning_rate, save_progress]),
        save_every_row,
        max_row,
        #NumberPicker(label="Number of Images: ", min=1, max=20, value=deep_daze_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')),
        Row([batch_folder_name, file_prefix]),
        upscaler,
        Row([ElevatedButton(content=Text("😶   Get DeepDaze Generation", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_deep_daze(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_deep_daze(page, from_list=True))
        ]),
      ]
    )), page.deep_daze_output,
        clear_button,
    ], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

CLIPstyler_prefs = {
    'source':'a photo',
    'prompt_text': 'Detailed oil painting',
    'batch_folder_name': 'clipstyler',
    'crop_size': 128,
    'num_crops': 64,
    'original_image': '',
    'image_dir': "",
    'training_iterations': 100,
    'width': 512,
    'height': 512,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildCLIPstyler(page):
    global CLIPstyler, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          CLIPstyler_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def CLIPstyler_help(e):
      def close_CLIPstyler_dlg(e):
        nonlocal CLIPstyler_help_dlg
        CLIPstyler_help_dlg.open = False
        page.update()
      CLIPstyler_help_dlg = AlertDialog(title=Text("💁   Help with CLIP Styler"), content=Column([
          Text("Existing neural style transfer methods require reference style images to transfer texture information of style images to content images. However, in many practical situations, users may not have reference style images but still be interested in transferring styles by just imagining them. In order to deal with such applications, we propose a new framework that enables a style transfer `without' a style image, but only with a text description of the desired style. Using the pre-trained text-image embedding model of CLIP, we demonstrate the modulation of the style of content images only with a single text condition. Specifically, we propose a patch-wise text-image matching loss with multiview augmentations for realistic texture transfer. Extensive experimental results confirmed the successful image style transfer with realistic textures that reflect semantic query texts."),
          Text("Style transfer aims to transform a content image by transferring the semantic texture of a style image. The seminar work of neural style transfer proposed by Gatys et al. uses pre-trained VGG network to transfer the style texture by calculating the style loss that matches the Gram matrices of the content and style features. Their style loss has become a standard in later works including stylization through pixel optimization for a single content image, arbitrary style transfer which operates in real-time for various style images, and optimizing feedforward network for stylizing each image."),
          Text("Although these approaches for style transfer can successfully create visually pleasing new artworks by transferring styles of famous artworks to common images, they require a reference style image to change the texture of the content image. However, in many practical applications, reference style images are not available to users, but the users are still interested in ‘imitiating’ the texture of the style images. For example, users can imagine being able to convert their own photos into Monet or Van Gogh styles without ever owning paintings by the famous painters. Or you can convert your daylight images into night images by mere imagination. In fact, to overcome this limitation of the existing style transfer and create a truly creative artwork, we should be able to transfer a completely novel style that we imagine."),
          Markdown("[GitHub Project](https://github.com/cyclomon/CLIPstyler) | [Paper](https://arxiv.org/abs/2112.00374) | [Colab](https://colab.research.google.com/drive/1dg8PXi-TVtzdpbaoI7ty72SSY7xdBgwo?usp=sharing)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😖  Stylish... ", on_click=close_CLIPstyler_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(CLIPstyler_help_dlg)
      CLIPstyler_help_dlg.open = True
      page.update()
    prompt_text = TextField(label="Stylized Prompt Text", value=CLIPstyler_prefs['prompt_text'], filled=True, on_change=lambda e:changed(e,'prompt_text'))
    batch_folder_name = TextField(label="Batch Folder Name", value=CLIPstyler_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    source = TextField(label="Source Type", value=CLIPstyler_prefs['source'], on_change=lambda e:changed(e,'source'))
    #training_iterations = TextField(label="Training Iterations", value=CLIPstyler_prefs['training_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'training_iterations', ptype="int"))
    crop_size = TextField(label="Crop Size", value=CLIPstyler_prefs['crop_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'crop_size', ptype="int"))
    num_crops = TextField(label="Number of Crops", value=CLIPstyler_prefs['num_crops'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_crops', ptype="int"))
    param_rows = Column([Row([batch_folder_name, source]), Row([crop_size, num_crops])])
    iterations = SliderRow(label="Training Iterations", min=50, max=500, divisions=90, pref=CLIPstyler_prefs, key='training_iterations')
    width_slider = SliderRow(label="Width", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=CLIPstyler_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=CLIPstyler_prefs, key='height')
    original_image = FileInput(label="Original Image", pref=CLIPstyler_prefs, key='original_image', page=page, expand=True, col={"*":1, "md":3})
    #original_image = TextField(label="Original Image", value=CLIPstyler_prefs['original_image'], on_change=lambda e:changed(e,'original_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original, col={"*":1, "md":3}))
    #mask_image = TextField(label="Mask Image", value=CLIPstyler_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask, col={"*":1, "md":3}))
    #invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=CLIPstyler_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
    image_picker = Container(content=Row([original_image]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    #prompt_strength = Slider(min=0.1, max=0.9, divisions=16, label="{value}%", value=CLIPstyler_prefs['prompt_strength'], on_change=change_strength, expand=True)
    #strength_value = Text(f" {int(CLIPstyler_prefs['prompt_strength'] * 100)}%", weight=FontWeight.BOLD)
    #strength_slider = Row([Text("Prompt Strength: "), strength_value, prompt_strength])
    #img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    upscaler = UpscaleBlock(CLIPstyler_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="📎   Run CLIP-Styler", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_CLIPstyler(page))
    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)
    page.CLIPstyler_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("😎   CLIP-Styler", "Transfers a Text Guided Style onto your Image From Prompt Description...", actions=[save_default(CLIPstyler_prefs, ['original_image', 'image_dir']), IconButton(icon=icons.HELP, tooltip="Help with CLIP Styler Settings", on_click=CLIPstyler_help)]),
            image_picker, prompt_text,
            param_rows, iterations, width_slider, height_slider, #Divider(height=9, thickness=2),
            upscaler,
            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
            parameters_row,
            page.CLIPstyler_output
        ],
    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, crop_size, num_crops,
    return c

semantic_prefs = {
    'prompt': '',
    'editing_prompt': '',
    'negative_prompt': '',
    'num_inference_steps': 100,
    'guidance_scale': 7.5,
    'edit_momentum_scale': 0.1,
    'edit_mom_beta': 0.4,
    'editing_prompts': [],
    'eta': 0.0,
    'seed': 0,
    'width': 960,
    'height': 768,
    'num_images': 1,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildSemanticGuidance(page):
    global semantic_prefs, prefs, pipe_semantic
    editing_prompt = {'editing_prompt':'', 'edit_warmup_steps':10, 'edit_guidance_scale':5, 'edit_threshold':0.9, 'edit_weights':1, 'reverse_editing_direction': False}
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          semantic_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_semantic_output(o):
      page.semantic_output.controls.append(o)
      page.semantic_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      play_snd(Snd.DELETE, page)
      page.semantic_output.controls = []
      page.semantic_output.update()
      clear_button.visible = False
      clear_button.update()
    def semantic_help(e):
      def close_semantic_dlg(e):
        nonlocal semantic_help_dlg
        semantic_help_dlg.open = False
        page.update()
      semantic_help_dlg = AlertDialog(title=Text("💁   Help with Semantic Guidance"), content=Column([
          Text("SEGA allows applying or removing one or more concepts from an image. The strength of the concept can also be controlled. I.e. the smile concept can be used to incrementally increase or decrease the smile of a portrait. Similar to how classifier free guidance provides guidance via empty prompt inputs, SEGA provides guidance on conceptual prompts. Multiple of these conceptual prompts can be applied simultaneously. Each conceptual prompt can either add or remove their concept depending on if the guidance is applied positively or negatively."),
          Text("Unlike Pix2Pix Zero or Attend and Excite, SEGA directly interacts with the diffusion process instead of performing any explicit gradient-based optimization."),
          Text("Semantic Guidance for Diffusion Models was proposed in SEGA: Instructing Diffusion using Semantic Dimensions and provides strong semantic control over the image generation. Small changes to the text prompt usually result in entirely different output images. However, with SEGA a variety of changes to the image are enabled that can be controlled easily and intuitively, and stay true to the original image composition."),
          Text("Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. However, achieving one-shot generation that aligns with the user's intent is nearly impossible, yet small changes to the input prompt often result in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion process to flexibly steer it along semantic directions. This semantic guidance (SEGA) allows for subtle and extensive edits, changes in composition and style, as well as optimizing the overall artistic conception. We demonstrate SEGA's effectiveness on a variety of tasks and provide evidence for its versatility and flexibility."),
          Markdown("[HuggingFace Documentation](https://huggingface.co/docs/diffusers/main/en/api/pipelines/semantic_stable_diffusion) - [Paper](https://arxiv.org/abs/2301.12247)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😖  More Control Please... ", on_click=close_semantic_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(semantic_help_dlg)
      semantic_help_dlg.open = True
      page.update()
    def change_eta(e):
        changed(e, 'eta', ptype="float")
        eta_value.value = f" {semantic_prefs['eta']}"
        eta_value.update()
        eta_row.update()
    def semantic_tile(semantic_prompt):
        params = []
        for k, v in semantic_prompt.items():
            if k == 'editing_prompt': continue
            params.append(f'{to_title(k)}: {v}')
        sub = ', '.join(params)
        return ListTile(title=Text(semantic_prompt['editing_prompt'], max_lines=6, theme_style=TextThemeStyle.BODY_LARGE), subtitle=Text(sub), dense=True, data=semantic_prompt, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[PopupMenuItem(icon=icons.EDIT, text="Edit Semantic Prompt", on_click=lambda e: edit_semantic(semantic_prompt), data=semantic_prompt),
                 PopupMenuItem(icon=icons.DELETE, text="Delete Semantic Prompt", on_click=lambda e: del_semantic(semantic_prompt), data=semantic_prompt)]), on_click=lambda e: edit_semantic(semantic_prompt))
    def edit_semantic(edit=None):
        semantic_prompt = edit if bool(edit) else editing_prompt.copy()
        edit_prompt = edit['editing_prompt'] if bool(edit) else ""
        if not bool(edit):
            semantic_prompt['editing_prompt'] = semantic_prefs['prompt']
        def close_dlg(e):
            dlg_edit.open = False
            page.update()
        def changed_p(e, pref=None):
            if pref is not None:
                semantic_prompt[pref] = e.control.value
        def save_semantic_prompt(e):
            if edit == None:
                semantic_prefs['editing_prompts'].append(semantic_prompt)
                page.semantic_prompts.controls.append(semantic_tile(semantic_prompt))
                page.semantic_prompts.update()
            else:
                for s in semantic_prefs['editing_prompts']:
                    if s['editing_prompt'] == edit_prompt:
                        s = semantic_prompt
                        break
                for t in page.semantic_prompts.controls:
                    if t.data['editing_prompt'] == edit_prompt:
                        params = []
                        for k, v in semantic_prompt.items():
                            if k == 'editing_prompt': continue
                            params.append(f'{to_title(k)}: {v}')
                        sub = ', '.join(params)
                        t.title = Text(semantic_prompt['editing_prompt'], max_lines=6, theme_style=TextThemeStyle.BODY_LARGE)
                        t.subtitle = Text(sub)
                        t.data = semantic_prompt
                        t.update()
                        break
            dlg_edit.open = False
            e.control.update()
            page.update()
        semantic_editing_prompt = TextField(label="Semantic Editing Prompt Modifier", value=semantic_prompt['editing_prompt'], autofocus=True, on_change=lambda e:changed_p(e,'editing_prompt'))
        edit_warmup_steps = SliderRow(label="Edit Warmup Steps", min=0, max=50, divisions=50, pref=semantic_prompt, key='edit_warmup_steps', tooltip="Number of diffusion steps (for each prompt) for which semantic guidance will not be applied. Momentum will still be calculated for those steps and applied once all warmup periods are over.")
        edit_guidance_scale = SliderRow(label="Edit Guidance Scale", min=0, max=20, divisions=40, round=1, pref=semantic_prompt, key='edit_guidance_scale', tooltip="Guidance scale for semantic guidance. If provided as list values should correspond to `editing_prompt`.")
        edit_threshold = SliderRow(label="Edit Threshold", min=0, max=1, divisions=40, round=3, pref=semantic_prompt, key='edit_threshold', tooltip="Threshold of semantic guidance.")
        edit_weights = SliderRow(label="Edit Weights", min=0, max=10, divisions=20, round=1, pref=semantic_prompt, key='edit_weights', tooltip="Indicates how much each individual concept should influence the overall guidance. If no weights are provided all concepts are applied equally.")
        reverse_editing_direction = Checkbox(label="Reverse Editing Direction", value=semantic_prompt['reverse_editing_direction'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed_p(e,'reverse_editing_direction'), tooltip="Whether the corresponding prompt in `editing_prompt` should be increased or decreased.")
        dlg_edit = AlertDialog(modal=False, title=Text(f"♟️ {'Edit' if bool(edit) else 'Add'} Semantic Prompt"), content=Container(Column([
            semantic_editing_prompt, edit_warmup_steps, edit_guidance_scale, edit_threshold, edit_weights, reverse_editing_direction,
        ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO), width=(page.width if page.web else page.window.width) - 180), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Prompt ", size=19, weight=FontWeight.BOLD), on_click=save_semantic_prompt)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(dlg_edit)
        dlg_edit.open = True
        page.update()
    def del_semantic(edit=None):
        for s in semantic_prefs['editing_prompts']:
            if s['editing_prompt'] == edit['editing_prompt']:
                semantic_prefs['editing_prompts'].remove(s)
                break
        for t in page.semantic_prompts.controls:
            if t.data['editing_prompt'] == edit['editing_prompt']:
                page.semantic_prompts.controls.remove(t)
                break
        page.semantic_prompts.update()
        play_snd(Snd.DELETE, page)
    def clear_semantic_prompts(e):
        semantic_prefs['editing_prompts'].clear()
        page.semantic_prompts.controls.clear()
        page.semantic_prompts.update()
        play_snd(Snd.DELETE, page)
    prompt = TextField(label="Base Prompt Text", value=semantic_prefs['prompt'], col={'md': 9}, filled=True, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=semantic_prefs['negative_prompt'], col={'md':3}, filled=True, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    seed = TextField(label="Seed", width=90, value=str(semantic_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=semantic_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=semantic_prefs, key='guidance_scale')
    edit_momentum_scale = SliderRow(label="Edit Momentum Scale", min=0, max=1, divisions=20, round=1, pref=semantic_prefs, key='edit_momentum_scale', tooltip="Scale of the momentum to be added to the semantic guidance at each diffusion step. Momentum is already built up during warmup, i.e. for diffusion steps smaller than `sld_warmup_steps`. Momentum will only be added to latent guidance once all warmup periods are finished.")
    edit_mom_beta = SliderRow(label="Edit Momentum Beta", min=0, max=1, divisions=20, round=1, pref=semantic_prefs, key='edit_mom_beta', tooltip="Defines how semantic guidance momentum builds up. `edit_mom_beta` indicates how much of the previous momentum will be kept. Momentum is already built up during warmup, i.e. for diffusion steps smaller than `edit_warmup_steps`.")
    #eta = TextField(label="ETA", value=str(semantic_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", round=2, value=float(semantic_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=change_eta)
    eta_value = Text(f" {semantic_prefs['eta']}", weight=FontWeight.BOLD)
    eta_row = Row([Text("ETA:"), eta_value, eta])
    page.etas.append(eta_row)
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=semantic_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=semantic_prefs, key='height')
    batch_folder_name = TextField(label="Batch Folder Name", value=semantic_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    upscaler = UpscaleBlock(semantic_prefs)
    page.upscalers.append(upscaler)
    page.semantic_prompts = Column([], spacing=0)
    page.semantic_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.semantic_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🧩  Semantic Guidance for Diffusion Models - SEGA", "Text-to-Image Generation with Latent Editing to apply or remove multiple concepts from an image with advanced controls....", actions=[save_default(semantic_prefs, ['editing_prompts']), IconButton(icon=icons.HELP, tooltip="Help with Semantic Guidance Settings", on_click=semantic_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        ResponsiveRow([prompt, negative_prompt]),
        Row([Text("Editing Semantic Prompts", theme_style=TextThemeStyle.TITLE_LARGE, weight=FontWeight.BOLD),
                    Row([ft.FilledTonalButton("Clear Prompts", on_click=clear_semantic_prompts), ft.FilledButton("Add Editing Prompt", on_click=lambda e: edit_semantic(None))])], alignment=MainAxisAlignment.SPACE_BETWEEN),
        page.semantic_prompts,
        Divider(thickness=2, height=4),
        num_inference_row,
        guidance,
        edit_momentum_scale, edit_mom_beta,
        eta_row,
        width_slider, height_slider,
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=semantic_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        upscaler,
        #Row([jump_length, jump_n_sample, seed]),
        Row([
            ElevatedButton(content=Text("🎳  Run Semantic Guidance", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_semantic(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_semantic(page, from_list=True))
        ]),
        page.semantic_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

demofusion_prefs = {
    "prompt": '',
    "negative_prompt": '',
    'init_image': '',
    'view_batch_size': 16,
    'cosine_scale_1': 3.0,
    'cosine_scale_2': 1.0,
    'cosine_scale_3': 1.0,
    'sigma': 0.8,
    'multi_decoder': True,
    'show_image': False,
    "steps":50,
    "width": 3072,
    "height": 3072,
    'sigma': 4.0,
    'stride': 64,
    "guidance_scale":7.5,
    "seed": 0,
    'cpu_offload': True,
    "batch_folder_name": '',
    "file_prefix": "demofusion-",
    "num_images": 1,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildDemoFusion(page):
    global prefs, demofusion_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          demofusion_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def demofusion_help(e):
      def close_demofusion_dlg(e):
        nonlocal demofusion_help_dlg
        demofusion_help_dlg.open = False
        page.update()
      demofusion_help_dlg = AlertDialog(title=Text("🙅   Help with DemoFusion Pipeline"), content=Column([
          Text('High-resolution image generation with Generative Artificial Intelligence (GenAI) has immense potential but, due to the enormous capital investment required for training, it is increasingly centralised to a few large corporations, and hidden behind paywalls. This paper aims to democratise high-resolution GenAI by advancing the frontier of high-resolution generation while remaining accessible to a broad audience. We demonstrate that existing Latent Diffusion Models (LDMs) possess untapped potential for higher-resolution image generation. Our novel DemoFusion framework seamlessly extends open-source GenAI models, employing Progressive Upscaling, Skip Residual, and Dilated Sampling mechanisms to achieve higher-resolution image generation. The progressive nature of DemoFusion requires more passes, but the intermediate results can serve as "previews", facilitating rapid prompt iteration.'),
          #Text(""),
          Markdown("[Paper](https://arxiv.org/abs/2311.16973) | [Original GitHub](https://github.com/PRIS-CV/DemoFusion) | [Ruoyi Du](https://github.com/RuoyiDu)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🏡  Go Big or...", on_click=close_demofusion_dlg)], actions_alignment=MainAxisAlignment.END)
      page.overlay.append(demofusion_help_dlg)
      demofusion_help_dlg.open = True
      page.update()
    prompt = TextField(label="Prompt Text", value=demofusion_prefs['prompt'], filled=True, multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=demofusion_prefs['negative_prompt'], filled=True, multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = FileInput(label="Init Image (optional)", pref=demofusion_prefs, key='init_image', page=page)
    batch_folder_name = TextField(label="Batch Folder Name", value=demofusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=demofusion_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #steps = TextField(label="Number of Steps", value=demofusion_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype="int"))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=demofusion_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    steps = SliderRow(label="Number of Steps", min=0, max=100, divisions=100, pref=demofusion_prefs, key='steps')
    sigma = SliderRow(label="Sigma", min=0, max=6, divisions=60, round=1, pref=demofusion_prefs, key='sigma', col={'xs':12, 'md':6}, tooltip="The standard value of the Gaussian filter. Larger sigma promotes the global guidance of dilated sampling, but has the potential of over-smoothing.")
    stride = SliderRow(label="Stride", min=0, max=100, divisions=100, pref=demofusion_prefs, key='stride', tooltip="The stride of moving local patches. A smaller stride is better for alleviating seam issues, but it also introduces additional computational overhead and inference time.")
    view_batch_size = SliderRow(label="View Batch Size", min=0, max=50, divisions=50, pref=demofusion_prefs, key='view_batch_size', tooltip="The batch size for multiple denoising paths. Typically, a larger batch size can result in higher efficiency but comes with increased GPU memory requirements.")
    cosine_scale_1 = SliderRow(label="Cosine Scale 1", min=0, max=5, divisions=10, round=1, expand=True, pref=demofusion_prefs, key='cosine_scale_1', col={'xs':12, 'md':4}, tooltip="Control the strength of skip-residual. For specific impacts, please refer to Appendix C in the DemoFusion paper.")
    cosine_scale_2 = SliderRow(label="Cosine Scale 2", min=0, max=5, divisions=10, round=1, expand=True, pref=demofusion_prefs, key='cosine_scale_2', col={'xs':12, 'md':4}, tooltip="Control the strength of dilated sampling. For specific impacts, please refer to Appendix C in the DemoFusion paper.")
    cosine_scale_3 = SliderRow(label="Cosine Scale 3", min=0, max=5, divisions=10, round=1, expand=True, pref=demofusion_prefs, key='cosine_scale_3', col={'xs':12, 'md':4}, tooltip="Control the strength of the Gaussian filter. For specific impacts, please refer to Appendix C in the DemoFusion paper.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=20, divisions=40, round=1, pref=demofusion_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=4096, divisions=31, multiple=128, suffix="px", pref=demofusion_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=4096, divisions=31, multiple=128, suffix="px", pref=demofusion_prefs, key='height')
    multi_decoder = Switcher(label="Multi Decoder", value=demofusion_prefs['multi_decoder'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'multi_decoder'), tooltip="Whether to use a tiled decoder. Generally, when the resolution exceeds 3072x3072, a tiled decoder becomes necessary.")
    show_image = Switcher(label="Show Intermediates", value=demofusion_prefs['show_image'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'show_image'), tooltip="Whether to show intermediate results during generation.")
    cpu_offload = Switcher(label="CPU Offload", value=demofusion_prefs['cpu_offload'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'cpu_offload'), tooltip="Saves VRAM if you have less than 16GB VRAM. Otherwise can run out of memory.")
    seed = TextField(label="Seed", width=90, value=str(demofusion_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    upscaler = UpscaleBlock(demofusion_prefs)
    page.upscalers.append(upscaler)
    parameters_button = ElevatedButton(content=Text(value="💥   Run DemoFusion", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_demofusion(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_demofusion(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_demofusion(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.demofusion_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("💣  DemoFusion", "Democratising High-Resolution Image Generation With No $$$. SDXL with Clean Upscaling, 3 Phase Denoising/Decoding, slow but real quality...", actions=[save_default(demofusion_prefs, ['init_image']), IconButton(icon=icons.HELP, tooltip="Help with DemoFusion Settings", on_click=demofusion_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            init_image,
            #ResponsiveRow([stride, sigma]),
            stride, sigma,
            view_batch_size,
            ResponsiveRow([cosine_scale_1, cosine_scale_2, cosine_scale_3]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            Row([multi_decoder, show_image, cpu_offload]),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            upscaler,
            parameters_row,
            page.demofusion_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

class State:
    x: float
    y: float

state = State()
#TODO: Waiting for Scribbler addon integration
def buildDreamMask(page):
    #prog_bars: Dict[str, ProgressRing] = {}
    files = Ref[Column]()
    #upload_button = Ref[ElevatedButton]()

    def file_picker_result(e: FilePickerResultEvent):
        files.current.controls.clear()
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        save_file(e.file_name)
    def save_file(file_name):
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
        else:
          fname = file_name
        load_img = PILImage.open(fname)
        w, h = load_img.size
        bg_img.width=w
        bg_img.height=h
        stack_box.width=w
        stack_box.height=h
        bg_img.src = fname
        bg_img.update()
        stack_box.update()
        clear_canvas(True)
        #files.current.controls.append(Row([Text(f"Done uploading {root_dir}{e.file_name}")]))
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    bg_img = Img(src="https://picsum.photos/200/300", fit=ImageFit.CONTAIN, width=1024, height=512)#float("inf")
    stroke_width = 50
    def pan_start(e: ft.DragStartEvent):
        state.x = e.local_x
        state.y = e.local_y
    def pan_update(e: ft.DragUpdateEvent):
        cp.shapes.append(cv.Line(state.x, state.y, e.local_x, e.local_y, paint=ft.Paint(stroke_width=stroke_width, stroke_join=ft.StrokeJoin.ROUND, stroke_cap=ft.StrokeCap.ROUND, style=ft.PaintingStyle.STROKE))) #, blend_mode=ft.BlendMode.CLEAR
        cp.update()
        state.x = e.local_x
        state.y = e.local_y
    cp = cv.Canvas(
        content=ft.GestureDetector(
            on_pan_start=pan_start,
            on_pan_update=pan_update,
            drag_interval=10,
        ),
        expand=False,
    )
    def clear_canvas(e):
        cp.shapes.clear()
        cp.update()
    def change_stroke(e):
        nonlocal stroke_width
        stroke_width = e.control.value
    stack_box = Container(
            Stack(
                [
                    #Container(content=Text("Picture")),
                    bg_img,
                    #Img(src="https://picsum.photos/200/300", fit=ImageFit.FILL, width=float("inf")),
                    cp,
                ]
            ),
            #border_radius=5,
            border=ft.border.all(2, ft.colors.BLACK),
            width=1024,#float("inf"),
            height=512,
            expand=False,
        )
    c = Column([
        Header("🎭   Dream Mask Maker"),
        ElevatedButton(
            "Select Init Image to Mask...",
            icon=icons.FOLDER_OPEN,
            on_click=lambda _: file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Init Image File" ),
        ),
        Column(ref=files),
        stack_box,
        Row([IconButton(icon=icons.CLEAR, tooltip="Clear", on_click=clear_canvas), Text("Stroke Width"), Slider(min=1, max=100, divisions=99, round=0, expand=True, label="{value}px", value=stroke_width, on_change=change_stroke)])
    ], scroll=ScrollMode.AUTO)
    return c

dreambooth_prefs = {
    'instance_prompt': '',
    'prior_preservation': False,
    'prior_preservation_class_prompt': "",
    'num_class_images': 12,
    'sample_batch_size': 2,
    'train_batch_size': 1,
    'prior_loss_weight': 0.5,
    'prior_preservation_class_folder': os.path.join(root_dir, "class_images"),
    'learning_rate': 5e-06,
    'max_train_steps': 450,
    'seed': 222476,
    'name_of_your_concept': "",
    'save_concept': True,
    'where_to_save_concept': "Public Library",
    'max_size': 512,
    'image_path': '',
    'readme_description': '',
    'urls': [],
}

def buildDreamBooth(page):
    global prefs, dreambooth_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            dreambooth_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_dreambooth_output(o):
        page.dreambooth_output.controls.append(o)
        page.dreambooth_output.update()
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.dreambooth_output.controls = []
        page.dreambooth_output.update()
        clear_button.visible = False
        clear_button.update()
    def db_help(e):
        def close_db_dlg(e):
          nonlocal db_help_dlg
          db_help_dlg.open = False
          page.update()
        db_help_dlg = AlertDialog(title=Text("💁   Help with DreamBooth"), content=Column([
            Text("First thing is to collect all your own images that you want to teach it to dream.  Feed it at least 5 square pictures of the object or style to learn, and it'll save your Custom Model Checkpoint."),
            Text("Fine-tune your perameters, but be aware that the training process takes a long time to run, so careful with the settings if you don't have the patience or processor. Dream at your own risk."),
          ], scroll=ScrollMode.AUTO), actions=[TextButton(emojize(':sleepy_face:') + "  Got it... ", on_click=close_db_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(db_help_dlg)
        db_help_dlg.open = True
        page.update()
    def delete_image(e):
        f = e.control.data
        if os.path.isfile(f):
          os.remove(f)
          for i, fl in enumerate(page.db_file_list.controls):
            if fl.title.value == f:
              del page.db_file_list.controls[i]
              page.db_file_list.update()
              continue
    def delete_all_images(e):
        for fl in page.db_file_list.controls:
          f = fl.title.value
          if os.path.isfile(f):
            os.remove(f)
        page.db_file_list.controls.clear()
        page.db_file_list.update()
    def add_file(fpath, update=True):
        page.db_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[#TODO: View Image
              PopupMenuItem(icon=icons.DELETE, text="Delete Image", on_click=delete_image, data=fpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All", on_click=delete_all_images, data=fpath),
          ])))
        if update: page.db_file_list.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    save_dir = os.path.join(root_dir, 'my_concept')
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
          save_file(e.file_name)
    def save_file(file_name):
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
          fpath = os.path.join(save_dir, file_name)
        else:
          fname = file_name
          fpath = os.path.join(save_dir, file_name.rpartition(slash)[2])
        original_img = PILImage.open(fname)
        width, height = original_img.size
        width, height = scale_dimensions(width, height, dreambooth_prefs['max_size'])
        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
        original_img.save(fpath)
        if page.web: os.remove(fname)
        #shutil.move(fname, fpath)
        add_file(fpath)
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Image File to Enlarge")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def add_image(e):
        save_dir = os.path.join(root_dir, 'my_concept')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        if image_path.value.startswith('http'):
          import requests
          from io import BytesIO
          response = requests.get(image_path.value)
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          concept_image = PILImage.open(BytesIO(response.content)).convert("RGB")
          width, height = concept_image.size
          width, height = scale_dimensions(width, height, dreambooth_prefs['max_size'])
          concept_image = concept_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          concept_image.save(fpath)
          add_file(fpath)
        elif os.path.isfile(image_path.value):
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(image_path.value)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, dreambooth_prefs['max_size'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          #shutil.copy(image_path.value, fpath)
          add_file(fpath)
        elif os.path.isdir(image_path.value):
          for f in os.listdir(image_path.value):
            file_path = os.path.join(image_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              fpath = os.path.join(save_dir, f)
              original_img = PILImage.open(file_path)
              width, height = original_img.size
              width, height = scale_dimensions(width, height, dreambooth_prefs['max_size'])
              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              original_img.save(fpath)
              #shutil.copy(file_path, fpath)
              add_file(fpath)
        else:
          if bool(image_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        image_path.value = ""
        image_path.update()
    def load_images():
        if os.path.exists(save_dir):
          for f in os.listdir(save_dir):
            existing = os.path.join(save_dir, f)
            if os.path.isdir(existing): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              add_file(existing, update=False)
    instance_prompt = TextField(label="Instance Prompt Token Text", value=dreambooth_prefs['instance_prompt'], on_change=lambda e:changed(e,'instance_prompt'))
    prior_preservation_class_prompt = TextField(label="Prior Preservation Class Prompt", value=dreambooth_prefs['prior_preservation_class_prompt'], on_change=lambda e:changed(e,'prior_preservation_class_prompt'))
    prior_preservation = Checkbox(label="Prior Preservation", tooltip="If you'd like class of the concept (e.g.: toy, dog, painting) is guaranteed to be preserved. This increases the quality and helps with generalization at the cost of training time", value=dreambooth_prefs['prior_preservation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'prior_preservation'))
    num_class_images = TextField(label="Number of Class Images", value=dreambooth_prefs['num_class_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_class_images', ptype='int'), width = 160)
    sample_batch_size = TextField(label="Sample Batch Size", value=dreambooth_prefs['sample_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_batch_size', ptype='int'), width = 160)
    prior_loss_weight = TextField(label="Prior Loss Weight", value=dreambooth_prefs['prior_loss_weight'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'prior_loss_weight', ptype='float'), width = 160)
    max_train_steps = TextField(label="Max Training Steps", value=dreambooth_prefs['max_train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_train_steps', ptype='int'), width = 160)
    learning_rate = TextField(label="Learning Rate", value=dreambooth_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160)
    seed = TextField(label="Seed", value=dreambooth_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    save_concept = Checkbox(label="Save Concept    ", tooltip="", value=dreambooth_prefs['save_concept'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_concept'))
    where_to_save_concept = Dropdown(label="Where to Save Concept", width=250, options=[dropdown.Option("Public Library"), dropdown.Option("Privately to my Profile")], value=dreambooth_prefs['where_to_save_concept'], on_change=lambda e: changed(e, 'where_to_save_concept'))
    prior_preservation_class_folder = TextField(label="Prior Preservation Class Folder", value=dreambooth_prefs['prior_preservation_class_folder'], on_change=lambda e:changed(e,'prior_preservation_class_folder'))
    name_of_your_concept = TextField(label="Name of your Concept", value=dreambooth_prefs['name_of_your_concept'], on_change=lambda e:changed(e,'name_of_your_concept'))
    readme_description = TextField(label="Extra README Description", value=dreambooth_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=dreambooth_prefs, key='max_size')
    image_path = TextField(label="Image File or Folder Path or URL to Train", value=dreambooth_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    add_image_button = ElevatedButton(content=Text("Add File or Folder"), on_click=add_image)
    page.db_file_list = Column([], tight=True, spacing=0)
    load_images()
    #seed = TextField(label="Seed", value=dreambooth_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    #lambda_entropy = TextField(label="Lambda Entropy", value=dreamfusdreambooth_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    #max_steps = TextField(label="Max Steps", value=dreambooth_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    page.dreambooth_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.dreambooth_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("😶‍🌫️  Create Custom DreamBooth Concept Model", "Provide a collection of images to conceptualize. Warning: May take over an hour to run the training...", actions=[save_default(dreambooth_prefs, ['image_path', 'urls']), IconButton(icon=icons.HELP, tooltip="Help with DreamBooth Settings", on_click=db_help)]),
        Row([instance_prompt, name_of_your_concept]),
        Row([num_class_images, sample_batch_size, prior_loss_weight]),
        Row([max_train_steps, learning_rate, seed]),
        Row([save_concept, where_to_save_concept]),
        readme_description,
        #Row([prior_preservation_class_folder]),
        max_row,
        Row([image_path, add_image_button]),
        page.db_file_list,
        Row([ElevatedButton(content=Text("👨‍🎨️  Run DreamBooth", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dreambooth(page))]),
        page.dreambooth_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

textualinversion_prefs = {
    'what_to_teach': 'object',
    'placeholder_token': '',
    'initializer_token': '',
    'scale_lr': True,
    'max_train_steps': 3000,
    'train_batch_size': 1,
    'gradient_accumulation_steps': 4,
    'seed': 22276,
    'repeats': 100,
    'validation_prompt': '',
    'validation_steps': 1,
    'num_vectors': 2,
    'output_dir': os.path.join(root_dir, "sd-concept-output"),
    'learning_rate': 5e-04,
    'name_of_your_concept': "",
    'save_concept': True,
    'where_to_save_concept': "Public Library",
    'max_size': 512,
    'image_path': '',
    'readme_description': '',
    'use_SDXL': False,
    'urls': [],
}
def buildTextualInversion(page):
    global prefs, textualinversion_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            textualinversion_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_textualinversion_output(o):
        page.textualinversion_output.controls.append(o)
        page.textualinversion_output.update()
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.textualinversion_output.controls = []
        page.textualinversion_output.update()
        clear_button.visible = False
        clear_button.update()
    def ti_help(e):
        def close_ti_dlg(e):
          nonlocal ti_help_dlg
          ti_help_dlg.open = False
          page.update()
        ti_help_dlg = AlertDialog(title=Text("💁   Help with Textual-Inversion"), content=Column([
            Text(""),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("😪  I'll figure it out... ", on_click=close_ti_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(ti_help_dlg)
        ti_help_dlg.open = True
        page.update()
    def delete_image(e):
        f = e.control.data
        if os.path.isfile(f):
          os.remove(f)
          for i, fl in enumerate(page.ti_file_list.controls):
            if fl.title.value == f:
              del page.ti_file_list.controls[i]
              page.ti_file_list.update()
              continue
    def delete_all_images(e):
        for fl in page.ti_file_list.controls:
          f = fl.title.value
          if os.path.isfile(f):
            os.remove(f)
        page.ti_file_list.controls.clear()
        page.ti_file_list.update()
    def add_file(fpath, update=True):
        page.ti_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[#TODO: View Image
              PopupMenuItem(icon=icons.DELETE, text="Delete Image", on_click=delete_image, data=fpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All", on_click=delete_all_images, data=fpath),
          ])))
        if update: page.ti_file_list.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    save_dir = os.path.join(root_dir, 'my_concept')
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
          save_file(e.file_name)
    def save_file(file_name):
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
          fpath = os.path.join(save_dir, file_name)
        else:
          fname = file_name
          fpath = os.path.join(save_dir, file_name.rpartition(slash)[2])
        original_img = PILImage.open(fname)
        width, height = original_img.size
        width, height = scale_dimensions(width, height, textualinversion_prefs['max_size'])
        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
        original_img.save(fpath)
        if page.web: os.remove(fname)
        #shutil.move(fname, fpath)
        add_file(fpath)
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Image File to Enlarge")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def add_image(e):
        save_dir = os.path.join(root_dir, 'my_concept')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        if image_path.value.startswith('http'):
          import requests
          from io import BytesIO
          response = requests.get(image_path.value)
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          concept_image = PILImage.open(BytesIO(response.content)).convert("RGB")
          width, height = concept_image.size
          width, height = scale_dimensions(width, height, textualinversion_prefs['max_size'])
          concept_image = concept_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          concept_image.save(fpath)
          add_file(fpath)
        elif os.path.isfile(image_path.value):
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(image_path.value)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, textualinversion_prefs['max_size'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          #shutil.copy(image_path.value, fpath)
          add_file(fpath)
        elif os.path.isdir(image_path.value):
          for f in os.listdir(image_path.value):
            file_path = os.path.join(image_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              fpath = os.path.join(save_dir, f)
              original_img = PILImage.open(file_path)
              width, height = original_img.size
              width, height = scale_dimensions(width, height, textualinversion_prefs['max_size'])
              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              original_img.save(fpath)
              #shutil.copy(file_path, fpath)
              add_file(fpath)
        else:
          if bool(image_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        image_path.value = ""
        image_path.update()
    def load_images():
        if os.path.exists(save_dir):
          for f in os.listdir(save_dir):
            existing = os.path.join(save_dir, f)
            if os.path.isdir(existing): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              add_file(existing, update=False)
    what_to_teach = Dropdown(label="What to Teach", width=250, options=[dropdown.Option("object"), dropdown.Option("style")], value=textualinversion_prefs['what_to_teach'], on_change=lambda e: changed(e, 'what_to_teach'))
    placeholder_token = TextField(label="Placeholder <Token> Keyword", value=textualinversion_prefs['placeholder_token'], on_change=lambda e:changed(e,'placeholder_token'))
    initializer_token = TextField(label="Initializer Token Category Summary", value=textualinversion_prefs['initializer_token'], on_change=lambda e:changed(e,'initializer_token'))
    validation_prompt = TextField(label="Validation <Token> Prompt", value=textualinversion_prefs['validation_prompt'], on_change=lambda e:changed(e,'validation_prompt'))
    gradient_accumulation_steps = TextField(label="Gradient Accumulation Steps", value=textualinversion_prefs['gradient_accumulation_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'gradient_accumulation_steps', ptype='int'), width = 160)
    scale_lr = Checkbox(label="Scale Learning Rate", tooltip="", value=textualinversion_prefs['scale_lr'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'scale_lr'))
    validation_steps = TextField(label="Validation Steps", value=textualinversion_prefs['validation_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'validation_steps', ptype='int'), width = 145)
    num_vectors = TextField(label="Number of Vectors", value=textualinversion_prefs['num_vectors'], tooltip="How many textual inversion vectors shall be used to learn the concept.", keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_vectors', ptype='int'), width = 145)
    repeats = TextField(label="Repeats", value=textualinversion_prefs['repeats'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'repeats', ptype='int'), width = 160)
    train_batch_size = TextField(label="Train Batch Size", value=textualinversion_prefs['train_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'train_batch_size', ptype='float'), width = 160)
    max_train_steps = TextField(label="Max Training Steps", value=textualinversion_prefs['max_train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_train_steps', ptype='int'), width = 160)
    learning_rate = TextField(label="Learning Rate", value=textualinversion_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160)
    seed = TextField(label="Seed", value=textualinversion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    use_SDXL = Switcher(label="Train with SDXL", value=textualinversion_prefs['use_SDXL'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e: changed(e, 'use_SDXL'))
    save_concept = Checkbox(label="Save Concept    ", tooltip="", value=textualinversion_prefs['save_concept'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_concept'))
    where_to_save_concept = Dropdown(label="Where to Save Concept", width=250, options=[dropdown.Option("Public Library"), dropdown.Option("Privately to my Profile")], value=textualinversion_prefs['where_to_save_concept'], on_change=lambda e: changed(e, 'where_to_save_concept'))
    output_dir = TextField(label="Prior Preservation Class Folder", value=textualinversion_prefs['output_dir'], on_change=lambda e:changed(e,'output_dir'))
    name_of_your_concept = TextField(label="Name of your Concept", value=textualinversion_prefs['name_of_your_concept'], on_change=lambda e:changed(e,'name_of_your_concept'))
    readme_description = TextField(label="Extra README Description", value=textualinversion_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=textualinversion_prefs, key='max_size')
    image_path = TextField(label="Image File or Folder Path or URL to Train", value=textualinversion_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    add_image_button = ElevatedButton(content=Text("Add File or Folder"), on_click=add_image)
    page.ti_file_list = Column([], tight=True, spacing=0)
    load_images()
    #seed = TextField(label="Seed", value=textualinversion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    #lambda_entropy = TextField(label="Lambda Entropy", value=dreamfustextualinversion_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    #max_steps = TextField(label="Max Steps", value=textualinversion_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    page.textualinversion_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.textualinversion_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("😶‍🌫️  Create Cusom Textual-Inversion Concept Model", "Provide a collection of images to conceptualize. Warning: May take over an hour to run the training...", actions=[save_default(textualinversion_prefs, ['image_path', 'urls']), IconButton(icon=icons.HELP, tooltip="Help with Textual-Inversion Settings", on_click=ti_help)]),
        Row([what_to_teach, initializer_token]),
        Row([placeholder_token, name_of_your_concept]),
        use_SDXL,
        Row([validation_prompt, validation_steps, num_vectors]),
        scale_lr,
        Row([gradient_accumulation_steps, repeats, train_batch_size]),
        Row([max_train_steps, learning_rate, seed]),
        Row([save_concept, where_to_save_concept]),
        readme_description,
        #Row([output_dir]),
        max_row,
        Row([image_path, add_image_button]),
        page.ti_file_list,
        ElevatedButton(content=Text("👨‍🎨️  Run Textual-Inversion", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_textualinversion(page)),
        page.textualinversion_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

LoRA_dreambooth_prefs = {
    'instance_prompt': '', #The prompt with identifier specifying the instance
    'class_prompt': '',
    'prior_preservation': False, #Flag to add prior preservation loss.
    'num_class_images': 100, #Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.
    'sample_batch_size': 4, #Batch size (per device) for sampling images.
    'train_batch_size': 1, #"Batch size (per device) for the training dataloader.
    'gradient_accumulation_steps': 1, #Number of updates steps to accumulate before performing a backward/update pass.
    'gradient_checkpointing': True, #Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.
    'checkpointing_steps': 100,#Number of training steps between saving model checkpoints
    'lr_scheduler': 'constant', #["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"]
    'lr_warmup_steps': 500, #Number of steps for the warmup in the lr scheduler.
    'lr_num_cycles': 1, #Number of hard resets of the lr in cosine_with_restarts scheduler.
    'lr_power': 1, #Power factor of the polynomial scheduler.
    'prior_loss_weight': 1.0, #The weight of prior preservation loss.
    'class_data_dir': os.path.join(root_dir, "class_images"),
    'learning_rate': 1e-4, #Initial learning rate (after the potential warmup period) to use.
    'scale_lr': False, #Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.
    'max_train_steps': 500, #Total number of training steps to perform.  If provided, overrides num_train_epochs.
    'seed': 0,
    'use_SDXL': False,
    'name_of_your_model': '',
    'save_model': True,
    'where_to_save_model': 'Public HuggingFace',
    'resolution': 512,
    'image_path': '',
    'readme_description': '',
    'urls': [],
}

    #--lr_num_cycles=1 --lr_power=1 --prior_loss_weight=1.0 --sample_batch_size=4 --num_class_images=100
LoRA_prefs = {
    'instance_prompt': '', #The prompt with identifier specifying the instance
    'class_prompt': '',
    'prior_preservation': False, #Flag to add prior preservation loss.
    #'num_class_images': 100, #Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.
    #'sample_batch_size': 4, #Batch size (per device) for sampling images.
    'train_batch_size': 1, #"Batch size (per device) for the training dataloader.
    'gradient_accumulation_steps': 1, #Number of updates steps to accumulate before performing a backward/update pass.
    'gradient_checkpointing': True, #Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.
    'checkpointing_steps': 100,#Number of training steps between saving model checkpoints
    'resume_from_checkpoint': '', #Whether training should be resumed from a previous checkpoint. Use a path saved by" `--checkpointing_steps`, or `latest` to automatically select the last available checkpoint.
    'lr_scheduler': 'constant', #["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"]
    'lr_warmup_steps': 500, #Number of steps for the warmup in the lr scheduler.
    #'lr_num_cycles': 1, #Number of hard resets of the lr in cosine_with_restarts scheduler.
    #'lr_power': 1, #Power factor of the polynomial scheduler.
    #'prior_loss_weight': 1.0, #The weight of prior preservation loss.
    'class_data_dir': os.path.join(root_dir, "class_images"),
    'learning_rate': 1e-4, #Initial learning rate (after the potential warmup period) to use.
    'scale_lr': False, #Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.
    'max_train_steps': 500, #Total number of training steps to perform.  If provided, overrides num_train_epochs.
    'seed': 0,
    'validation_prompt': '', #A prompt that is sampled during training for inference.
    'num_validation_images': 4, #Number of images that should be generated during validation with `validation_prompt`.
    'validation_epochs': 1, #Run fine-tuning validation every X epochs. The validation process consists of running the prompt
    'use_SDXL': False,
    'name_of_your_model': '',
    'save_model': True,
    'where_to_save_model': 'Public HuggingFace',
    'resolution': 512,
    'image_path': '',
    'readme_description': '',
    'urls': [],
    'dream_training': False,
    'dream_detail_preservation': 1.0,
}

def buildLoRA_Dreambooth(page):
    global prefs, LoRA_dreambooth_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            LoRA_dreambooth_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_LoRA_dreambooth_output(o):
        page.LoRA_dreambooth_output.controls.append(o)
        page.LoRA_dreambooth_output.update()
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.LoRA_dreambooth_output.controls = []
        page.LoRA_dreambooth_output.update()
        clear_button.visible = False
        clear_button.update()
    def lora_dreambooth_help(e):
        def close_lora_dreambooth_dlg(e):
          nonlocal lora_dreambooth_help_dlg
          lora_dreambooth_help_dlg.open = False
          page.update()
        lora_dreambooth_help_dlg = AlertDialog(title=Text("💁   Help with LoRA DreamBooth"), content=Column([
            Text("First thing is to collect all your own images that you want to teach it to dream.  Feed it at least 5 square pictures of the object or style to learn, and it'll save your Custom Model Checkpoint."),
            Markdown("""Low-Rank Adaption of Large Language Models was first introduced by Microsoft in [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) by *Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen*
In a nutshell, LoRA allows to adapt pretrained models by adding pairs of rank-decomposition matrices to existing weights and **only** training those newly added weights. This has a couple of advantages:
- Previous pretrained weights are kept frozen so that the model is not prone to [catastrophic forgetting](https://www.pnas.org/doi/10.1073/pnas.1611835114)
- Rank-decomposition matrices have significantly fewer parameters than the original model, which means that trained LoRA weights are easily portable.
- LoRA attention layers allow to control to which extent the model is adapted torwards new training images via a `scale` parameter.""", on_tap_link=lambda e: e.page.launch_url(e.data)),
            Text("Fine-tune your perameters, but be aware that the training process takes a long time to run, so careful with the settings if you don't have the patience or processor. Dream at your own risk."),
          ], scroll=ScrollMode.AUTO), actions=[TextButton(emojize(':sun_with_face:') + "  Neato... ", on_click=close_lora_dreambooth_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(lora_dreambooth_help_dlg)
        lora_dreambooth_help_dlg.open = True
        page.update()
    def delete_image(e):
        f = e.control.data
        if os.path.isfile(f):
          os.remove(f)
          for i, fl in enumerate(page.lora_dreambooth_file_list.controls):
            if fl.title.value == f:
              del page.lora_dreambooth_file_list.controls[i]
              page.lora_dreambooth_file_list.update()
              continue
    def delete_all_images(e):
        for fl in page.lora_dreambooth_file_list.controls:
          f = fl.title.value
          if os.path.isfile(f):
            os.remove(f)
        page.lora_dreambooth_file_list.controls.clear()
        page.lora_dreambooth_file_list.update()
    def image_details(e):
        img = e.control.data
        alert_msg(e.page, "Image Details", content=Image(src=img), sound=False)
    def add_file(fpath, update=True):
        page.lora_dreambooth_file_list.controls.append(ListTile(title=Text(fpath), dense=False, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[#TODO: View Image
              PopupMenuItem(icon=icons.INFO, text="Image Details", on_click=image_details, data=fpath),
              PopupMenuItem(icon=icons.DELETE, text="Delete Image", on_click=delete_image, data=fpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All", on_click=delete_all_images, data=fpath),
          ]), data=fpath, on_click=image_details))
        if update: page.lora_dreambooth_file_list.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    save_dir = os.path.join(root_dir, 'my_model')
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
          save_file(e.file_name)
    def save_file(file_name):
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
          fpath = os.path.join(save_dir, file_name)
        else:
          fname = file_name
          fpath = os.path.join(save_dir, file_name.rpartition(slash)[2])
        original_img = PILImage.open(fname)
        width, height = original_img.size
        width, height = scale_dimensions(width, height, LoRA_dreambooth_prefs['resolution'])
        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
        original_img.save(fpath)
        if page.web: os.remove(fname)
        #shutil.move(fname, fpath)
        add_file(fpath)
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Image File to Enlarge")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def add_image(e):
        save_dir = os.path.join(root_dir, 'my_model')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        if image_path.value.startswith('http'):
          import requests
          from io import BytesIO
          response = requests.get(image_path.value)
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          model_image = PILImage.open(BytesIO(response.content)).convert("RGB")
          width, height = model_image.size
          width, height = scale_dimensions(width, height, LoRA_dreambooth_prefs['resolution'])
          model_image = model_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          model_image.save(fpath)
          add_file(fpath)
        elif os.path.isfile(image_path.value):
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(image_path.value)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, LoRA_dreambooth_prefs['resolution'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          #shutil.copy(image_path.value, fpath)
          add_file(fpath)
        elif os.path.isdir(image_path.value):
          for f in os.listdir(image_path.value):
            file_path = os.path.join(image_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              fpath = os.path.join(save_dir, f)
              original_img = PILImage.open(file_path)
              width, height = original_img.size
              width, height = scale_dimensions(width, height, LoRA_dreambooth_prefs['resolution'])
              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              original_img.save(fpath)
              #shutil.copy(file_path, fpath)
              add_file(fpath)
        else:
          if bool(image_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        image_path.value = ""
        image_path.update()
    def load_images():
        if os.path.exists(save_dir):
          for f in os.listdir(save_dir):
            existing = os.path.join(save_dir, f)
            if os.path.isdir(existing): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              add_file(existing, update=False)
    def toggle_save(e):
        changed(e, 'save_model')
        where_to_save_model.visible = LoRA_dreambooth_prefs['save_model']
        where_to_save_model.update()
        readme_description.visible = LoRA_dreambooth_prefs['save_model']
        readme_description.update()
    instance_prompt = Container(content=Tooltip(message="The prompt with identifier specifying the instance", content=TextField(label="Instance Prompt Token Text", value=LoRA_dreambooth_prefs['instance_prompt'], on_change=lambda e:changed(e,'instance_prompt'))), col={'md':9})
    name_of_your_model = TextField(label="Name of your Model", value=LoRA_dreambooth_prefs['name_of_your_model'], on_change=lambda e:changed(e,'name_of_your_model'), col={'md':3})
    class_prompt = TextField(label="Class Prompt", value=LoRA_dreambooth_prefs['class_prompt'], on_change=lambda e:changed(e,'class_prompt'))
    lr_scheduler = Dropdown(label="Learning Rate Scheduler", width=250, options=[dropdown.Option("constant"), dropdown.Option("constant_with_warmup"), dropdown.Option("linear"), dropdown.Option("cosine"), dropdown.Option("cosine_with_restarts"), dropdown.Option("polynomial")], value=LoRA_dreambooth_prefs['lr_scheduler'], on_change=lambda e: changed(e, 'lr_scheduler'))
    prior_preservation = Checkbox(label="Prior Preservation", tooltip="If you'd like class of the model (e.g.: toy, dog, painting) is guaranteed to be preserved. This increases the quality and helps with generalization at the cost of training time", value=LoRA_dreambooth_prefs['prior_preservation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'prior_preservation'))
    gradient_checkpointing = Checkbox(label="Gradient Checkpointing   ", tooltip="Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.", value=LoRA_dreambooth_prefs['gradient_checkpointing'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'gradient_checkpointing'))
    num_class_images = Tooltip(message="Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.", content=TextField(label="Number of Class Images", value=LoRA_dreambooth_prefs['num_class_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_class_images', ptype='int'), width = 160))
    sample_batch_size = Tooltip(message="Batch size (per device) for sampling images.", content=TextField(label="Sample Batch Size", value=LoRA_dreambooth_prefs['sample_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_batch_size', ptype='int'), width = 160))
    train_batch_size = Tooltip(message="Batch size (per device) for the training dataloader.", content=TextField(label="Train Batch Size", value=LoRA_dreambooth_prefs['train_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'train_batch_size', ptype='int'), width = 160))
    prior_loss_weight = Tooltip(message="The weight of prior preservation loss.", content=TextField(label="Prior Loss Weight", value=LoRA_dreambooth_prefs['prior_loss_weight'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'prior_loss_weight', ptype='float'), width = 160))
    max_train_steps = Tooltip(message="Total number of training steps to perform.  If provided, overrides num_train_epochs.", content=TextField(label="Max Training Steps", value=LoRA_dreambooth_prefs['max_train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_train_steps', ptype='int'), width = 160))
    gradient_accumulation_steps = Tooltip(message="Number of updates steps to accumulate before performing a backward/update pass.", content=TextField(label="Gradient Accumulation Steps", value=LoRA_dreambooth_prefs['gradient_accumulation_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'gradient_accumulation_steps', ptype='int'), width = 160))
    learning_rate = Tooltip(message="Initial learning rate (after the potential warmup period) to use.", content=TextField(label="Learning Rate", value=LoRA_dreambooth_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160))
    lr_warmup_steps = Tooltip(message="Number of steps for the warmup in the lr scheduler.", content=TextField(label="LR Warmup Steps", value=LoRA_dreambooth_prefs['lr_warmup_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_warmup_steps', ptype='int'), width = 160))
    lr_num_cycles = Tooltip(message="Number of hard resets of the lr in cosine_with_restarts scheduler.", content=TextField(label="LR Number of Cycles", value=LoRA_dreambooth_prefs['lr_num_cycles'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_num_cycles', ptype='int'), width = 160))
    lr_power = Tooltip(message="Power factor of the polynomial scheduler.", content=TextField(label="LR Power", value=LoRA_dreambooth_prefs['lr_power'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_power', ptype='int'), width = 160))
    seed = Tooltip(message="0 or -1 for Random. Pick any number.", content=TextField(label="Seed", value=LoRA_dreambooth_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160))
    #save_model = Checkbox(label="Save Model to HuggingFace   ", tooltip="", value=LoRA_dreambooth_prefs['save_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_model'))
    use_SDXL = Switcher(label="Train with SDXL", value=LoRA_dreambooth_prefs['use_SDXL'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e: changed(e, 'use_SDXL'))
    save_model = Switcher(label="Save Model to HuggingFace", value=LoRA_dreambooth_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save, tooltip="Requires WRITE access on API Key to Upload Checkpoint")
    where_to_save_model = Dropdown(label="Where to Save Model", width=250, options=[dropdown.Option("Public HuggingFace"), dropdown.Option("Private HuggingFace")], value=LoRA_dreambooth_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))
    #class_data_dir = TextField(label="Prior Preservation Class Folder", value=LoRA_dreambooth_prefs['class_data_dir'], on_change=lambda e:changed(e,'class_data_dir'))
    readme_description = TextField(label="Extra README Description", value=LoRA_dreambooth_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=6, multiple=64, suffix="px", pref=LoRA_dreambooth_prefs, key='resolution')
    image_path = TextField(label="Image File or Folder Path or URL to Train", value=LoRA_dreambooth_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    add_image_button = ElevatedButton(content=Text("Add File or Folder"), on_click=add_image)
    page.lora_dreambooth_file_list = Column([], tight=True, spacing=0)
    load_images()
    where_to_save_model.visible = LoRA_dreambooth_prefs['save_model']
    readme_description.visible = LoRA_dreambooth_prefs['save_model']
    #lambda_entropy = TextField(label="Lambda Entropy", value=dreamfusLoRA_dreambooth_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    #max_steps = TextField(label="Max Steps", value=LoRA_dreambooth_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    page.LoRA_dreambooth_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.LoRA_dreambooth_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🌇  Training with Low-Rank Adaptation of Large Language Models (LoRA DreamBooth)", "Provide a collection of images to train. Adds on to the currently loaded Model Checkpoint...", actions=[save_default(LoRA_dreambooth_prefs, ['image_path', 'urls']), IconButton(icon=icons.HELP, tooltip="Help with LoRA DreamBooth Settings", on_click=lora_dreambooth_help)]),
        ResponsiveRow([instance_prompt, name_of_your_model]),
        use_SDXL,
        Row([num_class_images, sample_batch_size, train_batch_size, prior_loss_weight]),
        Row([prior_preservation, gradient_checkpointing, lr_scheduler]),
        Row([learning_rate, lr_warmup_steps, lr_num_cycles, lr_power]),
        Row([max_train_steps, gradient_accumulation_steps, seed]),
        Row([save_model, where_to_save_model]),
        readme_description,
        #Row([class_data_dir]),
        max_row,
        Row([image_path, add_image_button]),
        page.lora_dreambooth_file_list,
        Row([ElevatedButton(content=Text("🌄  Run LoRA DreamBooth", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_LoRA_dreambooth(page))]),
        page.LoRA_dreambooth_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c


def buildLoRA(page):
    global prefs, LoRA_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            LoRA_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_LoRA_output(o):
        page.LoRA_output.controls.append(o)
        page.LoRA_output.update()
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.LoRA_output.controls = []
        page.LoRA_output.update()
        clear_button.visible = False
        clear_button.update()
    def lora_help(e):
        def close_lora_dlg(e):
          nonlocal lora_help_dlg
          lora_help_dlg.open = False
          page.update()
        lora_help_dlg = AlertDialog(title=Text("💁   Help with LoRA DreamBooth"), content=Column([
            Text("First thing is to collect all your own images that you want to teach it to dream.  Feed it at least 5 square pictures of the object or style to learn, and it'll save your Custom Model Checkpoint."),
            Markdown("""Low-Rank Adaption of Large Language Models was first introduced by Microsoft in [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) by *Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen*
In a nutshell, LoRA allows to adapt pretrained models by adding pairs of rank-decomposition matrices to existing weights and **only** training those newly added weights. This has a couple of advantages:
- Previous pretrained weights are kept frozen so that the model is not prone to [catastrophic forgetting](https://www.pnas.org/doi/10.1073/pnas.1611835114)
- Rank-decomposition matrices have significantly fewer parameters than the original model, which means that trained LoRA weights are easily portable.
- LoRA attention layers allow to control to which extent the model is adapted torwards new training images via a `scale` parameter.""", on_tap_link=lambda e: e.page.launch_url(e.data)),
            Text("Fine-tune your perameters, but be aware that the training process takes a long time to run, so careful with the settings if you don't have the patience or processor. Dream at your own risk."),
          ], scroll=ScrollMode.AUTO), actions=[TextButton(emojize(':sun_with_face:') + "  Neato... ", on_click=close_lora_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(lora_help_dlg)
        lora_help_dlg.open = True
        page.update()
    def delete_image(e):
        f = e.control.data
        if os.path.isfile(f):
          os.remove(f)
          for i, fl in enumerate(page.lora_file_list.controls):
            if fl.title.value == f:
              del page.lora_file_list.controls[i]
              page.lora_file_list.update()
              continue
    def delete_all_images(e):
        for fl in page.lora_file_list.controls:
          f = fl.title.value
          if os.path.isfile(f):
            os.remove(f)
        page.lora_file_list.controls.clear()
        page.lora_file_list.update()
    def image_details(e):
        img = e.control.data
        #TODO: Get file size & resolution
        alert_msg(e.page, "Image Details", content=Column([Text(img), Img(src=asset_dir(img), gapless_playback=True)]), sound=False)
    def add_file(fpath, update=True):
        page.lora_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[#TODO: View Image
              PopupMenuItem(icon=icons.INFO, text="Image Details", on_click=image_details, data=fpath),
              PopupMenuItem(icon=icons.DELETE, text="Delete Image", on_click=delete_image, data=fpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All", on_click=delete_all_images, data=fpath),
          ]), subtitle=TextField(label="Caption Image Description", height=55, filled=True, content_padding=padding.only(top=12, left=12)), data=fpath, on_click=image_details))
        if update: page.lora_file_list.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    save_dir = os.path.join(root_dir, 'my_model')
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
          save_file(e.file_name)
    def save_file(file_name):
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
          fpath = os.path.join(save_dir, file_name)
        else:
          fname = file_name
          fpath = os.path.join(save_dir, file_name.rpartition(slash)[2])
        original_img = PILImage.open(fname)
        width, height = original_img.size
        width, height = scale_dimensions(width, height, LoRA_prefs['resolution'])
        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
        original_img.save(fpath)
        if page.web: os.remove(fname)
        #shutil.move(fname, fpath)
        add_file(fpath)
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Image File to Enlarge")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def add_image(e):
        save_dir = os.path.join(root_dir, 'my_model')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        if image_path.value.startswith('http'):
          import requests
          from io import BytesIO
          response = requests.get(image_path.value)
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          model_image = PILImage.open(BytesIO(response.content)).convert("RGB")
          width, height = model_image.size
          width, height = scale_dimensions(width, height, LoRA_prefs['resolution'])
          model_image = model_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          model_image.save(fpath)
          add_file(fpath)
        elif os.path.isfile(image_path.value):
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(image_path.value)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, LoRA_prefs['resolution'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          #shutil.copy(image_path.value, fpath)
          add_file(fpath)
        elif os.path.isdir(image_path.value):
          for f in os.listdir(image_path.value):
            file_path = os.path.join(image_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              fpath = os.path.join(save_dir, f)
              original_img = PILImage.open(file_path)
              width, height = original_img.size
              width, height = scale_dimensions(width, height, LoRA_prefs['resolution'])
              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              original_img.save(fpath)
              #shutil.copy(file_path, fpath)
              add_file(fpath)
        else:
          if bool(image_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        image_path.value = ""
        image_path.update()
    def load_images():
        if os.path.exists(save_dir):
          for f in os.listdir(save_dir):
            existing = os.path.join(save_dir, f)
            if os.path.isdir(existing): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              add_file(existing, update=False)
    def toggle_save(e):
        changed(e, 'save_model')
        where_to_save_model.visible = LoRA_prefs['save_model']
        where_to_save_model.update()
        readme_description.visible = LoRA_prefs['save_model']
        readme_description.update()
    def toggle_dream(e):
        changed(e, 'dream_training')
        #dream_detail_preservation.visible = LoRA_prefs['dream_training']
        #dream_detail_preservation.update()
    validation_prompt = Container(content=Tooltip(message="A prompt that is sampled during training for inference.", content=TextField(label="Validation Prompt Text", value=LoRA_prefs['validation_prompt'], on_change=lambda e:changed(e,'validation_prompt'))), col={'md':9})
    name_of_your_model = TextField(label="Name of your Model", value=LoRA_prefs['name_of_your_model'], on_change=lambda e:changed(e,'name_of_your_model'), col={'md':3})
    #class_prompt = TextField(label="Class Prompt", value=LoRA_prefs['class_prompt'], on_change=lambda e:changed(e,'class_prompt'))
    #'num_validation_images': 4, #Number of images that should be generated during validation with `validation_prompt`.
    #'validation_epochs': 1, #Run fine-tuning validation every X epochs. The validation process consists of running the prompt
    num_validation_images = Tooltip(message="Number of images that should be generated during validation with `validation_prompt`", content=TextField(label="# of Validation Images", value=LoRA_prefs['num_validation_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_validation_images', ptype='int'), width = 160))
    validation_epochs = Tooltip(message="Run fine-tuning validation every X epochs. The validation process consists of running the prompt", content=TextField(label="Validation Epochs", value=LoRA_prefs['validation_epochs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'validation_epochs', ptype='int'), width = 160))
    lr_scheduler = Dropdown(label="Learning Rate Scheduler", width=250, options=[dropdown.Option("constant"), dropdown.Option("constant_with_warmup"), dropdown.Option("linear"), dropdown.Option("cosine"), dropdown.Option("cosine_with_restarts"), dropdown.Option("polynomial")], value=LoRA_prefs['lr_scheduler'], on_change=lambda e: changed(e, 'lr_scheduler'))
    prior_preservation = Checkbox(label="Prior Preservation", tooltip="If you'd like class of the model (e.g.: toy, dog, painting) is guaranteed to be preserved. This increases the quality and helps with generalization at the cost of training time", value=LoRA_prefs['prior_preservation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'prior_preservation'))
    gradient_checkpointing = Checkbox(label="Gradient Checkpointing   ", tooltip="Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.", value=LoRA_prefs['gradient_checkpointing'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'gradient_checkpointing'))
    #dream_training = Switcher(label="DREAM Training   ", value=LoRA_prefs['dream_training'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_dream, tooltip="Diffusion Rectification and Estimation-Adaptive Models. Trains epsilon (noise) prediction models to make training more efficient and accurate at the expense of doing an extra forward pass.")
    #dream_detail_preservation = SliderRow(label="DREAM Detail Preservation", min=0, max=2, divisions=40, round=2, pref=LoRA_prefs, key='dream_detail_preservation', tooltip="Controls the Dream detail preservation variable p.", visible=LoRA_prefs['dream_training'])
    #num_class_images = Tooltip(message="Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.", content=TextField(label="Number of Class Images", value=LoRA_prefs['num_class_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_class_images', ptype='int'), width = 160))
    #sample_batch_size = Tooltip(message="Batch size (per device) for sampling images.", content=TextField(label="Sample Batch Size", value=LoRA_prefs['sample_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_batch_size', ptype='int'), width = 160))
    train_batch_size = Tooltip(message="Batch size (per device) for the training dataloader.", content=TextField(label="Train Batch Size", value=LoRA_prefs['train_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'train_batch_size', ptype='int'), width = 160))
    #prior_loss_weight = Tooltip(message="The weight of prior preservation loss.", content=TextField(label="Prior Loss Weight", value=LoRA_prefs['prior_loss_weight'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'prior_loss_weight', ptype='float'), width = 160))
    max_train_steps = Tooltip(message="Total number of training steps to perform.  If provided, overrides num_train_epochs.", content=TextField(label="Max Training Steps", value=LoRA_prefs['max_train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_train_steps', ptype='int'), width = 160))
    gradient_accumulation_steps = Tooltip(message="Number of updates steps to accumulate before performing a backward/update pass.", content=TextField(label="Gradient Accumulation Steps", value=LoRA_prefs['gradient_accumulation_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'gradient_accumulation_steps', ptype='int'), width = 160))
    learning_rate = Tooltip(message="Initial learning rate (after the potential warmup period) to use.", content=TextField(label="Learning Rate", value=LoRA_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160))
    lr_warmup_steps = Tooltip(message="Number of steps for the warmup in the lr scheduler.", content=TextField(label="LR Warmup Steps", value=LoRA_prefs['lr_warmup_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_warmup_steps', ptype='int'), width = 160))
    #lr_num_cycles = Tooltip(message="Number of hard resets of the lr in cosine_with_restarts scheduler.", content=TextField(label="LR Number of Cycles", value=LoRA_prefs['lr_num_cycles'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_num_cycles', ptype='int'), width = 160))
    #lr_power = Tooltip(message="Power factor of the polynomial scheduler.", content=TextField(label="LR Power", value=LoRA_prefs['lr_power'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_power', ptype='int'), width = 160))
    seed = Tooltip(message="0 or -1 for Random. Pick any number.", content=TextField(label="Seed", value=LoRA_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160))
    #save_model = Checkbox(label="Save Model to HuggingFace   ", tooltip="", value=LoRA_prefs['save_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_model'))
    save_model = Tooltip(message="Requires WRITE access on API Key to Upload Checkpoint", content=Switcher(label="Save Model to HuggingFace    ", value=LoRA_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))
    where_to_save_model = Dropdown(label="Where to Save Model", width=250, options=[dropdown.Option("Public HuggingFace"), dropdown.Option("Private HuggingFace")], value=LoRA_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))
    #class_data_dir = TextField(label="Prior Preservation Class Folder", value=LoRA_prefs['class_data_dir'], on_change=lambda e:changed(e,'class_data_dir'))
    readme_description = TextField(label="Extra README Description", value=LoRA_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=12, multiple=64, suffix="px", pref=LoRA_prefs, key='resolution')
    image_path = TextField(label="Image File or Folder Path or URL to Train", value=LoRA_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    add_image_button = ElevatedButton(content=Text("Add File or Folder"), on_click=add_image)
    page.lora_file_list = Column([], tight=True, spacing=0)
    load_images()
    where_to_save_model.visible = LoRA_prefs['save_model']
    readme_description.visible = LoRA_prefs['save_model']
    #lambda_entropy = TextField(label="Lambda Entropy", value=dreamfusLoRA_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    #max_steps = TextField(label="Max Steps", value=LoRA_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    page.LoRA_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.LoRA_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🌫️   Training Text-to-Image Low-Rank Adaptation of Large Language Models (LoRA)", "Provide a collection of images to train. Smaller sized. Adds on to the currently loaded Model Checkpoint...", actions=[save_default(LoRA_prefs, ['image_path', 'urls']), IconButton(icon=icons.HELP, tooltip="Help with LoRA DreamBooth Settings", on_click=lora_help)]),
        ResponsiveRow([validation_prompt, name_of_your_model]),
        Row([num_validation_images, validation_epochs, train_batch_size]),
        Row([prior_preservation, gradient_checkpointing]),
        Row([learning_rate, lr_warmup_steps, lr_scheduler]),
        Row([max_train_steps, gradient_accumulation_steps, seed]),
        #Row([dream_training, dream_detail_preservation]),
        Row([save_model, where_to_save_model]),
        readme_description,
        #Row([class_data_dir]),
        max_row,
        Row([image_path, add_image_button]),
        page.lora_file_list,
        Row([ElevatedButton(content=Text("🏄  Run LoRA Training", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_LoRA(page))]),
        page.LoRA_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

converter_prefs = {
    'from_format': 'ckpt',
    'to_format': 'pytorch',
    'model_path': '',
    'model_name': '',
    'base_model': '',
    'model_type': 'SD v1.x text2image',
    'scheduler_type': 'pndm',
    'half_percision': True,
    'save_model': False,
    'where_to_save_model': "Public HuggingFace",
    'readme_description': '',
    'load_custom_model': True,
}

def buildConverter(page):
    global prefs, converter_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            converter_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_converter_output(o):
        page.converter_output.controls.append(o)
        page.converter_output.update()
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.converter_output.controls = []
        page.converter_output.update()
        clear_button.visible = False
        clear_button.update()
    def converter_help(e):
        def close_converter_dlg(e):
          nonlocal converter_help_dlg
          converter_help_dlg.open = False
          page.update()
        converter_help_dlg = AlertDialog(title=Text("💁   Help with Converters"), content=Column([
            Text("Because there have been so many competing formats for Stable Diffusion models, we here have standardized with HuggingFace Diffusers, which is great but doesn't support all the Checkpoint Model types that are out there in the wild.  This should allow you to take other peoples custom trained model files and convert it to the better Diffusers PyTorch format, and then it'll save your Custom Model Checkpoint to HuggingFace (free) to reuse and/or share."),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🎈  Handy... ", on_click=close_converter_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(converter_help_dlg)
        converter_help_dlg.open = True
        page.update()
    def toggle_save(e):
        changed(e, 'save_model')
        where_to_save_model.visible = converter_prefs['save_model']
        where_to_save_model.update()
        readme_description.visible = converter_prefs['save_model']
        readme_description.update()
    def change_from_format(e):
        changed(e, 'from_format')
        base_model_row.visible = converter_prefs['from_format'] == "lora_safetensors"
        base_model_row.update()

    from_format = Dropdown(label="From Format", width=250, options=[dropdown.Option("ckpt"), dropdown.Option("safetensors"), dropdown.Option("lora_safetensors"), dropdown.Option("controlnet"), dropdown.Option("KerasCV")], value=converter_prefs['from_format'], on_change=change_from_format, col={'lg':6})
    to_format = Dropdown(label="To Format", width=250, options=[dropdown.Option("pytorch"), dropdown.Option("safetensors"), dropdown.Option("dance_diffusion")], value=converter_prefs['to_format'], on_change=lambda e: changed(e, 'to_format'), col={'lg':6})
    #instance_prompt = Container(content=Tooltip(message="The prompt with identifier specifying the instance", content=TextField(label="Instance Prompt Token Text", value=converter_prefs['instance_prompt'], on_change=lambda e:changed(e,'instance_prompt'))), col={'md':9})
    from_model_path = TextField(label="Model Path to HuggingFace or .ckpt or .safetensors file", value=converter_prefs['model_path'], on_change=lambda e:changed(e,'model_path'), col={'md':6})
    from_model_name = TextField(label="Name of your Model", value=converter_prefs['model_name'], on_change=lambda e:changed(e,'model_name'), col={'md':6})
    model_type = Dropdown(label="Model Type", width=250, options=[dropdown.Option("SD v1.x text2image"), dropdown.Option("SD v2.x text2image")], value=converter_prefs['model_type'], on_change=lambda e: changed(e, 'model_type'), col={'lg':6})
    base_model = TextField(label="Base Model Path to HuggingFace Diffusers", value=converter_prefs['base_model'], on_change=lambda e:changed(e,'base_model'), col={'md':6})
    base_model_row = ResponsiveRow([base_model])
    base_model_row.visible = converter_prefs['from_format'] == "lora_safetensors"
    #sd_version = Dropdown(label="Stable Diffusion Version", width=250, options=[dropdown.Option("text2image")], value=converter_prefs['model_type'], on_change=lambda e: changed(e, 'model_type'), col={'lg':6})
    #class_prompt = TextField(label="Class Prompt", value=converter_prefs['class_prompt'], on_change=lambda e:changed(e,'class_prompt'))
    scheduler_type = Dropdown(label="Original Scheduler Mode", hint_text="Hopefuly you know what Scheduler/Sampler they used in training", width=200,
            options=[
                dropdown.Option("pndm"),
                dropdown.Option("lms"),
                dropdown.Option("ddim"),
                dropdown.Option("euler"),
                dropdown.Option("euler-ancestral"),
                dropdown.Option("dpm"),
            ], value=converter_prefs['scheduler_type'], autofocus=False, on_change=lambda e:changed(e, 'scheduler_type'), col={'lg':6},
        )
    #save_model = Checkbox(label="Save Model to HuggingFace   ", tooltip="", value=converter_prefs['save_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_model'))
    save_model = Tooltip(message="Requires WRITE access on API Key to Upload Checkpoint", content=Switcher(label="Save Model to HuggingFace    ", value=converter_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))
    half_percision = Tooltip(message="Save weights in half precision.", content=Switcher(label="Save Half Percision Float16    ", value=converter_prefs['half_percision'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e: changed(e, 'half_percision')))
    where_to_save_model = Dropdown(label="Where to Save Model", width=250, options=[dropdown.Option("Public HuggingFace"), dropdown.Option("Private HuggingFace")], value=converter_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))
    #class_data_dir = TextField(label="Prior Preservation Class Folder", value=converter_prefs['class_data_dir'], on_change=lambda e:changed(e,'class_data_dir'))
    readme_description = TextField(label="Extra README Description", value=converter_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))
    load_custom_model = Checkbox(label="Load Custom Model", tooltip="After conversion is done, will put it in your Custom Model setting, ready to test out", value=converter_prefs['load_custom_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'load_custom_model'))
    #resolution = Slider(min=256, max=1024, divisions=6, label="{value}px", value=float(converter_prefs['resolution']), expand=True, on_change=lambda e:changed(e,'resolution', ptype='int'))
    #max_row = Row([Text("Max Resolution Size: "), resolution])
    #image_path = TextField(label="Image File or Folder Path or URL to Train", value=converter_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    #add_image_button = ElevatedButton(content=Text("Add File or Folder"), on_click=add_image)
    #page.converter_file_list = Column([], tight=True, spacing=0)
    #load_images()
    where_to_save_model.visible = converter_prefs['save_model']
    readme_description.visible = converter_prefs['save_model']
    #lambda_entropy = TextField(label="Lambda Entropy", value=dreamfusconverter_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    #max_steps = TextField(label="Max Steps", value=converter_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    page.converter_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.converter_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🔀  Model Converter Tool", "Lets you Convert Format of Model Checkpoints to work with Diffusers...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Model Converters Settings", on_click=converter_help)]),
        ResponsiveRow([from_format, to_format]),
        ResponsiveRow([from_model_path, from_model_name]),
        base_model_row,
        ResponsiveRow([model_type, scheduler_type]),
        half_percision,
        Row([save_model, where_to_save_model]),
        readme_description,
        load_custom_model,
        #max_row,
        #Row([image_path, add_image_button]),
        #page.converter_file_list,
        Row([ElevatedButton(content=Text("〽️  Run Model Converter", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_converter(page))]),
        page.converter_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

checkpoint_merger_prefs = {
    'pretrained_model': '',
    'selected_model': 'Stable Diffusion v1.5',
    'pretrained_models': [], #A list of valid pretrained model names in the HuggingFace hub or paths to locally stored models in the HuggingFace format.
    'alpha': 0.5, #The interpolation parameter. Ranges from 0 to 1.  It affects the ratio in which the checkpoints are merged. A 0.8 alpha would mean that the first model checkpoints would affect the final result far less than an alpha of 0.2
    'interp': 'weighted_sum', #The interpolation method to use for the merging. Supports "sigmoid", "inv_sigmoid", "add_difference" and None. Passing None uses the default interpolation which is weighted sum interpolation. For merging three checkpoints, only "add_difference" is supported.
    'force': False, #Whether to ignore mismatch in model_config.json for the current models. Defaults to False.
    'validation_prompt': '',
    'name_of_your_model': '',
    'save_model': True,
    'where_to_save_model': 'Public HuggingFace',
    'readme_description': '',
}

def buildCheckpointMerger(page):
    global prefs, checkpoint_merger_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            checkpoint_merger_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def checkpoint_merger_help(e):
        def close_checkpoint_merger_dlg(e):
          nonlocal checkpoint_merger_help_dlg
          checkpoint_merger_help_dlg.open = False
          page.update()
        checkpoint_merger_help_dlg = AlertDialog(title=Text("💁   Help with Checkpoint Merger"), content=Column([
            Text("Provide a list of valid pretrained model names in the HuggingFace hub or paths to locally stored models in the HuggingFace format.  Merges the Checkpoint Weights into a new model that you can save for free to HuggingFace to reuse."),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🍻  Sure thing... ", on_click=close_checkpoint_merger_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(checkpoint_merger_help_dlg)
        checkpoint_merger_help_dlg.open = True
        page.update()
    def toggle_save(e):
        changed(e, 'save_model')
        where_to_save_model.visible = checkpoint_merger_prefs['save_model']
        where_to_save_model.update()
        readme_description.visible = checkpoint_merger_prefs['save_model']
        readme_description.update()
    def remove_model(e):
        f = e.control.data
        for i, fl in enumerate(page.checkpoint_merger_file_list.controls):
            if fl.title.value == f:
                del page.checkpoint_merger_file_list.controls[i]
                page.checkpoint_merger_file_list.update()
                continue
    def remove_all_models(e):
        checkpoint_merger_prefs['pretrained_models'].clear()
        page.checkpoint_merger_file_list.controls.clear()
        page.checkpoint_merger_file_list.update()
    def move_down(e):
        idx = checkpoint_merger_prefs['pretrained_models'].index(e.control.data)
        if idx < (len(checkpoint_merger_prefs['pretrained_models']) - 1):
          d = checkpoint_merger_prefs['pretrained_models'].pop(idx)
          checkpoint_merger_prefs['pretrained_models'].insert(idx+1, d)
          dr = page.checkpoint_merger_file_list.controls.pop(idx)
          page.checkpoint_merger_file_list.controls.insert(idx+1, dr)
          page.checkpoint_merger_file_list.update()
    def move_up(e):
        idx = checkpoint_merger_prefs['pretrained_models'].index(e.control.data)
        if idx > 0:
          d = checkpoint_merger_prefs['pretrained_models'].pop(idx)
          checkpoint_merger_prefs['pretrained_models'].insert(idx-1, d)
          dr = page.checkpoint_merger_file_list.controls.pop(idx)
          page.checkpoint_merger_file_list.controls.insert(idx-1, dr)
          page.checkpoint_merger_file_list.update()
    def add_selected_model(e):
        name = checkpoint_merger_prefs['selected_model']
        m = {'name':''}
        if name == "Stable Diffusion v2.1 x768":
            m = {'name':'Stable Diffusion v2.1 x768', 'path':'stabilityai/stable-diffusion-2-1'}
        elif name == "Stable Diffusion v2.1 x512":
            m = {'name':'Stable Diffusion v2.1 x512', 'path':'stabilityai/stable-diffusion-2-1-base'}
        elif name == "Stable Diffusion v2.0":
            m = {'name':'Stable Diffusion v2.0', 'path':'stabilityai/stable-diffusion-2'}
        elif name == "Stable Diffusion v2.0 x768":
            m = {'name':'Stable Diffusion v2.0 x768', 'path':'stabilityai/stable-diffusion-2'}
        elif name == "Stable Diffusion v2.0 x512":
            m = {'name':'Stable Diffusion v2.0 x512', 'path':'stabilityai/stable-diffusion-2-base'}
        elif name == "Stable Diffusion v1.5":
            m = {'name':'Stable Diffusion v1.5', 'path':'runwayml/stable-diffusion-v1-5'}
        elif name == "Stable Diffusion v1.4":
            m = {'name':'Stable Diffusion v1.4', 'path':'CompVis/stable-diffusion-v1-4'}
        else:
            m = get_finetuned_model(name)
            if not bool(m['name']):
                m = get_dreambooth_model(name)
        if bool(m['path']):
            add_model(m['path'])
    def add_custom_model(e):
        mpath = checkpoint_merger_prefs['pretrained_model']
        add_model(mpath)
    def add_model(mpath, update=True):
        if mpath in checkpoint_merger_prefs['pretrained_models']:
            alert_msg(page, "That model path is already in your list...")
            return
        page.checkpoint_merger_file_list.controls.append(ListTile(title=Text(mpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[#TODO: View Image
              PopupMenuItem(icon=icons.DELETE, text="Remove Model", on_click=remove_model, data=mpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Remove All", on_click=remove_all_models, data=mpath),
              PopupMenuItem(icon=icons.ARROW_UPWARD, text="Move Up", on_click=move_up, data=mpath),
              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text="Move Down", on_click=move_down, data=mpath),
          ]), data=mpath))
        checkpoint_merger_prefs['pretrained_models'].append(mpath)
        if update: page.checkpoint_merger_file_list.update()
    validation_prompt = Container(content=Tooltip(message="Optional prompt to test after the merger is finished.", content=TextField(label="Validation Test Prompt", value=checkpoint_merger_prefs['validation_prompt'], on_change=lambda e:changed(e,'validation_prompt'))), col={'md':9})
    name_of_your_model = TextField(label="Name of New Model", value=checkpoint_merger_prefs['name_of_your_model'], on_change=lambda e:changed(e,'name_of_your_model'), col={'md':3})
    force = Checkbox(label="Force if Mismatch", tooltip="Whether to ignore mismatch in model_config.json for the current models.", value=checkpoint_merger_prefs['force'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'force'))
    alpha_row = SliderRow(label="Alpha Interpolation", min=0.0, max=1.0, divisions=20, round=2, pref=checkpoint_merger_prefs, key='alpha', tooltip="The interpolation parameter. Ranges from 0 to 1.  It affects the ratio in which the checkpoints are merged. A 0.8 alpha would mean that the first model checkpoints would affect the final result far less than an alpha of 0.2")
    interp = Dropdown(label="Interpolation Method", width=250, options=[dropdown.Option("weighted_sum"), dropdown.Option("sigmoid"), dropdown.Option("inv_sigmoid"), dropdown.Option("add_difference")], value=checkpoint_merger_prefs['interp'], on_change=lambda e: changed(e, 'interp'))
    #The interpolation method to use for the merging. Supports "sigmoid", "inv_sigmoid", "add_difference" and None. For merging three checkpoints, only "add_difference" is supported.
    model_ckpt = Dropdown(label="Model Checkpoint", width=300, options=[
        dropdown.Option("Stable Diffusion v2.1 x768"), dropdown.Option("Stable Diffusion v2.1 x512"),
        dropdown.Option("Stable Diffusion v2.0 x768"), dropdown.Option("Stable Diffusion v2.0 x512"), dropdown.Option("Stable Diffusion v1.5"), dropdown.Option("Stable Diffusion v1.4")], value=checkpoint_merger_prefs['selected_model'], on_change=lambda e: changed(e, 'selected_model'))
    for mod in finetuned_models:
        model_ckpt.options.append(dropdown.Option(mod["name"]))
    for db in dreambooth_models:
        model_ckpt.options.append(dropdown.Option(db["name"]))
    add_selected_model_button = ElevatedButton(content=Text("Add Selected Model"), on_click=add_selected_model)
    pretrained_model = TextField(label="HuggingFace Path or Local Path to Merge", value=checkpoint_merger_prefs['pretrained_model'], on_change=lambda e:changed(e,'pretrained_model'), expand=1)
    add_model_button = ElevatedButton(content=Text("Add Model Path"), on_click=add_custom_model)
    save_model = Tooltip(message="Requires WRITE access on API Key to Upload Checkpoint", content=Switcher(label="Save Model to HuggingFace    ", value=checkpoint_merger_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))
    where_to_save_model = Dropdown(label="Where to Save Model", width=250, options=[dropdown.Option("Public HuggingFace"), dropdown.Option("Private HuggingFace")], value=checkpoint_merger_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))
    readme_description = TextField(label="Extra README Description", value=checkpoint_merger_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))

    page.checkpoint_merger_file_list = Column([], tight=True, spacing=0)
    page.checkpoint_merger_output = Column([])
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("👥  Checkpoint Merger Tool", "Combine together two or more custom models to create a mixture of weights...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Checkpoint Merger Settings", on_click=checkpoint_merger_help)]),
        Row([model_ckpt, add_selected_model_button]),
        Row([pretrained_model, add_model_button]),
        page.checkpoint_merger_file_list,
        Divider(thickness=3, height=6),
        Row([interp, force]),
        alpha_row,
        ResponsiveRow([name_of_your_model, validation_prompt]),
        Row([save_model, where_to_save_model]),
        readme_description,
        Row([ElevatedButton(content=Text("🤗  Run Checkpoint Merger", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_checkpoint_merger(page))]),
        page.checkpoint_merger_output,
        #clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c


tortoise_prefs = {
    'text': '',
    'preset': 'standard', #"ultra_fast", "fast", "standard", "high_quality"
    'voice': [],
    'voices': ['angie', 'applejack', 'daniel', 'deniro', 'emma', 'freeman', 'geralt', 'halle', 'jlaw', 'lj', 'mol', 'myself', 'pat', 'pat2', 'rainbow', 'snakes', 'tim_reynolds', 'tom', 'train_atkins', 'train_daws', 'train_dotrice', 'train_dreams', 'train_empire', 'train_grace', 'train_kennard', 'train_lescault', 'train_mouse', 'weaver', 'william'],
    'train_custom': False,
    'custom_voice_name': '',
    'custom_wavs': [],
    'wav_path': '',
    'batch_folder_name': '',
    'file_prefix': 'tts-',
}

def buildTortoiseTTS(page):
    global prefs, tortoise_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            tortoise_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_tortoise_output(o):
        page.tortoise_output.controls.append(o)
        page.tortoise_output.update()
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.tortoise_output.controls = []
        page.tortoise_output.update()
        clear_button.visible = False
        clear_button.update()
    def tortoise_help(e):
        def close_tortoise_dlg(e):
          nonlocal tortoise_help_dlg
          tortoise_help_dlg.open = False
          page.update()
        tortoise_help_dlg = AlertDialog(title=Text("💁   Help with Tortoise-TTS"), content=Column([
            Text("Tortoise was specifically trained to be a multi-speaker model. It accomplishes this by consulting reference clips. These reference clips are recordings of a speaker that you provide to guide speech generation. These clips are used to determine many properties of the output, such as the pitch and tone of the voice, speaking speed, and even speaking defects like a lisp or stuttering. The reference clip is also used to determine non-voice related aspects of the audio output like volume, background noise, recording quality and reverb."),
            Text("This comes with several pre-packaged voices. Voices prepended with 'train_' came from the training set and perform far better than the others. If your goal is high quality speech, we recommend you pick one of them. If you want to see what Tortoise can do for zero-shot mimicing, take a look at the others."),
            Text("To add new voices to Tortoise, you will need to do the following: Gather audio clips of your speaker(s). Good sources are YouTube interviews (you can use youtube-dl to fetch the audio), audiobooks or podcasts. Guidelines for good clips are in the next section. Cut your clips into ~10 second segments. You want at least 3 clips. More is better, but I only experimented with up to 5 in my testing. Save the clips as a WAV file with floating point format and a 22,050 sample rate."),
            Text("You can do prompt engineering with Tortoise to get the performance you want. For example, you can evoke emotion by including things like [I am really sad], before your text. It redacts the phrase in brackets, but keeps the context of the meaning to the voice reading. Experiment with grammar and spelling, and use an audio editor to perfect the vocals later."),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("👄  What to say... ", on_click=close_tortoise_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(tortoise_help_dlg)
        tortoise_help_dlg.open = True
        page.update()
    def delete_audio(e):
        f = e.control.data
        if os.path.isfile(f):
          os.remove(f)
          for i, fl in enumerate(page.tortoise_file_list.controls):
            if fl.title.value == f:
              del page.tortoise_file_list.controls[i]
              page.tortoise_file_list.update()
              if f in tortoise_prefs['custom_wavs']:
                tortoise_prefs['custom_wavs'].remove(f)
              continue
    def delete_all_audios(e):
        for fl in page.tortoise_file_list.controls:
          f = fl.title.value
          if os.path.isfile(f):
            os.remove(f)
        page.tortoise_file_list.controls.clear()
        page.tortoise_file_list.update()
        tortoise_prefs['custom_wavs'].clear()
    def add_file(fpath, update=True):
        page.tortoise_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[#TODO: View Image
              PopupMenuItem(icon=icons.DELETE, text="Delete Audio", on_click=delete_audio, data=fpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All", on_click=delete_all_audios, data=fpath),
          ])))
        tortoise_prefs['custom_wavs'].append(fpath)
        if update: page.tortoise_file_list.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    save_dir = os.path.join(root_dir, 'tortoise-audio')
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
          save_file(e.file_name)
    def save_file(file_name):
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        if not slash in file_name:
          fname = os.path.join(root_dir, file_name)
          fpath = os.path.join(save_dir, file_name)
          shutil.move(fname, fpath)
        else:
          fname = file_name
          fpath = os.path.join(save_dir, file_name.rpartition(slash)[2])
          shutil.copy(fname, fpath)
        add_file(fpath)
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["wav", "WAV", "mp3", "MP3"], dialog_title="Pick Voice WAV or MP3 Files to Train")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                save_file(f.path)
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def add_wav(e):
        if not os.path.exists(save_dir):
            os.mkdir(save_dir)
        if wav_path.value.startswith('http'):
            import requests
            from io import BytesIO
            #response = requests.get(wav_path.value)
            fpath = download_file(wav_path.value)
            #fpath = os.path.join(save_dir, wav_path.value.rpartition(slash)[2])
            add_file(fpath)
        elif os.path.isfile(wav_path.value):
          fpath = os.path.join(save_dir, wav_path.value.rpartition(slash)[2])
          shutil.copy(wav_path.value, fpath)
          add_file(fpath)
        elif os.path.isdir(wav_path.value):
          for f in os.listdir(wav_path.value):
            file_path = os.path.join(wav_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.wav', '.WAV', '.mp3', '.MP3')):
              fpath = os.path.join(save_dir, f)
              shutil.copy(file_path, fpath)
              add_file(fpath)
        else:
          if bool(wav_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        wav_path.value = ""
        wav_path.update()
    def load_wavs():
        if os.path.exists(save_dir):
          for f in os.listdir(save_dir):
            existing = os.path.join(save_dir, f)
            if os.path.isdir(existing): continue
            if f.lower().endswith(('.wav', '.WAV', '.mp3', '.MP3')):
              add_file(existing, update=False)
    def toggle_custom(e):
        changed(e, 'train_custom')
        custom_box.height = None if tortoise_prefs['train_custom'] else 0
        custom_box.update()
        custom_voice_name.visible = tortoise_prefs['train_custom']
        custom_voice_name.update()
    text = TextField(label="Text to Read", value=tortoise_prefs['text'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'))
    preset = Dropdown(label="Quality Preset", width=250, options=[dropdown.Option("ultra_fast"), dropdown.Option("fast"), dropdown.Option("standard"), dropdown.Option("high_quality")], value=tortoise_prefs['preset'], on_change=lambda e: changed(e, 'preset'))
    batch_folder_name = TextField(label="Batch Folder Name", value=tortoise_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=tortoise_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    page.tortoise_voices = ResponsiveRow(controls=[])
    for v in tortoise_prefs['voices']:
      page.tortoise_voices.controls.append(Checkbox(label=v, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
    if len(prefs['tortoise_custom_voices']) > 0:
      for custom in prefs['tortoise_custom_voices']:
        page.tortoise_voices.controls.append(Checkbox(label=custom['name'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
    custom_voice_name = TextField(label="Custom Voice Name", value=tortoise_prefs['custom_voice_name'], on_change=lambda e:changed(e,'custom_voice_name'))
    train_custom = Switcher(label="Train Custom Voice  ", value=tortoise_prefs['train_custom'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_custom)
    wav_path = TextField(label="Audio Files or Folder Path or URL to Train", value=tortoise_prefs['wav_path'], on_change=lambda e:changed(e,'wav_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    add_wav_button = ElevatedButton(content=Text("Add Audio Files"), on_click=add_wav)
    page.tortoise_file_list = Column([], tight=True, spacing=0)
    custom_box = Container(Column([Text("Provide 3 or more ~10 second clips of voice as mp3 or wav files with 22050 sample rate:"),
        Row([wav_path, add_wav_button]),
        page.tortoise_file_list,]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN_CIRC), clip_behavior=ClipBehavior.HARD_EDGE)
    custom_box.height = None if tortoise_prefs['train_custom'] else 0
    custom_voice_name.visible = tortoise_prefs['train_custom']
    load_wavs()
    #seed = TextField(label="Seed", value=tortoise_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    #lambda_entropy = TextField(label="Lambda Entropy", value=dreamfustortoise_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    #max_steps = TextField(label="Max Steps", value=tortoise_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    page.tortoise_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.tortoise_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🐢  Tortoise Text-to-Speech Voice Modeling", "Reads your text in a realistic AI voice, train your own to mimic vocal performances...", actions=[save_default(tortoise_prefs, ['wav_path']), IconButton(icon=icons.HELP, tooltip="Help with Tortoise-TTS Settings", on_click=tortoise_help)]),
        text,
        preset,
        Row([Text("Select one or more voices:", weight=FontWeight.BOLD), Text("(none for random or custom)")]),
        page.tortoise_voices,
        Row([train_custom, custom_voice_name], vertical_alignment=CrossAxisAlignment.START),
        #Row([output_dir]),
        custom_box,
        Row([batch_folder_name, file_prefix]),
        ElevatedButton(content=Text("🗣️  Run Tortoise-TTS", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_tortoise_tts(page)),
        page.tortoise_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

openai_tts_prefs = {
    'text': '',
    'preset': 'tts-1-hd',
    'voice': 'Alloy',
    'speed': 1.0,
    'format': 'mp3',
    'batch_folder_name': '',
    'file_prefix': 'tts-',
}

def buildOpenAI_TTS(page):
    global prefs, openai_tts_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            openai_tts_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def openai_tts_help(e):
        def close_openai_tts_dlg(e):
          nonlocal openai_tts_help_dlg
          openai_tts_help_dlg.open = False
          page.update()
        openai_tts_help_dlg = AlertDialog(title=Text("💁   Help with OpenAI Text-to-Speech"), content=Column([
            Text("The Audio API provides a speech endpoint based on our TTS (text-to-speech) model. It comes with 6 built-in voices and can be used to: Narrate a written blog post, Produce spoken audio in multiple languages, Give real time audio output using streaming"),
            Text("The speech endpoint takes in three key inputs: the model, the text that should be turned into audio, and the voice to be used for the audio generation. For real-time applications, the standard tts-1 model provides the lowest latency but at a lower quality than the tts-1-hd model. Due to the way the audio is generated, tts-1 is likely to generate content that has more static in certain situations than tts-1-hd. In some cases, the audio may not have noticeable differences depending on your listening device and the individual person."),
            Text("Experiment with different voices (alloy, echo, fable, onyx, nova, and shimmer) to find one that matches your desired tone and audience. The current voices are optimized for English."),
            Text("The TTS model generally follows the Whisper model in terms of language support. Whisper supports the following languages and performs well despite the current voices being optimized for English. The following language are supported: Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh. You can generate spoken audio in these languages by providing the input text in the language of your choice."),
            Markdown("[Text-to-Speech Guide](https://platform.openai.com/docs/guides/text-to-speech) | [API Reference](https://platform.openai.com/docs/api-reference/audio)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("👄  Speak to me... ", on_click=close_openai_tts_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(openai_tts_help_dlg)
        openai_tts_help_dlg.open = True
        page.update()
    def add_to_openai_tts_output(o):
        page.openai_tts_output.controls.append(o)
        page.openai_tts_output.update()
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.openai_tts_output.controls = []
        page.openai_tts_output.update()
        clear_button.visible = False
        clear_button.update()
    text = TextField(label="Text to Read", value=openai_tts_prefs['text'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'))
    preset = Dropdown(label="TTS Model", width=150, options=[dropdown.Option("tts-1"), dropdown.Option("tts-1-hd")], value=openai_tts_prefs['preset'], on_change=lambda e: changed(e, 'preset'))
    voice = Dropdown(label="Voice Preset", width=200, options=[dropdown.Option("Alloy"), dropdown.Option("Echo"), dropdown.Option("Fable"), dropdown.Option("Onyx"), dropdown.Option("Nova"), dropdown.Option("Shimmer")], value=openai_tts_prefs['voice'], on_change=lambda e: changed(e, 'voice'))
    format = Dropdown(label="Format", width=100, options=[dropdown.Option("mp3"), dropdown.Option("opus"), dropdown.Option("aac"), dropdown.Option("flac")], value=openai_tts_prefs['format'], on_change=lambda e: changed(e, 'format'))
    speed = SliderRow(label="Voice Speed", min=0.25, max=4.0, divisions=15, round=2, pref=openai_tts_prefs, key='speed', tooltip="The speed of the generated audio. Select a value from 0.25 to 4.0. 1.0 is the default.")
    batch_folder_name = TextField(label="Batch Folder Name", value=openai_tts_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=openai_tts_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    page.openai_tts_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.openai_tts_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🦜  OpenAI Text-to-Speech Voice Modeling", "Turn text into lifelike spoken audio... Uses your OpenAI credits.", actions=[save_default(openai_tts_prefs), IconButton(icon=icons.HELP, tooltip="Help with OpenAI-TTS Settings", on_click=openai_tts_help)]),
        text,
        Row([voice, preset, format]),
        speed,
        Row([batch_folder_name, file_prefix]),
        ElevatedButton(content=Text("📞  Run OpenAI-TTS", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_openai_tts(page)),
        page.openai_tts_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

audioLDM_prefs = {
    'text': '',
    'duration': 5.0,
    'guidance_scale': 2.5,
    'n_candidates': 3,#This number control the number of candidates (e.g., generate three audios and choose the best to show you). A Larger value usually lead to better quality with heavier computation
    'seed': 0,
    'wav_path': '',
    'batch_folder_name': '',
    'file_prefix': 'ldm-',
}

def buildAudioLDM(page):
    global prefs, audioLDM_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            audioLDM_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_audioLDM_output(o):
        page.audioLDM_output.controls.append(o)
        page.audioLDM_output.update()
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.audioLDM_output.controls = []
        page.audioLDM_output.update()
        clear_button.visible = False
        clear_button.update()
    def audioLDM_help(e):
        def close_audioLDM_dlg(e):
          nonlocal audioLDM_help_dlg
          audioLDM_help_dlg.open = False
          page.update()
        audioLDM_help_dlg = AlertDialog(title=Text("💁   Help with Audio-LDM"), content=Column([
            Text("AudioLDM is a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion."),
            Markdown("They built the model with data from [AudioSet](http://research.google.com/audioset/), [Freesound](https://freesound.org/) and [BBC Sound Effect library](https://sound-effects.bbcrewind.co.uk/). We share this demo based on the [UK copyright exception](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/375954/Research.pdf) of data for academic research.", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🔔  Good to hear... ", on_click=close_audioLDM_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(audioLDM_help_dlg)
        audioLDM_help_dlg.open = True
        page.update()
    duration_row = SliderRow(label="Duration", min=1, max=20, divisions=38, round=1, suffix="s", pref=audioLDM_prefs, key='duration')
    guidance = SliderRow(label="Guidance Scale", min=0, max=5, divisions=10, round=1, pref=audioLDM_prefs, key='guidance_scale', tooltip="Large => better quality and relavancy to text; Small => better diversity")
    text = TextField(label="Text Prompt to Auditorialize", value=audioLDM_prefs['text'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'))
    batch_folder_name = TextField(label="Batch Folder Name", value=audioLDM_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=audioLDM_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_candidates = Tooltip(message="Automatic quality control. Generates candidates and choose the best. Larger value usually lead to better quality with heavier computation.", content=NumberPicker(label="Number of Candidates:   ", min=1, max=5, value=audioLDM_prefs['n_candidates'], on_change=lambda e: changed(e, 'n_candidates')))
    seed = TextField(label="Seed", value=audioLDM_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)
    page.audioLDM_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.audioLDM_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🦻  Audio LDM Modeling", "Text-to-Audio Generation with Latent Diffusion Model...", actions=[save_default(audioLDM_prefs, ['wav_path']), IconButton(icon=icons.HELP, tooltip="Help with Audio LDM-TTS Settings", on_click=audioLDM_help)]),
        text,
        duration_row,
        guidance,
        Row([n_candidates, seed]),
        Row([batch_folder_name, file_prefix]),
        ElevatedButton(content=Text("👏  Run AudioLDM", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_audio_ldm(page)),
        page.audioLDM_output,
        clear_button,
        #AudioPlayer(audio_file=os.path.join(assets, "snd-drop.mp3"), display="tester", page=page)
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

audioLDM2_prefs = {
    'text': '',
    'negative_prompt':'',
    'transcription': '',
    'model_name': 'cvssp/audioldm2',
    'duration': 10.0,
    'steps': 200,
    'guidance_scale': 3.5,
    'n_candidates': 3,#This number control the number of candidates (e.g., generate three audios and choose the best to show you). A Larger value usually lead to better quality with heavier computation
    'seed': 0,
    'batch_size': 1,
    'batch_folder_name': '',
    'file_prefix': 'ldm2-',
    'save_mp3': False,
}

def buildAudioLDM2(page):
    global prefs, audioLDM2_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            audioLDM2_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_audioLDM2_output(o):
        page.audioLDM2_output.controls.append(o)
        page.audioLDM2_output.update()
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.audioLDM2_output.controls = []
        page.audioLDM2_output.update()
        clear_button.visible = False
        clear_button.update()
    def audioLDM2_help(e):
        def close_audioLDM2_dlg(e):
          nonlocal audioLDM2_help_dlg
          audioLDM2_help_dlg.open = False
          page.update()
        audioLDM2_help_dlg = AlertDialog(title=Text("💁   Help with Audio-LDM 2"), content=Column([
            Text("AudioLDM-2 is a novel and versatile audio generation model that capable of performing conditional audio, music, and intelligible speech generation. The proposed method is based on a universal representation of audio, which enables large-scale self-supervised pretraining of the core latent diffusion model without audio annotation and helps to combine the advantages of both the auto-regressive and the latent diffusion model. AudioLDM 2 achieves state-of-the-art performance in text-to-audio and text-to-music generation, while also delivering competitive results in text-to-speech generation, comparable to the current SoTA."),
            Text("Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called 'language of audio' (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE and latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech demonstrate new state-of-the-art or competitive performance to previous approaches."),
            Markdown("[Project Page](https://audioldm.github.io/audioldm2/) | [Paper](https://arxiv.org/abs/2308.05734) | [GitHub Code](https://github.com/haoheliu/audioldm2)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🔔  Good to hear... ", on_click=close_audioLDM2_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(audioLDM2_help_dlg)
        audioLDM2_help_dlg.open = True
        page.update()
    def change_model(e):
        changed(e, 'model_name')
        if 'speech' in audioLDM2_prefs['model_name']:
            transcription.visible = True
        else:
            transcription.visible = False
        transcription.update()
    model_name = Dropdown(label="Audio-LDM2 Model", width=270, options=[dropdown.Option("cvssp/audioldm2"), dropdown.Option("cvssp/audioldm2-music"), dropdown.Option("cvssp/audioldm2-large"), dropdown.Option("anhnct/audioldm2_gigaspeech")], value=audioLDM2_prefs['model_name'], on_change=change_model)
    duration_row = SliderRow(label="Duration", min=1, max=320, divisions=319, round=1, suffix="s", pref=audioLDM2_prefs, key='duration')
    guidance = SliderRow(label="Guidance Scale", min=0, max=10, divisions=20, round=1, pref=audioLDM2_prefs, key='guidance_scale', tooltip="Large => better quality and relavancy to text; Small => better diversity")
    steps_row = SliderRow(label="Number of Steps", min=1, max=300, divisions=299, pref=audioLDM2_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    text = TextField(label="Text Prompt to Auditorialize", value=audioLDM2_prefs['text'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'), col={'md':9})
    negative_prompt = TextField(label="Negative Prompt", value=audioLDM2_prefs['negative_prompt'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'negative_prompt'), col={'md':3})
    transcription = TextField(label="Text Transcript to Speak", value=audioLDM2_prefs['transcription'], multiline=True, min_lines=1, max_lines=8, visible=False, on_change=lambda e:changed(e,'transcription'))
    save_mp3 = Checkbox(label="Save as mp3", tooltip="Otherwise saves larger wav file.", value=audioLDM2_prefs['save_mp3'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_mp3'))
    batch_folder_name = TextField(label="Batch Folder Name", value=audioLDM2_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=audioLDM2_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_candidates = Tooltip(message="Automatic quality control. Generates candidates and choose the best. Larger value usually lead to better quality with heavier computation.", content=NumberPicker(label="Number of Candidates:   ", min=1, max=5, value=audioLDM2_prefs['n_candidates'], on_change=lambda e: changed(e, 'n_candidates')))
    batch_size = NumberPicker(label="Batch Size:  ", min=1, max=10, value=audioLDM2_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    seed = TextField(label="Seed", value=audioLDM2_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)
    page.audioLDM2_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.audioLDM2_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("📢  Audio LDM-2 Modeling", "Holistic Audio Generation with Self-supervised Pretraining...", actions=[save_default(audioLDM2_prefs), IconButton(icon=icons.HELP, tooltip="Help with Audio LDM-TTS Settings", on_click=audioLDM2_help)]),
        ResponsiveRow([text, negative_prompt]),
        transcription,
        duration_row,
        guidance,
        steps_row,
        Row([model_name, batch_size, seed]),
        Row([batch_folder_name, file_prefix, save_mp3]),
        ElevatedButton(content=Text("🎙  Run AudioLDM-2", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_audio_ldm2(page)),
        page.audioLDM2_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

musiclang_prefs = {
    'midi_file': '',
    'bar_range': '1-4',
    'time_signature': '4/4',
    'nb_tokens': 512,
    'temperature': 0.95,
    'top_p': 0.98,
    'tempo': 120,
    'chord_progression': '',
    'seed': 0,
    'audio_name': '',
    'batch_folder_name': '',
    'file_prefix': 'musiclang-',
}

def buildMusicLang(page):
    global prefs, musiclang_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            musiclang_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def musiclang_help(e):
        def close_musiclang_dlg(e):
          nonlocal musiclang_help_dlg
          musiclang_help_dlg.open = False
          page.update()
        musiclang_help_dlg = AlertDialog(title=Text("💁   Help with MusicLang"), content=Column([
            Text("MusicLang Predict offers advanced controllability features and high-quality music generation by manipulating symbolic music. You can for example use it to continue your composition with a specific chord progression. We are using a LLAMA2 architecture (many thanks to Andrej Karpathy awesome llama2.c), trained on a large dataset of midi files (The CC0 licensed LAKH). We heavily rely on preprocessing the midi files to get an enriched tokenization that describe chords & scale for each bar. The is also helpful for normalizing melodies relative to the current chord/scale."),
            Text("You want to generate music that you can export to your favourite DAW in MIDI ? You want to control the chord progression of the generated music ? You need to run it fast on your laptop without a gpu ? You had a specific harmony in mind am I right ? That's why we allow a fine control over the chord progression of the generated music. Just specify it as a string like below, choose a time signature and let the magic happen."),
            Markdown("[Project Page](https://musiclang.github.io/) | [MusicLang Predict](https://github.com/musiclang/musiclang_predict) | [HuggingFace Space](https://huggingface.co/spaces/musiclang/musiclang-predict) | [MusicLang Model](https://huggingface.co/musiclang/musiclang-v2)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🚀  Blast da Beat... ", on_click=close_musiclang_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(musiclang_help_dlg)
        musiclang_help_dlg.open = True
        page.update()
    midi_file = FileInput(label="Prompt MIDI File (optional)", pref=musiclang_prefs, key='midi_file', ftype="midi", page=page)
    tempo = SliderRow(label="Tempo", min=60, max=240, divisions=180, suffix="bpm", pref=musiclang_prefs, key='tempo', tooltip="Only used if not using input file, otherwise bpm from MIDI.")
    nb_tokens = SliderRow(label="Nb Tokens", min=256, max=2048, divisions=7, pref=musiclang_prefs, key='nb_tokens', tooltip="")
    chord_progression = TextField(label="Chord Progression (optional)", value=musiclang_prefs['chord_progression'], expand=True, hint_text="Am CM Dm7/F E7 Asus4", on_change=lambda e:changed(e,'chord_progression'), tooltip="eg: Cm C7/E Fm F#dim G7, or Am Em CM G7 E7 Am Am E7 Am, or Cm AbM BbM G7 Cm")
    bar_range = TextField(label="Bar Range", value=musiclang_prefs['bar_range'], width=120, on_change=lambda e:changed(e,'bar_range'), tooltip="Number of bars from the Input File. eg: 0-4 for first four bars")
    time_signature = TextField(label="Time Signature", value=musiclang_prefs['time_signature'], width=120, on_change=lambda e:changed(e,'time_signature'), tooltip="Numbers seperated with a slash")
    temperature = SliderRow(label="Temperature", min=0, max=1, divisions=20, round=2, pref=musiclang_prefs, key='temperature', tooltip="Softmax value used to module the next token probabilities", col={'lg':6})
    top_p = SliderRow(label="Top-P Samples", min=0, max=1, divisions=100, round=2, pref=musiclang_prefs, key='top_p', tooltip="Highest probability vocabulary tokens to keep.", col={'lg':6})
    batch_folder_name = TextField(label="Batch Folder Name", value=musiclang_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=musiclang_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    audio_name = TextField(label="Audio File Name", value=musiclang_prefs['audio_name'], on_change=lambda e:changed(e,'audio_name'))
    #n_candidates = Tooltip(message="Automatic quality control. Generates candidates and choose the best. Larger value usually lead to better quality with heavier computation.", content=NumberPicker(label="Number of Candidates:   ", min=1, max=5, value=musiclang_prefs['n_candidates'], on_change=lambda e: changed(e, 'n_candidates')))
    seed = TextField(label="Seed", value=musiclang_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎹  MusicLang Predict", "Controllable Symbolic Music Generation with MusicLang Predict...", actions=[save_default(musiclang_prefs, ['midi_file']), IconButton(icon=icons.HELP, tooltip="Help with Music Lang-TTS Settings", on_click=musiclang_help)]),
        midi_file,
        Row([bar_range, time_signature, chord_progression]),
        tempo,
        nb_tokens,
        ResponsiveRow([temperature, top_p]),
        Row([audio_name, batch_folder_name, file_prefix, seed]),
        ElevatedButton(content=Text("🎶  Run MusicLang", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_music_lang(page)),
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

zeta_editing_prefs = {
    'target_prompt': '',
    'negative_prompt':'',
    'source_prompt':'',
    'audio_file': '',
    'model_name': 'cvssp/audioldm2',
    't_start': 45,
    'duration': 30.0,
    'steps': 200,
    'source_guidance_scale': 3.0,
    'guidance_scale': 12.0,
    'seed': 0,
    'batch_size': 1,
    'batch_folder_name': '',
    'file_prefix': 'zeta-',
    'save_mp3': False,
}

def buildZetaEditing(page):
    global prefs, zeta_editing_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            zeta_editing_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def zeta_editing_help(e):
        def close_zeta_editing_dlg(e):
          nonlocal zeta_editing_help_dlg
          zeta_editing_help_dlg.open = False
          page.update()
        zeta_editing_help_dlg = AlertDialog(title=Text("💁   Help with Zeta Editing"), content=Column([
            Text("Editing signals using large pre-trained models, in a zero-shot manner, has recently seen rapid advancements in the image domain. However, this wave has yet to reach the audio domain. In this paper, we explore two zero-shot editing techniques for audio signals, which use DDPM inversion on pre-trained diffusion models. The first, adopted from the image domain, allows text-based editing. The second, is a novel approach for discovering semantically meaningful editing directions without supervision. When applied to music signals, this method exposes a range of musically interesting modifications, from controlling the participation of specific instruments to improvisations on the melody."),
            Text("Our methods are based on the recently introduced editfriendly DDPM inversion method, which we use for extracting latent noise vectors corresponding to the source signal. To generate the edited signal, we use those noise vectors in a DDPM sampling process, while drifting the diffusion towards the desired edit. In text-based editing, we achieve this by changing the text prompt supplied to the denoiser model. In our unsupervised method, we perturb the output of the denoiser in the directions of the top principal components (PCs) of the posterior, which we efficiently compute based on Manor & Michaeli (2024). As we show, these perturbations are particularly useful for editing music excerpts, in which they can uncover improvisations and other musically plausible modifications."),
            Markdown("[Project Page](https://hilamanor.github.io/AudioEditing/) | [Paper](https://arxiv.org/abs/2402.10009) | [GitHub Code](https://github.com/HilaManor/AudioEditingCode)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🤐  Hear the Changes... ", on_click=close_zeta_editing_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(zeta_editing_help_dlg)
        zeta_editing_help_dlg.open = True
        page.update()
    def change_model(e):
        changed(e, 'model_name')
    model_name = Dropdown(label="Audio-LDM2 Model", width=225, options=[dropdown.Option("cvssp/audioldm2"), dropdown.Option("cvssp/audioldm2-music"), dropdown.Option("cvssp/audioldm2-large")], value=zeta_editing_prefs['model_name'], on_change=change_model)
    duration_row = SliderRow(label="Duration", min=1, max=320, divisions=319, round=1, suffix="s", pref=zeta_editing_prefs, key='duration')
    guidance = SliderRow(label="Target Guidance Scale", min=0, max=20, divisions=40, round=1, pref=zeta_editing_prefs, key='guidance_scale', tooltip="Large => better quality and relavancy to text; Small => better diversity")
    souce_guidance = SliderRow(label="Source Guidance Scale", min=0, max=20, divisions=40, round=1, pref=zeta_editing_prefs, key='source_guidance_scale', tooltip="Large => better quality and relavancy to text; Small => better diversity")
    steps_row = SliderRow(label="Number of Steps", min=1, max=300, divisions=299, pref=zeta_editing_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    t_start = SliderRow(label="T-Start", min=15, max=85, divisions=70, pref=zeta_editing_prefs, key='t_start', suffix="%", tooltip="Lower T-start -> closer to original audio. Higher T-start -> stronger edit.")
    target_prompt = TextField(label="Editing Target Prompt (describe your desired edited output)", value=zeta_editing_prefs['target_prompt'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'target_prompt'), col={'md':9})
    source_prompt = TextField(label="Input Source Prompt (describe the original audio input)", value=zeta_editing_prefs['source_prompt'], filled=False, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'source_prompt'), col={'md':9})
    negative_prompt = TextField(label="Negative Prompt", value=zeta_editing_prefs['negative_prompt'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'negative_prompt'), col={'md':3})
    audio_file = FileInput(label="Input Audio File (mp3 or wav)", pref=zeta_editing_prefs, key='audio_file', ftype="audio", page=page)
    #transcription = TextField(label="Text Transcript to Speak", value=zeta_editing_prefs['transcription'], multiline=True, min_lines=1, max_lines=8, visible=False, on_change=lambda e:changed(e,'transcription'))
    save_mp3 = Checkbox(label="Save as mp3", tooltip="Otherwise saves larger wav file.", value=zeta_editing_prefs['save_mp3'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_mp3'))
    batch_folder_name = TextField(label="Batch Folder Name", value=zeta_editing_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=zeta_editing_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #n_candidates = Tooltip(message="Automatic quality control. Generates candidates and choose the best. Larger value usually lead to better quality with heavier computation.", content=NumberPicker(label="Number of Candidates:   ", min=1, max=5, value=zeta_editing_prefs['n_candidates'], on_change=lambda e: changed(e, 'n_candidates')))
    batch_size = NumberPicker(label="Batch Size:  ", min=1, max=10, value=zeta_editing_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    seed = TextField(label="Seed", value=zeta_editing_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎧  ZETA Audio Editing with LDM-2", "Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion...", actions=[save_default(zeta_editing_prefs), IconButton(icon=icons.HELP, tooltip="Help with ZETA Audio Settings", on_click=zeta_editing_help)]),
        audio_file,
        source_prompt,
        ResponsiveRow([target_prompt, negative_prompt]),
        #transcription,
        t_start,
        #duration_row,
        souce_guidance,
        guidance,
        steps_row,
        Row([model_name, batch_size, seed]),
        Row([batch_folder_name, file_prefix, save_mp3]),
        ElevatedButton(content=Text("🎷  Run ZETA Edit", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_zeta_editing(page)),
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

musicLDM_prefs = {
    'text': '',
    'negative_prompt':'',
    'model_name': 'cvssp/musicldm',
    'duration': 10.0,
    'steps': 200,
    'guidance_scale': 3.5,
    'n_candidates': 3,#This number control the number of candidates (e.g., generate three audios and choose the best to show you). A Larger value usually lead to better quality with heavier computation
    'seed': 0,
    'batch_size': 1,
    'batch_folder_name': '',
    'file_prefix': 'ldm-',
    'save_mp3': False,
}

def buildMusicLDM(page):
    global prefs, musicLDM_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            musicLDM_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_musicLDM_output(o):
        page.musicLDM_output.controls.append(o)
        page.musicLDM_output.update()
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.musicLDM_output.controls = []
        page.musicLDM_output.update()
        clear_button.visible = False
        clear_button.update()
    def musicLDM_help(e):
        def close_musicLDM_dlg(e):
          nonlocal musicLDM_help_dlg
          musicLDM_help_dlg.open = False
          page.update()
        musicLDM_help_dlg = AlertDialog(title=Text("💁   Help with Music-LDM"), content=Column([
            Text("MusicLDM takes a text prompt as input and predicts the corresponding music sample. Inspired by Stable Diffusion and AudioLDM, MusicLDM is a text-to-music latent diffusion model (LDM) that learns continuous audio representations from CLAP latents. MusicLDM is trained on a corpus of 466 hours of music data. Beat-synchronous data augmentation strategies are applied to the music samples, both in the time domain and in the latent space. Using beat-synchronous data augmentation strategies encourages the model to interpolate between the training samples, but stay within the domain of the training data. The result is generated music that is more diverse while staying faithful to the corresponding style."),
            Text("It was proposed in MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies by Ke Chen, Yusong Wu, Haohe Liu, Marianna Nezhurina, Taylor Berg-Kirkpatrick, Shlomo Dubnov."),
            Markdown("[Paper](https://huggingface.co/papers/2308.01546) | [GitHub Code](https://github.com/haoheliu/musicldm)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🪨  Let's Rock... ", on_click=close_musicLDM_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(musicLDM_help_dlg)
        musicLDM_help_dlg.open = True
        page.update()
    def change_model(e):
        changed(e, 'model_name') #dropdown.Option("cvssp/audioldm-s-full-v2"), 
    model_name = Dropdown(label="Music-LDM Model", width=350, options=[dropdown.Option("cvssp/musicldm"), dropdown.Option("sanchit-gandhi/musicldm-full")], value=musicLDM_prefs['model_name'], on_change=change_model)
    duration_row = SliderRow(label="Duration", min=1, max=320, divisions=319, round=1, suffix="s", pref=musicLDM_prefs, key='duration')
    guidance = SliderRow(label="Guidance Scale", min=0, max=10, divisions=20, round=1, pref=musicLDM_prefs, key='guidance_scale', tooltip="Large => better quality and relavancy to text; Small => better diversity")
    steps_row = SliderRow(label="Number of Steps", min=1, max=300, divisions=299, pref=musicLDM_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    text = TextField(label="Text Prompt to Describe Music", value=musicLDM_prefs['text'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'), col={'md':9})
    negative_prompt = TextField(label="Negative Prompt", value=musicLDM_prefs['negative_prompt'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'negative_prompt'), col={'md':3})
    #transcription = TextField(label="Text Transcript to Speak", value=musicLDM_prefs['transcription'], multiline=True, min_lines=1, max_lines=8, visible=False, on_change=lambda e:changed(e,'transcription'))
    save_mp3 = Checkbox(label="Save as mp3", tooltip="Otherwise saves larger wav file.", value=musicLDM_prefs['save_mp3'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_mp3'))
    batch_folder_name = TextField(label="Batch Folder Name", value=musicLDM_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=musicLDM_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_candidates = Tooltip(message="Automatic quality control. Generates candidates and choose the best. Larger value usually lead to better quality with heavier computation.", content=NumberPicker(label="Number of Candidates:   ", min=1, max=5, value=musicLDM_prefs['n_candidates'], on_change=lambda e: changed(e, 'n_candidates')))
    batch_size = NumberPicker(label="Batch Size:  ", min=1, max=10, value=musicLDM_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    seed = TextField(label="Seed", value=musicLDM_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)
    page.musicLDM_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.musicLDM_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎸  MusicLDM Song Modeling", "Text-to-Music Generation: Enhancing Novelty in Beat-Synchronous Mixup...", actions=[save_default(musicLDM_prefs), IconButton(icon=icons.HELP, tooltip="Help with Music LDM-TTS Settings", on_click=musicLDM_help)]),
        ResponsiveRow([text, negative_prompt]),
        model_name,
        duration_row,
        guidance,
        steps_row,
        Row([batch_size, seed, save_mp3]),
        Row([batch_folder_name, file_prefix]),
        ElevatedButton(content=Text("🤘  Run MusicLDM", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_music_ldm(page)),
        page.musicLDM_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

stable_audio_prefs = {
    'text': '',
    'negative_prompt':'',
    'model_name': 'stabilityai/stable-audio-open-1.0',
    'custom_model': '',
    'sampler': 'dpmpp-3m-sde',
    'duration': 30.0,
    'steps': 100,
    'guidance_scale': 7.0,
    'sigma_min': 0.3,
    'sigma_max': 500,
    'seed': 0,
    'batch_size': 1,
    'batch_folder_name': '',
    'file_prefix': 'stableaudio-',
    'save_mp3': False,
}

def buildStableAudio(page):
    global prefs, stable_audio_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            stable_audio_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def stable_audio_help(e):
        def close_stable_audio_dlg(e):
          nonlocal stable_audio_help_dlg
          stable_audio_help_dlg.open = False
          page.update()
        stable_audio_help_dlg = AlertDialog(title=Text("💁   Help with Stable Audio"), content=Column([
            Text("Stable Audio Open 1.0 generates variable-length (up to 47s) stereo audio at 44.1kHz from text prompts. It comprises three components: an autoencoder that compresses waveforms into a manageable sequence length, a T5-based text embedding for text conditioning, and a transformer-based diffusion (DiT) model that operates in the latent space of the autoencoder.  This model is made to be used with the stable-audio-tools library for inference."),
            Text("Our dataset consists of 486492 audio recordings, where 472618 are from Freesound and 13874 are from the Free Music Archive (FMA). All audio files are licensed under CC0, CC BY, or CC Sampling+. This data is used to train our autoencoder and DiT. We use a publicly available pre-trained T5 model (t5-base) for text conditioning. The primary use of Stable Audio Open is research and experimentation on AI-based music and audio generation."),
            Text("Limitations: The model is not able to generate realistic vocals. The model has been trained with English descriptions and will not perform as well in other languages. The model does not perform equally well for all music styles and cultures. The model is better at generating sound effects and field recordings than music. It is sometimes difficult to assess what types of text descriptions provide the best generations. Prompt engineering may be required to obtain satisfying results."),
            Markdown("[Project Page](https://stableaudio.com) | [Paper](https://stability.ai/research/stable-audio-efficient-timing-latent-diffusion) | [Model Card](https://huggingface.co/stabilityai/stable-audio-open-1.0) | [GitHub Code](https://github.com/Stability-AI/stable-audio-tools) | [Blog](https://stability.ai/news/stable-audio-2-0)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🎛  How Stable is it? ", on_click=close_stable_audio_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(stable_audio_help_dlg)
        stable_audio_help_dlg.open = True
        page.update()
    def change_model(e):
        changed(e, 'model_name')
        custom_model.visible = e.control.value == "Custom"
        accept_model.visible = e.control.value != "Custom"
        custom_model.update()
        accept_model.update()
    model_name = Dropdown(label="Stable Audio Model", width=320, options=[dropdown.Option("stabilityai/stable-audio-open-1.0"), dropdown.Option("Custom")], value=stable_audio_prefs['model_name'], on_change=change_model)
    custom_model = TextField(label="Custom StableAudio Model (URL or HF Path)", value=stable_audio_prefs['custom_model'], expand=True, visible=stable_audio_prefs['model_name']=="Custom", on_change=lambda e:changed(e,'custom_model'))
    accept_model = Markdown(" [ ACCEPT HF Model Agreement First](https://huggingface.co/stabilityai/stable-audio-open-1.0)", on_tap_link=lambda e: e.page.launch_url(e.data), visible=stable_audio_prefs['model_name']!="Custom")
    sampler = Dropdown(label="Sampler Type", width=230, options=[dropdown.Option(s) for s in ["dpmpp-2m-sde", "dpmpp-3m-sde", "k-heun", "k-lms", "k-dpmpp-2s-ancestral", "k-dpm-2", "k-dpm-fast"]], value=stable_audio_prefs['sampler'], on_change=lambda e:changed(e,'sampler'))
    duration_row = SliderRow(label="Duration", min=1, max=47, divisions=46, round=0, suffix="s", pref=stable_audio_prefs, key='duration')
    guidance = SliderRow(label="Guidance Scale", min=0, max=15, divisions=30, round=1, pref=stable_audio_prefs, key='guidance_scale', tooltip="Large => better quality and relavancy to text; Small => better diversity")
    steps_row = SliderRow(label="Number of Steps", min=1, max=300, divisions=299, pref=stable_audio_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality audio at the expense of slower inference.")
    sigma_min = SliderRow(label="Sigma Min", min=0, max=5, divisions=100, round=2, pref=stable_audio_prefs, key='sigma_min', col={'md':6}, tooltip="")
    sigma_max = SliderRow(label="Sigma Max", min=0, max=1000, divisions=2000, round=1, pref=stable_audio_prefs, key='sigma_max', col={'md':6}, tooltip="")
    text = TextField(label="Audio Prompt to Generate", value=stable_audio_prefs['text'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'), col={'md':9})
    negative_prompt = TextField(label="Negative Prompt", value=stable_audio_prefs['negative_prompt'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'negative_prompt'), col={'md':3})
    save_mp3 = Checkbox(label="Save as mp3", tooltip="Otherwise saves larger wav file.", value=stable_audio_prefs['save_mp3'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_mp3'))
    batch_folder_name = TextField(label="Batch Folder Name", value=stable_audio_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=stable_audio_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    batch_size = NumberPicker(label=" Number of Iterations:  ", min=1, max=10, value=stable_audio_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    seed = TextField(label="Seed", value=stable_audio_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)
    page.stable_audio_output = Column([])
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🫢   Stable Audio Open v1.0 (Under construction)", "Generate Variable-Length Stereo Audio at 44.1kHz from Text Prompts using StabilityAI Open Model...", actions=[save_default(stable_audio_prefs), IconButton(icon=icons.HELP, tooltip="Help with StableAudio Settings", on_click=stable_audio_help)]),
        ResponsiveRow([text, negative_prompt]),
        duration_row,
        guidance,
        steps_row,
        ResponsiveRow([sigma_min, sigma_max]),
        Row([model_name, accept_model, custom_model]),
        Row([sampler, batch_size, seed]),
        Row([batch_folder_name, file_prefix, save_mp3]),
        ElevatedButton(content=Text("🎺  Run Stable Audio", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_stable_audio(page)),
        page.stable_audio_output,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

bark_prefs = {
    'text': '',
    'text_temp': 0.7,
    'waveform_temp': 0.7,
    'acoustic_prompt': 'Unconditional',
    'n_iterations': 1,
    'seed': 0,
    'use_bettertransformer': False,
    'batch_folder_name': '',
    'file_prefix': 'bark-',
}

def buildBark(page):
    global prefs, bark_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            bark_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_bark_output(o):
        page.bark_output.controls.append(o)
        page.bark_output.update()
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.bark_output.controls = []
        page.bark_output.update()
        clear_button.visible = False
        clear_button.update()
    def bark_help(e):
        def close_bark_dlg(e):
          nonlocal bark_help_dlg
          bark_help_dlg.open = False
          page.update()
        bark_help_dlg = AlertDialog(title=Text("💁   Help with Bark AI"), content=Column([
            Text("Bark is a transformer-based text-to-audio model created by Suno. Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects. The model can also produce nonverbal communications like laughing, sighing and crying. To support the research community, we are providing access to pretrained model checkpoints ready for inference."),
            Text("Below is a list of some known non-speech sounds, but we are finding more every day. Please let us know if you find patterns that work particularly well on Discord!"),
            Text("[laughter], [laughs], [sighs], [music], [gasps], [clears throat], — or … for hesitations, ♪ for song lyrics, capitalization for emphasis of a word, MAN/WOMAN: for bias towards speaker"),
            Markdown("Checkout their [GitHub Project](https://github.com/suno-ai/bark), [HuggingFace Space](https://huggingface.co/spaces/suno/bark) and [Suno Discord](https://discord.com/invite/J2B2vsjKuE).", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🦊  Woof... ", on_click=close_bark_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(bark_help_dlg)
        bark_help_dlg.open = True
        page.update()
    #duration_row = SliderRow(label="Duration", min=1, max=20, divisions=38, round=1, suffix="s", pref=bark_prefs, key='duration')
    text_temp = SliderRow(label="Text Temperature", min=0, max=1, divisions=20, round=2, pref=bark_prefs, key='text_temp', tooltip="1.0 more diverse, 0.0 more conservative")
    waveform_temp = SliderRow(label="Wave Temperature", min=0, max=1, divisions=20, round=2, pref=bark_prefs, key='waveform_temp', tooltip="1.0 more diverse, 0.0 more conservative")
    text = TextField(label="Text Prompt to Vocalize", value=bark_prefs['text'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'))
    acoustic_prompt = Dropdown(label="Acoustic Prompt", width=250, options=[dropdown.Option("Unconditional"), dropdown.Option("Announcer")], value=bark_prefs['acoustic_prompt'], on_change=lambda e: changed(e, 'acoustic_prompt'))
    langs = ["en", "de", "es", "fr", "hi", "it", "ja", "ko", "pl", "pt", "ru", "tr", "zh"]
    for lang in langs:
        for n in range(10):
            #label = f"Speaker {n} ({lang})"
            acoustic_prompt.options.append(dropdown.Option(f"{lang}_speaker_{n}"))
    batch_folder_name = TextField(label="Batch Folder Name", value=bark_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=bark_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_iterations = Tooltip(message="Make multiple for quality control. Generates candidates and choose the best.", content=NumberPicker(label="Number of Iterations:   ", min=1, max=10, value=bark_prefs['n_iterations'], on_change=lambda e: changed(e, 'n_iterations')))
    seed = TextField(label="Seed", value=bark_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)
    use_bettertransformer = Switcher(label="Use Optimum BetterTransformer", value=bark_prefs['use_bettertransformer'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_bettertransformer'), tooltip="Enables HuggingFace Optimum Transformers to go 25% faster.")
    page.bark_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.bark_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🐶  Bark AI", "Text-to-Audio Generation for Multilingual Speech, Music and Sound Effects...", actions=[save_default(bark_prefs), IconButton(icon=icons.HELP, tooltip="Help with Audio LDM-TTS Settings", on_click=bark_help)]),
        text,
        text_temp,
        waveform_temp,
        Row([acoustic_prompt, n_iterations]),
        #Row([n_iterations, seed]),
        Row([batch_folder_name, file_prefix]),
        use_bettertransformer,
        ElevatedButton(content=Text("🐕  Run Bark", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_bark(page)),
        page.bark_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c


riffusion_prefs = {
    'prompt': '',
    'negative_prompt': '',
    'audio_file': '',
    'duration': 5.0,
    'steps': 50,
    'strength': 0.5,
    'guidance_scale': 7.0,
    'batch_size': 1,
    'max_size': 768,
    'seed': 0,
    'wav_path': '',
    'batch_folder_name': '',
    'file_prefix': 'riff-',
    'loaded_pipe': '',
}

def buildRiffusion(page):
    global prefs, riffusion_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            riffusion_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_riffusion_output(o):
        page.riffusion_output.controls.append(o)
        page.riffusion_output.update()
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.riffusion_output.controls = []
        page.riffusion_output.update()
        clear_button.visible = False
        clear_button.update()
    def riffusion_help(e):
        def close_riffusion_dlg(e):
          nonlocal riffusion_help_dlg
          riffusion_help_dlg.open = False
          page.update()
        riffusion_help_dlg = AlertDialog(title=Text("💁   Help with Riffusion"), content=Column([
            Text("This is the v1.5 stable diffusion model with no modifications, just fine-tuned on images of spectrograms paired with text. Audio processing happens downstream of the model. It can generate infinite variations of a prompt by varying the seed. All the same web UIs and techniques like img2img, inpainting, negative prompts, and interpolation work out of the box."),
            Markdown("[Project Page](https://www.riffusion.com/about), [Codebase](https://github.com/riffusion/riffusion), [Discord](https://discord.gg/yu6SRwvX4v)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🎺  Let's Jam... ", on_click=close_riffusion_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(riffusion_help_dlg)
        riffusion_help_dlg.open = True
        page.update()
    audio_file = FileInput(label="Input Audio File (optional)", pref=riffusion_prefs, key='audio_file', ftype="audio", page=page)
    def change_duration(e):
        changed(e, 'duration', ptype="float")
        duration_value.value = f" {riffusion_prefs['duration']}s"
        duration_value.update()
    duration = Slider(min=1, max=20, divisions=38, label="{value}s", round=1, value=float(riffusion_prefs['duration']), expand=True, on_change=change_duration)
    duration_value = Text(f" {float(riffusion_prefs['duration'])}s", weight=FontWeight.BOLD)
    duration_row = Row([Text("Duration: "), duration_value, duration])
    steps_row = SliderRow(label="Number of Steps", min=1, max=100, divisions=99, pref=riffusion_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=10, divisions=20, round=1, pref=riffusion_prefs, key='guidance_scale', tooltip="Large => better quality and relavancy to text; Small => better diversity")
    prompt = TextField(label="Musical Text Prompt", value=riffusion_prefs['prompt'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'prompt'), col={'md':9})
    negative_prompt = TextField(label="Negative Prompt", value=riffusion_prefs['negative_prompt'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'negative_prompt'), col={'md':3})
    max_size_row = SliderRow(label="Max Size", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=riffusion_prefs, key='max_size')
    batch_folder_name = TextField(label="Batch Folder Name", value=riffusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=riffusion_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    batch_size = NumberPicker(label="Batch Size:   ", min=1, max=5, value=riffusion_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    seed = TextField(label="Seed", value=riffusion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)
    page.riffusion_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.riffusion_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("💽  Riffusion Spectrogram Sound Modeling", "Stable Diffusion for real-time music generation...", actions=[save_default(riffusion_prefs, ['wav_path']), IconButton(icon=icons.HELP, tooltip="Help with Audio LDM-TTS Settings", on_click=riffusion_help)]),
        ResponsiveRow([prompt, negative_prompt]),
        #audio_file,
        #duration_row,
        guidance,
        steps_row,
        max_size_row,
        Row([batch_size, seed]),
        Row([batch_folder_name, file_prefix]),
        ElevatedButton(content=Text("👨‍🎤️  Run Riffusion", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_riffusion(page)),
        page.riffusion_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

mubert_prefs = {
    'prompt': '',
    'duration': 30,
    'is_loop': False,
    'email': '',
    'tags': '',
    'tag_list': '',
    'batch_folder_name': '',
    'file_prefix': 'mu-'
}

def buildMubert(page):
    global mubert_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            mubert_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_mubert_output(o):
        page.mubert_output.controls.append(o)
        page.mubert_output.update()
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.mubert_output.controls = []
        page.mubert_output.update()
        clear_button.visible = False
        clear_button.update()
    def mubert_help(e):
        def close_mubert_dlg(e):
          nonlocal mubert_help_dlg
          mubert_help_dlg.open = False
          page.update()
        mubert_help_dlg = AlertDialog(title=Text("💁   Help with Mubert"), content=Column([
            Text("Mubert AI is a cutting-edge tool that allows you to create realistic and infinite music by learning from a large dataset of existing music. The result is a high-quality and original music stream that sounds like it was composed by a professional musician."),
            Text("This uses the Mubert Developer API which is actually quite expensive to use, so don't be surprised if your generation fails because the API Key has used it's monthy quota. Sorry, it's not free/opensource..."),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🥁  Gimme a Beat... ", on_click=close_mubert_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(mubert_help_dlg)
        mubert_help_dlg.open = True
        page.update()
    prompt = TextField(label="Prompt to generate a track (genre, theme, etc.)", value=mubert_prefs['prompt'], filled=True, multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'prompt'))
    duration_row = SliderRow(label="Duration", min=1, max=250, divisions=249, suffix="s", pref=mubert_prefs, key='duration')
    is_loop = Checkbox(label="Is Audio Loop   ", value=mubert_prefs['is_loop'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'is_loop'))
    email = TextField(label="Email Address (for API use)", keyboard_type=KeyboardType.EMAIL, value=mubert_prefs['email'], on_change=lambda e:changed(e,'email'))
    batch_folder_name = TextField(label="Batch Folder Name", value=mubert_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=mubert_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #page.mubert_songs = ResponsiveRow(controls=[])
    #for v in mubert_prefs['tag_list']:
    #  page.mubert_songs.controls.append(Checkbox(label=v, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
    page.mubert_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.mubert_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎼  Mubert Music Generator", "AI music is generated by Mubert API. Pretty good grooves...(may not work if API maxed)", actions=[save_default(mubert_prefs), IconButton(icon=icons.HELP, tooltip="Help with Mubert Settings", on_click=mubert_help)]),
        prompt,
        duration_row,
        is_loop,
        email,
        Row([batch_folder_name, file_prefix]),
        #Row([Text("Select one or more tags:", weight=FontWeight.BOLD), Text("(none for random or custom)")]),
        #page.mubert_songs,
        ElevatedButton(content=Text("🎻  Run Mubert Music", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_mubert(page)),
        page.mubert_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

whisper_prefs = {
    'audio_file': '',
    'model_size': 'base',
    'trim_audio': True,
    'AI_engine': 'ChatGPT-3.5 Turbo',
    'AI_temperature': 0.7,
    'simple_transcribe': True,
    'detect_language': False,
    'reformat': False,
    'rewrite': False,
    'summarize': False,
    'describe': False,
    'article': False,
    'keypoints': False,
    'keywords': False,
    'translate': False,
    'to_language': '',
}
whisper_requests = {
        'reformat': 'Format the following text to have correct punctuations, grammar and sentence structure.',
        'rewrite': 'Rewrite and edit the following text to be more proper.',
        'summarize': 'Write a summary of the following text to reflect the most important information.',
        'describe': 'Write a description of the following text to explain the subject and discussed topics.',
        'article': 'Write an article based on the following text, using the interesting points to discuss.',
        'keypoints': 'Outline the key points of the following text to summarize the useful information contained.',
        'keywords': 'List the relevant keywords and phrases of the following text for SEO search engine optimization.',
        'translate': 'Translate the following text into ',
    }

def buildWhisper(page):
    global prefs, whisper_prefs, whisper_requests
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            whisper_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.whisper_output.controls = []
        page.whisper_output.update()
        clear_button.visible = False
        clear_button.update()
    def whisper_help(e):
        def close_whisper_dlg(e):
          nonlocal whisper_help_dlg
          whisper_help_dlg.open = False
          page.update()
        whisper_help_dlg = AlertDialog(title=Text("💁   Help with Whisper-AI"), content=Column([
            Text("Whisper is an automatic speech recognition (ASR) system trained on 680,000 hours of multilingual and multitask supervised data collected from the web. We show that the use of such a large and diverse dataset leads to improved robustness to accents, background noise and technical language. Moreover, it enables transcription in multiple languages, as well as translation from those languages into English. We are open-sourcing models and inference code to serve as a foundation for building useful applications and for further research on robust speech processing."),
            Text("The Whisper architecture is a simple end-to-end approach, implemented as an encoder-decoder Transformer. Input audio is split into 30-second chunks, converted into a log-Mel spectrogram, and then passed into an encoder. A decoder is trained to predict the corresponding text caption, intermixed with special tokens that direct the single model to perform tasks such as language identification, phrase-level timestamps, multilingual speech transcription, and to-English speech translation."),
            Markdown("[Project Page](https://openai.com/research/whisper) | [GitHub Code](https://github.com/openai/whisper)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🙉  Understood... ", on_click=close_whisper_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(whisper_help_dlg)
        whisper_help_dlg.open = True
        page.update()
    #audio_file = TextField(label="Input Media File (MP3, MP4, AVI, URL or YouTube URL)", value=whisper_prefs['audio_file'], on_change=lambda e:changed(e,'audio_file'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_audio))
    audio_file = FileInput(label="Input Media File (MP3, MP4, AVI, URL or YouTube URL)", pref=whisper_prefs, key='audio_file', ftype="media", page=page)
    model_size = Dropdown(label="Whisper Model Size", width=200, options=[dropdown.Option("tiny"), dropdown.Option("base"), dropdown.Option("small"), dropdown.Option("medium"), dropdown.Option("large"), dropdown.Option("large-v2"), dropdown.Option("large-v3")], value=whisper_prefs['model_size'], on_change=lambda e: changed(e, 'model_size'))
    trim_audio = Checkbox(label="Trim Audio to 30s", value=whisper_prefs['trim_audio'], tooltip="Prefers a short audio chunk", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'trim_audio'))
    AI_engine = Dropdown(label="AI Engine", width=200, options=[dropdown.Option("OpenAI GPT-3"), dropdown.Option("ChatGPT-3.5 Turbo"), dropdown.Option("OpenAI GPT-4"), dropdown.Option("GPT-4 Turbo"), dropdown.Option("GPT-4o"), dropdown.Option("Google Gemini")], value=whisper_prefs['AI_engine'], on_change=lambda e: changed(e, 'AI_engine'))
    AI_temperature = SliderRow(label="AI Temperature", min=0, max=1, divisions=10, round=1, expand=True, pref=whisper_prefs, key="AI_temperature")
    reformat = Checkbox(label="Reformat grammar and structure of transcript", value=whisper_prefs['reformat'], tooltip=whisper_requests['reformat'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'reformat'))
    rewrite = Checkbox(label="Rewrite and edit content of transcript", value=whisper_prefs['rewrite'], tooltip=whisper_requests['rewrite'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'rewrite'))
    summarize = Checkbox(label="Summarize the information of transcript", value=whisper_prefs['summarize'], tooltip=whisper_requests['summarize'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'summarize'))
    describe = Checkbox(label="Describe the subject of transcript", value=whisper_prefs['describe'], tooltip=whisper_requests['describe'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'describe'))
    article = Checkbox(label="Write article about content of transcript", value=whisper_prefs['article'], tooltip=whisper_requests['article'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'article'))
    keypoints = Checkbox(label="Outline key points of transcript", value=whisper_prefs['keypoints'], tooltip=whisper_requests['keypoints'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'keypoints'))
    keywords = Checkbox(label="List SEO Keywords in transcript", value=whisper_prefs['keywords'], tooltip=whisper_requests['keywords'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'keywords'))
    translate = Checkbox(label="Translate transcript", value=whisper_prefs['translate'], tooltip=whisper_requests['translate'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'translate'))
    to_language = TextField(label="to Language", value=whisper_prefs['to_language'], width=120, on_change=lambda e:changed(e,'to_language'))
    translate_to = Container(Row([translate, to_language], vertical_alignment=CrossAxisAlignment.START), col={'md':6, 'lg':4, 'xl':3})
    page.whisper_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.whisper_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🧏  OpenAI Whisper-AI Speech-To-Text", "Generate Text Transcriptions from Speech Recordings, then optionally process text with GPT...", actions=[save_default(whisper_prefs, ['audio_file']), IconButton(icon=icons.HELP, tooltip="Help with Audio Diffusion-TTS Settings", on_click=whisper_help)]),
        audio_file,
        Row([model_size, trim_audio]),
        Row([AI_engine, AI_temperature]),
        ResponsiveRow([reformat, rewrite, summarize, describe, article, keypoints, keywords, translate_to]),
        ElevatedButton(content=Text("👂  Run Whisper", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_whisper(page)),
        page.whisper_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

voice_fixer_prefs = {
    'audio_file': '',
    'mode': 0,
    'mode_0': True,
    'mode_1': False,
    'mode_2': False,
    'audio_name': '',
    'wav_path': '',
    'batch_folder_name': '',
    'file_prefix': 'voicefixer-',
}

def buildVoiceFixer(page):
    global prefs, voice_fixer_prefs, voice_fixer_requests
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            voice_fixer_prefs[pref] = int(e.control.value) if ptype == "int" else float(e.control.value) if ptype == "float" else e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def clear_output(e):
        play_snd(Snd.DELETE, page)
        page.voice_fixer_output.controls = []
        page.voice_fixer_output.update()
        clear_button.visible = False
        clear_button.update()
    def voice_fixer_help(e):
        def close_voice_fixer_dlg(e):
          nonlocal voice_fixer_help_dlg
          voice_fixer_help_dlg.open = False
          page.update()
        voice_fixer_help_dlg = AlertDialog(title=Text("💁   Help with VoiceFixer"), content=Column([
            Text("Voicefixer aims to restore human speech regardless how serious it's degraded. It can handle noise, reveberation, low resolution (2kHz~44.1kHz) and clipping (0.1-1.0 threshold) effect within one model.  This package provides a pretrained Voicefixer, which is build based on neural vocoder and pretrained 44.1k universal speaker-independent neural vocoder."),
            Markdown("[Arxiv Paper](https://arxiv.org/pdf/2109.13731.pdf) | [GitHub Code](https://github.com/haoheliu/voicefixer) | [Demo Page](https://haoheliu.github.io/demopage-voicefixer/)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🦜  Loud & clear... ", on_click=close_voice_fixer_dlg)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(voice_fixer_help_dlg)
        voice_fixer_help_dlg.open = True
        page.update()
    audio_file = FileInput(label="Input Audio File (WAV, MP3, URL or YouTube URL)", pref=voice_fixer_prefs, key='audio_file', ftype="audio", page=page)
    #model_size = Dropdown(label="VoiceFixer Model Size", width=200, options=[dropdown.Option("tiny"), dropdown.Option("base"), dropdown.Option("small"), dropdown.Option("medium"), dropdown.Option("large")], value=voice_fixer_prefs['model_size'], on_change=lambda e: changed(e, 'model_size'))
    #trim_audio = Checkbox(label="Trim Audio to 30s", value=voice_fixer_prefs['trim_audio'], tooltip="Prefers a short audio chunk", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'trim_audio'))
    mode_0 = Checkbox(label="Mode 0", value=voice_fixer_prefs['mode_0'], tooltip="", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'mode_0'))
    mode_1 = Checkbox(label="Mode 1", value=voice_fixer_prefs['mode_1'], tooltip="", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'mode_1'))
    mode_2 = Checkbox(label="Mode 2", value=voice_fixer_prefs['mode_2'], tooltip="", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'mode_2'))
    audio_name = TextField(label="Audio File Name", value=voice_fixer_prefs['audio_name'], on_change=lambda e:changed(e,'audio_name'))
    batch_folder_name = TextField(label="Batch Folder Name", value=voice_fixer_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=voice_fixer_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    page.voice_fixer_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.voice_fixer_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("💬  Voice Fixer - Speech Restoration with Neural Vocoder", "Cleans up bad vocals and fixes the unwanted noise...", actions=[save_default(voice_fixer_prefs, ['audio_file']), IconButton(icon=icons.HELP, tooltip="Help with Audio Diffusion-TTS Settings", on_click=voice_fixer_help)]),
        audio_file,
        Row([mode_0, mode_1, mode_2]),
        #Row([model_size, trim_audio]),
        Row([audio_name, batch_folder_name, file_prefix]),
        ElevatedButton(content=Text("🗣  Run VoiceFixer", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_voice_fixer(page)),
        page.voice_fixer_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c


def buildCustomModelManager(page):
    global prefs
    def title_header(title, type):
        return Row([Text(title, theme_style=TextThemeStyle.BODY_LARGE),
                    ft.FilledButton(f"Add {type} Model", on_click=lambda e: add_model(type))], alignment=MainAxisAlignment.SPACE_BETWEEN)
    def model_tile(name, path, token, type, weights=""):
        return ListTile(title=Row([Text(name, weight=FontWeight.BOLD), Text(path + (f" - {weights}" if bool(weights) else "")), Text(token)], alignment=MainAxisAlignment.SPACE_BETWEEN), data=type, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[PopupMenuItem(icon=icons.EDIT, text="Edit Custom Model", on_click=lambda e: edit_model(e, name), data=type),
                 PopupMenuItem(icon=icons.DELETE, text="Delete Custom Model", on_click=lambda e: del_model(e, name), data=type)]), on_click=lambda e: edit_model(e, name))
    def load_customs():
        #custom_models.controls.append(title_header("Stable Diffusion Finetuned Models", type="Finetuned"))
        for mod in prefs['custom_models']:
            token = mod['prefix'] if 'prefix' in mod else ""
            custom_models.controls.append(model_tile(mod['name'], mod['path'], token, "Finetuned"))
            #page.custom_models.controls.append(ListTile(title=Row([Text(f".{slash}{dir.rpartition(slash)[2]}", weight=FontWeight.BOLD), Text("")], alignment=MainAxisAlignment.SPACE_BETWEEN), data=dir, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
            #  items=[PopupMenuItem(icon=icons.DELETE, text="Delete Custom Model", on_click=del_model, data=dir)])))
        #custom_models.update()
        for mod in prefs['custom_LoRA_models']:
            token = mod['prefix'] if 'prefix' in mod else ""
            weights = mod['weights'] if 'weights' in mod else ""
            custom_LoRA_models.controls.append(model_tile(mod['name'], mod['path'], token, "LoRA", weights=weights))
        #custom_LoRA_models.update()
        for mod in prefs['custom_SDXL_LoRA_models']:
            token = mod['prefix'] if 'prefix' in mod else ""
            weights = mod['weights'] if 'weights' in mod else ""
            custom_SDXL_LoRA_models.controls.append(model_tile(mod['name'], mod['path'], token, "SDXL LoRA", weights=weights))
        for mod in prefs['custom_CivitAI_LoRA_models']:
            token = mod['inject_trigger'] if 'inject_trigger' in mod else ""
            clip = mod['clip'] if 'clip' in mod else 1
            custom_CivitAI_LoRA_models.controls.append(model_tile(mod['name'], mod['model'], token, "CivitAI LoRA", weights="Clip: "+clip))
        for mod in prefs['custom_dance_diffusion_models']:
            token = mod['prefix'] if 'prefix' in mod else ""
            custom_dance_diffusion_models.controls.append(model_tile(mod['name'], mod['path'], token, "DanceDiffusion"))
        #custom_dance_diffusion_models.update()
        for mod in prefs['tortoise_custom_voices']:
            token = mod['prefix'] if 'prefix' in mod else ""
            tortoise_custom_voices.controls.append(model_tile(mod['name'], mod['folder'], token, "Tortoise"))
        #tortoise_custom_voices.update()
    def model_list(type):
        if type == "Finetuned":
            return prefs["custom_models"]
        elif type == "LoRA":
            return prefs["custom_LoRA_models"]
        elif type == "SDXL LoRA":
            return prefs["custom_SDXL_LoRA_models"]
        elif type == "CivitAI LoRA":
            return prefs["custom_CivitAI_LoRA_models"]
        elif type == "DanceDiffusion":
            return prefs["custom_dance_diffusion_models"]
        elif type == "Tortoise":
            return prefs["tortoise_custom_voices"]
    def edit_model(e, name):
        #name = e.control.title.controls[0].value
        #path = e.control.title.controls[1].value
        type = e.control.data
        mod = None
        for sub in model_list(type):
            if sub['name'] == name:
                mod = sub
                break
        if mod is None: return
        path = mod['model' if type == "CivitAI LoRA" else 'path' if type != "Tortoise" else 'folder']
        if type == "CivitAI LoRA":
            weights = mod['clip']
        elif 'LoRA' in type:
            weights = mod['weights'] if 'LoRA' in type and 'weights' in mod else ""
        else: weights = None
        #print(str(mod))
        def close_dlg(e):
            dlg_edit.open = False
            page.update()
        def save_model(e):
            nonlocal weights
            if type == "Finetuned":
                for m in custom_models.controls:
                    if m.title.controls[0].value == name:
                        m.title.controls[0].value = model_name.value
                        m.title.controls[1].value = model_path.value
                        m.update()
            elif type == "LoRA":
                for m in custom_LoRA_models.controls:
                    if m.title.controls[0].value == name:
                        m.title.controls[0].value = model_name.value
                        m.title.controls[1].value = model_path.value
                        m.title.controls[2].value = model_weights.value
                        m.update()
            elif type == "SDXL LoRA":
                for m in custom_SDXL_LoRA_models.controls:
                    if m.title.controls[0].value == name:
                        m.title.controls[0].value = model_name.value
                        m.title.controls[1].value = model_path.value
                        m.title.controls[2].value = model_weights.value
                        m.update()
            elif type == "CivitAI LoRA":
                for m in custom_CivitAI_LoRA_models.controls:
                    if m.title.controls[0].value == name:
                        m.title.controls[0].value = model_name.value
                        m.title.controls[1].value = model_path.value
                        m.title.controls[2].value = "Clip: "+model_weights.value
                        m.update()
            elif type == "DanceDiffusion":
                for m in custom_dance_diffusion_models.controls:
                    if m.title.controls[0].value == name:
                        m.title.controls[0].value = model_name.value
                        m.title.controls[1].value = model_path.value
                        m.update()
            elif type == "Tortoise":
                for m in tortoise_custom_voices.controls:
                    if m.title.controls[0].value == name:
                        m.title.controls[0].value = model_name.value
                        m.title.controls[1].value = model_path.value
                        m.update()
            mod['name'] = model_name.value
            mod['model' if type == "CivitAI LoRA" else 'path' if type != "Tortoise" else 'folder'] = model_path.value
            if weights != None:
                mod['clip' if type == "CivitAI LoRA" else 'weights'] = model_weights.value
            dlg_edit.open = False
            e.control.update()
            page.update()
        model_name = TextField(label="Custom Model Name", value=name)
        model_path = TextField(label="Model Path", value=path)
        model_weights = TextField(label="Model Weights (safetensor)", value=weights or "")
        if type == "CivitAI LoRA":
            model_path.label = "CivitAI Model ID #"
            model_weights.label = "LoRA Clip Skip"
            model_weights.value = "1"
        dlg_edit = AlertDialog(modal=False, title=Text(f"🧳 Edit {type} Model Info"), content=Container(
          Column([model_name, model_path, model_weights if weights != None else Container(content=None)], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO)), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Model ", size=19, weight=FontWeight.BOLD), on_click=save_model)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(dlg_edit)
        dlg_edit.open = True
        page.update()
    def add_model(type):
        mod_list = model_list(type)
        def close_dlg(e):
            dlg_edit.open = False
            page.update()
        def save_model(e):
            mod = {'name': model_name.value, 'path' if type != "Tortoise" else 'folder': model_path.value}
            if 'LoRA' in type:
                mod['weights'] = model_weights.value
            if type == "CivitAI LoRA":
                mod = {'name': model_name.value, 'model': model_path.value, 'clip': model_weights.value}
            mod_list.append(mod)
            if type == "Finetuned":
                custom_models.controls.append(model_tile(mod['name'], mod['path'], "", "Finetuned"))
                custom_models.update()
            elif type == "LoRA":
                weights = mod['weights'] if 'weights' in mod else ""
                custom_LoRA_models.controls.append(model_tile(mod['name'], mod['path'], "", "LoRA", weights=weights))
                custom_LoRA_models.update()
            elif type == "SDXL LoRA":
                weights = mod['weights'] if 'weights' in mod else ""
                custom_SDXL_LoRA_models.controls.append(model_tile(mod['name'], mod['path'], "", "SDXL LoRA", weights=weights))
                custom_SDXL_LoRA_models.update()
            elif type == "CivitAI LoRA":
                weights = mod['clip'] if 'clip' in mod else "1"
                custom_CivitAI_LoRA_models.controls.append(model_tile(mod['name'], mod['model'], "", "CivitAI LoRA", weights="Clip: "+weights))
                custom_CivitAI_LoRA_models.update()
            elif type == "DanceDiffusion":
                custom_dance_diffusion_models.controls.append(model_tile(mod['name'], mod['path'], "", "DanceDiffusion"))
                custom_dance_diffusion_models.update()
            elif type == "Tortoise":
                tortoise_custom_voices.controls.append(model_tile(mod['name'], mod['folder'], "", "Tortoise"))
                tortoise_custom_voices.update()
            dlg_edit.open = False
            e.control.update()
            page.update()
        model_name = TextField(label="Custom Model Name")
        model_path = TextField(label="Model Path")
        model_weights = TextField(label="Model Weights (safetensor)")
        if type == "CivitAI LoRA":
            model_path.label = "CivitAI Model ID #"
            model_weights.label = "LoRA Clip Skip"
            model_weights.value = "1"
        dlg_edit = AlertDialog(modal=False, title=Text(f"🧳 Add Custom {type} Model"), content=Container(Column([model_name, model_path, model_weights if 'LoRA' in type else Container(content=None)], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO)), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Model ", size=19, weight=FontWeight.BOLD), on_click=save_model)], actions_alignment=MainAxisAlignment.END)
        page.overlay.append(dlg_edit)
        dlg_edit.open = True
        page.update()
    def del_model(e, name):
        type = e.control.data
        mod_list = model_list(type)
        for i, sub in enumerate(mod_list):
            if sub['name'] == name:
                mod = sub
                del mod_list[i]
                break
        if type == "Finetuned":
            for i, l in enumerate(custom_models.controls):
                if l.title.controls[0].value == name:
                    del custom_models.controls[i]
                    custom_models.update()
                    break
        elif type == "LoRA":
            for i, l in enumerate(custom_LoRA_models.controls):
                if l.title.controls[0].value == name:
                    del custom_LoRA_models.controls[i]
                    custom_LoRA_models.update()
                    break
        elif type == "SDXL LoRA":
            for i, l in enumerate(custom_SDXL_LoRA_models.controls):
                if l.title.controls[0].value == name:
                    del custom_SDXL_LoRA_models.controls[i]
                    custom_SDXL_LoRA_models.update()
                    break
        elif type == "CivitAI LoRA":
            for i, l in enumerate(custom_CivitAI_LoRA_models.controls):
                if l.title.controls[0].value == name:
                    del custom_CivitAI_LoRA_models.controls[i]
                    custom_CivitAI_LoRA_models.update()
                    break
        elif type == "DanceDiffusion":
            for i, l in enumerate(custom_dance_diffusion_models.controls):
                if l.title.controls[0].value == name:
                    del custom_dance_diffusion_models.controls[i]
                    custom_dance_diffusion_models.update()
                    break
        elif type == "Tortoise":
            for i, l in enumerate(tortoise_custom_voices.controls):
                if l.title.controls[0].value == name:
                    del tortoise_custom_voices.controls[i]
                    tortoise_custom_voices.update()
                    break
        play_snd(Snd.DELETE, e.page)
    def update_list(e):
        page.finetuned_model.options.clear()
        for cust in model_list("Finetuned"):
            page.finetuned_model.options.append(dropdown.Option(cust["name"]))
        for mod in finetuned_models:
            page.finetuned_model.options.append(dropdown.Option(mod["name"]))
        try: page.finetuned_model.update()
        except: pass
        page.LoRA_model.options.clear()
        for cust in model_list("LoRA"):
            page.LoRA_model.options.append(dropdown.Option(cust["name"]))
        for mod in LoRA_models:
            page.LoRA_model.options.append(dropdown.Option(mod["name"]))
        page.LoRA_model.options.append(dropdown.Option("Custom LoRA Path"))
        try: page.LoRA_model.update()
        except: pass
        page.SDXL_LoRA_model.options.clear()
        for cust in model_list("SDXL LoRA"):
            page.SDXL_LoRA_model.options.append(dropdown.Option(cust["name"]))
        for mod in SDXL_LoRA_models:
            page.SDXL_LoRA_model.options.append(dropdown.Option(mod["name"]))
        page.SDXL_LoRA_model.options.append(dropdown.Option("Custom SDXL LoRA Path"))
        try: page.SDXL_LoRA_model.update()
        except: pass
        page.AIHorde_lora_layer.options.clear()
        page.AIHorde_lora_layer.options.append(dropdown.Option("Custom"))
        for cust in model_list("CivitAI LoRA"):
            page.AIHorde_lora_layer.options.append(dropdown.Option(cust["name"]))
        for mod in CivitAI_LoRAs:
            page.AIHorde_lora_layer.options.append(dropdown.Option(mod['name']))
        try: page.AIHorde_lora_layer.update()
        except: pass
        page.community_dance_diffusion_model.options.clear()
        for cust in model_list("DanceDiffusion"):
            page.community_dance_diffusion_model.options.append(dropdown.Option(cust["name"]))
        for mod in community_dance_diffusion_models:
            page.community_dance_diffusion_model.options.append(dropdown.Option(mod["name"]))
        try: page.community_dance_diffusion_model.update()
        except: pass
        page.tortoise_voices.controls.clear()
        for v in tortoise_prefs['voices']:
            page.tortoise_voices.controls.append(Checkbox(label=v, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
        if len(prefs['tortoise_custom_voices']) > 0:
            for custom in prefs['tortoise_custom_voices']:
                page.tortoise_voices.controls.append(Checkbox(label=custom['name'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
        try: page.tortoise_voices.update()
        except: pass
        save_settings_file(e.page)
        play_snd(Snd.DROP, page)
    custom_models = Column([], spacing=0)
    custom_LoRA_models = Column([], spacing=0)
    custom_SDXL_LoRA_models = Column([], spacing=0)
    custom_CivitAI_LoRA_models = Column([], spacing=0)
    custom_dance_diffusion_models = Column([], spacing=0)
    tortoise_custom_voices = Column([], spacing=0)
    load_customs()
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🧧   Manage your Saved Custom Models", "Add or Edit your favorite models from HuggingFace, URL or Local Path"),
        title_header("Custom Finetuned Models", "Finetuned"),
        custom_models,
        title_header("Custom LoRA Models", "LoRA"),
        custom_LoRA_models,
        title_header("Custom SDXL LoRA Models", "SDXL LoRA"),
        custom_SDXL_LoRA_models,
        title_header("Custom CivitAI LoRA Models", "CivitAI LoRA"),
        custom_CivitAI_LoRA_models,
        title_header("Custom Tortoise Voice Models", "Tortoise"),
        tortoise_custom_voices,
        title_header("Custom Dance Diffusion Models", "DanceDiffusion"),
        custom_dance_diffusion_models,
        ElevatedButton(content=Text("🛄  Update Custom Dropdowns", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=update_list),
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

def get_directory_size(directory):
    total = 0
    for entry in os.scandir(directory):
        if entry.is_file():
            total += entry.stat().st_size
        elif entry.is_dir():
            try:
                total += get_directory_size(entry.path)
            except FileNotFoundError:
                pass
    return total
def convert_bytes(num):
    step_unit = 1000.0 #1024 bad the size
    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:
        if num < step_unit:
            return "%3.1f %s" % (num, x)
        num /= step_unit

def buildCachedModelManager(page):
    global prefs
    def scan_cache(e):
      if not bool(prefs['cache_dir']):
        alert_msg(page, "You haven't set a Cache Directory in your Settings...")
        return
      elif not os.path.isdir(prefs['cache_dir']):
        alert_msg(page, "The Cache Directory in your Settings can't be found...")
        return
      if len(page.cached_folders.controls) > 1:
        page.cached_folders.controls.clear()
        page.cached_folders.update()
      page.cached_folders.controls.append(Installing(f"Scanning {prefs['cache_dir']}"))
      dirs = [f.path for f in os.scandir(prefs['cache_dir']) if f.is_dir()]
      del page.cached_folders.controls[-1]
      page.cached_folders.update()
      for dir in dirs:
        page.cached_folders.controls.append(ListTile(title=Row([Text(f".{slash}{dir.rpartition(slash)[2]}", weight=FontWeight.BOLD), Text("")], alignment=MainAxisAlignment.SPACE_BETWEEN), data=dir, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[PopupMenuItem(icon=icons.DELETE, text="Delete Model Directory", on_click=del_dir, data=dir)])))
        page.cached_folders.update()
      for l in page.cached_folders.controls:
        size = convert_bytes(get_directory_size(l.data))
        l.title.controls[1].value = size
        l.title.controls[1].update()
        page.cached_folders.update()
    def del_dir(e):
      dir = e.control.data
      shutil.rmtree(dir, ignore_errors=True)
      for i, l in enumerate(page.cached_folders.controls):
        if l.data == dir:
          del page.cached_folders.controls[i]
          page.cached_folders.update()
          break
      play_snd(Snd.DELETE, e.page)
    page.cached_folders = Column([])
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🗂️   Manage your Cache Directory Saved Models", "If you're cacheing your model files, it can fill up your drive space quickly, so you can trim the fat as needed... Redownloads when used."),
        page.cached_folders,
        ElevatedButton(content=Text("🔍  Scan Cache Directory", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=scan_cache),
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

def get_var_name(var):
    for name, value in globals().items():
        if value is var:
            return name

if 'default_prefs' in prefs:
    for pref_name, saved_prefs in prefs['default_prefs'].items():
        if isinstance(saved_prefs, dict):
            globals()[pref_name].update(saved_prefs)

def save_default(save_prefs, exclude=[]):
    global prefs
    def save_pref(e):
        pref_name = get_var_name(save_prefs)
        if 'default_prefs' not in prefs:
            prefs['default_prefs'] = {}
        if pref_name in prefs['default_prefs']:
            prefs['default_prefs'][pref_name].update(save_prefs)
        else:
            prefs['default_prefs'][pref_name] = save_prefs
        if exclude:
            for ex in exclude:
                if ex in prefs['default_prefs'][pref_name]:
                    del prefs['default_prefs'][pref_name][ex]
        #print(f"Saved {pref_name} = {prefs['default_prefs'][pref_name]}")
        save_settings_file(e.page)
    def clear_pref(e): # Todo: Add to right click or long press
        pref_name = get_var_name(save_prefs)
        if pref_name in prefs['default_prefs']:
            del prefs['default_prefs'][pref_name]
        save_settings_file(e.page)
        play_snd(Snd.DELETE, e.page)
    return IconButton(icon=icons.SAVE, tooltip="Save as Default Preference", on_click=save_pref)

#if 'pipe' in locals():
#    clear_pipes()
use_custom_scheduler = False
retry_attempts_if_NSFW = 3
unet = None
pipe = None
pipe_img2img = None
pipe_SD = None
pipe_SDXL = None
pipe_SDXL_refiner = None
pipe_SD3 = None
compel_proc = None
compel_base = None
compel_refiner = None
pipe_interpolation = None
pipe_clip_guided = None
pipe_conceptualizer = None
pipe_repaint = None
pipe_imagic = None
pipe_composable = None
pipe_safe = None
pipe_versatile = None
pipe_versatile_text2img = None
pipe_versatile_variation = None
pipe_versatile_dualguided = None
pipe_upscale = None
pipe_depth = None
pipe_image_variation = None
pipe_semantic = None
pipe_DiffEdit = None
pipe_EDICT = None
text_encoder_EDICT = None
pipe_null_text = None
pipe_unCLIP = None
pipe_unCLIP_image_variation = None
pipe_unCLIP_interpolation = None
pipe_unCLIP_image_interpolation = None
pipe_wuerstchen = None
pipe_stable_cascade_prior = None
pipe_stable_cascade_decoder = None
pipe_pixart_alpha = None
pipe_pixart_alpha_encoder = None
pipe_pixart_sigma = None
pipe_pixart_sigma_encoder = None
pipe_hunyuan = None
pipe_lumina = None
pipe_kolors = None
pipe_auraflow = None
pipe_layer_diffusion, ld_text_encoder, ld_text_encoder_2, ld_vae, ld_unet, ld_transparent_decoder, ld_transparent_encoder = [None] * 7
pipe_differential_diffusion = None
pipe_magic_mix = None
pipe_paint_by_example = None
pipe_instruct_pix2pix = None
pipe_ledits = None
pipe_alt_diffusion = None
pipe_alt_diffusion_img2img = None
pipe_demofusion = None
pipe_SAG = None
pipe_attend_and_excite = None
pipe_lmd_plus = None
pipe_lcm = None
pipe_lcm_interpolation = None
pipe_instaflow = None
pipe_PAG = None
pipe_task_matrix = None
pipe_ldm3d = None
pipe_ldm3d_upscale = None
pipe_svd = None
pipe_diffsynth, pipe_diffsynth_image, diffsynth_model_manager = [None] * 3
pipe_animatediff_img2video = None
pipe_animatediff_sdxl = None
pipe_pia = None
pipe_easyanimate, easyanimate_transformer = [None] *2
pipe_i2vgen_xl = None
pipe_panorama = None
pipe_DiT = None
pipe_dance = None
pipe_kandinsky = None
pipe_kandinsky_prior = None
pipe_tortoise_tts = None
pipe_audio_ldm = None
pipe_audio_ldm2 = None
pipe_music_ldm = None
pipe_stable_audio = None
config_stable_audio = None
pipe_riffusion = None
pipe_audio_diffusion = None
pipe_music_gen = None
pipe_music_lang = None
pipe_voice_fixer = None
pipe_whisper = None
pipe_text_to_video = None
pipe_text_to_video_zero = None
pipe_video_to_video = None
pipe_fresco_v2v = None
pipe_latte = None
pipe_open_sora_plan = None
pipe_video_infinity = None
pipe_infinite_zoom = None
pipe_deepfloyd = None
pipe_deepfloyd2 = None
pipe_deepfloyd3 = None
pipe_amused = None
pipe_gpt2 = None
pipe_distil_gpt2 = None
pipe_superprompt = None
tokenizer_superprompt = None
pipe_background_remover = None
pipe_shap_e = None
pipe_zoe_depth = None
pipe_marigold_depth = None
pipe_tripo = None
pipe_crm = None
crm_rembg_session=None
pipe_instantmesh = None
instantmesh_model = None
instantmesh_rembg_session=None
pipe_splatter_image = None
splatter_image_model = None
splatter_image_rembg_session=None
pipe_stable_lm = None
tokenizer_stable_lm = None
depth_estimator = None
fuyu_tokenizer = None
fuyu_model = None
fuyu_processor = None
moondream2_model = None
moondream2_tokenizer = None
pipe_blip_diffusion = None
pipe_anytext = None
pipe_reference = None
pipe_ip_adapter = None
pipe_hd_painter = None
pipe_controlnet_qr = None
pipe_controlnet_segment = None
pipe_kandinsky_controlnet_prior = None
pipe_controlnet = None
controlnet = None
controlnet_models = {"Canny Map Edge":None, "Scribble":None, "OpenPose":None, "Depth":None, "HED":None, "M-LSD":None, "Normal Map":None, "Segmented":None, "LineArt":None, "Shuffle":None, "Instruct Pix2Pix":None}
controlnet_xl_models = {"Canny Map Edge":None, "OpenPose":None, "Depth":None, "Softedge":None, "Segmented":None, "LineArt":None, "Shuffle":None, "Instruct Pix2Pix":None}
controlnet_sd3_models = {"Canny Map Edge":None, "OpenPose":None, "Depth":None, "Softedge":None, "Segmented":None, "LineArt":None, "Shuffle":None, "Tile":None}
controlnet_xs_models = {"Canny Map Edge":None, "OpenPose":None, "Depth":None, "Softedge":None, "Segmented":None, "LineArt":None, "Shuffle":None, "Instruct Pix2Pix":None}
controlnet_hunyuan_models = {"Canny Map Edge":None, "OpenPose":None, "Depth":None, "Marigold Depth":None}
stability_api = None
safety = {'safety_checker':None, 'requires_safety_checker':False, 'feature_extractor':None} if prefs['disable_nsfw_filter'] else {}
model_path = "CompVis/stable-diffusion-v1-4"
inpaint_model = "stabilityai/stable-diffusion-2-inpainting"
#"runwayml/stable-diffusion-inpainting"
scheduler = None
scheduler_clip = None
if is_Colab:
  from google.colab import output
  output.enable_custom_widget_manager()


def get_model(name):
  #dropdown.Option("Stable Diffusion v1.5"), dropdown.Option("Stable Diffusion v1.4", dropdown.Option("Community Finetuned Model", dropdown.Option("DreamBooth Library Model"), dropdown.Option("Custom Model Path")
  if name == "Stable Diffusion v2.1 x768":
    return {'name':'Stable Diffusion v2.1 x768', 'path':'stabilityai/stable-diffusion-2-1', 'prefix':'', 'revision': 'fp16'}
  elif name == "Stable Diffusion v2.1 x512":
    return {'name':'Stable Diffusion v2.1 x512', 'path':'stabilityai/stable-diffusion-2-1-base', 'prefix':''}
  elif name == "Stable Diffusion v2.0":
    return {'name':'Stable Diffusion v2.0', 'path':'stabilityai/stable-diffusion-2', 'prefix':'', 'revision': 'fp16'}
  elif name == "Stable Diffusion v2.0 x768":
    return {'name':'Stable Diffusion v2.0 x768', 'path':'stabilityai/stable-diffusion-2', 'prefix':'', 'revision': 'fp16'}
  elif name == "Stable Diffusion v2.0 x512":
    return {'name':'Stable Diffusion v2.0 x512', 'path':'stabilityai/stable-diffusion-2-base', 'prefix':'', 'revision': 'fp16'}
  elif name == "Stable Diffusion v1.5":
    return {'name':'Stable Diffusion v1.5', 'path':'runwayml/stable-diffusion-v1-5', 'prefix':'', 'revision': 'fp16'}
  elif name == "Stable Diffusion v1.4":
    return {'name':'Stable Diffusion v1.4', 'path':'CompVis/stable-diffusion-v1-4', 'prefix':'', 'revision': 'fp16'}
  elif name == "Community Finetuned Model":
    return get_finetuned_model(prefs['finetuned_model'])
  elif name == "DreamBooth Library Model":
    return get_dreambooth_model(prefs['dreambooth_model'])
  elif name == "Custom Model Path":
    return {'name':'Custom Model', 'path':prefs['custom_model'], 'prefix':''}
  else:
    return {'name':'', 'path':'', 'prefix':''}

def get_finetuned_model(name):
  for mod in finetuned_models:
      if mod['name'] == name:
        return mod
  for mod in prefs['custom_models']:
      if mod['name'] == name:
        return mod
  return {'name':'', 'path':'', 'prefix':''}
def get_dreambooth_model(name):
  for mod in dreambooth_models:
      if mod['name'] == name:
        return {'name':mod['name'], 'path':f'sd-dreambooth-library/{mod["name"]}', 'prefix':mod['token']}
  return {'name':'', 'path':'', 'prefix':''}
def get_LoRA_model(name):
  if name == "Custom LoRA Path":
      return {'name':"Custom LoRA Model", 'path':prefs['custom_LoRA_model'], 'weights':None}
  for mod in LoRA_models:
      if mod['name'] == name:
        return {'name':mod['name'], 'path':mod['path'], 'weights':None if 'weights' not in mod else mod['weights']}
  if len(prefs['custom_LoRA_models']) > 0:
    for mod in prefs['custom_LoRA_models']:
      if mod['name'] == name:
        return {'name':mod['name'], 'path':mod['path'], 'weights':None if 'weights' not in mod else mod['weights']}
  return {'name':'', 'path':''}
def get_SDXL_LoRA_model(name):
  if name == "Custom SDXL LoRA Path":
      return {'name':"Custom SDXL LoRA Model", 'path':prefs['custom_SDXL_LoRA_model'], 'weights':None}
  for mod in SDXL_LoRA_models:
      if mod['name'] == name:
        return {'name':mod['name'], 'path':mod['path'], 'weights':None if 'weights' not in mod else mod['weights']}
  if len(prefs['custom_SDXL_LoRA_models']) > 0:
    for mod in prefs['custom_SDXL_LoRA_models']:
      if mod['name'] == name:
        return {'name':mod['name'], 'path':mod['path'], 'weights':None if 'weights' not in mod else mod['weights']}
  return {'name':'', 'path':''}
def get_SDXL_model(name):
  if name == "Custom Model":
      safetensors = True if '.safetensors' in prefs['SDXL_custom_model'] else False
      return {'name':"Custom SDXL Model", 'path':prefs['SDXL_custom_model'], 'prefix':'', 'variant': 'fp16', 'use_safetensors': safetensors}
  for mod in SDXL_models:
      safetensors = mod['use_safetensors'] if 'use_safetensors' in mod else False
      if mod['name'] == name:
        extra = {key: mod.get(key) for key in ['variant', 'revision', 'vae'] if key in mod}
        return {'name':mod['name'], 'path':mod['path'], 'prefix':mod['prefix'] if 'prefix' in mod else '', 'use_safetensors': safetensors, **extra}
def get_SD3_model(name):
  if name == "Custom Model":
      safetensors = True if '.safetensors' in prefs['SD3_custom_model'] else False
      return {'name':"Custom SD3 Model", 'path':prefs['SD3_custom_model'], 'prefix':'', 'variant': 'fp16', 'use_safetensors': safetensors}
  for mod in SD3_models:
      safetensors = mod['use_safetensors'] if 'use_safetensors' in mod else False
      if mod['name'] == name:
        extra = {key: mod.get(key) for key in ['variant', 'revision', 'vae'] if key in mod}
        return {'name':mod['name'], 'path':mod['path'], 'prefix':mod['prefix'] if 'prefix' in mod else '', 'use_safetensors': safetensors, **extra}

def get_seed(seed, min=0, max=4294967295):
    return int(seed) if int(seed) > 0 else rnd.randint(min, max)

HFapi = None
def get_diffusers(page):
    global scheduler, model_path, prefs, status, HFapi
    torch_installed = False
    try:
        import torch
        torch_installed = True
    except:
        pass
    if torch_installed:
        if version.parse(torch.__version__) < version.parse("2.0.0"):
            torch_installed = False
    if not torch_installed:
        import importlib
        page.console_msg("Upgrading Torch 2.1.0 Packages... You may need to restart session.")
        run_process("pip uninstall --yes torch torchaudio torchvision torchtext torchdata", page=page)
        if is_Colab:
            run_process("pip install torch torchaudio torchvision torchtext torchdata", page=page)
        else: #TODO: Check OS and run platform specific
            run_process("pip install torch torchvision torchaudio torchtext torchdata --index-url https://download.pytorch.org/whl/cu121", page=page)
        import torch
        importlib.reload(torch)
    if prefs['enable_xformers']:#prefs['memory_optimization'] == 'Xformers Mem Efficient Attention':
        try:
            import xformers
            if force_update("xformers"): raise ModuleNotFoundError("Forcing update")
        except ModuleNotFoundError:
            page.console_msg("Installing FaceBook's Xformers Memory Efficient Package...")
            run_process("pip install --pre -U triton", page=page)
            run_process(f"pip install -U xformers=={'0.0.25' if upgrade_torch else '0.0.22.post7'} --index-url https://download.pytorch.org/whl/cu121", page=page)
            import xformers
            page.console_msg("Installing Hugging Face Diffusers Pipeline...")
            pass
        #run_process("pip install pyre-extensions==0.0.23", page=page)
        #run_process("pip install -i https://test.pypi.org/simple/ formers==0.0.15.dev376", page=page)
        #run_process("pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl", page=page)
        #run_process("pip install https://github.com/metrolobo/xformers_wheels/releases/download/1d31a3ac/xformers-0.0.14.dev0-cp37-cp37m-linux_x86_64.whl", page=page)
        #if install_xformers(page):
        status['installed_xformers'] = True
    page.console_msg("Installing Hugging Face Diffusers Pipeline...")
    if prefs['enable_bitsandbytes'] or (prefs['install_SD3'] and prefs['SD3_bitsandbytes_8bit']):
        try:
            import bitsandbytes
        except ModuleNotFoundError:
            page.status("...installing bitsandbytes")
            run_sp("pip install bitsandbytes", realtime=False)
            import bitsandbytes
            page.status()
            pass
        pip_install("sentencepiece")
    try:
        import imwatermark
    except ModuleNotFoundError:
        page.status("...installing invisible_watermark")
        run_sp("pip install invisible-watermark", realtime=False) #pip install --no-deps invisible-watermark>=0.2.0
        page.status()
        pass
    try:
        import peft
    except ModuleNotFoundError:
        page.status("...installing peft")
        run_sp("pip install --upgrade git+https://github.com/huggingface/peft.git", realtime=False)
        page.status()
        pass
    
    page.console_msg("Installing Hugging Face Diffusers Pipeline...")
    try:
        import transformers
        #print(f"transformers=={transformers.__version__}")
        if version.parse(transformers.__version__) == version.parse("4.21.3"): #Workaround because CLIP-Interrogator required other version
            page.status("...uninstalling transformers")
            run_process("pip uninstall -y git+https://github.com/pharmapsychotic/BLIP.git@lib#egg=blip", realtime=False)
            run_process("pip uninstall -y clip-interrogator", realtime=False)
            run_process("pip uninstall -y transformers", realtime=False)
        elif version.parse(transformers.__version__).base_version < version.parse("4.39.0").base_version:
          import importlib
          page.status(f"...uninstalling transformers {transformers.__version__}")
          run_process("pip uninstall -y transformers", realtime=False)
          page.status("...installing transformers")
          run_process("pip install --upgrade transformers", page=page)
          #run_process("pip install --upgrade git+https://github.com/huggingface/transformers.git@main#egg=transformers[sentencepiece]", page=page)
          importlib.reload(transformers)
    except ModuleNotFoundError:
        pass
    try:
        import transformers
        #if force_update("transformers"): raise ModuleNotFoundError("Forcing update")
    except ModuleNotFoundError:
        page.status("...installing transformers")
        run_process("pip install --upgrade transformers", page=page)
        #run_process("pip install --upgrade git+https://github.com/huggingface/transformers.git@main#egg=transformers[sentencepiece]", page=page)
        #run_process("pip install --upgrade transformers~=4.28", page=page)
        page.status()
        pass
    #run_process("pip install -q huggingface_hub", page=page)
    '''try:
      from huggingface_hub import notebook_login, HfApi, HfFolder, login
      from diffusers import StableDiffusionPipeline, logging
      import transformers
    except ModuleNotFoundError as e:#ModuleNotFoundError as e:'''
    try:
        import accelerate
    except ModuleNotFoundError:
        page.status("...installing Accelerate")
        #run_sp("pip install --upgrade git+https://github.com/huggingface/accelerate.git", realtime=False)
        run_sp("pip install --upgrade accelerate", realtime=False)
        import accelerate
        page.status()
        pass
    try:
        import diffusers
        if force_update("diffusers"): raise ModuleNotFoundError("Forcing update")
    except ModuleNotFoundError:
        page.status("...installing Diffusers")
        #run_process("pip install --upgrade git+https://github.com/Skquark/diffusers.git", page=page)
        run_process("pip install --upgrade git+https://github.com/Skquark/diffusers.git@main#egg=diffusers[torch]", page=page)
        page.status()
        pass
    if prefs['enable_hidiffusion']:
        try:
            import hidiffusion
        except ModuleNotFoundError:
            page.status("...installing HiDiffusion")
            run_process("pip install -upgrade hidiffusion", page=page)
            page.status()
            pass
    if prefs['enable_deepcache']:
        try:
            import DeepCache
        except ModuleNotFoundError:
            page.status("...installing DeepCache")
            run_process("pip install -upgrade DeepCache", page=page)
            page.status()
            pass
    try:
        import scipy
    except ModuleNotFoundError:
        page.status("...installing scipy")
        run_process("pip install -upgrade scipy", page=page)
        page.status()
        pass
    try:
        import ftfy
    except ModuleNotFoundError:
        page.status("...installing ftfy")
        run_process("pip install --upgrade ftfy", page=page)
        page.status()
        pass
    try:
        import safetensors
    except ModuleNotFoundError:
        page.status("...installing safetensors")
        run_process("pip install --upgrade safetensors~=0.3", page=page)
        import safetensors
        from safetensors import safe_open
        page.status()
        pass
    try:
        import ipywidgets
    except ModuleNotFoundError:
        page.status("...installing ipywidgets")
        run_process('pip install -qq "ipywidgets>=7,<8"', page=page)
        page.status()
        pass

    page.status("...setting up credentials")
    from huggingface_hub import notebook_login, HfApi, HfFolder, login
    #from diffusers import StableDiffusionPipeline, logging
    from diffusers import logging
    if not os.path.exists(HfFolder.path_token):
        run_process("git config --global credential.helper store", page=page)
    logging.set_verbosity_error()
    if not os.path.exists(HfFolder.path_token):
        page.status("...logging into HuggingFace")
        #from huggingface_hub.commands.user import _login
        #_login(HfApi(), token=prefs['HuggingFace_api_key'])
        try:
          login(token=prefs['HuggingFace_api_key'], add_to_git_credential=True)
        except Exception:
          alert_msg(page, "ERROR Logging into HuggingFace... Check your API Key or Internet conenction.")
          return
    # TODO: Get Username to prefs
    HFapi = HfApi()
    page.status("...getting model")
    prefs['HuggingFace_username'] = HFapi.whoami()["name"]
    #if prefs['model_ckpt'] == "Stable Diffusion v1.5": model_path =  "runwayml/stable-diffusion-v1-5"
    #elif prefs['model_ckpt'] == "Stable Diffusion v1.4": model_path =  "CompVis/stable-diffusion-v1-4"
    model = get_model(prefs['model_ckpt'])
    model_path = model['path']
    #try:
    #  scheduler = model_scheduler(model_path)
    #except Exception as e:
    #  alert_msg(page, f"ERROR: {prefs['scheduler_mode']} Scheduler couldn't load for {model_path}", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
    #  pass
    status['finetuned_model'] = False if model['name'].startswith("Stable") else True
    page.status()

def save_file(file, folder="", filename=None, media="images"):
    if 'save_to_HF' in prefs and prefs['save_to_HF']:
        upload_HF(file=file, folder=folder, filename=filename, media=media)
    
def upload_HF(file, folder="", filename=None, media="images"):
    global HFapi
    import huggingface_hub
    from huggingface_hub import HfApi
    if HFapi == None:
        HFapi = HfApi()
    if 'HuggingFace_username' not in prefs or prefs['HuggingFace_username'] == "":
        prefs['HuggingFace_username'] = HFapi.whoami()["name"]
    repo_name = f"{prefs['HuggingFace_username']/{media}}"
    if folder=="":
        path=filename
    else:
        path=f"{folder}/{filename}"
    if filename==None:
        print("upload_HF Error: Must give filename")
    try:
        HFapi.upload_file(
            path_or_fileobj=file,
            path_in_repo=path,#f"{folder}{filename}",#path.split("/")[-1],
            repo_id=repo_name,
            repo_type="dataset",
        )
    except huggingface_hub.utils._errors.RepositoryNotFoundError:
        HFapi.create_repo(
            repo_id=repo_name,
            repo_type="dataset",
            private=True,
        )
        try:
            HFapi.upload_file(
                path_or_fileobj=file,
                path_in_repo=path,
                repo_id=repo_name,
                repo_type="dataset",
            )
        except Exception as e:
            print(f"Create upload_HF Error: {e}")
            return None
        pass
    except Exception as e:
        print(f"upload_HF Error: {e}")
        return None
    print(f"Saved https://huggingface.co/datasets/{repo_name}/blob/main/{path}")
    return f" https://huggingface.co/datasets/{repo_name}/blob/main/{path}"

def model_scheduler(model, big3=False, **kwargs):
    scheduler_mode = prefs['scheduler_mode']
    if scheduler_mode == "LMS Discrete":
      from diffusers import LMSDiscreteScheduler
      s = LMSDiscreteScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif scheduler_mode == "PNDM":
      from diffusers import PNDMScheduler
      s = PNDMScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif scheduler_mode == "DDIM":
      from diffusers import DDIMScheduler
      s = DDIMScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif big3:
      from diffusers import DDIMScheduler
      s = DDIMScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif scheduler_mode == "DPM Solver":
      from diffusers import DPMSolverMultistepScheduler #"hf-internal-testing/tiny-stable-diffusion-torch"
      s = DPMSolverMultistepScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif scheduler_mode == "DPM Solver Singlestep":
      from diffusers import DPMSolverSinglestepScheduler
      s = DPMSolverSinglestepScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif scheduler_mode == "DPM Solver Inverse":
      from diffusers import DPMSolverMultistepInverseScheduler
      s = DPMSolverMultistepInverseScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif scheduler_mode == "K-Euler Discrete":
      from diffusers import EulerDiscreteScheduler
      s = EulerDiscreteScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif scheduler_mode == "K-Euler Ancestral":
      from diffusers import EulerAncestralDiscreteScheduler
      s = EulerAncestralDiscreteScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif scheduler_mode == "Flow Match Euler Discrete":
      from diffusers import FlowMatchEulerDiscreteScheduler
      s = FlowMatchEulerDiscreteScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif scheduler_mode == "EDM Euler":
      from diffusers import EDMEulerScheduler
      s = EDMEulerScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif scheduler_mode == "Karras-LMS":
      from diffusers import LMSDiscreteScheduler
      s = LMSDiscreteScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
      scheduler_config = s.get_scheduler_config()
      s = LMSDiscreteScheduler(**scheduler_config, use_karras_sigmas=True)
    elif scheduler_mode == "DPM Stochastic":
      from diffusers import DPMSolverSDEScheduler
      s = DPMSolverSDEScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif scheduler_mode == "SDE-DPM Solver++":
      from diffusers import DPMSolverMultistepScheduler
      s = DPMSolverMultistepScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
      s.config.algorithm_type = 'sde-dpmsolver++'
    elif scheduler_mode == "DPM Solver++":
      from diffusers import DPMSolverMultistepScheduler
      s = DPMSolverMultistepScheduler.from_pretrained(model, subfolder="scheduler",
        beta_start=0.00085,
        beta_end=0.012,
        beta_schedule="scaled_linear",
        num_train_timesteps=1000,
        trained_betas=None,
        #predict_epsilon=True,
        prediction_type="v_prediction" if model.startswith('stabilityai') else "epsilon",
        thresholding=False,
        algorithm_type="dpmsolver++",
        solver_type="midpoint",
        solver_order=2,
        #denoise_final=True,
        lower_order_final=True,
        **kwargs,
      )
    elif scheduler_mode == "Heun Discrete":
      from diffusers import HeunDiscreteScheduler
      s = HeunDiscreteScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif scheduler_mode == "Karras Heun Discrete":
      from diffusers import HeunDiscreteScheduler
      s = HeunDiscreteScheduler.from_pretrained(model, subfolder="scheduler", use_karras_sigmas=True, **kwargs)
    elif scheduler_mode == "K-DPM2 Ancestral":
      from diffusers import KDPM2AncestralDiscreteScheduler
      s = KDPM2AncestralDiscreteScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif scheduler_mode == "K-DPM2 Discrete":
      from diffusers import KDPM2DiscreteScheduler
      s = KDPM2DiscreteScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif scheduler_mode == "IPNDM":
      from diffusers import IPNDMScheduler
      s = IPNDMScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif scheduler_mode == "DEIS Multistep":
      from diffusers import DEISMultistepScheduler
      s = DEISMultistepScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif scheduler_mode == "UniPC Multistep":
      from diffusers import UniPCMultistepScheduler
      s = UniPCMultistepScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    elif scheduler_mode == "TCD":
      from diffusers import TCDScheduler
      s = TCDScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    #elif scheduler_mode == "Score-SDE-Vp":
    #  from diffusers import ScoreSdeVpScheduler
    #  s = ScoreSdeVpScheduler() #(num_train_timesteps=2000, beta_min=0.1, beta_max=20, sampling_eps=1e-3, tensor_format="np")
    #  use_custom_scheduler = True
    #elif scheduler_mode == "Score-SDE-Ve":
    #  from diffusers import ScoreSdeVeScheduler
    #  s = ScoreSdeVeScheduler() #(num_train_timesteps=2000, snr=0.15, sigma_min=0.01, sigma_max=1348, sampling_eps=1e-5, correct_steps=1, tensor_format="pt"
    #  use_custom_scheduler = True
    #elif scheduler_mode == "Karras-Ve":
    #  from diffusers import KarrasVeScheduler
    #  s = KarrasVeScheduler() #(sigma_min=0.02, sigma_max=100, s_noise=1.007, s_churn=80, s_min=0.05, s_max=50, tensor_format="pt")
    #  use_custom_scheduler = True
    elif scheduler_mode == "DDPM":
      from diffusers import DDPMScheduler
      s = DDPMScheduler(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule="linear", trained_betas=None, variance_type="fixed_small", clip_sample=True, tensor_format="pt", **kwargs)
      use_custom_scheduler = True
    elif scheduler_mode == "LMS": #no more
      from diffusers import LMSScheduler
      s = LMSScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", **kwargs)
      #(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule="linear", trained_betas=None, timestep_values=None, tensor_format="pt")
      use_custom_scheduler = True
    #print(f"Loaded Schedueler {scheduler_mode} {type(scheduler)}")
    else:
      print(f"Unknown scheduler request {scheduler_mode} - Using LMS Discrete")
      from diffusers import LMSDiscreteScheduler
      s = LMSDiscreteScheduler.from_pretrained(model, subfolder="scheduler", **kwargs)
    return s

def pipeline_scheduler(p, big3=False, from_scheduler = True, scheduler=None, trailing=False, use_karras_sigmas=False, **kwargs):
    global status
    scheduler_mode = prefs['scheduler_mode'] if scheduler is None else scheduler
    args = {} if not trailing else {'timestep_spacing': 'trailing'}
    if use_karras_sigmas: args['use_karras_sigmas'] = True
    if scheduler_mode == "LMS Discrete":
      from diffusers import LMSDiscreteScheduler
      s = LMSDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "PNDM":
      from diffusers import PNDMScheduler
      s = PNDMScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "DDIM":
      from diffusers import DDIMScheduler
      s = DDIMScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif big3:
      from diffusers import DDIMScheduler
      s = DDIMScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "DPM Solver":
      from diffusers import DPMSolverMultistepScheduler #"hf-internal-testing/tiny-stable-diffusion-torch"
      s = DPMSolverMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "DPM Solver Singlestep":
      from diffusers import DPMSolverSinglestepScheduler
      s = DPMSolverSinglestepScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "DPM Solver Inverse":
      from diffusers import DPMSolverMultistepInverseScheduler
      s = DPMSolverMultistepInverseScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "DPM Stochastic":
      from diffusers import DPMSolverSDEScheduler
      s = DPMSolverSDEScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "SDE-DPM Solver++":
      from diffusers import DPMSolverMultistepScheduler #"hf-internal-testing/tiny-stable-diffusion-torch"
      s = DPMSolverMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
      s.config.algorithm_type = 'sde-dpmsolver++'
    elif scheduler_mode == "K-Euler Discrete":
      from diffusers import EulerDiscreteScheduler
      s = EulerDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "K-Euler Ancestral":
      from diffusers import EulerAncestralDiscreteScheduler
      s = EulerAncestralDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "Flow Match Euler Discrete":
      from diffusers import FlowMatchEulerDiscreteScheduler
      s = FlowMatchEulerDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "EDM Euler":
      from diffusers import EDMEulerScheduler
      s = EDMEulerScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "Karras-LMS":
      from diffusers import LMSDiscreteScheduler
      s = LMSDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
      scheduler_config = s.get_scheduler_config()
      s = LMSDiscreteScheduler(**scheduler_config, use_karras_sigmas=True, **args)
    elif scheduler_mode == "DPM Solver++":
      from diffusers import DPMSolverMultistepScheduler
      try:
        p_model = p.model
      except Exception:
        p_model = ""
        pass
      s = DPMSolverMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config,
        beta_start=0.00085,
        beta_end=0.012,
        beta_schedule="scaled_linear",
        num_train_timesteps=1000,
        trained_betas=None,
        #predict_epsilon=True,
        prediction_type="v_prediction" if p_model.startswith('stabilityai') else "epsilon",
        thresholding=False,
        algorithm_type="dpmsolver++",
        solver_type="midpoint",
        solver_order=2,
        #denoise_final=True,
        lower_order_final=True,
        **args
      )
    elif scheduler_mode == "Heun Discrete":
      from diffusers import HeunDiscreteScheduler
      s = HeunDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "Karras Heun Discrete":
      from diffusers import HeunDiscreteScheduler
      s = HeunDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config, use_karras_sigmas=True, **args)
    elif scheduler_mode == "K-DPM2 Ancestral":
      from diffusers import KDPM2AncestralDiscreteScheduler
      s = KDPM2AncestralDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "K-DPM2 Discrete":
      from diffusers import KDPM2DiscreteScheduler
      s = KDPM2DiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "IPNDM":
      from diffusers import IPNDMScheduler
      s = IPNDMScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "DEIS Multistep":
      from diffusers import DEISMultistepScheduler
      s = DEISMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "UniPC Multistep":
      from diffusers import UniPCMultistepScheduler
      s = UniPCMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "LCM":
      from diffusers import LCMScheduler
      s = LCMScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    elif scheduler_mode == "TCD":
      from diffusers import TCDScheduler
      s = TCDScheduler.from_config(p.scheduler.config if from_scheduler else p.config, **args)
    #elif scheduler_mode == "Score-SDE-Vp":
    #  from diffusers import ScoreSdeVpScheduler
    #  s = ScoreSdeVpScheduler() #(num_train_timesteps=2000, beta_min=0.1, beta_max=20, sampling_eps=1e-3, tensor_format="np")
    #  use_custom_scheduler = True
    #elif scheduler_mode == "Score-SDE-Ve":
    #  from diffusers import ScoreSdeVeScheduler
    #  s = ScoreSdeVeScheduler() #(num_train_timesteps=2000, snr=0.15, sigma_min=0.01, sigma_max=1348, sampling_eps=1e-5, correct_steps=1, tensor_format="pt"
    #  use_custom_scheduler = True
    #elif scheduler_mode == "Karras-Ve":
    #  from diffusers import KarrasVeScheduler
    #  s = KarrasVeScheduler() #(sigma_min=0.02, sigma_max=100, s_noise=1.007, s_churn=80, s_min=0.05, s_max=50, tensor_format="pt")
    #  use_custom_scheduler = True
    elif scheduler_mode == "DDPM":
      from diffusers import DDPMScheduler
      s = DDPMScheduler(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule="linear", trained_betas=None, variance_type="fixed_small", clip_sample=True, tensor_format="pt", **args)
      use_custom_scheduler = True
    elif scheduler_mode == "LMS": #no more
      from diffusers import LMSScheduler
      s = LMSScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", **args)
      #(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule="linear", trained_betas=None, timestep_values=None, tensor_format="pt")
      use_custom_scheduler = True
    #print(f"Loaded Schedueler {scheduler_mode} {type(scheduler)}")
    else:
      print(f"Unknown scheduler request {scheduler_mode} - Using LMS Discrete")
      from diffusers import LMSDiscreteScheduler
      s = LMSDiscreteScheduler.from_config(p.scheduler.config, **args)
    p.scheduler = s
    status['loaded_scheduler'] = scheduler_mode
    return p

#if is_Colab:
#    os.remove("/usr/local/lib/python3.8/dist-packages/torch/lib/libcudnn.so.8")
#    download_file("https://github.com/Skquark/diffusers/blob/main/utils/libcudnn.so.8?raw=true", to="/usr/local/lib/python3.8/dist-packages/torch/lib/")
torch_device = "cuda"
try:
    from packaging import version
    import torch
    if version.parse(version.parse(torch.__version__).base_version) < version.parse("2.2.1") and torch.cuda.is_available():
      if upgrade_torch:
        raise ModuleNotFoundError("Upgrade Torch")
    else:
        upgrade_torch = True
except ModuleNotFoundError:
    import platform
    #page.console_msg("Installing PyTorch with CUDA 1.17")
    pt_ver = latest_version("torch")
    print(f"Installing PyTorch {pt_ver} with CUDA 1.21...")
    if platform.system() == 'Darwin':  # macOS
      run_sp("pip install -qq -U --force-reinstall torch torchvision torchaudio")
    else:
      run_sp("pip install -qq -U --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121", realtime=False)
    #run_sp("pip install -U --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121", realtime=False)
    #pip install --pre torch torchvision torchaudio --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu118
    #run_sp("pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu117", realtime=False)
    try:
      import torch
    except ModuleNotFoundError:
      print("Failed Installing Torch Packages...")
      run_sp("pip install -q torch")
      import torch
      pass
    pass
finally:
    torch_device = "cuda" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else "cpu"
    if torch_device == "cpu":
        print("WARNING: CUDA is only available with CPU, so GPU tasks are limited. Can use Stability-API, AIHorde & OpenAI, Gemini, but not Diffusers...")

if torch_device == "cuda":
    try:
        import accelerate
    except ModuleNotFoundError:
        print("Installing HuggingFace Accelerate packages...")
        run_sp("pip install --upgrade -q accelerate", realtime=False)
        pass
    try:
        import transformers
        if version.parse(version.parse(transformers.__version__).base_version) < version.parse("4.40.0"):
            #import importlib
            print(f"Uninstalling old Transformers v{transformers.__version__}")
            run_sp("pip uninstall -y transformers", realtime=False)
            t_ver = latest_version("transformers")
            t_ver = f" v{t_ver}" if t_ver is not None else ""
            print(f"Installing latest Transformers{t_ver} package...")
            run_sp("pip install --upgrade -q git+https://github.com/huggingface/transformers.git", realtime=False)
            print("Installing latest HuggingFace packages...")
            run_sp("pip install --upgrade -q git+https://github.com/huggingface/peft.git", realtime=False)
            try:
                import huggingface_hub
                if version.parse(huggingface_hub.__version__) < version.parse("0.20.3"):
                    raise ModuleNotFoundError("")
            except ModuleNotFoundError:
                run_sp("pip install --upgrade -q huggingface_hub", realtime=False)
                pass
            try:
                import gdown
                if gdown.__version__ < "4.7.3":
                    raise ModuleNotFoundError("")
            except ModuleNotFoundError:
                run_sp("pip install -q gdown==4.7.3")
                pass
            print("Goto Runtime -> Restart session to apply updates...")
            raise SystemExit("Please Restart Session and run all again to Upgrade... Sorry, only workaround.")
    except ModuleNotFoundError:
        print(f"Installing latest Transformers package...")
        run_sp("pip install --upgrade -q git+https://github.com/huggingface/transformers.git", realtime=False)
        pass
#print(latest_version("torch"))
if not is_Colab:
    try:
        import setuptools
    except ImportError:
        run_sp("pip install setuptools==69.5.1", realtime=False)
        pass
    try:
        subprocess.check_call(["git", "--version"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    except subprocess.CalledProcessError:
        import platform
        #os_name, _, _ = os.uname()
        #os_name = os_name.lower()
        os_name = platform.uname().system.lower()
        if os_name == "posix":
            install_command = "curl -fsSL https://git-scm.com/download/linux/latest.tar.gz | tar xz && mv git-*/bin/git /usr/bin/git"
        elif os_name == "nt":
            install_command = "curl -fsSL https://git-scm.com/download/Win64/Git-2.39.0-windows-x64.exe | tar xz && mv Git/bin/git.exe /usr/bin/git.exe"
        elif os_name == "linux":
            install_command = ["sudo", "apt-get", "install", "-y", "git"]
        elif os_name == "darwin":
            install_command = ["brew", "install", "git"]
        elif os_name == "windows":
            url = "https://github.com/git-for-windows/git/releases/download/v2.40.0.windows.1/MinGit-2.40.0-64-bit.zip"
            install_command = ["powershell", "-Command", f"Invoke-WebRequest -Uri {url} -OutFile git.zip; Expand-Archive git.zip -DestinationPath C:\\Git"]
        else:
            print(f"Unsupported OS for Git: {os_name}. Install git manually from https://git-scm.com/downloads")
            pass
        try:
            print("Installing latest GIT library....")
            subprocess.run(install_command, shell=True, check=True)
        except subprocess.CalledProcessError:
            print(f"Git installation failed on {os_name}. Please manually install it...")
        try:
            subprocess.check_call(["git", "--version"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        except subprocess.CalledProcessError:
            print("Error installing Git. Install git manually from https://git-scm.com/downloads")
        pass

status['cpu_memory'] = psutil.virtual_memory().total / (1024 * 1024 * 1024)
try:
    status['gpu_memory'] = torch.cuda.get_device_properties(0).total_memory / (1024 * 1024 * 1024)
except Exception:
    status['gpu_memory'] = "N/A"
    pass

pb = ProgressBar(width=420, bar_height=8)
abort_run = False
start_step = 0
start_callback = 0
total_steps = args['steps']
def callback_fn(step: int, timestep: int, latents: torch.FloatTensor) -> None:
    global total_steps, pb
    callback_fn.has_been_called = True
    if total_steps is None: total_steps = timestep
    if total_steps == 0: total_steps = len(latents)
    multiplier = 2 if prefs['scheduler_mode'].startswith("Heun") or prefs['scheduler_mode'].startswith("K-DPM") else 1
    percent = (step +1)/ (total_steps * multiplier)
    pb.value = percent
    pb.tooltip = f"[{step +1} / {total_steps * multiplier}] (Timestep: {int(timestep)})"
    #print(f"step: {step}, total: {total_steps}, latent: {len(latents)}")
    #if step == 0:
        #latents = latents.detach().cpu().numpy()
        #assert latents.shape == (1, 4, 64, 64)
        #latents_slice = latents[0, -3:, -3:, -1]
        #expected_slice = np.array([1.8285, 1.2857, -0.1024, 1.2406, -2.3068, 1.0747, -0.0818, -0.6520, -2.9506])
        #assert np.abs(latents_slice.flatten() - expected_slice).max() < 1e-3
    pb.update()

def callback_step(pipe, i, t, callback_kwargs):
    global pb, start_step, start_callback, abort_run
    callback_step.has_been_called = True
    now = time.time()
    itsec = ""
    try:
        steps = pipe.num_timesteps
    except:
        global total_steps
        steps = total_steps
        pass
    if i < 2:
        start_callback = now
    else:
        itsec = f" - {its(now - start_step)} - Elapsed: {elapsed(start_callback, now)}"
    start_step = now
    percent = (i +1)/ (steps)
    pb.value = percent
    pb.tooltip = f"[{i +1} / {steps}] (Timestep: {t}){itsec}"
    pb.update()
    if abort_run:
        pipe._interrupt = True
    return callback_kwargs

def abort_reset():
  global abort_run
  time.sleep(10)
  abort_run = False
  
def abort(page=None):
  global abort_run
  abort_run = True
  thread = threading.Thread(target=abort_reset)
  thread.start
  if page is not None:
    page.snd_error.play()
    page.snd_delete.play()


def optimize_pipe(p, vae_slicing=False, unet=False, no_cpu=False, vae_tiling=False, to_gpu=True, tome=True, torch_compile=True, model_offload=False, freeu=True, lora=True):
    global prefs, status
    if model_offload:
      p.enable_model_cpu_offload()
    #if prefs['memory_optimization'] == 'Attention Slicing':
    if prefs['enable_attention_slicing']:
      #if not model['name'].startswith('Stable Diffusion v2'): #TEMP hack until it updates my git with fix
      if prefs['sequential_cpu_offload'] and not no_cpu:
        p.enable_attention_slicing(1)
      else:
        p.enable_attention_slicing()#prefs['memory_optimization'] == 'Xformers Mem Efficient Attention'
    elif prefs['enable_xformers'] and status['installed_xformers']:
      #p.set_use_memory_efficient_attention_xformers(True)
      p.enable_xformers_memory_efficient_attention()
    elif prefs['enable_xformers']:
      p.enable_attention_slicing()
    if prefs['vae_slicing'] and vae_slicing:
      p.enable_vae_slicing()
    if prefs['vae_tiling'] and vae_tiling:
      p.enable_vae_tiling()
    if unet:
      p.unet = torch.compile(p.unet)
    if lora:
      p = apply_LoRA(p, fuse=prefs['enable_torch_compile'] and torch_compile)
    '''if prefs['use_LoRA_model']:
      lora = get_LoRA_model(prefs['LoRA_model'])
      if bool(lora['weights']):
        p.load_lora_weights(lora['path'], weight_name=lora['weights'])
      else:
        p.load_lora_weights(lora['path'])'''
      #TODO: , weight_name=lora_filename
      #p.unet.load_attn_procs(lora['path'])
    if prefs['enable_freeu'] and freeu:
      p.enable_freeu(**prefs['freeu_args'])
    if prefs['sequential_cpu_offload'] and not no_cpu:
      p.enable_sequential_cpu_offload()
    else:
      if to_gpu and not (prefs['enable_torch_compile'] and torch_compile) and not model_offload:
        p = p.to(torch_device)
    if prefs['enable_torch_compile'] and torch_compile:
      p.unet.to(memory_format=torch.channels_last)
      p.unet = torch.compile(p.unet, mode="reduce-overhead", fullgraph=True)
    if prefs['enable_tome'] and tome:
      try:
        import tomesd
      except Exception:
        run_sp("pip install tomesd", realtime=False)
        import tomesd
        pass
      tomesd.apply_patch(p, ratio=prefs['tome_ratio'])
    if prefs['enable_hidiffusion']:
        from hidiffusion import apply_hidiffusion
        apply_hidiffusion(p)
    if prefs['enable_deepcache']:
        try:
            from DeepCache import DeepCacheSDHelper
        except Exception:
            run_sp("pip install DeepCache", realtime=False)
            from DeepCache import DeepCacheSDHelper
            pass
        helper = DeepCacheSDHelper(pipe=p)
        helper.set_params(cache_interval=3, cache_branch_id=0)
        helper.enable()
    status['loaded_scheduler'] = prefs['scheduler_mode']
    status['loaded_model'] = get_model(prefs['model_ckpt'])['path']
    return p

def optimize_SDXL(p, vae_slicing=False, no_cpu=False, vae_tiling=True, torch_compile=True, model_offload=False, freeu=True, tome=True, lora=True):
    global prefs, status
    low_ram = int(status['cpu_memory']) <= 12
    to_gpu = True
    if model_offload and not low_ram:
      p.enable_model_cpu_offload()
      to_gpu = False
    elif prefs['sequential_cpu_offload'] and not no_cpu:
      p.enable_sequential_cpu_offload()
      to_gpu = False
    if prefs['enable_xformers'] and status['installed_xformers']:
      #p.set_use_memory_efficient_attention_xformers(True)
      p.enable_xformers_memory_efficient_attention()
    if prefs['vae_slicing'] and vae_slicing:
      p.enable_vae_slicing()
    if prefs['vae_tiling'] and vae_tiling:
      p.enable_vae_tiling()
    if lora:
      p = apply_LoRA(p, SDXL=True, fuse=prefs['enable_torch_compile'] and torch_compile)
    '''if prefs['use_LoRA_model']:
      lora = get_SDXL_LoRA_model(prefs['SDXL_LoRA_model'])
      if bool(lora['weights']):
        p.load_lora_weights(lora['path'], weight_name=lora['weights'], torch_dtype=torch.float16)
      else:
        p.load_lora_weights(lora['path'], torch_dtype=torch.float16)'''
      #p.unfuse_lora() TODO
      #p.fuse_lora(lora_scale=0.5)
      #p.unet.load_attn_procs(lora['path'])
    #if to_gpu and not (prefs['enable_torch_compile'] and torch_compile) and not model_offload:
    if prefs['enable_freeu'] and freeu:
      p.enable_freeu(s1=0.6, s2=0.4, b1=1.1, b2=1.2)#**prefs['freeu_args'])
      #s1=0.9, s2=0.2, b1=1.2, b2=1.4
    if prefs['enable_torch_compile'] and torch_compile:
      #p.unet.to(memory_format=torch.channels_last)
      p.unet = torch.compile(p.unet, mode="reduce-overhead", fullgraph=True)
    if prefs['enable_tome'] and tome:
      try:
        import tomesd
      except Exception:
        run_sp("pip install tomesd", realtime=False)
        import tomesd
        pass
      tomesd.apply_patch(p, ratio=prefs['tome_ratio'])
    if prefs['enable_hidiffusion']:
        from hidiffusion import apply_hidiffusion
        apply_hidiffusion(p)
    if prefs['enable_deepcache']:
        try:
            from DeepCache import DeepCacheSDHelper
        except Exception:
            run_sp("pip install DeepCache", realtime=False)
            from DeepCache import DeepCacheSDHelper
            pass
        helper = DeepCacheSDHelper(pipe=p)
        helper.set_params(cache_interval=3, cache_branch_id=0)
        helper.enable()
    if to_gpu:
      p = p.to(torch_device)
    p = pipeline_scheduler(p)
    status['loaded_scheduler'] = prefs['scheduler_mode']
    status['loaded_SDXL_model'] = get_SDXL_model(prefs['SDXL_model'])['path']
    p.set_progress_bar_config(disable=True)
    return p

def apply_LoRA(p, SDXL=False, SD3=False, fuse=False):
    active = p.get_active_adapters() if not SD3 else []
    layers = 'active_SDXL_LoRA_layers' if SDXL else 'active_SD3_LoRA_layers' if SD3 else 'active_LoRA_layers'
    if prefs['use_LoRA_model'] and len(prefs[layers]) > 0:
      if SD3:
        from peft import LoraConfig
        lora_config = LoraConfig(
            r=4,
            lora_alpha=4,
            target_modules=["to_q", "to_k", "to_v", "to_out.0"],
            init_lora_weights=False,
            use_dora=False,
        )
        p.transformer.add_adapter(lora_config)
      adapters = []
      scales = []
      for l in prefs[layers]:
        adapters.append(l['name'])
        scales.append(l['scale'])
        weight_args = {}
        if 'weights' in l and bool(l['weights']):
          weight_args['weight_name'] = l['weights']
        p.load_lora_weights(l['path'], adapter_name=l['name'], torch_dtype=torch.float16, **weight_args)
      p.set_adapters(adapters, adapter_weights=scales)
      if fuse:
        p.fuse_lora()
        p.unload_lora_weights()
    else:
      if len(active) > 0:
        p.disable_lora()
    return p

def install_xformers(page):
    ''' No longer needed, they finally updated to make it easier'''
    run_process("pip install -U --pre triton", page=page)
    from subprocess import getoutput
    s = getoutput('nvidia-smi')
    if 'T4' in s:
      gpu = 'T4'
    elif 'P100' in s:
      gpu = 'P100'
    elif 'V100' in s:
      gpu = 'V100'
    elif 'A100' in s:
      gpu = 'A100'
    if not (gpu=='T4'or gpu=='P100'or gpu=='V100'or gpu=='A100'):
      alert_msg(page, "Xformers Error: It seems that your GPU is not supported at the moment")
      return False
    print(f"Installing Xformers for {gpu}")
    if (gpu=='T4'):
      run_process("pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl", page=page, show=True)
    elif (gpu=='P100'):
      run_process("pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl", page=page)
    elif (gpu=='V100'):
      run_process("pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl", page=page)
    elif (gpu=='A100'):
      run_process("pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl", page=page)
    return True

def get_text2image(page):
    os.chdir(root_dir)
    global pipe, unet, scheduler, prefs, model
    def open_url(e):
      page.launch_url(e.data)
    try:
      if use_custom_scheduler: # Not really using anymore, maybe later
        from transformers import CLIPTextModel, CLIPTokenizer
        from diffusers import AutoencoderKL, UNet2DConditionModel
        # 1. Load the autoencoder model which will be used to decode the latents into image space.
        vae = AutoencoderKL.from_pretrained(model_path, subfolder="vae")
        # 2. Load the tokenizer and text encoder to tokenize and encode the text.
        tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")
        text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14")
        if prefs['higher_vram_mode']:
          unet = UNet2DConditionModel.from_pretrained(model_path, subfolder="unet", device_map="auto")
        else:
          unet = UNet2DConditionModel.from_pretrained(model_path, variant="fp16", torch_dtype=torch.float16, subfolder="unet", device_map="auto")
        vae = vae.to(torch_device)
        text_encoder = text_encoder.to(torch_device)
        #if enable_attention_slicing:
        #  unet.enable_attention_slicing() #slice_size
        unet = unet.to(torch_device)
      else:
        #if status['finetuned_model']: pipe = get_txt2img_pipe()
        #else:
        #pipe = get_lpw_pipe()
        pipe = get_SD_pipe()
    except EnvironmentError as e:
      model = get_model(prefs['model_ckpt'])
      model_url = f"https://huggingface.co/{model['path']}"
      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace {model["name"]} Model Cards to use Checkpoint',
                content=Column([Markdown(f'[{model_url}]({model_url})', selectable=True, on_tap_link=open_url), Text(str(e), selectable=True), Text(str(traceback.format_exc()).strip(), selectable=True)]))
    except Exception as e:
      model = get_model(prefs['model_ckpt'])
      model_url = f"https://huggingface.co/{model['path']}"
      alert_msg(page, f'ERROR Loading Pipeline with {model["name"]}',
                content=Column([Markdown(f'[{model_url}]({model_url})', selectable=True, on_tap_link=open_url), Text(str(e), selectable=True), Text(str(traceback.format_exc()).strip(), selectable=True)]))

# I thought it's what I wanted, but current implementation does same as mine but doesn't clear memory between
def get_mega_pipe():
  global pipe, scheduler, model_path, prefs
  from diffusers import DiffusionPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker

  if prefs['higher_vram_mode']:
    pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="stable_diffusion_mega", safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker"))
    #pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker"))
  else:
    pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="stable_diffusion_mega", revision="fp16", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker"))
    #pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, revision="fp16", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker"))
  #pipe = pipe.to(torch_device)
  pipe = pipeline_scheduler(pipe)
  pipe = optimize_pipe(pipe)
  pipe.set_progress_bar_config(disable=True)
  return pipe

def get_lpw_pipe(): #Not using anymore after things broke. Switched to Compel.
  global pipe, scheduler, model_path, prefs
  from diffusers import DiffusionPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  from diffusers import AutoencoderKL, UNet2DConditionModel
  model = get_model(prefs['model_ckpt'])
  model_path = model['path']
  os.chdir(root_dir)
  #if not os.path.isfile(os.path.join(root_dir, 'lpw_stable_diffusion.py')):
  #  run_sp("wget -q --show-progress --no-cache --backups=1 https://raw.githubusercontent.com/Skquark/diffusers/main/examples/community/lpw_stable_diffusion.py")
  #from lpw_stable_diffusion import StableDiffusionLongPromptWeightingPipeline
  if pipe is not None:
    if model['path'] != status['loaded_model']:
      #clear_txt2img_pipe()
      clear_pipes()
    elif prefs['scheduler_mode'] != status['loaded_scheduler']:
      pipe = pipeline_scheduler(pipe)
      return pipe
    else:
      return pipe
  if 'revision' in model:
    pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/lpw_stable_diffusion_update", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, variant=model['revision'], torch_dtype=torch.float16, **safety)
  else:
    if 'vae' in model:
      from diffusers import AutoencoderKL, UNet2DConditionModel
      vae = AutoencoderKL.from_pretrained(model_path, subfolder="vae", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)
      unet = UNet2DConditionModel.from_pretrained(model_path, subfolder="unet", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)
      pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/lpw_stable_diffusion_update", vae=vae, unet=unet, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
    else:
      if 'from_ckpt' in model:
        pipe = DiffusionPipeline.from_single_file(model_path, custom_pipeline="AlanB/lpw_stable_diffusion_update", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
      else:
        pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/lpw_stable_diffusion_update", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
  pipe = optimize_pipe(pipe, vae_slicing=True)
  pipe.set_progress_bar_config(disable=True)
  return pipe

def get_SD_pipe(task="txt2img"):
  global pipe, model_path, prefs, compel_proc
  from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image, AutoPipelineForInpainting
  model = get_model(prefs['model_ckpt'])
  model_path = model['path']
  safetensors = model['use_safetensors'] if 'use_safetensors' in model else False
  if pipe is not None:
    if 'from_ckpt' in model and task != status['loaded_task']:
      clear_pipes()
    elif task != status['loaded_task']:
      if task == "txt2img":
        pipe = AutoPipelineForText2Image.from_pipe(pipe)
      elif task == "img2img":
        pipe = AutoPipelineForImage2Image.from_pipe(pipe)
      elif task == "inpaint":
        pipe = AutoPipelineForInpainting.from_pipe(pipe)
      pipe = pipeline_scheduler(pipe)
      pipe = apply_LoRA(pipe)
      status['loaded_task'] = task
      return pipe
    if model['path'] != status['loaded_model'] or task != status['loaded_task']:
      clear_pipes()
    elif prefs['scheduler_mode'] != status['loaded_scheduler']:
      pipe = pipeline_scheduler(pipe)
      pipe = apply_LoRA(pipe)
      return pipe
    else:
      pipe = apply_LoRA(pipe)
      return pipe
  if prefs['SD_compel']:
      pip_install("compel", upgrade=True)
  variant = {'variant': model['revision']} if 'revision' in model else {}
  variant = {'variant': model['variant']} if 'variant' in model else variant
  if 'vae' in model:
    from diffusers import AutoencoderKL, UNet2DConditionModel
    vae = AutoencoderKL.from_single_file(model_path, subfolder="vae", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)
    unet = UNet2DConditionModel.from_single_file(model_path, subfolder="unet", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)
    vae_args = {'vae': vae, 'unet': unet}
  else:
    vae_args = {}
  if 'from_ckpt' in model:
    from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipeline
    if task == "txt2img":
      pipe = StableDiffusionPipeline.from_single_file(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, use_safetensors=True, **vae_args, **variant, **safety)
    elif task == "img2img":
      pipe = StableDiffusionImg2ImgPipeline.from_single_file(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, use_safetensors=True, **vae_args, **variant, **safety)
    elif task == "inpaint":
      pipe = StableDiffusionInpaintPipeline.from_single_file(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, use_safetensors=True,**vae_args, **variant, **safety)
    #pipe = StableDiffusionPipeline.from_single_file(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
  else:
    if task == "txt2img":
      pipe = AutoPipelineForText2Image.from_pretrained(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, use_safetensors=True, **vae_args, **variant, **safety)
    elif task == "img2img":
      pipe = AutoPipelineForImage2Image.from_pretrained(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, use_safetensors=True, **vae_args, **variant, **safety)
    elif task == "inpaint":
      pipe = AutoPipelineForInpainting.from_pretrained(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, use_safetensors=True, **vae_args, **variant, **safety)
      #pipe = StableDiffusionPipeline.from_pretrained(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
  pipe = optimize_pipe(pipe)
  if prefs['SD_compel']:
      from compel import Compel
      compel_proc = Compel(tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder, truncate_long_prompts=False)
  pipe.set_progress_bar_config(disable=True)
  status['loaded_task'] = task
  status['loaded_model'] = model_path
  return pipe

def get_txt2img_pipe():
  global pipe#, scheduler, model_path, prefs, status
  #from diffusers import StableDiffusionPipeline
  #from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  #from diffusers import AutoencoderKL, UNet2DConditionModel
  #if status['finetuned_model']:
  #  vae = AutoencoderKL.from_pretrained(model_path, subfolder="vae", torch_dtype=torch.float16)
  #  unet = UNet2DConditionModel.from_pretrained(model_path, subfolder="unet", torch_dtype=torch.float16)
  #pipe = optimize_pipe(pipe, vae=True)
  #pipe.set_progress_bar_config(disable=True)
  #pipe = pipe.to(torch_device)
  pipe = get_lpw_pipe()
  return pipe

def get_compel_pipe():
  global pipe, scheduler, model_path, prefs
  from diffusers import DiffusionPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  from diffusers import AutoencoderKL, UNet2DConditionModel
  model = get_model(prefs['model_ckpt'])
  os.chdir(root_dir)
  #if not os.path.isfile(os.path.join(root_dir, 'lpw_stable_diffusion.py')):
  #  run_sp("wget -q --show-progress --no-cache --backups=1 https://raw.githubusercontent.com/Skquark/diffusers/main/examples/community/lpw_stable_diffusion.py")
  #from lpw_stable_diffusion import StableDiffusionLongPromptWeightingPipeline
  if pipe is not None:
    if model['path'] != status['loaded_model']:
      #clear_txt2img_pipe()
      clear_pipes()
    elif prefs['scheduler_mode'] != status['loaded_scheduler']:
      pipe = pipeline_scheduler(pipe)
      return pipe
    else:
      return pipe
  if 'revision' in model:
    pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/lpw_stable_diffusion_update", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, variant=model['revision'], torch_dtype=torch.float16, **safety)
  else:
    if 'vae' in model:
      from diffusers import AutoencoderKL, UNet2DConditionModel
      vae = AutoencoderKL.from_pretrained(model_path, subfolder="vae", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)
      unet = UNet2DConditionModel.from_pretrained(model_path, subfolder="unet", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)
      pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/lpw_stable_diffusion_update", vae=vae, unet=unet, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
    else:
      if 'from_ckpt' in model:
        pipe = DiffusionPipeline.from_single_file(model_path, custom_pipeline="AlanB/lpw_stable_diffusion_update", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
      else:
        pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/lpw_stable_diffusion_update", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
    #pipe = DiffusionPipeline.from_pretrained(model_path, community="lpw_stable_diffusion", scheduler=scheduler, revision="fp16", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker"))
  #if prefs['enable_attention_slicing']: pipe.enable_attention_slicing()
  #pipe = pipe.to(torch_device)
  pipe = pipeline_scheduler(pipe)
  pipe = optimize_pipe(pipe, vae_slicing=True)
  pipe.set_progress_bar_config(disable=True)
  return pipe

def get_unet_pipe():
  global unet, scheduler, model_path, prefs
  from transformers import CLIPTextModel, CLIPTokenizer
  from diffusers import AutoencoderKL, UNet2DConditionModel
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  # 1. Load the autoencoder model which will be used to decode the latents into image space.
  vae = AutoencoderKL.from_pretrained(model_path, subfolder="vae")
  # 2. Load the tokenizer and text encoder to tokenize and encode the text.
  tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")
  text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14")
  if prefs['higher_vram_mode']:
    unet = UNet2DConditionModel.from_pretrained(model_path, subfolder="unet", **safety, device_map="auto")
  else:
    unet = UNet2DConditionModel.from_pretrained(model_path, revision="fp16", torch_dtype=torch.float16, subfolder="unet", **safety, device_map="auto")
  vae = vae.to(torch_device)
  text_encoder = text_encoder.to(torch_device)
  #if enable_attention_slicing:
  #  unet.enable_attention_slicing() #slice_size
  unet = unet.to(torch_device)
  return unet

def get_interpolation(page):
    from diffusers import DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler
    import torch, gc
    global pipe_interpolation
    torch_device = "cuda" if torch.cuda.is_available() else "cpu"
    if pipe_interpolation is not None:
      #print("Clearing the ol' pipe first...")
      del pipe_interpolation
      flush()
      pipe_interpolation = None
    pipe_interpolation = get_interpolation_pipe()
    pip_install("watchdog")
    status['loaded_interpolation'] = True

def get_interpolation_pipe():
    global pipe_interpolation, scheduler, model_path, prefs
    from diffusers import StableDiffusionPipeline
    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
    os.chdir(root_dir)
    if not os.path.isfile(os.path.join(root_dir, 'clip_guided_stable_diffusion.py')):
      download_file("https://raw.githubusercontent.com/Skquark/diffusers/main/examples/community/interpolate_stable_diffusion.py", root_dir, raw=False)
      #run_sp("wget -q --show-progress --no-cache --backups=1 https://raw.githubusercontent.com/Skquark/diffusers/main/examples/community/interpolate_stable_diffusion.py")
    from interpolate_stable_diffusion import StableDiffusionWalkPipeline
    model = get_model(prefs['model_ckpt'])
    if pipe_interpolation is not None:
      if model['path'] != status['loaded_model']:
        clear_interpolation_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_interpolation = pipeline_scheduler(pipe_interpolation)
        return pipe_interpolation
      else:
        return pipe_interpolation
    if 'revision' in model:
      pipe_interpolation = StableDiffusionWalkPipeline.from_pretrained(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, revision=model['revision'], torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
    else:
      pipe_interpolation = StableDiffusionWalkPipeline.from_pretrained(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
    pipe_interpolation = pipeline_scheduler(pipe_interpolation)
    pipe_interpolation = optimize_pipe(pipe_interpolation, freeu=False)
    pipe_interpolation.set_progress_bar_config(disable=True)
    return pipe_interpolation

def get_image2image(page):
    from diffusers import StableDiffusionInpaintPipeline, DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler
    import torch, gc
    global pipe_img2img
    def open_url(e):
      page.launch_url(e.data)
    torch_device = "cuda" if torch.cuda.is_available() else "cpu"
    if pipe_img2img is not None:
      if model['path'] != status['loaded_model']:
        clear_img2img_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_img2img = pipeline_scheduler(pipe_img2img)
        return pipe_img2img
      else:
        return pipe_img2img
    try:
      pipe_img2img = get_img2img_pipe()
    except EnvironmentError:
      model_url = f"https://huggingface.co/{inpaint_model}"
      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Inpainting Model Card to use Checkpoint',
                content=Markdown(f'[{model_url}]({model_url})', on_tap_link=open_url))
    loaded_img2img = True

def get_img2img_pipe():
  global pipe_img2img, scheduler, model_path, inpaint_model, prefs, callback_fn
  from diffusers import DiffusionPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  if pipe_img2img is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_img2img = pipeline_scheduler(pipe_img2img)
        return pipe_img2img
      else:
        return pipe_img2img
  if prefs['higher_vram_mode']:
    pipe_img2img = DiffusionPipeline.from_pretrained(
        inpaint_model,
        custom_pipeline="img2img_inpainting",
        #scheduler=model_scheduler(inpaint_model),
        use_safetensors=True,
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        **safety
    )
  else:
      pipe_img2img = DiffusionPipeline.from_pretrained(
      inpaint_model,
      custom_pipeline="img2img_inpainting",
      #scheduler=model_scheduler(inpaint_model),
      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
      use_safetensors=True, variant="fp16",
      torch_dtype=torch.float16,
      **safety)
  #pipe_img2img.to(torch_device)
  #if prefs['enable_attention_slicing']: pipe_img2img.enable_attention_slicing() #slice_size
  pipe_img2img = pipeline_scheduler(pipe_img2img)
  pipe_img2img = optimize_pipe(pipe_img2img, vae_slicing=True)
  pipe_img2img.set_progress_bar_config(disable=True)
  #def dummy(images, **kwargs): return images, False
  #pipe_img2img.safety_checker = dummy
  return pipe_img2img

def get_imagic(page):
    global pipe_imagic
    pipe_imagic = get_imagic_pipe()

def get_imagic_pipe():
  global pipe_imagic, scheduler, model_path, prefs
  from diffusers import DiffusionPipeline#, DDIMScheduler
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  #ddim = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", clip_sample=False, set_alpha_to_one=False)
  #if prefs['higher_vram_mode']:
  if pipe_imagic is not None:
      if model_path != status['loaded_model']:
        clear_imagic_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_imagic = pipeline_scheduler(pipe_imagic)
        return pipe_imagic
      else:
        return pipe_imagic
  if True:
    pipe_imagic = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/imagic_stable_diffusion_mod", **safety)
  else:
    pipe_imagic = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/imagic_stable_diffusion_mod", revision="fp16", torch_dtype=torch.float16, **safety)
  #pipe_imagic = pipe_imagic.to(torch_device)
  def dummy(images, **kwargs):
    return images, False
  if prefs['disable_nsfw_filter']:
    pipe_imagic.safety_checker = dummy
  pipe_imagic = pipeline_scheduler(pipe_imagic, big3=True)
  pipe_imagic = optimize_pipe(pipe_imagic, freeu=False)
  #pipe_imagic.set_progress_bar_config(disable=True)
  return pipe_imagic

def get_composable(page):
    global pipe_composable
    pipe_composable = get_composable_pipe()

def get_composable_pipe():
  global pipe_composable, scheduler, model_path, prefs
  from diffusers import DiffusionPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  if pipe_composable is not None:
      if model_path != status['loaded_model']:
        clear_composable_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_composable = pipeline_scheduler(pipe_composable, big3=True)
        return pipe_composable
      else:
        return pipe_composable
  #if prefs['higher_vram_mode']:
  if True:
    pipe_composable = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/composable_stable_diffusion_mod", feature_extractor=None, safety_checker=None)
  else:
    pipe_composable = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/composable_stable_diffusion_mod", revision="fp16", torch_dtype=torch.float16, feature_extractor=None, safety_checker=None)
  #pipe_composable = pipe_composable.to(torch_device)
  def dummy(images, **kwargs):
    return images, False
  if prefs['disable_nsfw_filter']:
    pipe_composable.safety_checker = dummy
  pipe_composable = pipeline_scheduler(pipe_composable, big3=True)
  pipe_composable = optimize_pipe(pipe_composable, freeu=False)
  #pipe_composable.set_progress_bar_config(disable=True)
  return pipe_composable

def get_SDXL(page):
    global pipe_SDXL
    def open_url(e):
      page.launch_url(e.data)
    try:
      pipe_SDXL = get_SDXL_pipe()
      return True
    except Exception as er:
      SDXL_model = get_SDXL_model(prefs['SDXL_model'])
      model_id = SDXL_model['path']
      model_url = f"https://huggingface.co/{model_id if model_id.count('/') == 1 else 'stabilityai/stable-diffusion-xl-base-1.0'}"
      alert_msg(page, f'ERROR Loading Model {SDXL_model["name"]} from HuggingFace. Maybe accept model card or try another model for now.',
                content=[Markdown(f'[{model_id}]({model_url})', on_tap_link=open_url), Text(er, weight=FontWeight.BOLD), Text(traceback.format_exc(), selectable=True)])
      return False

def get_SDXL_pipe(task="text2image"):
  global pipe_SDXL, pipe_SDXL_refiner, prefs, status, compel_base, compel_refiner
  from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline, StableDiffusionXLInpaintPipeline, AutoencoderKL # , AutoencoderTiny
  try:
      from imwatermark import WatermarkEncoder
  except ModuleNotFoundError:
      run_sp("pip install --no-deps invisible-watermark>=0.2.0", realtime=False)
      pass
  if prefs['SDXL_compel']:
      pip_install("compel", upgrade=True)
  SDXL_model = get_SDXL_model(prefs['SDXL_model'])
  model_id = SDXL_model['path']#"stabilityai/stable-diffusion-xl-base-1.0"
  safetensors = SDXL_model['use_safetensors'] if 'use_safetensors' in SDXL_model else False
  refiner_id = "stabilityai/stable-diffusion-xl-refiner-1.0"
  if SDXL_model['path'] != status['loaded_SDXL_model'] or task != status['loaded_SDXL']:
      clear_pipes()
  if pipe_SDXL is not None:
      pipe_SDXL = apply_LoRA(pipe_SDXL, SDXL=True)
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
          pipe_SDXL = pipeline_scheduler(pipe_SDXL)
          if pipe_SDXL_refiner is not None:
              pipe_SDXL_refiner = pipeline_scheduler(pipe_SDXL_refiner)
      return pipe_SDXL
  watermark = prefs['SDXL_watermark']
  low_ram = int(status['cpu_memory']) <= 12
  variant = {'variant': SDXL_model['revision']} if 'revision' in SDXL_model else {}
  variant = {'variant': SDXL_model['variant']} if 'variant' in SDXL_model else variant
  #vae = AutoencoderTiny.from_pretrained("madebyollin/taesdxl", torch_dtype=torch.float16)
  vae_model = "madebyollin/sdxl-vae-fp16-fix" if not prefs['higher_vram_mode'] else "stabilityai/sdxl-vae"
  if 'vae' in SDXL_model:
      vae_model = SDXL_model['vae']
  vae = AutoencoderKL.from_pretrained(vae_model, torch_dtype=torch.float16, use_safetensors=True)
  if task == "text2image":
      status['loaded_SDXL'] = task
      pipe_SDXL = StableDiffusionXLPipeline.from_pretrained(
          model_id,
          torch_dtype=torch.float16,# if not prefs['higher_vram_mode'] else torch.float32,
          vae=vae,
          use_safetensors=safetensors,
          add_watermarker=watermark,
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          **variant, **safety,
      )
      pipe_SDXL = optimize_SDXL(pipe_SDXL, vae_slicing=True)
      pipe_SDXL_refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(refiner_id, torch_dtype=torch.float16, use_safetensors=True,
          text_encoder_2=pipe_SDXL.text_encoder_2,
          vae=pipe_SDXL.vae,
          add_watermarker=watermark,
          variant="fp16",
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          **safety,
      )
      pipe_SDXL_refiner = optimize_SDXL(pipe_SDXL_refiner, lora=False, vae_slicing=True)

  elif task == "image2image":
      status['loaded_SDXL'] = task
      pipe_SDXL = StableDiffusionXLImg2ImgPipeline.from_pretrained(
          model_id, torch_dtype=torch.float16, use_safetensors=safetensors,
          vae=vae,
          add_watermarker=watermark,
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          **variant, **safety,
      )
      pipe_SDXL = optimize_SDXL(pipe_SDXL, vae_slicing=True)
      pipe_SDXL_refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(
          refiner_id, torch_dtype=torch.float16, use_safetensors=True,
          vae=pipe_SDXL.vae,
          add_watermarker=watermark,
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          **variant, **safety,
      )
      pipe_SDXL_refiner = optimize_SDXL(pipe_SDXL_refiner, lora=False, vae_slicing=True)

  elif task == "inpainting":
      status['loaded_SDXL'] = task
      from diffusers import StableDiffusionXLInpaintPipeline
      pipe_SDXL = StableDiffusionXLInpaintPipeline.from_pretrained(
          "diffusers/stable-diffusion-xl-1.0-inpainting-0.1", torch_dtype=torch.float16, use_safetensors=safetensors,
          vae=vae,
          add_watermarker=watermark,
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          **variant, **safety,
      )
      pipe_SDXL = optimize_SDXL(pipe_SDXL, vae_slicing=True)

      pipe_SDXL_refiner = StableDiffusionXLInpaintPipeline.from_pretrained(
          refiner_id,
          text_encoder_2=pipe_SDXL.text_encoder_2,
          vae=pipe_SDXL.vae,
          torch_dtype=torch.float16,
          use_safetensors=True,
          add_watermarker=watermark,
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          **variant, **safety,
      )
      pipe_SDXL_refiner = optimize_SDXL(pipe_SDXL_refiner, lora=False, vae_slicing=True)
  if prefs['SDXL_compel']:
      from compel import Compel, ReturnedEmbeddingsType
      compel_base = Compel(tokenizer=[pipe_SDXL.tokenizer, pipe_SDXL.tokenizer_2], text_encoder=[pipe_SDXL.text_encoder, pipe_SDXL.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True])
      compel_refiner = Compel(tokenizer=[pipe_SDXL_refiner.tokenizer_2], text_encoder=[pipe_SDXL_refiner.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[True])
      #compel_refiner = Compel(tokenizer=[pipe_SDXL_refiner.tokenizer, pipe_SDXL_refiner.tokenizer_2] , text_encoder=[pipe_SDXL_refiner.text_encoder, pipe_SDXL_refiner.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True])
      #compel_refiner = Compel(tokenizer=pipe_SDXL_refiner.tokenizer_2, text_encoder=pipe_SDXL_refiner.text_encoder_2, returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True])
      #truncate_long_prompts=True,
  return pipe_SDXL

def get_SD3(page):
    global pipe_SD3
    def open_url(e):
      page.launch_url(e.data)
    try:
      pipe_SD3 = get_SD3_pipe()
      return True
    except Exception as er:
      SD3_model = get_SD3_model(prefs['SD3_model'])
      model_id = SD3_model['path']
      model_url = f"https://huggingface.co/{model_id if model_id.count('/') == 1 else 'stabilityai/stable-diffusion-3-medium-diffusers'}"
      alert_msg(page, f'ERROR Loading Model {SD3_model["name"]} from HuggingFace. Maybe accept model card or try another model for now.',
                content=[Markdown(f'[{model_id}]({model_url})', on_tap_link=open_url), Text(er, weight=FontWeight.BOLD), Text(traceback.format_exc(), selectable=True)])
      return False

def get_SD3_pipe(task="text2image"):
  global pipe_SD3, prefs, status, compel_base
  from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image, AutoPipelineForInpainting#, StableDiffusionXLImg2ImgPipeline, StableDiffusionXLInpaintPipeline, AutoencoderKL # , AutoencoderTiny
  if prefs['SD3_compel']:
      pip_install("compel", upgrade=True)
  SD3_model = get_SD3_model(prefs['SD3_model'])
  model_id = SD3_model['path']#"stabilityai/stable-diffusion-xl-base-1.0"
  safetensors = SD3_model['use_safetensors'] if 'use_safetensors' in SD3_model else False
  if SD3_model['path'] != status['loaded_SD3_model'] or task != status['loaded_SD3']:
      clear_pipes()
  if pipe_SD3 is not None:
      if task != status['loaded_SD3']:
          if task == "text2image":
              pipe_SD3 = AutoPipelineForText2Image.from_pipe(pipe_SD3)
          elif task == "image2image":
              pipe_SD3 = AutoPipelineForImage2Image.from_pipe(pipe_SD3)
          elif task == "inpainting":
              pipe_SD3 = AutoPipelineForInpainting.from_pipe(pipe_SD3)
          status['loaded_SD3'] = task
      pipe_SD3 = apply_LoRA(pipe_SD3, SD3=True)
      #if prefs['scheduler_mode'] != status['loaded_scheduler']:
      #    pipe_SD3 = pipeline_scheduler(pipe_SD3)
      return pipe_SD3
  low_ram = int(status['cpu_memory']) <= 12
  variant = {'variant': SD3_model['revision']} if 'revision' in SD3_model else {}
  variant = {'variant': SD3_model['variant']} if 'variant' in SD3_model else variant
  #vae = AutoencoderTiny.from_pretrained("madebyollin/taesdxl", torch_dtype=torch.float16)
  #vae_model = "madebyollin/sdxl-vae-fp16-fix" if not prefs['higher_vram_mode'] else "stabilityai/sdxl-vae"
  if 'vae' in SD3_model:
      vae_model = SD3_model['vae']
  #vae = AutoencoderKL.from_pretrained(vae_model, torch_dtype=torch.float16, use_safetensors=True)
  if task == "text2image":
      status['loaded_SD3'] = task
      if prefs['enable_torch_compile']:
          torch.set_float32_matmul_precision("high")
          torch._inductor.config.conv_1x1_as_mm = True
          torch._inductor.config.coordinate_descent_tuning = True
          torch._inductor.config.epilogue_fusion = False
          torch._inductor.config.coordinate_descent_check_all_directions = True
      if not prefs['SD3_bitsandbytes_8bit']:
          pipe_SD3 = AutoPipelineForText2Image.from_pretrained(
              model_id,
              torch_dtype=torch.float16,# if not prefs['higher_vram_mode'] else torch.float32,
              #vae=vae,
              #use_safetensors=safetensors,
              #add_watermarker=watermark,
              cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
              **variant, **safety,
          )
      else:
          from transformers import T5EncoderModel, BitsAndBytesConfig
          quantization_config = BitsAndBytesConfig(load_in_8bit=True)
          text_encoder = T5EncoderModel.from_pretrained(
              model_id,
              subfolder="text_encoder_3",
              quantization_config=quantization_config,
          )
          pipe_SD3 = AutoPipelineForText2Image.from_pretrained(
              model_id,
              text_encoder_3=text_encoder,
              device_map="balanced",
              torch_dtype=torch.float16,# if not prefs['higher_vram_mode'] else torch.float32,
              cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
              **variant, **safety,
          )
      if prefs['enable_torch_compile']:
          pipe_SD3.to("cuda")
          pipe_SD3.transformer.to(memory_format=torch.channels_last)
          pipe_SD3.vae.to(memory_format=torch.channels_last)
          pipe_SD3.transformer = torch.compile(pipe_SD3.transformer, mode="max-autotune", fullgraph=True)
          pipe_SD3.vae.decode = torch.compile(pipe_SD3.vae.decode, mode="max-autotune", fullgraph=True)
      else:
          if prefs['SD3_cpu_offload']:
              pipe_SD3.enable_model_cpu_offload()
          elif not prefs['SD3_bitsandbytes_8bit']:
              pipe_SD3.to("cuda")
      if prefs['vae_tiling']:
          pipe_SD3.vae.enable_tiling()
      #pipe_SD3 = optimize_SDXL(pipe_SDXL, vae_slicing=True)
  elif task == "image2image":
      status['loaded_SD3'] = task
      if prefs['enable_torch_compile']:
          torch.set_float32_matmul_precision("high")
          torch._inductor.config.conv_1x1_as_mm = True
          torch._inductor.config.coordinate_descent_tuning = True
          torch._inductor.config.epilogue_fusion = False
          torch._inductor.config.coordinate_descent_check_all_directions = True
      if not prefs['SD3_bitsandbytes_8bit']:
          pipe_SD3 = AutoPipelineForImage2Image.from_pretrained(
              model_id,
              torch_dtype=torch.float16,# if not prefs['higher_vram_mode'] else torch.float32,
              #vae=vae,
              #use_safetensors=safetensors,
              #add_watermarker=watermark,
              cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
              **variant, **safety,
          )
      else:
          from transformers import T5EncoderModel, BitsAndBytesConfig
          quantization_config = BitsAndBytesConfig(load_in_8bit=True)
          text_encoder = T5EncoderModel.from_pretrained(
              model_id,
              subfolder="text_encoder_3",
              quantization_config=quantization_config,
          )
          pipe_SD3 = AutoPipelineForImage2Image.from_pretrained(
              model_id,
              text_encoder_3=text_encoder,
              device_map="balanced",
              torch_dtype=torch.float16,# if not prefs['higher_vram_mode'] else torch.float32,
              cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
              **variant, **safety,
          )
      if prefs['enable_torch_compile']:
          pipe_SD3.to("cuda")
          pipe_SD3.transformer.to(memory_format=torch.channels_last)
          pipe_SD3.vae.to(memory_format=torch.channels_last)
          pipe_SD3.transformer = torch.compile(pipe_SD3.transformer, mode="max-autotune", fullgraph=True)
          pipe_SD3.vae.decode = torch.compile(pipe_SD3.vae.decode, mode="max-autotune", fullgraph=True)
      else:
          if prefs['SD3_cpu_offload']:
              pipe_SD3.enable_model_cpu_offload()
          elif not prefs['SD3_bitsandbytes_8bit']:
              pipe_SD3.to("cuda")
      if prefs['vae_tiling']:
          pipe_SD3.vae.enable_tiling()
  elif task == "inpainting":
      status['loaded_SD3'] = task
      if prefs['enable_torch_compile']:
          torch.set_float32_matmul_precision("high")
          torch._inductor.config.conv_1x1_as_mm = True
          torch._inductor.config.coordinate_descent_tuning = True
          torch._inductor.config.epilogue_fusion = False
          torch._inductor.config.coordinate_descent_check_all_directions = True
      if not prefs['SD3_bitsandbytes_8bit']:
          pipe_SD3 = AutoPipelineForInpainting.from_pretrained(
              model_id,
              torch_dtype=torch.float16,# if not prefs['higher_vram_mode'] else torch.float32,
              #vae=vae,
              #use_safetensors=safetensors,
              #add_watermarker=watermark,
              cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
              **variant, **safety,
          )
      else:
          from transformers import T5EncoderModel, BitsAndBytesConfig
          quantization_config = BitsAndBytesConfig(load_in_8bit=True)
          text_encoder = T5EncoderModel.from_pretrained(
              model_id,
              subfolder="text_encoder_3",
              quantization_config=quantization_config,
          )
          pipe_SD3 = AutoPipelineForInpainting.from_pretrained(
              model_id,
              text_encoder_3=text_encoder,
              device_map="balanced",
              torch_dtype=torch.float16,# if not prefs['higher_vram_mode'] else torch.float32,
              cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
              **variant, **safety,
          )
      if prefs['enable_torch_compile']:
          pipe_SD3.to("cuda")
          pipe_SD3.transformer.to(memory_format=torch.channels_last)
          pipe_SD3.vae.to(memory_format=torch.channels_last)
          pipe_SD3.transformer = torch.compile(pipe_SD3.transformer, mode="max-autotune", fullgraph=True)
          pipe_SD3.vae.decode = torch.compile(pipe_SD3.vae.decode, mode="max-autotune", fullgraph=True)
      else:
          if prefs['SD3_cpu_offload']:
              pipe_SD3.enable_model_cpu_offload()
          elif not prefs['SD3_bitsandbytes_8bit']:
              pipe_SD3.to("cuda")
      if prefs['vae_tiling']:
          pipe_SD3.vae.enable_tiling()
  if prefs['SD3_compel']:
      from compel import Compel, ReturnedEmbeddingsType
      compel_base = Compel(tokenizer=[pipe_SD3.tokenizer, pipe_SD3.tokenizer_2], text_encoder=[pipe_SD3.text_encoder, pipe_SD3.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True])
      #compel_refiner = Compel(tokenizer=[pipe_SDXL_refiner.tokenizer_2], text_encoder=[pipe_SDXL_refiner.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[True])
      #compel_refiner = Compel(tokenizer=[pipe_SDXL_refiner.tokenizer, pipe_SDXL_refiner.tokenizer_2] , text_encoder=[pipe_SDXL_refiner.text_encoder, pipe_SDXL_refiner.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True])
      #compel_refiner = Compel(tokenizer=pipe_SDXL_refiner.tokenizer_2, text_encoder=pipe_SDXL_refiner.text_encoder_2, returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True])
      #truncate_long_prompts=True,
      print(f"Compel Base: {compel_base}")
  pipe_SD3 = apply_LoRA(pipe_SD3, SD3=True)
  pipe_SD3.set_progress_bar_config(disable=True)
  status['loaded_SD3_model'] = model_id
  return pipe_SD3

def get_versatile(page):
    import torch, gc
    global pipe_versatile_text2img
    def open_url(e):
      page.launch_url(e.data)
    try:
      pipe_versatile_text2img = get_versatile_text2img_pipe()
    except Exception as er:
      model_url = f"https://huggingface.co/shi-labs/versatile-diffusion"
      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Versatile Diffusion Model Card to use Checkpoint',
                content=Markdown(f'[{model_url}]({model_url})<br>{er}', on_tap_link=open_url))

def get_versatile_pipe(): # Mega was taking up too much vram and crashing the system
  global pipe_versatile, scheduler, model_path, prefs
  from diffusers import VersatileDiffusionPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  model_id = "shi-labs/versatile-diffusion"
  if pipe_composable is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_composable = pipeline_scheduler(pipe_composable)
        return pipe_composable
      else:
        return pipe_composable
  pipe_versatile = VersatileDiffusionPipeline.from_pretrained(
      model_id,
      #scheduler=model_scheduler(model_id),
      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
      #revision="fp16",
      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,
      **safety
  )
  #pipe_versatile.to(torch_device)
  pipe_versatile = pipeline_scheduler(pipe_versatile)
  pipe_versatile = optimize_pipe(pipe_versatile, freeu=False)
  pipe_versatile.set_progress_bar_config(disable=True)
  return pipe_versatile

def get_versatile_text2img_pipe():
  global pipe_versatile_text2img, scheduler, model_path, prefs
  from diffusers import VersatileDiffusionTextToImagePipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  model_id = "shi-labs/versatile-diffusion"
  if pipe_versatile_text2img is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_versatile_text2img = pipeline_scheduler(pipe_versatile_text2img)
        return pipe_versatile_text2img
      else:
        return pipe_versatile_text2img
  pipe_versatile_text2img = VersatileDiffusionTextToImagePipeline.from_pretrained(
      model_id,
      #scheduler=model_scheduler(model_id),
      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
      #revision="fp16",
      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,
      **safety
  )
  #pipe_versatile_text2img.to(torch_device)
  pipe_versatile_text2img = pipeline_scheduler(pipe_versatile_text2img)
  pipe_versatile_text2img = optimize_pipe(pipe_versatile_text2img, freeu=False)
  pipe_versatile_text2img.set_progress_bar_config(disable=True)
  return pipe_versatile_text2img

def get_versatile_variation_pipe():
  global pipe_versatile_variation, scheduler, model_path, prefs
  from diffusers import VersatileDiffusionImageVariationPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  model_id = "shi-labs/versatile-diffusion"
  if pipe_versatile_variation is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_versatile_variation = pipeline_scheduler(pipe_versatile_variation)
        return pipe_versatile_variation
      else:
        return pipe_versatile_variation
  if prefs['higher_vram_mode']:
    pipe_versatile_variation = VersatileDiffusionImageVariationPipeline.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        **safety
    )
  else:
    pipe_versatile_variation = VersatileDiffusionImageVariationPipeline.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        #revision="fp16",
        torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,
        **safety
    )
  #pipe_versatile_variation.to(torch_device)
  pipe_versatile_variation = pipeline_scheduler(pipe_versatile_variation)
  pipe_versatile_variation = optimize_pipe(pipe_versatile_variation, freeu=False)
  pipe_versatile_variation.set_progress_bar_config(disable=True)
  return pipe_versatile_variation

def get_versatile_dualguided_pipe():
  global pipe_versatile_dualguided, scheduler, model_path, prefs
  from diffusers import VersatileDiffusionDualGuidedPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  model_id = "shi-labs/versatile-diffusion"
  if pipe_versatile_dualguided is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_versatile_dualguided = pipeline_scheduler(pipe_versatile_dualguided)
        return pipe_versatile_dualguided
      else:
        return pipe_versatile_dualguided
  if prefs['higher_vram_mode']:
    pipe_versatile_dualguided = VersatileDiffusionDualGuidedPipeline.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        **safety
    )
  else:
    pipe_versatile_dualguided = VersatileDiffusionDualGuidedPipeline.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        #revision="fp16",
        torch_dtype=torch.float16,
        **safety
    )
  #pipe_versatile_dualguided.to(torch_device)
  pipe_versatile_dualguided = pipeline_scheduler(pipe_versatile_dualguided)
  pipe_versatile_dualguided = optimize_pipe(pipe_versatile_dualguided, freeu=False)
  pipe_versatile_dualguided.set_progress_bar_config(disable=True)
  return pipe_versatile_dualguided

def get_safe(page):
    import torch, gc
    global pipe_safe
    def open_url(e):
      page.launch_url(e.data)
    try:
      pipe_safe = get_safe_pipe()
    except Exception as er:
      model_url = f"https://huggingface.co/AIML-TUDA/stable-diffusion-safe"
      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Safe Model Card to use Checkpoint. Reinstall after accepting TOS.',
                content=Markdown(f'[{model_url}]({model_url})<br>{er}', on_tap_link=open_url))

def get_safe_pipe():
  global pipe_safe, scheduler, model_path, prefs, callback_fn
  from diffusers import StableDiffusionPipelineSafe
  from diffusers.pipelines.stable_diffusion_safe import StableDiffusionPipelineSafe
  #from diffusers.pipelines.safety_checker import SafeStableDiffusionPipelineSafe
  #from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  model_id = "AIML-TUDA/stable-diffusion-safe"
  if pipe_safe is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_safe = pipeline_scheduler(pipe_safe)
        return pipe_safe
      else:
        return pipe_safe
  #if prefs['higher_vram_mode']:
  if True:
    pipe_safe = StableDiffusionPipelineSafe.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        **safety,
    )
  else:
      pipe_safe = StableDiffusionPipelineSafe.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        revision="fp16",
        torch_dtype=torch.float16,
        **safety
      )
  #pipe_safe.to(torch_device)
  pipe_safe = pipeline_scheduler(pipe_safe)
  pipe_safe = optimize_pipe(pipe_safe, freeu=False)
  pipe_safe.set_progress_bar_config(disable=True)
  return pipe_safe

def get_SAG(page):
  global pipe_SAG
  #clear_SAG_pipe()
  pipe_SAG = get_SAG_pipe()

def get_SAG_pipe():
  global pipe_SAG, scheduler, model_path, prefs
  from diffusers import StableDiffusionSAGPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  if pipe_SAG is not None:
      if model_path != status['loaded_model']:
        clear_SAG_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_SAG = pipeline_scheduler(pipe_SAG, big3=True)
        return pipe_SAG
      else:
        return pipe_SAG
  if prefs['higher_vram_mode']:
    pipe_SAG = StableDiffusionSAGPipeline.from_pretrained(
        model_path,
        #scheduler=model_scheduler(model_path, big3=True),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        **safety
    )
  else:
      pipe_SAG = StableDiffusionSAGPipeline.from_pretrained(
      model_path,
      #scheduler=model_scheduler(model_path, big3=True),
      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,
      **safety)
  pipe_SAG = pipeline_scheduler(pipe_SAG, big3=True)
  pipe_SAG = optimize_pipe(pipe_SAG, vae_slicing=False, freeu=False)
  pipe_SAG.set_progress_bar_config(disable=True)
  return pipe_SAG

def get_attend_and_excite(page):
  global pipe_attend_and_excite
  #clear_attend_and_excite_pipe()
  pipe_attend_and_excite = get_attend_and_excite_pipe()

def get_attend_and_excite_pipe():
  global pipe_attend_and_excite, scheduler, model_path, prefs
  from diffusers import StableDiffusionAttendAndExcitePipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  if pipe_attend_and_excite is not None:
      if model_path != status['loaded_model']:
        clear_attend_and_excite_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_attend_and_excite = pipeline_scheduler(pipe_attend_and_excite)
        return pipe_attend_and_excite
      else:
        return pipe_attend_and_excite
  if prefs['higher_vram_mode']:
    pipe_attend_and_excite = StableDiffusionAttendAndExcitePipeline.from_pretrained(
        model_path,
        #scheduler=model_scheduler(model_path),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        **safety
    )
  else:
      pipe_attend_and_excite = StableDiffusionAttendAndExcitePipeline.from_pretrained(
      model_path,
      #scheduler=model_scheduler(model_path),
      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,
      **safety)
  pipe_attend_and_excite = pipeline_scheduler(pipe_attend_and_excite)
  pipe_attend_and_excite = optimize_pipe(pipe_attend_and_excite, vae_slicing=True, freeu=False)
  pipe_attend_and_excite.set_progress_bar_config(disable=True)
  return pipe_attend_and_excite

def get_panorama(page):
  global pipe_panorama
  #clear_panorama_pipe()
  pipe_panorama = get_panorama_pipe()

def get_panorama_pipe():
  global pipe_panorama, scheduler, model_path, prefs
  from diffusers import StableDiffusionPanoramaPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  if pipe_panorama is not None:
      if model_path != status['loaded_model']:
        clear_panorama_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_panorama = pipeline_scheduler(pipe_panorama)
        return pipe_panorama
      else:
        return pipe_panorama
  if prefs['higher_vram_mode']:
    pipe_panorama = StableDiffusionPanoramaPipeline.from_pretrained(
        model_path,
        #scheduler=model_scheduler(model_path, big3=True),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        **safety
    )
  else:
      pipe_panorama = StableDiffusionPanoramaPipeline.from_pretrained(
      model_path,
      #scheduler=model_scheduler(model_path, big3=True),
      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,
      **safety)
  pipe_panorama = pipeline_scheduler(pipe_panorama)
  pipe_panorama = optimize_pipe(pipe_panorama, vae_slicing=True, freeu=False)
  pipe_panorama.set_progress_bar_config(disable=True)
  return pipe_panorama

def get_upscale(page):
    import torch, gc
    global pipe_upscale
    def open_url(e):
      page.launch_url(e.data)
    if pipe_upscale is None:
      try:
        pipe_upscale = get_upscale_pipe()
      except Exception as er:
        model_url = f"https://huggingface.co/{model_path}"
        alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Upscale Model Card to use Checkpoint',
                  content=Markdown(f'[{model_url}]({model_url})<br>{er}', on_tap_link=open_url))

def get_upscale_pipe():
  global pipe_upscale, scheduler, prefs
  from diffusers import StableDiffusionUpscalePipeline
  model_id = "stabilityai/stable-diffusion-x4-upscaler"
  if pipe_upscale is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_upscale = pipeline_scheduler(pipe_upscale, big3=True)
        return pipe_upscale
      else:
        return pipe_upscale
  if prefs['higher_vram_mode']:
    pipe_upscale = StableDiffusionUpscalePipeline.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id, big3=True),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        #safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker"),
    )
  else:
    pipe_upscale = StableDiffusionUpscalePipeline.from_pretrained(
      model_id,
      #scheduler=model_scheduler(model_id, big3=True),
      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
      revision="fp16",
      torch_dtype=torch.float16,
      #safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker")
    )
  #pipe_upscale.to(torch_device)
  pipe_upscale = pipeline_scheduler(pipe_upscale, big3=True)
  pipe_upscale = optimize_pipe(pipe_upscale)
  pipe_upscale.set_progress_bar_config(disable=True)
  return pipe_upscale

def get_clip(page):
    global pipe_clip_guided, model_path
    pipe_clip_guided = get_clip_guided_pipe()

def get_clip_guided_pipe():
    global pipe_clip_guided, scheduler_clip, prefs, model_path
    from diffusers import DiffusionPipeline
    from diffusers import LMSDiscreteScheduler, PNDMScheduler, StableDiffusionPipeline
    from transformers import CLIPModel, CLIPFeatureExtractor #, CLIPGuidedStableDiffusion
    if pipe_clip_guided is not None:
      if model_path != status['loaded_model']:
        clear_clip_guided_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_clip_guided = pipeline_scheduler(pipe_clip_guided, big3=True)
        return pipe_clip_guided
      else:
        return pipe_clip_guided
    #if isinstance(scheduler, LMSDiscreteScheduler) or isinstance(scheduler, PNDMScheduler):
    #  scheduler_clip = scheduler
    #else:
    #  scheduler_clip = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear")
    model = get_model(prefs['model_ckpt'])

    clip_model = CLIPModel.from_pretrained(prefs['clip_model_id'], torch_dtype=torch.float16)
    feature_extractor = CLIPFeatureExtractor.from_pretrained(prefs['clip_model_id'])

    if 'revision' in model:
      pipe_clip_guided = DiffusionPipeline.from_pretrained(
              model_path,
              custom_pipeline="AlanB/clip_guided_stable_diffusion_mod",
              clip_model=clip_model,
              feature_extractor=feature_extractor,
              #scheduler=model_scheduler(model_path, big3=True),
              cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
              safety_checker=None,
              torch_dtype=torch.float16,
              revision=model['revision'],
              #device_map="auto",
          )
    else:
      pipe_clip_guided = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/clip_guided_stable_diffusion_mod", clip_model=clip_model, feature_extractor=feature_extractor, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16)
    #pipe_clip_guided = pipe_clip_guided.to(torch_device)
    '''
    pipe_clip_guided = CLIPGuidedStableDiffusion(
        unet=pipeline.unet,
        vae=pipeline.vae,
        tokenizer=pipeline.tokenizer,
        text_encoder=pipeline.text_encoder,
        scheduler=scheduler_clip,
        clip_model=clip_model,
        feature_extractor=feature_extractor,
    )'''
    pipe_clip_guided = pipeline_scheduler(pipe_clip_guided, big3=True)
    pipe_clip_guided = optimize_pipe(pipe_clip_guided, freeu=False)
    return pipe_clip_guided

def get_clip_guided_img2img_pipe():
    global pipe_clip_guided, scheduler_clip, prefs, model_path
    from diffusers import DiffusionPipeline
    from diffusers import LMSDiscreteScheduler, PNDMScheduler, StableDiffusionPipeline
    from transformers import CLIPModel, CLIPFeatureExtractor #, CLIPGuidedStableDiffusion
    if pipe_clip_guided is not None:
      if model_path != status['loaded_model']:
        clear_clip_guided_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_clip_guided = pipeline_scheduler(pipe_clip_guided, big3=True)
        return pipe_clip_guided
      else:
        return pipe_clip_guided
    #if isinstance(scheduler, LMSDiscreteScheduler) or isinstance(scheduler, PNDMScheduler):
    #  scheduler_clip = scheduler
    #else:
    #  scheduler_clip = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear")
    model = get_model(prefs['model_ckpt'])

    clip_model = CLIPModel.from_pretrained(prefs['clip_model_id'], torch_dtype=torch.float16)
    feature_extractor = CLIPFeatureExtractor.from_pretrained(prefs['clip_model_id'])

    if 'revision' in model:
      pipe_clip_guided = DiffusionPipeline.from_pretrained(
              model_path,
              custom_pipeline="AlanB/clip_guided_stable_diffusion_mod",
              clip_model=clip_model,
              feature_extractor=feature_extractor,
              #scheduler=model_scheduler(model_path, big3=True),
              cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
              safety_checker=None,
              torch_dtype=torch.float16,
              revision=model['revision'],
              #device_map="auto",
          )
    else:
      pipe_clip_guided = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/clip_guided_stable_diffusion_mod", clip_model=clip_model, feature_extractor=feature_extractor, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16)
    #pipe_clip_guided = pipe_clip_guided.to(torch_device)
    '''
    pipe_clip_guided = CLIPGuidedStableDiffusion(
        unet=pipeline.unet,
        vae=pipeline.vae,
        tokenizer=pipeline.tokenizer,
        text_encoder=pipeline.text_encoder,
        scheduler=scheduler_clip,
        clip_model=clip_model,
        feature_extractor=feature_extractor,
    )'''
    pipe_clip_guided = pipeline_scheduler(pipe_clip_guided, big3=True)
    pipe_clip_guided = optimize_pipe(pipe_clip_guided, freeu=False)
    return pipe_clip_guided

def get_repaint(page):
    global pipe_repaint
    pipe_repaint = get_repaint_pipe()

def get_repaint_pipe():
    global pipe_repaint
    from diffusers import UNet2DModel, RePaintScheduler, RePaintPipeline
    #model = get_model(prefs['model_ckpt'])
    #model_path = model['path']
    model_id = "google/ddpm-ema-celebahq-256"
    unet = UNet2DModel.from_pretrained(model_id)
    repaint_scheduler = RePaintScheduler.from_pretrained(model_id)
    pipe_repaint = RePaintPipeline(unet=unet, scheduler=repaint_scheduler).to(torch_device)
    return pipe_repaint

def get_depth2img(page):
  global pipe_depth
  pipe_depth = get_depth_pipe()

def get_depth_pipe():
  global pipe_depth, prefs
  from diffusers import StableDiffusionDepth2ImgPipeline
  model_id = "stabilityai/stable-diffusion-2-depth"
  if pipe_depth is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_depth = pipeline_scheduler(pipe_depth)
        return pipe_depth
      else:
        return pipe_depth
  if prefs['higher_vram_mode']:
    pipe_depth = StableDiffusionDepth2ImgPipeline.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
    )
  else:
    pipe_depth = StableDiffusionDepth2ImgPipeline.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        variant="fp16",
        torch_dtype=torch.float16,
    )
  #pipe_depth.to(torch_device)
  pipe_depth = pipeline_scheduler(pipe_depth)
  pipe_depth = optimize_pipe(pipe_depth, freeu=False)
  pipe_depth.set_progress_bar_config(disable=True)
  return pipe_depth

def get_alt_diffusion(page):
    global pipe_alt_diffusion
    run_process("pip install -q sentencepiece", page=page)
    pipe_alt_diffusion = get_alt_diffusion_pipe()

def get_alt_diffusion_pipe():
    global pipe_alt_diffusion
    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
    from diffusers import AltDiffusionPipeline, StableDiffusionPipeline
    #from diffusers.pipelines.alt_diffusion.modeling_roberta_series import (RobertaSeriesConfig, RobertaSeriesModelWithTransformation)
    model_id = "BAAI/AltDiffusion-m9"
    if pipe_alt_diffusion is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_alt_diffusion = pipeline_scheduler(pipe_alt_diffusion)
        return pipe_alt_diffusion
      else:
        return pipe_alt_diffusion
    if prefs['higher_vram_mode']:
      pipe_alt_diffusion = StableDiffusionPipeline.from_pretrained(
          model_id,
          #scheduler=model_scheduler(model_id),
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          **safety
      )
    else:
      pipe_alt_diffusion = StableDiffusionPipeline.from_pretrained(
          model_id,
          #scheduler=model_scheduler(model_id),
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          torch_dtype=torch.float16,
          **safety
      )
    #pipe_alt_diffusion.to(torch_device)
    pipe_alt_diffusion = pipeline_scheduler(pipe_alt_diffusion)
    pipe_alt_diffusion = optimize_pipe(pipe_alt_diffusion)
    pipe_alt_diffusion.set_progress_bar_config(disable=True)
    return pipe_alt_diffusion

def get_alt_diffusion_img2img(page):
    global pipe_alt_diffusion_img2img
    pipe_alt_diffusion_img2img = get_alt_diffusion_img2img_pipe()

def get_alt_diffusion_img2img_pipe():
    global pipe_alt_diffusion_img2img
    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
    from diffusers import AltDiffusionImg2ImgPipeline, StableDiffusionImg2ImgPipeline
    model_id = "BAAI/AltDiffusion-m9"
    if pipe_alt_diffusion_img2img is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_alt_diffusion_img2img = pipeline_scheduler(pipe_alt_diffusion_img2img)
        return pipe_alt_diffusion_img2img
      else:
        return pipe_alt_diffusion_img2img
    if prefs['higher_vram_mode']:
      pipe_alt_diffusion_img2img = StableDiffusionImg2ImgPipeline.from_pretrained(
          model_id,
          #scheduler=model_scheduler(model_id),
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          **safety
      )
    else:
      pipe_alt_diffusion_img2img = StableDiffusionImg2ImgPipeline.from_pretrained(
          model_id,
          #scheduler=model_scheduler(model_id),
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          torch_dtype=torch.float16,
          **safety
      )
    #pipe_alt_diffusion_img2img.to(torch_device)
    pipe_alt_diffusion_img2img = pipeline_scheduler(pipe_alt_diffusion_img2img)
    pipe_alt_diffusion_img2img = optimize_pipe(pipe_alt_diffusion_img2img)
    pipe_alt_diffusion_img2img.set_progress_bar_config(disable=True)
    return pipe_alt_diffusion_img2img

def postprocess_latent(pipe, latent):
    vae_output = pipe.vae.decode(
        latent.images / pipe.vae.config.scaling_factor, return_dict=False
    )[0].detach()
    return pipe.image_processor.postprocess(vae_output, output_type="pil")[0]

def seamless_tiling(pipeline, x_axis=True, y_axis=True):
    from diffusers.models.lora import LoRACompatibleConv
    def asymmetric_conv2d_convforward(self, input: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor] = None):
        self.paddingX = (self._reversed_padding_repeated_twice[0], self._reversed_padding_repeated_twice[1], 0, 0)
        self.paddingY = (0, 0, self._reversed_padding_repeated_twice[2], self._reversed_padding_repeated_twice[3])
        working = torch.nn.functional.pad(input, self.paddingX, mode=x_mode)
        working = torch.nn.functional.pad(working, self.paddingY, mode=y_mode)
        return torch.nn.functional.conv2d(working, weight, bias, self.stride, torch.nn.modules.utils._pair(0), self.dilation, self.groups)
    x_mode = 'circular' if x_axis else 'constant'
    y_mode = 'circular' if y_axis else 'constant'
    targets = [pipeline.vae, pipeline.text_encoder, pipeline.unet]
    convolution_layers = []
    for target in targets:
        for module in target.modules():
            if isinstance(module, torch.nn.Conv2d):
                convolution_layers.append(module)
    for layer in convolution_layers:
        if isinstance(layer, LoRACompatibleConv) and layer.lora_layer is None:
            layer.lora_layer = lambda * x: 0
        layer._conv_forward = asymmetric_conv2d_convforward.__get__(layer, torch.nn.Conv2d)
    return pipeline

SD_sampler = None
def get_stability(page):
    global prefs, SD_sampler#, stability_api
    '''try:
      from stability_sdk import client
      import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation
    except ImportError as e:
      run_process("pip install stability-sdk -q", page=page)
      from stability_sdk import client
      import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation
      pass
    stability_api = client.StabilityInference(
        key=prefs['Stability_api_key'],
        verbose=True,
        engine=prefs['model_checkpoint']# if prefs['model_checkpoint'] == "stable-diffusion-v1-5" else "stable-diffusion-v1",
    )
    SD_sampler = client.get_sampler_from_str(prefs['generation_sampler'].lower())'''
    # New way, other is obsolete
    page.status("installing stability")
    import requests
    api_host = os.getenv('API_HOST', 'https://api.stability.ai')
    stability_url = f"{api_host}/v1/engines/list" #user/account"
    response = requests.get(stability_url, headers={"Authorization": prefs['Stability_api_key']})
    if response.status_code != 200:
      alert_msg(page, "ERROR with Stability-ai: " + str(response.text))
      return
    payload = response.json()
    #print(str(payload))
    status['installed_stability'] = True
    page.status()

'''
def update_stability():
    global SD_sampler, stability_api
    from stability_sdk import client
    stability_api = client.StabilityInference(
        key=prefs['Stability_api_key'],
        verbose=True,
        engine=prefs['model_checkpoint']
    )
    SD_sampler = client.get_sampler_from_str(prefs['generation_sampler'].lower())
'''
def get_AIHorde(page):
    global prefs, status
    import requests
    from requests.packages.urllib3.exceptions import InsecureRequestWarning
    requests.packages.urllib3.disable_warnings(InsecureRequestWarning)# Suppress InsecureRequestWarning
    api_host = os.getenv('API_HOST', 'https://aihorde.net/api/')
    horde_url = f"{api_host}/v2/find_user" #user/account"
    #horde_url = f"https://aihorde.net/api/v2/find_user" #user/account"
    try:
      response = requests.get(horde_url, headers={"apikey": prefs['AIHorde_api_key'], 'accept': 'application/json'})
    except Exception as e:
      alert_msg(page, "ERROR with AIHorde Authentication", content=Text(str(e), selectable=True))
      return
    if response.status_code != 200:
      alert_msg(page, "ERROR {response.status_code} with AIHorde Authentication", content=Text(str(response.text), selectable=True))
      return
    payload = response.json()
    print(str(payload))
    AI_Horde = os.path.join(root_dir, "AI-Horde-CLI")
    if not os.path.exists(AI_Horde) or force_update("AI-Horde"):
      if os.path.exists(AI_Horde):
        #run_sp(f"chmod -R u+w {AI_Horde}")
        chmod_recursive(AI_Horde, 0o755)
        shutil.rmtree(AI_Horde, ignore_errors=True)
      try:
        run_sp("git clone -q https://github.com/Haidra-Org/AI-Horde-CLI.git", cwd=root_dir, realtime=False)
      except Exception:
        pass
      pip_install("loguru omegaconf tqdm pyyaml|yaml")
      #run_sp("pip install -r cli_requirements.txt --user", cwd=AI_Horde, realtime=False)
    status['installed_AIHorde'] = True

def get_ESRGAN(page, model=None, installer=None):
    global status
    def stat(msg):
        if installer is not None: installer.status(f"...{msg}")
    ESRGAN_folder = os.path.join(dist_dir, 'Real-ESRGAN')
    pretrained = os.path.join(ESRGAN_folder, 'experiments', 'pretrained_models')
    if model == None: model = prefs['upscale_model']
    if not os.path.isdir(ESRGAN_folder):
        os.chdir(dist_dir)
        stat(f"Installing Real-ESRGAN")
        run_sp(f"git clone https://github.com/xinntao/Real-ESRGAN.git -q", cwd=dist_dir)
        os.chdir(ESRGAN_folder)
        pip_install("basicsr facexlib gfpgan tqdm", q=True, cwd=ESRGAN_folder, installer=installer)
        stat(f"Setup Real-ESRGAN")
        run_sp(f"pip install -r requirements.txt --quiet", realtime=False, cwd=ESRGAN_folder)
        run_sp(f"python setup.py develop --quiet", realtime=False, cwd=ESRGAN_folder)
    if 'BSRGAN' in model:
        run_sp(f"git clone https://github.com/cszn/BSRGAN.git -q", cwd=ESRGAN_folder)
        run_sp(f"rm -r BSRGAN/testsets/RealSRSet", cwd=ESRGAN_folder)
    # Search https://openmodeldb.info/?q=real-esrgan
    model_url = ""
    for m in Real_ESRGAN_models:
        if m['name'] == model:
            model_url = m['url']
            break
    if not bool(model_url):
        print(f"ESRGAN model {model} not found.")
        return
    '''if model =="realesr-general-x4v3":
        model_url = "https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth"
    elif model == "RealESRGAN_x2plus":
        model_url = "https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth"
    elif model == "RealESRGAN_x4plus":
        model_url = "https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth"
    elif model == "BSRGANx2":
        model_url = "https://github.com/cszn/KAIR/releases/download/v1.0/BSRGANx2.pth"
    elif model == "realesr-animevideov3":
        model_url = "https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-animevideov3.pth"
    elif model == "4xLSDIRplus":
        model_url = "https://github.com/Phhofm/models/raw/main/4xLSDIRplus/4xLSDIRplus.pth"
    elif model == "2xLexicaRRDBNet_Sharp":
        model_url = "https://github.com/Phhofm/models/raw/main/2xLexicaRRDBNet/2xLexicaRRDBNet_Sharp.pth"
    elif model == "RealisticRescaler":
        model_url = "https://drive.google.com/drive/folders/13OC-hQNz_S-kX0EVjVgNO1eoGvcXrTfk?usp=sharing"
    elif model == "RealESRGAN_x4plus_anime":
        model_url = "https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth"'''
    if not os.path.isfile(os.path.join(pretrained, f"{model}.pth")):
        stat(f"downloading {model}.pth")
        if 'drive.google' in model_url:
            import gdown
            if 'folders' in model_url:
                gdown.download_folder(url=model_url, output=os.path.join(ESRGAN_folder, 'experiments', 'pretrained_models', f'{model}.pth'), quiet=True)
            else:
                gdown.download(url=model_url, output=os.path.join(ESRGAN_folder, 'experiments', 'pretrained_models', f'{model}.pth'), quiet=True)
        else:
            download_file(model_url, os.path.join(ESRGAN_folder, "experiments", "pretrained_models"))
            #run_sp(f"wget {model_url} -P experiments/pretrained_models --quiet", cwd=ESRGAN_folder)
    #run_process(f"wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models --quiet", page=page, cwd=os.path.join(dist_dir, 'Real-ESRGAN'))
    os.chdir(root_dir)
    status['installed_ESRGAN'] = True

def upscale_image(source, target, method="Real-ESRGAN", scale=4, face_enhance=False, model=None, installer=None):
    def stat(msg):
        if installer is not None: installer.status(f"...{msg}")
    if not isinstance(source, list): source = [source]
    if not os.path.isdir(target): target = os.path.dirname(target)
    saves = {}
    if model == None: model = prefs['upscale_model']
    if method=="Real-ESRGAN": #TODO: Add more ESRGAN model options
        ESRGAN_folder = os.path.join(dist_dir, 'Real-ESRGAN')
        if not status['installed_ESRGAN']:
            stat(f"Installing {method}")
            get_ESRGAN(None, model=model, installer=installer)
        model_file = os.path.join(ESRGAN_folder, 'experiments', 'pretrained_models', f'{model}.pth')
        if not os.path.isfile(model_file):
            stat(f"Downloading {model}")
            get_ESRGAN(None, model=model, installer=installer)
        stat("Initializing")
        upload_folder = os.path.join(ESRGAN_folder, 'upload')
        if 'BSRGAN' in model:
            upload_folder = os.path.join(ESRGAN_folder, 'BSRGAN', 'testsets', 'RealSRSet')
        result_folder = os.path.join(ESRGAN_folder, 'results')
        if os.path.isdir(upload_folder):
            shutil.rmtree(upload_folder)
        if os.path.isdir(result_folder):
            shutil.rmtree(result_folder)
        os.mkdir(upload_folder)
        os.mkdir(result_folder)
        #TODO: Support source as list
        for i in source:
            fname = os.path.basename(i)
            short_name = f'{fname[:80]}.png'
            dst_path = os.path.join(upload_folder, short_name)
            shutil.copy(i, dst_path)
            saves[short_name] = fname
        faceenhance = ' --face_enhance' if face_enhance else ''
        stat(f"Upscaling {method} {scale}X")
        try:
            run_sp(f'python inference_realesrgan.py -n {model} -i upload --outscale {scale}{faceenhance}', cwd=ESRGAN_folder, realtime=False)
        except Exception as e:
            stat(f"Error running {method}")
            print(f"Error running {method}: {e}")
            return
        out_file = short_name.rpartition('.')[0] + '_out.png'
        stat("Saving output")
        filenames = os.listdir(result_folder)
        for oname in filenames:
            #fparts = oname.rpartition('_out')
            #fname_clean = fparts[0] + fparts[2]
            fname_clean = saves[oname]
            opath = os.path.join(target, fname_clean)
            shutil.move(os.path.join(result_folder, oname), opath)
        #shutil.move(os.path.join(result_folder, out_file), target)
        # python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input upload --netscale 4 --outscale 3.5 --half --face_enhance
    elif method=="SRFormer":
        SRFormer_dir = os.path.join(dist_dir, "SRFormer")
        if not os.path.isdir(SRFormer_dir):
            stat(f"Installing {method}...")
            run_sp("git clone https://github.com/HVision-NKU/SRFormer", cwd=root_dir)
            pip_install("addict future lmdb pyyaml scikit-image scipy tb-nightly tqdm yapf", q=True, installer=installer)
            run_sp("python setup.py develop", cwd=SRFormer_dir)
        upload_folder = os.path.join(SRFormer_dir, 'upload')
        result_folder = os.path.join(SRFormer_dir, 'results')
        if os.path.isdir(upload_folder):
            shutil.rmtree(upload_folder)
        if os.path.isdir(result_folder):
            shutil.rmtree(result_folder)
        os.mkdir(upload_folder)
        os.mkdir(result_folder)
        for i in source:
            fname = os.path.basename(i)
            short_name = f'{fname[:80]}.png'
            dst_path = os.path.join(upload_folder, short_name)
            shutil.copy(i, dst_path)
            saves[short_name] = fname
        x = int(scale)
        if x < 2: x = 2
        if x > 4: x = 4
        stat(f"Upscaling {method} {scale}X")
        try:
            run_sp(f"python basicsr/infer_sr.py -opt options/test/SRFormer/test_SRFormer_DF2Ksrx{x}.yml --input_dir upload --output_dir results", cwd=SRFormer_dir)
        except Exception as e:
            print(f"Error running {method}: {e}")
            return
        stat("Saving output")
        to = os.path.dirname(target)
        for f in os.listdir(result_folder):
            img = os.path.join(result_folder, f)
            out = os.path.join(to, saves[f])
            shutil.move(img, out)
    else: #TODO: Add SwinIR, DAT and other Upscale methods
        print(f"Unknown upscale method {method}")
    #https://colab.research.google.com/gist/JingyunLiang/a5e3e54bc9ef8d7bf594f6fee8208533/swinir-demo-on-real-world-image-sr.ipynb
    

def get_concept(name):
  for con in concepts:
      if con['name'] == name:
        return con
  return {'name':'', 'token':''}

def get_conceptualizer(page):
    from huggingface_hub import hf_hub_download
    from diffusers import StableDiffusionPipeline
    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
    from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer
    global pipe_conceptualizer
    repo_id_embeds = f"sd-concepts-library/{prefs['concepts_model']}"
    embeds_url = "" #Add the URL or path to a learned_embeds.bin file in case you have one
    placeholder_token_string = "" #Add what is the token string in case you are uploading your own embed

    downloaded_embedding_folder = os.path.join(root_dir, "downloaded_embedding")
    if not os.path.exists(downloaded_embedding_folder):
      os.mkdir(downloaded_embedding_folder)
    try:
      if(not embeds_url):
        embeds_path = hf_hub_download(repo_id=repo_id_embeds, filename="learned_embeds.bin")
        token_path = hf_hub_download(repo_id=repo_id_embeds, filename="token_identifier.txt")
        shutil.copy(embeds_path, downloaded_embedding_folder)
        shutil.copy(token_path, downloaded_embedding_folder)
        with open(f'{downloaded_embedding_folder}/token_identifier.txt', 'r') as file:
          placeholder_token_string = file.read()
      else:
        download_file(embeds_url, downloaded_embedding_folder, "learned_embeds.bin")
        #run_sp(f"wget -q -O {downloaded_embedding_folder}/learned_embeds.bin {embeds_url}")
        #!wget -q -O $downloaded_embedding_folder/learned_embeds.bin $embeds_url
    except Exception as e:
      alert_msg(page, f"Error getting concept. May need to accept model at https://huggingface.co/sd-concepts-library/{prefs['concepts_model']}", content=Text(e))
      return
    learned_embeds_path = f"{downloaded_embedding_folder}/learned_embeds.bin"
    tokenizer = CLIPTokenizer.from_pretrained(model_path, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(model_path, subfolder="text_encoder", torch_dtype=torch.float16)
    def load_learned_embed_in_clip(learned_embeds_path, text_encoder, tokenizer, token=None):
      loaded_learned_embeds = torch.load(learned_embeds_path, map_location="cpu")
      trained_token = list(loaded_learned_embeds.keys())[0]
      embeds = loaded_learned_embeds[trained_token]
      dtype = text_encoder.get_input_embeddings().weight.dtype
      embeds.to(dtype)
      token = token if token is not None else trained_token
      num_added_tokens = tokenizer.add_tokens(token)
      if num_added_tokens == 0:
        alert_msg(page, f"The tokenizer already contains the token {token}. Please pass a different `token` that is not already in the tokenizer.")
        return
      text_encoder.resize_token_embeddings(len(tokenizer))
      token_id = tokenizer.convert_tokens_to_ids(token)
      text_encoder.get_input_embeddings().weight.data[token_id] = embeds
    try:
      load_learned_embed_in_clip(learned_embeds_path, text_encoder, tokenizer)
    except Exception as e:
      alert_msg(page, f"Error Loading Concept", content=Text(e))
      return
    pipe_conceptualizer = StableDiffusionPipeline.from_pretrained(
        model_path,
        variant="fp16",
        torch_dtype=torch.float16,
        text_encoder=text_encoder,
        tokenizer=tokenizer,
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        **safety,
    )
    pipe_conceptualizer = pipeline_scheduler(pipe_conceptualizer)
    pipe_conceptualizer = optimize_pipe(pipe_conceptualizer)
    pipe_conceptualizer.set_progress_bar_config(disable=True)
    #pipe_conceptualizer = pipe_conceptualizer.to(torch_device)
    return pipe_conceptualizer

def resize_for_condition_image(input_image: PILImage, resolution: int):
    input_image = input_image.convert("RGB")
    W, H = input_image.size
    k = float(resolution) / min(H, W)
    H *= k
    W *= k
    H = int(round(H / 64.0)) * 64
    W = int(round(W / 64.0)) * 64
    img = input_image.resize((W, H), resample=PILImage.Resampling.LANCZOS)
    return img

def flush():
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    #torch.cuda.clear_autocast_cache()

def clear_img2img_pipe():
  global pipe_img2img
  if pipe_img2img is not None:
    #print("Clearing out img2img pipeline for more VRAM")
    del pipe_img2img
    flush()
    pipe_img2img = None
def clear_txt2img_pipe():
  global pipe, compel_proc
  if pipe is not None:
    #print("Clearing out text2img pipeline for more VRAM")
    del pipe
    flush()
    pipe = None
    if prefs['SD_compel']:
      if compel_proc is not None:
        del compel_proc
        compel_proc = None
def clear_unet_pipe():
  global unet
  if unet is not None:
    #print("Clearing out unet custom pipeline for more VRAM")
    del unet
    flush()
    unet = None
def clear_SDXL_pipe():
  global pipe_SDXL, pipe_SDXL_refiner, compel_base, compel_refiner
  if prefs['SDXL_compel']:
    if compel_base is not None:
      del compel_base, compel_refiner
      compel_base = None
      compel_refiner = None
  if pipe_SDXL is not None:
    del pipe_SDXL
    flush()
    pipe_SDXL = None
  if pipe_SDXL_refiner is not None:
    del pipe_SDXL_refiner
    flush()
    pipe_SDXL_refiner = None
def clear_SD3_pipe():
  global pipe_SD3, compel_base
  if prefs['SD3_compel']:
    if compel_base is not None:
      del compel_base
      compel_base = None
  if pipe_SD3 is not None:
    del pipe_SD3
    flush()
    pipe_SD3 = None
def clear_clip_guided_pipe():
  global pipe_clip_guided
  if pipe_clip_guided is not None:
    #print("Clearing out CLIP Guided pipeline for more VRAM")
    del pipe_clip_guided
    flush()
    pipe_clip_guided = None
def clear_conceptualizer_pipe():
  global pipe_conceptualizer
  if pipe_conceptualizer is not None:
    #print("Clearing out CLIP Guided pipeline for more VRAM")
    del pipe_conceptualizer
    flush()
    pipe_conceptualizer = None
def clear_repaint_pipe():
  global pipe_repaint
  if pipe_repaint is not None:
    del pipe_repaint
    flush()
    pipe_repaint = None
def clear_imagic_pipe():
  global pipe_imagic
  if pipe_imagic is not None:
    del pipe_imagic
    flush()
    pipe_imagic = None
def clear_composable_pipe():
  global pipe_composable
  if pipe_composable is not None:
    del pipe_composable
    flush()
    pipe_composable = None
def clear_versatile_pipe():
  global pipe_versatile
  if pipe_versatile is not None:
    del pipe_versatile
    flush()
    pipe_versatile = None
def clear_versatile_text2img_pipe():
  global pipe_versatile_text2img
  if pipe_versatile_text2img is not None:
    del pipe_versatile_text2img
    flush()
    pipe_versatile_text2img = None
def clear_versatile_variation_pipe():
  global pipe_versatile_variation
  if pipe_versatile_variation is not None:
    del pipe_versatile_variation
    flush()
    pipe_versatile_variation = None
def clear_versatile_dualguided_pipe():
  global pipe_versatile_dualguided
  if pipe_versatile_dualguided is not None:
    del pipe_versatile_dualguided
    flush()
    pipe_versatile_dualguided = None
def clear_depth_pipe():
  global pipe_depth
  if pipe_depth is not None:
    del pipe_depth
    flush()
    pipe_depth = None
def clear_interpolation_pipe():
  global pipe_interpolation
  if pipe_interpolation is not None:
    del pipe_interpolation
    flush()
    pipe_interpolation = None
def clear_safe_pipe():
  global pipe_safe
  if pipe_safe is not None:
    del pipe_safe
    flush()
    pipe_safe = None
def clear_upscale_pipe():
  global pipe_upscale
  if pipe_upscale is not None:
    del pipe_upscale
    flush()
    pipe_upscale = None
def clear_image_variation_pipe():
  global pipe_image_variation
  if pipe_image_variation is not None:
    del pipe_image_variation
    flush()
    pipe_image_variation = None
def clear_semantic_pipe():
  global pipe_semantic
  if pipe_semantic is not None:
    del pipe_semantic
    flush()
    pipe_semantic = None
def clear_EDICT_pipe():
  global pipe_EDICT, text_encoder_EDICT
  if pipe_EDICT is not None:
    del pipe_EDICT
    del text_encoder_EDICT
    flush()
    pipe_EDICT = None
    text_encoder_EDICT = None
def clear_DiffEdit_pipe():
  global pipe_DiffEdit
  if pipe_DiffEdit is not None:
    del pipe_DiffEdit
    flush()
    pipe_DiffEdit = None
def clear_null_text_pipe():
  global pipe_null_text
  if pipe_null_text is not None:
    del pipe_null_text
    flush()
    pipe_null_text = None
def clear_unCLIP_pipe():
  global pipe_unCLIP
  if pipe_unCLIP is not None:
    del pipe_unCLIP
    flush()
    pipe_unCLIP = None
def clear_unCLIP_image_variation_pipe():
  global pipe_unCLIP_image_variation
  if pipe_unCLIP_image_variation is not None:
    del pipe_unCLIP_image_variation
    flush()
    pipe_unCLIP_image_variation = None
def clear_unCLIP_interpolation_pipe():
  global pipe_unCLIP_interpolation
  if pipe_unCLIP_interpolation is not None:
    del pipe_unCLIP_interpolation
    flush()
    pipe_unCLIP_interpolation = None
def clear_unCLIP_image_interpolation_pipe():
  global pipe_unCLIP_image_interpolation
  if pipe_unCLIP_image_interpolation is not None:
    del pipe_unCLIP_image_interpolation
    flush()
    pipe_unCLIP_image_interpolation = None
def clear_wuerstchen_pipe():
  global pipe_wuerstchen
  if pipe_wuerstchen is not None:
    del pipe_wuerstchen.prior_pipe
    del pipe_wuerstchen.decoder_pipe
    del pipe_wuerstchen
    flush()
    pipe_wuerstchen = None
def clear_stable_cascade_pipe():
  global pipe_stable_cascade_prior, pipe_stable_cascade_decoder
  if pipe_stable_cascade_prior is not None:
    del pipe_stable_cascade_prior
    del pipe_stable_cascade_decoder
    flush()
    pipe_stable_cascade_prior = None
    pipe_stable_cascade_decoder = None
def clear_pixart_alpha_pipe():
  global pipe_pixart_alpha, pipe_pixart_alpha_encoder
  if pipe_pixart_alpha is not None:
    del pipe_pixart_alpha, pipe_pixart_alpha_encoder
    flush()
    pipe_pixart_alpha = None
    pipe_pixart_alpha_encoder = None
def clear_pixart_sigma_pipe():
  global pipe_pixart_sigma, pipe_pixart_sigma_encoder
  if pipe_pixart_sigma is not None:
    del pipe_pixart_sigma, pipe_pixart_sigma_encoder
    flush()
    pipe_pixart_sigma = None
    pipe_pixart_sigma_encoder = None
    if pipe_SDXL_refiner is not None:
      del pipe_SDXL_refiner
      flush()
      pipe_SDXL_refiner = None
def clear_hunyuan_pipe():
  global pipe_hunyuan, pipe_SDXL_refiner
  if pipe_hunyuan is not None:
    del pipe_hunyuan
    flush()
    pipe_hunyuan = None
    if pipe_SDXL_refiner is not None:
      del pipe_SDXL_refiner
      flush()
      pipe_SDXL_refiner = None
def clear_lumina_pipe():
  global pipe_lumina
  if pipe_lumina is not None:
    del pipe_lumina
    flush()
    pipe_lumina = None
def clear_kolors_pipe():
  global pipe_kolors
  if pipe_kolors is not None:
    del pipe_kolors
    flush()
    pipe_kolors = None
def clear_auraflow_pipe():
  global pipe_auraflow
  if pipe_auraflow is not None:
    del pipe_auraflow
    flush()
    pipe_auraflow = None
def clear_layer_diffusion_pipe():
  global pipe_layer_diffusion
  if pipe_layer_diffusion is not None:
    global ld_text_encoder, ld_text_encoder_2, ld_vae, ld_unet, ld_transparent_decoder, ld_transparent_encoder
    del pipe_layer_diffusion, ld_text_encoder, ld_text_encoder_2, ld_vae, ld_unet, ld_transparent_decoder, ld_transparent_encoder
    flush()
    pipe_layer_diffusion = None
    ld_text_encoder, ld_text_encoder_2, ld_vae, ld_unet, ld_transparent_decoder, ld_transparent_encoder = [None] * 6
def clear_differential_diffusion_pipe():
  global pipe_differential_diffusion
  if pipe_differential_diffusion is not None:
    del pipe_differential_diffusion
    flush()
    pipe_differential_diffusion = None
def clear_magic_mix_pipe():
  global pipe_magic_mix
  if pipe_magic_mix is not None:
    del pipe_magic_mix
    flush()
    pipe_magic_mix = None
def clear_paint_by_example_pipe():
  global pipe_paint_by_example
  if pipe_paint_by_example is not None:
    del pipe_paint_by_example
    flush()
    pipe_paint_by_example = None
def clear_instruct_pix2pix_pipe():
  global pipe_instruct_pix2pix
  if pipe_instruct_pix2pix is not None:
    del pipe_instruct_pix2pix
    flush()
    pipe_instruct_pix2pix = None
def clear_ledits_pipe():
  global pipe_ledits
  if pipe_ledits is not None:
    del pipe_ledits
    flush()
    pipe_ledits = None
def clear_alt_diffusion_pipe():
  global pipe_alt_diffusion
  if pipe_alt_diffusion is not None:
    del pipe_alt_diffusion
    flush()
    pipe_alt_diffusion = None
def clear_alt_diffusion_img2img_pipe():
  global pipe_alt_diffusion_img2img
  if pipe_alt_diffusion_img2img is not None:
    del pipe_alt_diffusion_img2img
    flush()
    pipe_alt_diffusion_img2img = None
def clear_SAG_pipe():
  global pipe_SAG
  if pipe_SAG is not None:
    del pipe_SAG
    flush()
    pipe_SAG = None
def clear_PAG_pipe():
  global pipe_PAG
  if pipe_PAG is not None:
    del pipe_PAG
    flush()
    pipe_PAG = None
def clear_demofusion_pipe():
  global pipe_demofusion
  if pipe_demofusion is not None:
    del pipe_demofusion
    flush()
    pipe_demofusion = None
def clear_attend_and_excite_pipe():
  global pipe_attend_and_excite
  if pipe_attend_and_excite is not None:
    del pipe_attend_and_excite
    flush()
    pipe_attend_and_excite = None
def clear_lmd_plus_pipe():
  global pipe_lmd_plus
  if pipe_lmd_plus is not None:
    del pipe_lmd_plus
    flush()
    pipe_lmd_plus = None
def clear_lcm_pipe():
  global pipe_lcm
  if pipe_lcm is not None:
    del pipe_lcm
    flush()
    pipe_lcm = None
def clear_lcm_interpolation_pipe():
  global pipe_lcm_interpolation
  if pipe_lcm_interpolation is not None:
    del pipe_lcm_interpolation
    flush()
    pipe_lcm_interpolation = None
def clear_instaflow_pipe():
  global pipe_instaflow
  if pipe_instaflow is not None:
    del pipe_instaflow
    flush()
    pipe_instaflow = None
def clear_task_matrix_pipe():
  global pipe_task_matrix
  if pipe_task_matrix is not None:
    pipe_task_matrix.memory.clear()
    del pipe_task_matrix
    flush()
    pipe_task_matrix = None
def clear_ldm3d_pipe():
  global pipe_ldm3d, pipe_ldm3d_upscale
  if pipe_ldm3d is not None:
    del pipe_ldm3d, pipe_ldm3d_upscale
    flush()
    pipe_ldm3d = None
    pipe_ldm3d_upscale = None
def clear_svd_pipe():
  global pipe_svd
  if pipe_svd is not None:
    del pipe_svd
    flush()
    pipe_svd = None
def clear_diffsynth_pipe():
  global pipe_diffsynth, pipe_diffsynth_image, diffsynth_model_manager
  if pipe_diffsynth is not None:
    del pipe_diffsynth, pipe_diffsynth_image, diffsynth_model_manager
    flush()
    pipe_diffsynth, pipe_diffsynth_image, diffsynth_model_manager = [None] * 3
def clear_animatediff_img2video_pipe():
  global pipe_animatediff_img2video
  if pipe_animatediff_img2video is not None:
    del pipe_animatediff_img2video
    flush()
    pipe_animatediff_img2video = None
def clear_animatediff_sdxl_pipe():
  global pipe_animatediff_sdxl
  if pipe_animatediff_sdxl is not None:
    del pipe_animatediff_sdxl
    flush()
    pipe_animatediff_sdxl = None
def clear_pia_pipe():
  global pipe_pia
  if pipe_pia is not None:
    del pipe_pia
    flush()
    pipe_pia = None
def clear_easyanimate_pipe():
  global pipe_easyanimate, easyanimate_transformer
  if pipe_easyanimate is not None:
    del pipe_easyanimate, easyanimate_transformer
    flush()
    pipe_easyanimate, easyanimate_transformer = [None] *2
def clear_i2vgen_xl_pipe():
  global pipe_i2vgen_xl
  if pipe_i2vgen_xl is not None:
    del pipe_i2vgen_xl
    flush()
    pipe_i2vgen_xl = None
def clear_panorama_pipe():
  global pipe_panorama
  if pipe_panorama is not None:
    del pipe_panorama
    flush()
    pipe_panorama = None
def clear_dance_pipe():
  global pipe_dance
  if pipe_dance is not None:
    del pipe_dance
    flush()
    pipe_dance = None
def clear_audio_diffusion_pipe():
  global pipe_audio_diffusion
  if pipe_audio_diffusion is not None:
    del pipe_audio_diffusion
    flush()
    pipe_audio_diffusion = None
def clear_music_gen_pipe():
  global pipe_music_gen
  if pipe_music_gen is not None:
    del pipe_music_gen
    flush()
    pipe_music_gen = None
def clear_music_lang_pipe():
  global pipe_music_lang
  if pipe_music_lang is not None:
    del pipe_music_lang
    flush()
    pipe_music_lang = None
def clear_riffusion_pipe():
  global pipe_riffusion
  if pipe_riffusion is not None:
    del pipe_riffusion
    flush()
    pipe_riffusion = None
def clear_voice_fixer_pipe():
  global pipe_voice_fixer
  if pipe_voice_fixer is not None:
    del pipe_voice_fixer
    flush()
    pipe_voice_fixer = None
def clear_whisper_pipe():
  global pipe_whisper
  if pipe_whisper is not None:
    del pipe_whisper
    flush()
    pipe_whisper = None
def clear_text_to_video_pipe():
  global pipe_text_to_video
  if pipe_text_to_video is not None:
    del pipe_text_to_video
    flush()
    pipe_text_to_video = None
def clear_text_to_video_zero_pipe():
  global pipe_text_to_video_zero
  if pipe_text_to_video_zero is not None:
    del pipe_text_to_video_zero
    flush()
    torch.cuda.ipc_collect()
    torch.cuda.reset_peak_memory_stats()
    pipe_text_to_video_zero = None
def clear_video_to_video_pipe():
  global pipe_video_to_video
  if pipe_video_to_video is not None:
    del pipe_video_to_video
    flush()
    pipe_video_to_video = None
def clear_fresco_pipe():
  global pipe_fresco_v2v
  if pipe_fresco_v2v is not None:
    del pipe_fresco_v2v
    flush()
    pipe_fresco_v2v = None
def clear_latte_pipe():
  global pipe_latte
  if pipe_latte is not None:
    del pipe_latte
    flush()
    pipe_latte = None
def clear_open_sora_plan_pipe():
  global pipe_open_sora_plan
  if pipe_open_sora_plan is not None:
    del pipe_open_sora_plan
    flush()
    pipe_open_sora_plan = None
def clear_video_infinity_pipe():
  global pipe_video_infinity
  if pipe_video_infinity is not None:
    del pipe_video_infinity
    flush()
    pipe_video_infinity = None
def clear_infinite_zoom_pipe():
  global pipe_infinite_zoom
  if pipe_infinite_zoom is not None:
    del pipe_infinite_zoom
    flush()
    pipe_infinite_zoom = None
def clear_deepfloyd_pipe():
  global pipe_deepfloyd, pipe_deepfloyd2, pipe_deepfloyd3
  if pipe_deepfloyd is not None:
    del pipe_deepfloyd, pipe_deepfloyd2, pipe_deepfloyd3
    flush()
    pipe_deepfloyd = None
    pipe_deepfloyd2 = None
    pipe_deepfloyd3 = None
def clear_amused_pipe():
  global pipe_amused
  if pipe_amused is not None:
    del pipe_amused
    flush()
    pipe_amused = None
def clear_blip_diffusion_pipe():
  global pipe_blip_diffusion
  if pipe_blip_diffusion is not None:
    del pipe_blip_diffusion
    flush()
    pipe_blip_diffusion = None
def clear_anytext_pipe():
  global pipe_anytext
  if pipe_anytext is not None:
    del pipe_anytext
    flush()
    pipe_anytext = None
def clear_fuyu_pipe():
  global fuyu_tokenizer, fuyu_model, fuyu_processor
  if fuyu_tokenizer is not None:
    del fuyu_tokenizer
    del fuyu_model
    del fuyu_processor
    flush()
    fuyu_tokenizer = None
    fuyu_model = None
    fuyu_processor = None
def clear_moondream2_pipe():
  global moondream2_tokenizer, moondream2_model
  if moondream2_model is not None:
    del moondream2_model
    del moondream2_tokenizer
    flush()
    moondream2_model = None
    moondream2_tokenizer = None
def clear_reference_pipe():
  global pipe_reference
  if pipe_reference is not None:
    del pipe_reference
    flush()
    pipe_reference = None
def clear_ip_adapter_pipe():
  global pipe_ip_adapter
  if pipe_ip_adapter is not None:
    del pipe_ip_adapter
    flush()
    pipe_ip_adapter = None
def clear_hd_painter_pipe():
  global pipe_hd_painter
  if pipe_hd_painter is not None:
    del pipe_hd_painter
    flush()
    pipe_hd_painter = None
def clear_controlnet_qr_pipe():
  global pipe_controlnet_qr, pipe_controlnet
  if pipe_controlnet_qr is not None:
    del pipe_controlnet_qr
    flush()
    pipe_controlnet_qr = None
  if pipe_controlnet is not None:
    del pipe_controlnet
    flush()
    pipe_controlnet = None
def clear_controlnet_segment_pipe():
  global pipe_controlnet_segment, pipe_controlnet
  if pipe_controlnet_segment is not None:
    del pipe_controlnet_segment
    flush()
    pipe_controlnet_segment = None
  if pipe_controlnet is not None:
    del pipe_controlnet
    flush()
    pipe_controlnet = None
def clear_DiT_pipe():
  global pipe_DiT
  if pipe_DiT is not None:
    del pipe_DiT
    flush()
    pipe_DiT = None
def clear_kandinsky_pipe(all=True):
  global pipe_kandinsky, pipe_kandinsky_prior, pipe_kandinsky_controlnet_prior, loaded_kandinsky_task, depth_estimator
  if pipe_kandinsky is not None:
    del pipe_kandinsky
    if all:
      del pipe_kandinsky_prior
      del pipe_kandinsky_controlnet_prior
      del depth_estimator
    flush()
    pipe_kandinsky = None
    if all:
      pipe_kandinsky_prior = None
      pipe_kandinsky_controlnet_prior = None
      depth_estimator = None
    loaded_kandinsky_task = ""
def clear_tortoise_tts_pipe():
  global pipe_tortoise_tts
  if pipe_tortoise_tts is not None:
    del pipe_tortoise_tts
    flush()
    pipe_tortoise_tts = None
def clear_audio_ldm_pipe():
  global pipe_audio_ldm
  if pipe_audio_ldm is not None:
    del pipe_audio_ldm
    flush()
    pipe_audio_ldm = None
def clear_audio_ldm2_pipe():
  global pipe_audio_ldm2
  if pipe_audio_ldm2 is not None:
    del pipe_audio_ldm2
    flush()
    pipe_audio_ldm2 = None
def clear_music_ldm_pipe():
  global pipe_music_ldm
  if pipe_music_ldm is not None:
    del pipe_music_ldm
    flush()
    pipe_music_ldm = None
def clear_stable_audio_pipe():
  global pipe_stable_audio, config_stable_audio
  if pipe_stable_audio is not None:
    del pipe_stable_audio, config_stable_audio
    flush()
    pipe_stable_audio = None
    config_stable_audio = None
def clear_gpt2_pipe():
  global pipe_gpt2
  if pipe_gpt2 is not None:
    del pipe_gpt2
    flush()
    pipe_gpt2 = None
def clear_distil_gpt2_pipe():
  global pipe_distil_gpt2
  if pipe_distil_gpt2 is not None:
    del pipe_distil_gpt2
    flush()
    pipe_distil_gpt2 = None
def clear_superprompt_pipe():
  global pipe_superprompt, tokenizer_superprompt
  if pipe_superprompt is not None:
    del pipe_superprompt
    del tokenizer_superprompt
    flush()
    pipe_superprompt = None
    tokenizer_superprompt = None
def clear_background_remover_pipe():
  global pipe_background_remover
  if pipe_background_remover is not None:
    del pipe_background_remover
    flush()
    pipe_background_remover = None
def clear_shap_e_pipe():
  global pipe_shap_e
  if pipe_shap_e is not None:
    del pipe_shap_e
    flush()
    pipe_shap_e = None
def clear_marigold_depth_pipe():
  global pipe_marigold_depth
  if pipe_marigold_depth is not None:
    del pipe_marigold_depth
    flush()
    pipe_marigold_depth = None
def clear_tripo_pipe():
  global pipe_tripo
  if pipe_tripo is not None:
    del pipe_tripo
    flush()
    pipe_tripo = None
def clear_crm_pipe():
  global pipe_crm, crm_rembg_session
  if pipe_crm is not None:
    del pipe_crm, crm_rembg_session
    flush()
    pipe_crm = None
    crm_rembg_session=None
def clear_instantmesh_pipe():
  global pipe_instantmesh, instantmesh_model, instantmesh_rembg_session
  if pipe_instantmesh is not None:
    del pipe_instantmesh, instantmesh_model, instantmesh_rembg_session
    flush()
    pipe_instantmesh = None
    instantmesh_model = None
    instantmesh_rembg_session = None
def clear_splatter_image_pipe():
  global pipe_splatter_image, splatter_image_model, splatter_image_rembg_session
  if pipe_splatter_image is not None:
    del pipe_splatter_image, splatter_image_model, splatter_image_rembg_session
    flush()
    pipe_splatter_image = None
    splatter_image_model = None
    splatter_image_rembg_session = None
def clear_zoe_depth_pipe():
  global pipe_zoe_depth
  if pipe_zoe_depth is not None:
    del pipe_zoe_depth
    flush()
    pipe_zoe_depth = None
def clear_controlnet_pipe():
  global pipe_controlnet, controlnet, controlnet_models, controlnet_xl_models, status
  if pipe_controlnet is not None:
    del pipe_controlnet
    del controlnet
    for k, v in controlnet_models.items():
      if v != None:
        del v
        controlnet_models[k] = None
    for k, v in controlnet_xl_models.items():
      if v != None:
        del v
        controlnet_xl_models[k] = None
    flush()
    pipe_controlnet = None
    controlnet = None
    status['loaded_controlnet'] = None
def clear_stable_lm_pipe():
  global pipe_stable_lm, tokenizer_stable_lm
  if pipe_stable_lm is not None:
    del pipe_stable_lm
    del tokenizer_stable_lm
    flush()
    pipe_stable_lm = None
    tokenizer_stable_lm = None

def clear_pipes(allbut=None):
    if torch_device == "cpu": return
    but = [] if allbut == None else [allbut] if type(allbut) is str else allbut
    if not 'txt2img' in but: clear_txt2img_pipe()
    if not 'img2img' in but: clear_img2img_pipe()
    if not 'SDXL' in but: clear_SDXL_pipe()
    if not 'SD3' in but: clear_SD3_pipe()
    if not 'unet' in but: clear_unet_pipe()
    if not 'clip_guided' in but: clear_clip_guided_pipe()
    if not 'conceptualizer' in but: clear_conceptualizer_pipe()
    if not 'repaint' in but: clear_repaint_pipe()
    if not 'imagic' in but: clear_imagic_pipe()
    if not 'composable': clear_composable_pipe()
    if not 'versatile_text2img' in but: clear_versatile_text2img_pipe()
    if not 'versatile_variation' in but: clear_versatile_variation_pipe()
    if not 'versatile_dualguided' in but: clear_versatile_dualguided_pipe()
    if not 'depth' in but: clear_depth_pipe()
    if not 'interpolation' in but: clear_interpolation_pipe()
    if not 'safe' in but: clear_safe_pipe()
    if not 'upscale' in but: clear_upscale_pipe()
    if not 'unCLIP' in but: clear_unCLIP_pipe()
    if not 'EDICT' in but: clear_EDICT_pipe()
    if not 'DiffEdit' in but: clear_DiffEdit_pipe()
    if not 'null_text' in but: clear_null_text_pipe()
    if not 'unCLIP_image_variation' in but: clear_unCLIP_image_variation_pipe()
    if not 'unCLIP_interpolation' in but: clear_unCLIP_interpolation_pipe()
    if not 'image_variation' in but: clear_image_variation_pipe()
    if not 'semantic' in but: clear_semantic_pipe()
    if not 'wuerstchen' in but: clear_wuerstchen_pipe()
    if not 'stable_cascade' in but: clear_stable_cascade_pipe()
    if not 'pixart_alpha' in but: clear_pixart_alpha_pipe()
    if not 'pixart_sigma' in but: clear_pixart_sigma_pipe()
    if not 'hunyuan' in but: clear_hunyuan_pipe()
    if not 'lumina' in but: clear_lumina_pipe()
    if not 'kolors' in but: clear_kolors_pipe()
    if not 'auraflow' in but: clear_auraflow_pipe()
    if not 'layer_diffusion' in but: clear_layer_diffusion_pipe()
    if not 'differential_diffusion' in but: clear_differential_diffusion_pipe()
    if not 'magic_mix' in but: clear_magic_mix_pipe()
    if not 'alt_diffusion' in but: clear_alt_diffusion_pipe()
    if not 'alt_diffusion_img2img' in but: clear_alt_diffusion_img2img_pipe()
    if not 'paint_by_example' in but: clear_paint_by_example_pipe()
    if not 'instruct_pix2pix' in but: clear_instruct_pix2pix_pipe()
    if not 'ledits' in but: clear_ledits_pipe()
    if not 'SAG' in but: clear_SAG_pipe()
    if not 'demofusion' in but: clear_demofusion_pipe()
    if not 'attend_and_excite' in but: clear_attend_and_excite_pipe()
    if not 'lmd_plus' in but: clear_lmd_plus_pipe()
    if not 'lcm' in but: clear_lcm_pipe()
    if not 'lcm_interpolation' in but: clear_lcm_interpolation_pipe()
    if not 'instaflow' in but: clear_instaflow_pipe()
    if not 'PAG' in but: clear_PAG_pipe()
    if not 'task_matrix' in but: clear_task_matrix_pipe()
    if not 'ldm3d' in but: clear_ldm3d_pipe()
    if not 'svd' in but: clear_svd_pipe()
    if not 'diffsynth' in but: clear_diffsynth_pipe()
    if not 'animatediff_img2video' in but: clear_animatediff_img2video_pipe()
    if not 'animatediff_sdxl' in but: clear_animatediff_sdxl_pipe()
    if not 'pia' in but: clear_pia_pipe()
    if not 'easyanimate' in but: clear_easyanimate_pipe()
    if not 'i2vgen_xl' in but: clear_i2vgen_xl_pipe()
    if not 'deepfloyd' in but: clear_deepfloyd_pipe()
    if not 'amused' in but: clear_amused_pipe()
    if not 'blip_diffusion' in but: clear_blip_diffusion_pipe()
    if not 'anytext' in but: clear_anytext_pipe()
    if not 'fuyu' in but: clear_fuyu_pipe()
    if not 'moondream2' in but: clear_moondream2_pipe()
    if not 'ip_adapter' in but: clear_ip_adapter_pipe()
    if not 'hd_painter' in but: clear_hd_painter_pipe()
    if not 'reference' in but: clear_reference_pipe()
    if not 'controlnet_qr' in but: clear_controlnet_qr_pipe()
    if not 'controlnet_segment' in but: clear_controlnet_segment_pipe()
    if not 'DiT' in but: clear_DiT_pipe()
    if not 'controlnet' in but: clear_controlnet_pipe()
    if not 'panorama' in but: clear_panorama_pipe()
    if not 'kandinsky' in but: clear_kandinsky_pipe(all=not ('kandinsky_prior' in but or 'kandinsky_controlnet_prior' in but))
    if not 'dance' in but: clear_dance_pipe()
    if not 'riffusion' in but: clear_riffusion_pipe()
    if not 'audio_diffusion' in but: clear_audio_diffusion_pipe()
    if not 'music_gen' in but: clear_music_gen_pipe()
    if not 'music_lang' in but: clear_music_lang_pipe()
    if not 'voice_fixer' in but: clear_voice_fixer_pipe()
    if not 'whisper' in but: clear_whisper_pipe()
    if not 'text_to_video' in but: clear_text_to_video_pipe()
    if not 'text_to_video_zero' in but: clear_text_to_video_zero_pipe()
    if not 'video_to_video' in but: clear_video_to_video_pipe()
    if not 'fresco' in but: clear_fresco_pipe()
    if not 'latte' in but: clear_latte_pipe()
    if not 'open_sora_plan' in but: clear_open_sora_plan_pipe()
    if not 'video_infinity' in but: clear_video_infinity_pipe()
    if not 'infinite_zoom' in but: clear_infinite_zoom_pipe()
    if not 'tortoise_tts' in but: clear_tortoise_tts_pipe()
    if not 'audio_ldm' in but: clear_audio_ldm_pipe()
    if not 'audio_ldm2' in but: clear_audio_ldm2_pipe()
    if not 'music_ldm' in but: clear_music_ldm_pipe()
    if not 'stable_audio' in but: clear_stable_audio_pipe()
    if not 'gpt2' in but: clear_gpt2_pipe()
    if not 'distil_gpt2' in but: clear_distil_gpt2_pipe()
    if not 'superprompt' in but: clear_superprompt_pipe()
    if not 'shap_e' in but: clear_shap_e_pipe()
    if not 'zoe_depth' in but: clear_zoe_depth_pipe()
    if not 'marigold_depth' in but: clear_marigold_depth_pipe()
    if not 'tripo' in but: clear_tripo_pipe()
    if not 'instantmesh' in but: clear_instantmesh_pipe()
    if not 'splatter_image' in but: clear_splatter_image_pipe()
    if not 'crm' in but: clear_crm_pipe()
    if not 'background_remover' in but: clear_background_remover_pipe()
    if not 'controlnet' in but: clear_controlnet_pipe()
    if not 'stable_lm' in but: clear_stable_lm_pipe()
    try:
        torch.cuda.ipc_collect()
        #torch.cuda.reset_max_memory_allocated()
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
    except Exception:
        pass
import base64
def get_base64(image_path):
    with open(image_path, "rb") as img_file:
        base64_string = base64.b64encode(img_file.read()).decode('utf-8')
        return base64_string
def pil_to_base64(image):
    image_stream = io.BytesIO()
    image.save(image_stream, format='PNG')
    image_bytes = image_stream.getvalue()
    base64_string = base64.b64encode(image_bytes).decode('utf-8')
    return base64_string

def available_file(folder, name, idx=None, ext='png', no_num=False, zfill=None):
  available = False
  idx = int(idx) if idx is not None else 1 if prefs['file_from_1'] else 0
  while not available:
    # Todo, check if using PyDrive2
    if no_num:
      if not os.path.isfile(os.path.join(folder, f'{name}.{ext}')):
        return os.path.join(folder, f'{name}.{ext}')
    i = str(idx) if zfill is None else str(idx).zfill(zfill)
    if os.path.isfile(os.path.join(folder, f'{name}-{i}.{ext}')):
      idx += 1
    else: available = True
  return os.path.join(folder, f'{name}-{i}.{ext}')

def available_folder(folder, name, idx):
  available = False
  while not available:
    if os.path.isdir(os.path.join(folder, f'{name}-{idx}')):
      idx += 1
    else: available = True
  return os.path.join(folder, f'{name}-{idx}')

def filepath_to_url(path):
    if is_Colab:
        from urllib.parse import quote
        path = quote(asset_dir(path))
        return path
    windows_path_pattern = re.compile(r"(.)\:\/")
    linux_path_pattern = re.compile(r"^\/")
    path = path.replace('\\', '/')
    is_windows_path = windows_path_pattern.match(path)
    is_linux_path = linux_path_pattern.match(path)
    if is_windows_path:
        drive_letter = windows_path_pattern.match(path).group(1)
        path = re.sub(windows_path_pattern, f"file:///{drive_letter}:/", path)
    elif is_linux_path:
        path = re.sub(linux_path_pattern, "file:///", path)
    else:
        path = f"file:///{path}"
    path = path.replace(" ", "%20")
    return path

#import asyncio
#async
def start_diffusion(page):
  global pipe, unet, pipe_img2img, pipe_clip_guided, pipe_interpolation, pipe_conceptualizer, pipe_imagic, pipe_depth, pipe_composable, pipe_versatile_text2img, pipe_versatile_variation, pipe_versatile_dualguided, pipe_SAG, pipe_attend_and_excite, pipe_alt_diffusion, pipe_alt_diffusion_img2img, pipe_panorama, pipe_safe, pipe_upscale, pipe_SDXL, pipe_SDXL_refiner, pipe_SD3
  global SD_sampler, stability_api, total_steps, pb, prefs, args, total_steps, compel_proc, compel_base, compel_refiner, abort_run
  def prt(line, update=True):
    if type(line) == str:
      line = Text(line)
    try:
      page.imageColumn.controls.append(line)
      if update:
        page.imageColumn.update()
    except Exception:
      clear_image_output()
      pass
    if update:
      page.Images.update()
  def clear_last(update=True):
    del page.imageColumn.controls[-1]
    if update:
      page.imageColumn.update()
      page.Images.update()
  def abort_diffusion(e):
    global abort_run
    abort_run = True
    page.snd_error.play()
    page.snd_delete.play()
  def callback_cancel(cancel) -> None:
    callback_cancel.has_been_called = True
    if abort_run:
      return True
  def download_image(e):
    if is_Colab:
      print(f"{type(e.control.data)} {e.control.data}")
      from google.colab import files
      if os.path.isfile(e.control.data):
        files.download(e.control.data)
      else:
        time.sleep(5)
        files.download(e.control.data)
  def clear_image_output():
    for co in reversed(page.imageColumn.controls):
      del co
    page.imageColumn.controls.clear()
    try:
      page.imageColumn.update()
    except Exception as e:
      try:
        page.imageColumn = Column([], auto_scroll=True, scroll=ScrollMode.AUTO)
      except Exception as er:
        alert_msg(page, f"ERROR: Problem Clearing Image Output List. May need to stop script and restart app to recover, sorry...", content=Text(f'{e}\n{er}'))
        page.Images = buildImages(page)
        pass
      page.update()
      pass
# Why getting Exception: control with ID '_3607' not found when re-running after error
  #page.Images.content.controls = []
  if int(status['cpu_memory']) <= 8 and ((prefs['use_SDXL'] and status['installed_SDXL']) or (prefs['use_SD3'] and status['installed_SD3'])): # and prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']
      alert_msg(page, f"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run SD XL. Either Change runtime type to High-RAM mode and restart or use other pipelines.")
      return
  clear_image_output()
  #pb = ProgressBar(bar_height=8)
  pb.width=(page.width if page.web else page.window.width) - 50
  #prt(Row([Text("▶️   Running Stable Diffusion on Batch Prompts List", style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), IconButton(icon=icons.CANCEL, tooltip="Abort Current Diffusion Run", on_click=abort_diffusion)], alignment=MainAxisAlignment.SPACE_BETWEEN))
  prt(Header("▶️   Running Stable Diffusion on Batch Prompts List", actions=[IconButton(icon=icons.CANCEL, tooltip="Abort Current Diffusion Run", on_click=abort_diffusion)]))
  import string, shutil, random, gc, io, json
  from collections import ChainMap
  from PIL.PngImagePlugin import PngInfo
  from contextlib import contextmanager, nullcontext
  import copy

  if status['installed_diffusers']:
    from diffusers import StableDiffusionPipeline
  os.chdir(stable_dir)
  generator = None
  clear_repaint_pipe()
  output_files = []
  pipe_used = ""
  retry_attempts_if_NSFW = prefs['retry_attempts']
  #if (prefs['use_Stability_api'] and status['installed_stability']) or bool(not status['installed_diffusers'] and status['installed_stability']):
  #  update_stability()
  last_seed = args['seed']
  if args['seed'] < 1 or args['seed'] is None:
    rand_seed = random.randint(0,2147483647)
    if (not (prefs['use_Stability_api'] or (not status['installed_diffusers'] and status['installed_stability']))) and (not (prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde']))):
      if use_custom_scheduler:
        generator = torch.manual_seed(rand_seed)
      else:
        generator = torch.Generator("cuda").manual_seed(rand_seed)
    last_seed = rand_seed
  else:
    rand_seed = args['seed']
    if not (prefs['use_Stability_api'] or (not status['installed_diffusers'] and status['installed_stability'])) and (not (prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde']))):
      if use_custom_scheduler:
        generator = torch.manual_seed(args['seed'])
      else:
        generator = torch.Generator("cuda").manual_seed(args['seed'])
  strikes = 0
  p_idx = 0
  if prefs['centipede_prompts_as_init_images']:
    os.makedirs(os.path.join(root_dir, 'init_images'), exist_ok=True)
  last_image = None
  updated_prompts = []
  model = get_model(prefs['model_ckpt'])
      
  if not (prefs["use_interpolation"] and status['installed_interpolation']):
    for p in prompts:
      pr = None
      arg = {}
      if type(p) == list or type(p) == str:
        pr = p
        arg = args.copy()
      elif isinstance(p, Dream):
        pr = p.prompt
        arg = merge_dict(args, p.arg)
      else: prt(f'Unknown item in list of type {type(p)}')
      #print(str(arg))
      arg['width'] = int(arg['width'])
      arg['height'] = int(arg['height'])
      arg['seed'] = int(arg['seed'])
      arg['guidance_scale'] = float(arg['guidance_scale'])
      arg['batch_size'] = int(arg['batch_size'])
      arg['n_iterations'] = int(arg['n_iterations'])
      arg['steps'] = int(arg['steps'])
      arg['eta'] = float(arg['eta'])
      arg['init_image_strength'] = float(arg['init_image_strength'])
      p.arg = arg
      iterations = arg['n_iterations']
      updated_prompts.append(p)
      if iterations > 1:
        #print(f"Iterating {iterations} times - {pr}")
        for d in range(iterations - 1):
          new_dream = None
          if isinstance(p, Dream):
            new_dream = copy.copy(p)
            new_dream.prompt = pr[0] if type(pr) == list else pr
            new_arg = new_dream.arg.copy()
            new_arg['seed'] = random.randint(0,2147483647)
            new_arg['n_iterations'] = 1
            new_dream.arg = new_arg
            #new_dream.arg['seed'] = random.randint(0,4294967295)
          else:
            new_dream = Dream(p, seed=random.randint(0,2147483647), n_iterations=1)
          new_dream.arg['n_iterations'] = 1
          #prompts.insert(p_idx+1, new_dream)
          updated_prompts.append(new_dream)
    prefix = ""
    if bool(model['prefix']):
      if model['prefix'][-1] != ' ':
        model['prefix'] = model['prefix'] + ' '
      prefix = model['prefix']
    lcm_lora = False
    if prefs['use_LoRA_model']:
      if prefs['use_SDXL'] and status['installed_SDXL']:
        for l in prefs['active_SDXL_LoRA_layers']:
          if 'name' in l:
            if 'LCM' in l['name']:
              lcm_lora = True
          if 'prefix' in l:
           if bool(l['prefix']):
              prefix += l['prefix']
              if prefix[-1] != ' ':
                prefix += ' '
        #lora = get_SDXL_LoRA_model(prefs['SDXL_LoRA_model'])
      #TODO: Add SD3 LoRA
      else:
        for l in prefs['active_LoRA_layers']:
          if 'name' in l:
            if 'LCM' in l['name']:
              lcm_lora = True
          if 'prefix' in l:
           if bool(l['prefix']):
              prefix += l['prefix']
              if prefix[-1] != ' ':
                prefix += ' '
        #lora = get_LoRA_model(prefs['LoRA_model'])
    ip_adapter_arg = {}
    if prefs['use_ip_adapter']:
      ip_adapter_img = None
      if prefs['ip_adapter_image'].startswith('http'):
        i_response = requests.get(prefs['ip_adapter_image'])
        ip_adapter_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
      else:
        if os.path.isfile(prefs['ip_adapter_image']):
          ip_adapter_img = PILImage.open(prefs['ip_adapter_image'])
        else:
          clear_last()
          prt(f"ERROR: Couldn't find your ip_adapter_image {prefs['ip_adapter_image']}")
      if bool(ip_adapter_img):
        ip_adapter_arg['ip_adapter_image'] = ip_adapter_img
        
    for p in updated_prompts:
      pb.value = None
      pr = ""
      images = None
      usable_image = True
      arg = {}
      if type(p) == list or type(p) == str:
        if (status['installed_stability'] and prefs['use_Stability_api']) or (status['installed_AIHorde'] and prefs['use_AIHorde_api']):
          pr = p
        else:
          pr = prefix + p
        arg = args.copy()
      elif isinstance(p, Dream):
        if (status['installed_stability'] and prefs['use_Stability_api']) or (status['installed_AIHorde'] and prefs['use_AIHorde_api']):
          pr = prefix + p.prompt
        else:
          pr = p.prompt
        arg = merge_dict(args, p.arg)
      else: prt(f"Unknown object {type(p)} in the prompt list")
      if arg['batch_size'] > 1:
        pr = [pr] * arg['batch_size']
        #if bool(arg['negative_prompt']):
        arg['negative_prompt'] = [arg['negative_prompt']] * arg['batch_size']
      if last_seed != arg['seed']:
        if arg['seed'] < 1 or arg['seed'] is None:
          rand_seed = random.randint(0,2147483647)
          if (not (prefs['use_Stability_api'] or (not status['installed_diffusers'] and status['installed_stability']))) and (not (prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde']))):
            if use_custom_scheduler:
              generator = torch.manual_seed(rand_seed)
            else:
              generator = torch.Generator("cuda").manual_seed(rand_seed)
          arg['seed'] = rand_seed
        else:
          if (not(prefs['use_Stability_api'] or (not status['installed_diffusers'] and status['installed_stability']))) and (not (prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde']))):
            if use_custom_scheduler:
              generator = torch.manual_seed(arg['seed'])
            else:
              generator = torch.Generator("cuda").manual_seed(arg['seed'])
        last_seed = arg['seed']
      if prefs['centipede_prompts_as_init_images'] and last_image is not None:
        arg['init_image'] = last_image
      p_count = f'[{p_idx + 1} of {len(updated_prompts)}]  '
      #if p_idx % 30 == 0 and p_idx > 1:
      #  clear_output()
      #  print(f"{Color.BEIGE2}Cleared console display due to memory limit in console logging.  Images still saving.{Color.END}")
      #prt(Divider(height=6, thickness=2), update=False)
      prt(Row([Text(p_count), Text(pr[0] if type(pr) == list else pr, expand=True, weight=FontWeight.BOLD), Text(f'seed: {arg["seed"]}     ')]))
      time.sleep(0.1)
      page.auto_scrolling(False)
      #prt(p_count + ('─' * 90))
      #prt(f'{pr[0] if type(pr) == list else pr} - seed:{arg["seed"]}')
      total_steps = arg['steps']
      #if prefs['use_Stability_api'] or bool(arg['use_Stability'] or (not status['installed_diffusers'] and status['installed_stability'])):
      
      if prefs['install_AIHorde_api'] and (prefs['use_AIHorde_api'] or not status['installed_diffusers']):# and status['installed_AIHorde']:# or bool(prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde'])):
        if not status['installed_AIHorde']:
          prt(Installing("Installing AI-Horde Cloudsourced Pipeline..."))
          get_AIHorde(page)
          clear_last()
          #alert_msg(page, f"ERROR: To use AIHorde-API, you must run the install it first and have proper API key")
          #return
        stats = Text(f"Stable Horde API Diffusion - Using {prefs['AIHorde_model']} Model...")
        prt(stats)
        #prt('Stable Horde API Diffusion ')# + ('─' * 100))
        #print(f'"{SD_prompt}", height={SD_height}, width={SD_width}, steps={SD_steps}, cfg_scale={SD_guidance_scale}, seed={SD_seed}, sampler={generation_sampler}')
        #strikes = 0
        images = []
        arg['width'] = multiple_of_64(arg['width'])
        arg['height'] = multiple_of_64(arg['height'])
        pb.value = None
        prt(pb)
        nudge(page.imageColumn, page=page)
        answers = None
        response = None

        import requests
        from io import BytesIO
        import base64
        api_host = 'https://aihorde.net/api'
        engine_id = prefs['AIHorde_model']
        api_check_url = f"{api_host}/v2/generate/check/"
        api_get_result_url = f"{api_host}/v2/generate/status/"
        url = f"{api_host}/v2/generate/async"
        headers = {
            #'Content-Type': 'application/json',
            #'Accept': 'application/json',
            'apikey': prefs['AIHorde_api_key'],
        }
        text_prompt = pr[0] if type(pr) == list else pr
        if bool(arg['negative_prompt']):
          text_prompt += "###" +arg['negative_prompt'][0] if type(arg['negative_prompt']) == list else arg['negative_prompt']
        payload = {
          "prompt": text_prompt,
          "nsfw": prefs["disable_nsfw_filter"],
          "models": [prefs["AIHorde_model"]]
        }
        params = {
          "cfg_scale": arg['guidance_scale'],
          "denoising_strength": arg['init_image_strength'],
          "width": arg['width'],
          "height": arg['height'],
          "sampler_name": prefs['AIHorde_sampler'],
          "n": arg['batch_size'],
          "seed": str(arg['seed']),
          "seed_variation": 1,
          "steps": arg['steps'],
          "post_processing": [],
          "karras": prefs['AIHorde_karras'],
          "tiling": prefs['AIHorde_tiling'],
          "hires_fix": prefs['AIHorde_hires_fix'] and 'XL' not in prefs["AIHorde_model"],
        }
        if prefs['AIHorde_post_processing'] != "None":
          params['post_processing'] = [prefs['AIHorde_post_processing']]
        if prefs['AIHorde_strip_background']:
          params['post_processing'].append("strip_background")
        if prefs['AIHorde_transparent']:
          params['transparent'] = True
        AIHorde_loras = []
        if len(prefs['AIHorde_lora_map']) > 0:
          for l in prefs['AIHorde_lora_map']:
            lora = {
              "name": l['model'],
              "model": float(l['scale']),
              "clip": l['clip'] if 'clip' in l else 1,
              "is_version": True,#l['is_version'] if 'is_version' in l else False,
            }
            if 'inject_trigger' in l:
              lora['inject_trigger'] = l['inject_trigger']
            AIHorde_loras.append(lora)
        if AIHorde_loras:
          params['lora'] = AIHorde_loras #json.dumps(AIHorde_loras, indent = 4)
        if bool(arg['mask_image']) or (bool(arg['init_image']) and arg['alpha_mask']):
          if not bool(arg['init_image']):
            clear_last()
            prt(f"ERROR: You have not selected an init_image to go with your image mask..")
            continue
          if arg['init_image'].startswith('http'):
            i_response = requests.get(arg['init_image'])
            init_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
          else:
            if os.path.isfile(arg['init_image']):
              init_img = PILImage.open(arg['init_image'])
            else:
              clear_last()
              prt(f"ERROR: Couldn't find your init_image {arg['init_image']}")
          init_img = init_img.resize((arg['width'], arg['height']))
          img_str = pil_to_base64(init_img)
          #buff = BytesIO()
          #init_img.save(buff, format="PNG")
          #buff.seek(0)
          #img_str = io.BufferedReader(buff).read()
          #init_image = preprocess(init_img)
          if not arg['alpha_mask']:
            if arg['mask_image'].startswith('http'):
              i_response = requests.get(arg['mask_image'])
              mask_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
            else:
              if os.path.isfile(arg['mask_image']):
                mask_img = PILImage.open(arg['mask_image'])
              else:
                clear_last()
                prt(f"ERROR: Couldn't find your mask_image {arg['mask_image']}")
            mask_img = mask_img.resize((arg['width'], arg['height']))
            mask_str = pil_to_base64(mask_img)
            #buff = BytesIO()
            #mask.save(buff, format="PNG")
            #buff.seek(0)
            #mask_str = io.BufferedReader(buff).read()
          payload['source_image'] = img_str
          if not arg['alpha_mask']:
            payload['source_mask'] = mask_str
          pipe_used = "Stable Horde-API Inpainting"
          payload['source_processing'] = "inpainting"
          #engine_id = prefs['model_checkpoint'] if prefs['model_checkpoint'] == "stable-diffusion-v1-5" else "stable-diffusion-v1"
          #response = requests.post(url+"image-to-image/masking", headers=headers, files=files)
          #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], mask_image=mask, init_image=init_img, start_schedule= 1 - arg['init_image_strength'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], samples=arg['batch_size'], safety=not prefs["disable_nsfw_filter"], seed=arg['seed'], sampler=SD_sampler)
        elif bool(arg['init_image']):
          if arg['init_image'].startswith('http'):
            i_response = requests.get(arg['init_image'])
            init_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
          else:
            if os.path.isfile(arg['init_image']):
              init_img = PILImage.open(arg['init_image']).convert("RGB")
            else:
              clear_last()
              prt(f"ERROR: Couldn't find your init_image {arg['init_image']}")
          init_img = init_img.resize((arg['width'], arg['height']))
          img_str = pil_to_base64(init_img)
          #buff = BytesIO()
          #init_img.save(buff, format="PNG")
          #buff.seek(0)
          #img_str = io.BufferedReader(buff).read()
          #img_str = open(buff.read(), 'rb') #base64.b64encode(buff.getvalue())  init_img.tobytes("raw")
          payload['source_image'] = img_str
          if prefs["AIHorde_use_controlnet"]:
            params["image_is_control"] = True
            params["control_type"] = prefs["AIHorde_controlnet"].lower()
            params["return_control_map"] = False
            pipe_used = f"Stable Horde-API ControlNet {prefs['AIHorde_controlnet']}"
          else:
            pipe_used = "Stable Horde-API Image-to-Image"
          payload['source_processing'] = "img2img"
          #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], init_image=init_img, start_schedule= 1 - arg['init_image_strength'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], samples=arg['batch_size'], safety=not prefs["disable_nsfw_filter"], seed=arg['seed'], sampler=SD_sampler)
        else:
          pipe_used = "Stable Horde-API Text-to-Image"
          #response = requests.post(url+"text-to-image", headers=headers, json=payload)
          #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], seed=arg['seed'], samples=arg['batch_size'], safety=False, sampler=SD_sampler)
        payload["params"] = params
        #print(params)
        try:
            response = requests.post(url, headers=headers, json=payload)#json.dumps(payload, indent = 4))
        except Exception as e:
            alert_msg(page, f"ERROR: Problem sending JSON request and getting response.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]), debug_pref=payload)
            print(payload)
            return
        if response != None:
          if response.status_code != 202:
            clear_last()
            if response.status_code == 400:
              alert_msg(page, "Stable Horde-API ERROR: Validation Error...", content=Text(str(response.text)))
              return
            else:
              prt(Text(f"Stable Horde-API ERROR {response.status_code}: " + str(response.text), selectable=True))
              print(payload)
              continue
        artifacts = json.loads(response.content)
        q_id = artifacts['id']
        #print(str(artifacts))
        elapsed_seconds = 0
        try:
          while True:
            if abort_run: break
            check_response = requests.get(api_check_url + q_id)
            check = json.loads(check_response.content)
            #print(check)
            try:
              div = check['wait_time'] + elapsed_seconds
              percentage = (1 - check['wait_time'] / div)
            except Exception:
              div = 0
              continue
              pass
            if div == 0: continue
            pb.value = percentage
            pb.update()
            status_txt = f"Stable Horde API Diffusion - Queued Position: {check['queue_position']} - Waiting: {check['waiting']} - Wait Time: {check['wait_time']} - Elapsed: {elapsed_seconds}s"
            stats.value = status_txt #str(check)
            stats.update()
            if bool(check['done']):
              kudos = check['kudos']
              print(f"AI-Horde Kudos Used: {kudos}")
              break
            time.sleep(2)
            elapsed_seconds += 2
        except Exception as e:
          alert_msg(page, f"EXCEPTION ERROR: Unknown error processing image. Check parameters and try again. Restart app if persists.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
          return
        if abort_run:
          clear_last(False)
          clear_last()
          prt(Text("🛑   Aborting Current Diffusion Run..."))
          abort_run = False
          requests.delete(api_get_result_url + q_id)
          return
        attempts = 0
        success = False
        while not success:
          try:
            get_response = requests.get(api_get_result_url + q_id)
            success = True
          except Exception as e:
            attempts += 1
            if attempts < 3:
              print(f"ERROR: {e}. Trying again {attempts}/3")
              time.sleep(3)
              pass
            else:
              clear_last(update=False)
              clear_last()
              prt(f"💢  ERROR after 3 attempts: {e}")
              return
        final_results = json.loads(get_response.content)
        clear_last(update=False)
        clear_last()
        for gen in final_results["generations"]:
          #print(f'{type(resp)} - {resp["seed"]}')
          if gen["censored"]:
            prt(f"Couldn't process NSFW text in prompt.  Can't retry so change your request.")
            #TODO: Retry Attempts
            continue
          img_response = requests.get(gen['img'])
          webp_file = io.BytesIO(img_response.content)
          cv_img = cv2.imdecode(np.frombuffer(webp_file.read(), np.uint8), cv2.IMREAD_UNCHANGED)
          images.append(PILImage.fromarray(cv_img))
          #cv_img = cv2.imdecode(io.BytesIO(base64.b64decode(gen['img'])), cv2.IMREAD_COLOR)
          #cv_img = cv2.imdecode(np.frombuffer(base64.b64decode(gen['img']), dtype=np.uint8), cv2.IMREAD_UNCHANGED)
          #decoded_string = base64.b64decode(gen['img'])
          #nparr = np.frombuffer(decoded_string, np.uint8)
          #cv_img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
          #print(gen['img'])
          #print(str(type(gen['img'])))
          #img_bytes = io.BytesIO(decoded_string)
          #img = Image.open(io.BytesIO(img_response.content))
          #pil_image = PILImage.open(webp_file)
          #images.append(PILImage.open(io.BytesIO(img_response.content)))
          #images.append(PILImage.open(pil_image))
          #images.append(PILImage.open(img_response.content))
          #images.append(PILImage.open(io.BytesIO(base64.b64decode(gen['img']))).convert('RGB'))
          #print(f'{type(response.content)} {response.content}')
          '''if answers != None:
            for resp in answers:
              for artifact in resp.artifacts:
                #print("Artifact reason: " + str(artifact.finish_reason))
                if artifact.finish_reason == generation.FILTER:
                  usable_image = False
                if artifact.finish_reason == generation.ARTIFACT_TEXT:
                  usable_image = False
                  prt(f"Couldn't process NSFW text in prompt.  Can't retry so change your request.")
                if artifact.type == generation.ARTIFACT_IMAGE:
                  images.append(PILImage.open(io.BytesIO(artifact.binary)))'''
      
      elif prefs['install_Stability_api'] and (not status['installed_diffusers'] or prefs['use_Stability_api']):
      #elif status['installed_stability'] and (not status['installed_diffusers'] or prefs['use_Stability_api']) and not (status['installed_AIHorde'] and prefs['use_AIHorde_api']):
        #print('use_Stability_api')
        if not status['installed_stability']:
          prt(Installing("Installing Stability-API DreamStudio.ai Pipeline..."))
          get_stability(page)
          clear_last()
          #alert_msg(page, f"ERROR: To use Stability-API, you must run the install it first and have proper API key")
          #return
        #else:
        prt(f'Stability API Diffusion - Using {prefs["model_checkpoint"]} Model...')# + ('─' * 100))
        #print(f'"{SD_prompt}", height={SD_height}, width={SD_width}, steps={SD_steps}, cfg_scale={SD_guidance_scale}, seed={SD_seed}, sampler={generation_sampler}')
        #strikes = 0
        images = []
        arg['width'] = multiple_of_64(arg['width'])
        arg['height'] = multiple_of_64(arg['height'])
        prt(pb)
        import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation
        answers = response = None
        files = {}
        import requests
        from io import BytesIO
        import base64
        import uuid
        api_host = os.getenv('API_HOST', 'https://api.stability.ai')
        engine_id = prefs['model_checkpoint']# if prefs['model_checkpoint'] == "stable-diffusion-v1-5" else "stable-diffusion-v1"
        sd3 = "Stable Diffusion 3" in engine_id or "Stable Image" in engine_id
        if sd3:
            engine = "core" if "Stable Image" in engine_id else "sd3"
            url = f"{api_host}/v2beta/stable-image/generate/{engine}"#image-to-image"
            boundary = str(uuid.uuid4())
            headers = {
                #'Content-Type': f'multipart/form-data; boundary="{boundary}"',
                'Accept': 'image/*',#'application/json',#'image/png',
                'Authorization': f"Bearer {prefs['Stability_api_key']}",
            }
            payload = {
                "prompt": pr[0] if type(pr) == list else pr,
                "negative_prompt": arg['negative_prompt'] if "Turbo" not in engine_id else "",
                "aspect_ratio": closest_aspect_ratio(arg['width'], arg['height']),
                "seed": rand_seed,
                #"style_preset": 3d-model analog-film anime cinematic comic-book digital-art enhance fantasy-art isometric line-art low-poly modeling-compound neon-punk origami photographic pixel-art tile-texture
            }
            if "Turbo" in engine_id:
                payload['model'] = "sd3-turbo"
        else:
            url = f"{api_host}/v1/generation/{engine_id}/"#image-to-image"
            headers = {
                'Content-Type': 'application/json',
                'Accept': 'application/json',#'image/png',
                'Authorization': prefs['Stability_api_key'],
            }
            payload = {
                "cfg_scale": arg['guidance_scale'],
                "clip_guidance_preset": prefs['clip_guidance_preset'],
                "height": arg['height'],
                "width": arg['width'],
                "sampler": prefs['generation_sampler'],
                "samples": arg['batch_size'],
                "seed": rand_seed,
                "steps": arg['steps'],
                "text_prompts": [
                    {
                        "text": pr[0] if type(pr) == list else pr,
                        "weight": 1
                    }
                ],
            }
        if bool(arg['negative_prompt']) and not sd3:
          payload['text_prompts'].append({"text": arg['negative_prompt'][0] if type(arg['negative_prompt']) == list else arg['negative_prompt'], "weight": -10})
        if bool(arg['mask_image']) or (bool(arg['init_image']) and arg['alpha_mask']):
          if not bool(arg['init_image']):
            clear_last()
            prt(f"ERROR: You have not selected an init_image to go with your image mask..")
            continue
          if arg['init_image'].startswith('http'):
            i_response = requests.get(arg['init_image'])
            init_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
          else:
            if os.path.isfile(arg['init_image']):
              init_img = PILImage.open(arg['init_image'])
            else:
              clear_last()
              prt(f"ERROR: Couldn't find your init_image {arg['init_image']}")
          init_img = init_img.resize((arg['width'], arg['height']), resample=PILImage.Resampling.LANCZOS)
          #init_image = preprocess(init_img)
          if not arg['alpha_mask']:
            if arg['mask_image'].startswith('http'):
              i_response = requests.get(arg['mask_image'])
              mask_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
            else:
              if os.path.isfile(arg['mask_image']):
                mask_img = PILImage.open(arg['mask_image'])
              else:
                clear_last()
                prt(f"ERROR: Couldn't find your mask_image {arg['mask_image']}")
            mask = mask_img.resize((arg['width'], arg['height']), resample=PILImage.Resampling.LANCZOS)
          if not sd3:
            buff = BytesIO()
            init_img.save(buff, format="PNG")
            buff.seek(0)
            img_str = io.BufferedReader(buff).read()
            buff = BytesIO()
            mask.save(buff, format="PNG")
            buff.seek(0)
            mask_str = io.BufferedReader(buff).read()
            #payload["step_schedule_end"] = 0.01
            payload["step_schedule_start"] = 1# - arg['init_image_strength']
            files = {
                'init_image': img_str,#base64.b64encode(init_img.tobytes()).decode(),#open(init_img, 'rb'),
                #'mask_image': mask_str,
                'mask_source': "INIT_IMAGE_ALPHA" if arg['alpha_mask'] else "MASK_IMAGE_BLACK" if arg['invert_mask'] else "MASK_IMAGE_WHITE",
                'options': (None, json.dumps(payload)),
            }
            if not arg['alpha_mask']:
              files['mask_image'] = mask_str
            #engine_id = prefs['model_checkpoint'] if prefs['model_checkpoint'] == "stable-diffusion-v1-5" else "stable-diffusion-v1"
            response = requests.post(url+"image-to-image/masking", headers=headers, files=files)
            #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], mask_image=mask, init_image=init_img, start_schedule= 1 - arg['init_image_strength'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], samples=arg['batch_size'], safety=not prefs["disable_nsfw_filter"], seed=arg['seed'], sampler=SD_sampler)
          else:
            url = f"{api_host}/v2beta/stable-image/generate/edit/inpaint"
            #payload['mode'] = "mask"
            #payload['image'] = img_str
            del payload['aspect_ratio']
            img_byte_array = io.BytesIO()
            init_img.save(img_byte_array, format='PNG')
            img_byte_array.seek(0)
            mask_byte_array = io.BytesIO()
            mask_img.save(mask_byte_array, format='PNG')
            mask_byte_array.seek(0)
            payload['strength'] = arg['init_image_strength']
            files = {"image": img_byte_array, "mask": mask_byte_array}
            response = requests.post(url, headers=headers, data=payload, files=files)
          pipe_used = "Stability-API Inpainting"
        elif bool(arg['init_image']):
          if arg['init_image'].startswith('http'):
            i_response = requests.get(arg['init_image'])
            init_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
          else:
            if os.path.isfile(arg['init_image']):
              init_img = PILImage.open(arg['init_image']).convert("RGB")
            else:
              clear_last()
              prt(f"ERROR: Couldn't find your init_image {arg['init_image']}")
          init_img = init_img.resize((arg['width'], arg['height']), resample=PILImage.Resampling.LANCZOS)

          if not sd3:
            buff = BytesIO()
            init_img.save(buff, format="PNG")
            buff.seek(0)
            img_str = io.BufferedReader(buff).read()
            #img_str = open(buff.read(), 'rb') #base64.b64encode(buff.getvalue())  init_img.tobytes("raw")
            payload["step_schedule_end"] = 0.01
            payload["step_schedule_start"] = 1 - arg['init_image_strength']
            files = {
                'init_image': img_str,#base64.b64encode(init_img.tobytes()).decode(),#open(init_img, 'rb'),
                'options': (None, json.dumps(payload)),
            }
            pipe_used = "Stability-API Image-to-Image"
            response = requests.post(url+"image-to-image", headers=headers, files=files)
          else:
            payload['mode'] = "image-to-image"
            del payload['aspect_ratio']
            #payload['image'] = img_str
            img_byte_array = io.BytesIO()
            init_img.save(img_byte_array, format='PNG')
            img_byte_array.seek(0)
            payload['strength'] = arg['init_image_strength']
            files = {"image":img_byte_array}
            response = requests.post(url, headers=headers, data=payload, files=files)
          #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], init_image=init_img, start_schedule= 1 - arg['init_image_strength'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], samples=arg['batch_size'], safety=not prefs["disable_nsfw_filter"], seed=arg['seed'], sampler=SD_sampler)
        else:
          pipe_used = "Stability-API Text-to-Image"
          if not sd3:
            response = requests.post(url+"text-to-image", headers=headers, json=payload)
          else:
            #print(headers)
            response = requests.post(url, headers=headers, data=payload, files={'none':''})
          #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], seed=arg['seed'], samples=arg['batch_size'], safety=False, sampler=SD_sampler)
        clear_last(update=False)
        clear_last()
        if response != None:
          if response.status_code != 200:
            if response.status_code == 402:
              alert_msg(page, "Stability-API ERROR: Insufficient Credit Balance. Reload at DreamStudio.com...", content=Text(str(response.text)))
              return
            else:
              prt(Text(f"Stability-API ERROR {response.status_code}: " + str(response.text), selectable=True))
              #print(payload)
              continue
          #with open(output_file, "wb") as f:
          #  f.write(response.content)
          if sd3:
            finish_reason = response.headers.get("finish-reason")
            if finish_reason == 'CONTENT_FILTERED':
              usable_image = False
              prt(f"Couldn't process NSFW text in prompt.  Can't retry so change your request.")
              continue
            output_image = response.content
            images.append(PILImage.open(io.BytesIO(output_image)))
          else:
            artifacts = json.loads(response.content)
            if 'artifacts' in artifacts:
              for resp in artifacts['artifacts']:
                #print(f'{type(resp)} - {resp["seed"]}')
                if resp == None: continue
                images.append(PILImage.open(io.BytesIO(base64.b64decode(resp['base64']))))
            #print(f'{type(response.content)} {response.content}')
        if answers != None:
          for resp in answers:
            for artifact in resp.artifacts:
              print("Artifact reason: " + str(artifact.finish_reason))
              if artifact.finish_reason == generation.FILTER:
                usable_image = False
              if artifact.finish_reason == generation.ARTIFACT_TEXT:
                usable_image = False
                prt(f"Couldn't process NSFW text in prompt.  Can't retry so change your request.")
              if artifact.type == generation.ARTIFACT_IMAGE:
                images.append(PILImage.open(io.BytesIO(artifact.binary)))
      
      else:
        #from torch.amp.autocast_mode import autocast
        #precision_scope = autocast if prefs['precision']=="autocast" else nullcontext
        SDXL_negative_conditions = {'negative_original_size':(512, 512), 'negative_crops_coords_top_left':(0, 0), 'negative_target_size':(1024, 1024)} if not prefs['SDXL_negative_conditions'] else {}
        if lcm_lora:
          arg['guidance_scale'] = 0.0
        if (status['installed_SDXL'] and prefs['use_SDXL'] and prefs['SDXL_model'] == "SDXL-Turbo") or ((not status['installed_SDXL'] or not prefs['use_SDXL']) and prefs['model_ckpt'] == "SD-Turbo"):
          arg['guidance_scale'] = 0.0
          if arg['steps'] > 8:
            arg['steps'] = 4
        try:
          if use_custom_scheduler and not bool(arg['init_image']) and not bool(arg['mask_image']) and not bool(arg['prompt2']):
            # Not implemented correctly anymore, old code but might reuse custom
            text_input = tokenizer(pr[0] if type(pr) == list else pr, padding="max_length", max_length=tokenizer.model_max_length, truncation=True, return_tensors="pt")
            with torch.no_grad():
              text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]# We'll also get the unconditional text embeddings for classifier-free guidance, which are just the embeddings for the padding token (empty text). They need to have the same shape as the conditional text_embeddings (batch_size and seq_length)
            max_length = text_input.input_ids.shape[-1]
            uncond_input = tokenizer([""] * arg['batch_size'], padding="max_length", max_length=max_length, return_tensors="pt")
            with torch.no_grad():
              uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]   #For classifier-free guidance, we need to do two forward passes. One with the conditioned input (`text_embeddings`), and another with the unconditional embeddings (`uncond_embeddings`). In practice, we can concatenate both into a single batch to avoid doing two forward passes.

            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])#Generate the intial random noise.
            latents = torch.randn((arg['batch_size'], unet.in_channels, arg['height'] // 8,  arg['width'] // 8), generator=generator)
            latents = latents.to(torch_device)
            latents.shape
            #Cool  64×64  is expected. The model will transform this latent representation (pure noise) into a 512 × 512 image later on.
            #Next, we initialize the scheduler with our chosen num_inference_steps. This will compute the sigmas and exact time step values to be used during the denoising process.
            scheduler.set_timesteps(arg['steps'])#The LMS Discrete scheduler needs to multiple the `latents` by its `sigma` values. Let's do this here
            if prefs['scheduler_mode'] == "LMS Discrete" or prefs['scheduler_mode'] == "Score-SDE-Vp":
              latents = latents * scheduler.sigmas[0]#We are ready to write the denoising loop.
            from tqdm.auto import tqdm
            clear_pipes("unet")
            if unet is None:
              unet = get_unet_pipe()
            #with precision_scope("cuda"):
            #with autocast("cuda"):
            for i, t in tqdm(enumerate(scheduler.timesteps)):
              # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.
              latent_model_input = torch.cat([latents] * 2)
              if prefs['scheduler_mode'] == "LMS Discrete" or prefs['scheduler_mode'] == "Score-SDE-Vp":
                sigma = scheduler.sigmas[i]
                latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)
              # predict the noise residual
              if prefs['scheduler_mode'] == "DDPM":
                #TODO: Work in progress, still not perfect
                noisy_sample = torch.randn(1, unet.config.in_channels, unet.config.sample_size, unet.config.sample_size)
                noisy_residual = unet(sample=noisy_sample, timestep=2)["sample"]
                less_noisy_sample = scheduler.step(model_output=noisy_residual, timestep=2, sample=noisy_sample)["prev_sample"]
                less_noisy_sample.shape
              with torch.no_grad():
                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).images
              noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
              noise_pred = noise_pred_uncond + arg['guidance_scale'] * (noise_pred_text - noise_pred_uncond)
              latents = scheduler.step(noise_pred, i, latents)["prev_sample"]#We now use the vae to decode the generated latents back into the image.
            latents = 1 / 0.18215 * latents
            with torch.no_grad():
              images = vae.decode(latents)
            images = (images / 2 + 0.5).clip(0, 1)
            images = images.detach().cpu().permute(0, 2, 3, 1).numpy()
            uint8_images = (images * 255).round().astype("uint8")
            #for img in uint8_images: images.append(Image.fromarray(img))
            images = [PILImage.fromarray(img) for img in uint8_images]
          else:
            if bool(arg['use_clip_guided_model']) and status['installed_clip']:
              if bool(arg['init_image']) or bool(arg['mask_image']):
                #raise ValueError("Cannot use CLIP Guided Model with init or mask image yet.")
                alert_msg(page, "Cannot use CLIP Guided Model with init or mask image yet.")
                return
              clear_pipes("clip_guided")
              if pipe_clip_guided is None:
                prt(Installing("Initializing CLIP-Guided Pipeline..."))
                pipe_clip_guided = get_clip_guided_pipe()
                clear_last()
              clip_prompt = arg["clip_prompt"] if arg["clip_prompt"].strip() != "" else None
              if bool(arg["unfreeze_unet"]):
                pipe_clip_guided.unfreeze_unet()
              else:
                pipe_clip_guided.freeze_unet()
              if bool(arg["unfreeze_vae"]):
                pipe_clip_guided.unfreeze_vae()
              else:
                pipe_clip_guided.freeze_vae()
              # TODO: Figure out why it's broken with use_cutouts=False and doesn't generate, hacking it True for now
              arg["use_cutouts"] = True
              prt(pb)
              page.auto_scrolling(False)
              pipe_used = "CLIP Guided"
              images = pipe_clip_guided(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], clip_prompt=clip_prompt, clip_guidance_scale=arg["clip_guidance_scale"], num_cutouts=int(arg["num_cutouts"]) if arg["use_cutouts"] else None, use_cutouts=arg["use_cutouts"], generator=generator).images
              clear_last()
              page.auto_scrolling(True)
            elif bool(prefs['use_conceptualizer']) and status['installed_conceptualizer']:
              clear_pipes("conceptualizer")
              if pipe_conceptualizer is None:
                prt(Installing("Initializing Conceptualizer Pipeline..."))
                pipe_conceptualizer = get_conceptualizer(page)
                clear_last()
              total_steps = arg['steps']
              prt(pb)
              page.auto_scrolling(False)
              pipe_used = f"Conceptualizer {prefs['concepts_model']}"
              images = pipe_conceptualizer(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              clear_last()
              page.auto_scrolling(True)
            elif bool(arg['mask_image']) or (not bool(arg['mask_image']) and bool(arg['init_image']) and bool(arg['alpha_mask'])):
              if not bool(arg['init_image']):
                alert_msg(page, f"ERROR: You have not selected an init_image to go with your image mask..")
                return
              if prefs['use_inpaint_model'] and status['installed_img2img']:
                clear_pipes("img2img")
                if pipe_img2img is None:
                  prt(Installing("Initializing Inpaint Pipeline..."))
                  pipe_img2img = get_img2img_pipe()
                  clear_last()
              elif prefs['use_SDXL'] and status['installed_SDXL']:
                if status['loaded_SDXL'] == "inpainting" and get_SDXL_model(prefs['SDXL_model'])['path'] == status['loaded_SDXL_model']:
                  clear_pipes("SDXL")
                else:
                  clear_pipes()
                if pipe_SDXL is None:
                  prt(Installing("Initializing Stable Diffusion XL Inpainting Pipeline..."))
                  get_SDXL_pipe("inpainting")
                  clear_last()
                elif prefs['scheduler_mode'] != status['loaded_scheduler']:
                  pipe_SDXL = pipeline_scheduler(pipe_SDXL)
                  pipe_SDXL_refiner = pipeline_scheduler(pipe_SDXL_refiner)
                if prefs['SDXL_model'] == "SDXL-Turbo":
                  pipe_SDXL = pipeline_scheduler(pipe_SDXL, trailing=True)
                if bool(ip_adapter_arg):
                  ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == prefs['ip_adapter_SDXL_model'])
                  pipe_SDXL.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
                  pipe_SDXL.set_ip_adapter_scale(prefs['ip_adapter_strength'])
              elif prefs['use_SD3'] and status['installed_SD3']:
                if get_SD3_model(prefs['SD3_model'])['path'] == status['loaded_SD3_model']:
                  clear_pipes("SD3")
                else:
                  clear_pipes()
                if pipe_SD3 is None or status['loaded_SD3'] != "inpainting":
                  prt(Installing("Initializing Stable Diffusion 3 Inpainting Pipeline..."))
                  get_SD3_pipe("inpainting")
                  clear_last()
              else:
                clear_pipes("txt2img")
                if pipe is None or status['loaded_task'] != "inpaint" or status['loaded_model'] != get_model(prefs['model_ckpt'])['path']:
                  #prt(Installing("Initializing Long Prompt Weighting Inpaint Pipeline..."))
                  prt(Installing("Initializing Stable Diffusion Inpaint Pipeline..."))
                  pipe = get_SD_pipe(task="inpaint")
                  #pipe = get_txt2img_pipe()
                  clear_last()
                else:
                  pipe = get_SD_pipe(task="inpaint")
                if bool(ip_adapter_arg):
                  if 'FaceID' not in prefs['ip_adapter_model']:
                    ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == prefs['ip_adapter_model'])
                    pipe.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
                    pipe.set_ip_adapter_scale(prefs['ip_adapter_strength'])
                  else:
                    pipe.load_ip_adapter("h94/IP-Adapter-FaceID", subfolder=None, weight_name="ip-adapter-faceid_sd15.bin", image_encoder_folder=None)
                    pipe.set_ip_adapter_scale(prefs['ip_adapter_strength'])
                    if 'ip_adapter_image_embeds' not in ip_adapter_arg:
                      try:
                          import insightface
                      except Exception:
                          pip_install("insightface")
                          pass
                      from insightface.app import FaceAnalysis
                      image = ip_adapter_arg['ip_adapter_image']
                      ref_images_embeds = []
                      face_app = FaceAnalysis(name="buffalo_l", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
                      face_app.prepare(ctx_id=0, det_size=(640, 640))
                      image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)
                      faces = face_app.get(image)
                      image = torch.from_numpy(faces[0].normed_embedding)
                      ref_images_embeds.append(image.unsqueeze(0))
                      ref_images_embeds = torch.stack(ref_images_embeds, dim=0).unsqueeze(0)
                      neg_ref_images_embeds = torch.zeros_like(ref_images_embeds)
                      id_embeds = torch.cat([neg_ref_images_embeds, ref_images_embeds]).to(dtype=torch.float16, device="cuda")
                      ip_adapter_arg = {'ip_adapter_image_embeds': [id_embeds]}
              '''if pipe_img2img is None:
                try:
                  pipe_img2img = get_img2img_pipe()
                except NameError:
                  prt(f"{Color.RED}You must install the image2image Pipeline above.{Color.END}")
                finally:
                  raise NameError("You must install the image2image Pipeline above")'''
              import requests
              from io import BytesIO
              if arg['init_image'].startswith('http'):
                response = requests.get(arg['init_image'])
                init_img = PILImage.open(BytesIO(response.content))
              else:
                if os.path.isfile(arg['init_image']):
                  init_img = PILImage.open(arg['init_image'])
                else: prt(f"ERROR: Couldn't find your init_image {arg['init_image']}")
              #if bool(arg['alpha_mask']):
              #  init_img = init_img.convert("RGBA")
              #else:
              #  init_img = init_img.convert("RGB")
              init_img = init_img.resize((arg['width'], arg['height']), resample=PILImage.Resampling.LANCZOS)
              #init_image = preprocess(init_img)
              mask_img = None
              if not bool(arg['mask_image']) and bool(arg['alpha_mask']):
                mask_img = init_img.convert('RGBA')
                red, green, blue, alpha = PILImage.Image.split(init_img)
                mask_img = alpha.convert('L')
              else:
                if arg['mask_image'].startswith('http'):
                  response = requests.get(arg['mask_image'])
                  mask_img = PILImage.open(BytesIO(response.content))
                else:
                  if os.path.isfile(arg['mask_image']):
                    mask_img = PILImage.open(arg['mask_image'])
                  else: prt(f"ERROR: Couldn't find your mask_image {arg['mask_image']}")
              if arg['invert_mask'] and not arg['alpha_mask']:
                from PIL import ImageOps
                mask_img = ImageOps.invert(mask_img.convert('RGB'))
              mask_img = mask_img.convert("L")
              mask_img = mask_img.resize((arg['width'], arg['height']), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              init_img = init_img.convert("RGB")
              #mask = mask_img.resize((arg['width'], arg['height']))
              #mask = np.array(mask).astype(np.float32) / 255.0
              #mask = np.tile(mask,(4,1,1))
              #mask = mask[None].transpose(0, 1, 2, 3)
              #mask[np.where(mask != 0.0 )] = 1.0 #make sure mask is actually valid
              #mask_img = torch.from_numpy(mask)
              prt(pb)
              nudge(page.imageColumn, page=page)
              page.auto_scrolling(False)
              #with autocast("cuda"):
              if prefs['use_inpaint_model'] and status['installed_img2img']:
                pipe_used = "Diffusers Inpaint"
                images = pipe_img2img(prompt=pr, negative_prompt=arg['negative_prompt'], mask_image=mask_img, image=init_img, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions).images
              elif prefs['use_SD3'] and status['installed_SD3']:
                pipe_used = "Stable Diffusion 3 Inpaint"
                total_steps = int(arg['steps'])
                cross_attention_kwargs = {"joint_attention_kwargs": {"scale": 1.0}} if prefs['use_LoRA_model'] and len(prefs['active_SD3_LoRA_layers']) > 0 else {}
                if prefs['SD3_compel']:
                  prompt_embed, pooled = compel_base(pr)
                  negative_embed, negative_pooled = compel_base(arg['negative_prompt'] if bool(arg['negative_prompt']) else "blurry, ugly")
                  images = pipe_SD3(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, image=init_img, mask_image=mask_img, strength= 1 - arg['init_image_strength'], output_type="pil", height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], generator=generator, callback_on_step_end=callback_step).images#[0]
                  del pooled, negative_pooled
                else:
                  images = pipe_SD3(prompt=pr, negative_prompt=arg['negative_prompt'] if bool(arg['negative_prompt']) else "blurry, ugly", image=init_img, mask_image=mask_img, strength= 1 - arg['init_image_strength'], output_type="pil", height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], generator=generator, callback_on_step_end=callback_step).images#[0]
                flush()
              elif prefs['use_SDXL'] and status['installed_SDXL']:
                pipe_used = "Stable Diffusion XL Inpaint"
                high_noise_frac = prefs['SDXL_high_noise_frac']
                total_steps = int(arg['steps'] * high_noise_frac)
                cross_attention_kwargs = {"cross_attention_kwargs": {"scale": 1.0}} if prefs['use_LoRA_model'] and len(prefs['active_SDXL_LoRA_layers']) > 0 else {}
                if prefs['SDXL_compel']:
                  prompt_embed, pooled = compel_base(pr)
                  negative_embed, negative_pooled = compel_base(arg['negative_prompt'])
                  #[prompt_embed, negative_embed] = compel_base.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])
                  #, target_size=(arg['width'], arg['height'])
                  images = pipe_SDXL(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, image=init_img, mask_image=mask_img, strength=1 - arg['init_image_strength'], height=arg['height'], width=arg['width'], output_type="latent" if high_noise_frac != 1 else "pil", denoising_end=high_noise_frac, num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions, **cross_attention_kwargs, **ip_adapter_arg).images#[0]
                else:
                  if arg['batch_size'] > 1:
                    init_img = [init_img] * arg['batch_size']
                    mask_img = [mask_img] * arg['batch_size']
                  images = pipe_SDXL(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, mask_image=mask_img, strength=1 - arg['init_image_strength'], height=arg['height'], width=arg['width'], output_type="latent" if high_noise_frac != 1 else "pil", denoising_end=high_noise_frac, num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions, **cross_attention_kwargs, **ip_adapter_arg).images#[0]
                if high_noise_frac != 1:
                  total_steps = int(arg['steps'] * (1 - high_noise_frac))
                  if prefs['SDXL_compel']:
                    prompt_embed, pooled = compel_refiner(pr)
                    negative_embed, negative_pooled = compel_refiner(arg['negative_prompt'])
                    #[prompt_embed, negative_embed] = compel_refiner.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])
                    images = pipe_SDXL_refiner(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, image=images, mask_image=mask_img, strength=1 - arg['init_image_strength'], target_size=(arg['height'], arg['width']), num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step).images
                    del prompt_embed, negative_embed, pooled, negative_pooled
                  else:
                    images = pipe_SDXL_refiner(prompt=pr, negative_prompt=arg['negative_prompt'], image=images, mask_image=mask_img, strength=1 - arg['init_image_strength'], target_size=(arg['height'], arg['width']), num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step).images
                #images = pipe_SDXL_refiner(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, mask_image=mask_img, strength=arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                flush()
              else:
                #pipe_used = "Long Prompt Weight Inpaint"
                #images = pipe.inpaint(prompt=pr, negative_prompt=arg['negative_prompt'], mask_image=mask_img, image=init_img, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                pipe_used = "Stable Diffusion Inpaint"
                cross_attention_kwargs = {"cross_attention_kwargs": {"scale": 1.0}} if prefs['use_LoRA_model'] and len(prefs['active_LoRA_layers']) > 0 else {}
                if prefs['SD_compel']:
                  pipe_used += " w/ Compel"
                  prompt_embed = compel_proc.build_conditioning_tensor(pr)
                  negative_embed = compel_proc.build_conditioning_tensor(arg['negative_prompt'] if bool(arg['negative_prompt']) else "blurry")
                  [prompt_embed, negative_embed] = compel_proc.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])
                  images = pipe(prompt_embeds=prompt_embed, negative_prompt_embeds=negative_embed, mask_image=mask_img, image=init_img, height=arg['height'], width=arg['width'], strength=1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **cross_attention_kwargs, **ip_adapter_arg).images
                  del prompt_embed, negative_embed
                else:
                  images = pipe(prompt=pr, negative_prompt=arg['negative_prompt'] if bool(arg['negative_prompt']) else "blurry", mask_image=mask_img, image=init_img, height=arg['height'], width=arg['width'], strength=1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **cross_attention_kwargs, **ip_adapter_arg).images

              clear_last()
              page.auto_scrolling(True)
            elif bool(arg['init_image']):
              if not status['installed_txt2img'] and not (prefs['use_imagic'] and status['installed_imagic']) and not (prefs['use_depth2img'] and status['installed_depth2img']) and not (prefs['use_alt_diffusion'] and status['installed_alt_diffusion']) and not status['installed_SDXL']:
                alert_msg(page, f"CRITICAL ERROR: You have not installed a image2image pipeline yet.  Run in the Installer..")
                continue
              if prefs['use_versatile'] and status['installed_versatile']:
                if len(pr.strip()) > 2: # Find another way to know the difference
                  clear_pipes("versatile_dualguided")
                  if pipe_versatile_dualguided is None:
                    prt(Installing("Initializing Versatile Dual-Guided Pipeline..."))
                    pipe_versatile_dualguided = get_versatile_dualguided_pipe()
                    clear_last()
                else:
                  clear_pipes("versatile_variation")
                  if pipe_versatile_variation is None:
                    prt(Installing("Initializing Versatile Image Variation Pipeline..."))
                    pipe_versatile_variation = get_versatile_variation_pipe()
                    clear_last()
              elif prefs['use_SD3'] and status['installed_SD3']:
                if get_SD3_model(prefs['SD3_model'])['path'] == status['loaded_SD3_model']:
                  clear_pipes("SD3")
                else:
                  clear_pipes()
                if pipe_SD3 is None or status['loaded_SD3'] != "image2image":
                  prt(Installing("Initializing Stable Diffusion 3 Image2Image Pipeline..."))
                  get_SD3_pipe("image2image")
                  clear_last()
                #elif prefs['scheduler_mode'] != status['loaded_scheduler']:
                #  pipe_SD3 = pipeline_scheduler(pipe_SD3)
              elif prefs['use_SDXL'] and status['installed_SDXL']:
                if status['loaded_SDXL'] == "image2image" and get_SDXL_model(prefs['SDXL_model'])['path'] == status['loaded_SDXL_model']:
                  clear_pipes("SDXL")
                else:
                  clear_pipes()
                if pipe_SDXL_refiner is None:
                  prt(Installing("Initializing Stable Diffusion XL Image2Image Pipeline..."))
                  get_SDXL_pipe("image2image")
                  clear_last()
                elif prefs['scheduler_mode'] != status['loaded_scheduler']:
                  pipe_SDXL = pipeline_scheduler(pipe_SDXL)
                  pipe_SDXL_refiner = pipeline_scheduler(pipe_SDXL_refiner)
                if prefs['SDXL_model'] == "SDXL-Turbo":
                  pipe_SDXL = pipeline_scheduler(pipe_SDXL, trailing=True)
                if bool(ip_adapter_arg):
                  ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == prefs['ip_adapter_SDXL_model'])
                  pipe_SDXL.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
                  pipe_SDXL.set_ip_adapter_scale(prefs['ip_adapter_strength'])
              elif prefs['use_alt_diffusion'] and status['installed_alt_diffusion']:
                clear_pipes("alt_diffusion_img2img")
                if pipe_alt_diffusion_img2img is None:
                  prt(Installing("Initializing AltDiffusion Image2Image Pipeline..."))
                  pipe_alt_diffusion_img2img = get_alt_diffusion_img2img_pipe()
                  clear_last()
              elif prefs['use_depth2img'] and status['installed_depth2img']:
                clear_pipes("depth")
                if pipe_depth is None:
                  prt(Installing("Initializing SD2 Depth2Image Pipeline..."))
                  pipe_depth = get_depth_pipe()
                  clear_last()
              elif prefs['use_inpaint_model'] and status['installed_img2img']:
                clear_pipes("img2img")
                if pipe_img2img is None:
                  prt(Installing("Initializing Inpaint Pipeline..."))
                  pipe_img2img = get_img2img_pipe()
                  clear_last()
                  if bool(ip_adapter_arg):
                    ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == prefs['ip_adapter_model'])
                    pipe.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
                    pipe.set_ip_adapter_scale(prefs['ip_adapter_strength'])
              elif prefs['use_imagic'] and status['installed_imagic']:
                clear_pipes("imagic")
                if pipe_imagic is None:
                  prt(Installing("Initializing iMagic Image2Image Pipeline..."))
                  pipe_imagic = get_imagic_pipe()
                  clear_last()
              else:
                clear_pipes("txt2img")
                if pipe is None or status['loaded_task'] != "img2img" or status['loaded_model'] != get_model(prefs['model_ckpt'])['path']:
                  #prt(Installing("Initializing Long Prompt Weighting Image2Image Pipeline..."))
                  prt(Installing("Initializing Stable Diffusion Image2Image Pipeline..."))
                  pipe = get_SD_pipe(task="img2img")
                  #pipe = get_txt2img_pipe()
                  clear_last()
                else:
                  pipe = get_SD_pipe(task="img2img")
                if prefs['model_ckpt'] == "SD-Turbo":
                  pipe = pipeline_scheduler(pipe_SDXL, trailing=True)
                if bool(ip_adapter_arg):
                  ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == prefs['ip_adapter_model'])
                  pipe.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
                  pipe.set_ip_adapter_scale(prefs['ip_adapter_strength'])
              '''if pipe_img2img is None:
                try:
                  pipe_img2img = get_img2img_pipe()
                except NameError:
                  prt(f"{Color.RED}You must install the image2image Pipeline above.{Color.END}")
                  raise NameError("You must install the image2image Pipeline above")'''
                #finally:
              import requests
              from io import BytesIO
              if arg['init_image'].startswith('http'):
                response = requests.get(arg['init_image'])
                init_img = PILImage.open(BytesIO(response.content)).convert("RGB")
              else:
                if os.path.isfile(arg['init_image']):
                  init_img = PILImage.open(arg['init_image']).convert("RGB")
                else:
                  alert_msg(page, f"ERROR: Couldn't find your init_image {arg['init_image']}")
              init_img = init_img.resize((arg['width'], arg['height']))
              #init_image = preprocess(init_img)
              #white_mask = PILImage.new("RGB", (arg['width'], arg['height']), (255, 255, 255))
              prt(pb)
              nudge(page.imageColumn, page=page)
              page.auto_scrolling(False)
              #with autocast("cuda"):
              #images = pipe_img2img(prompt=pr, negative_prompt=arg['negative_prompt'], init_image=init_img, mask_image=white_mask, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              if prefs['use_versatile'] and status['installed_versatile']:
                if len(pr.strip()) > 2:
                  pipe_used = "Versatile Dual-Guided"
                  images = pipe_versatile_dualguided(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, text_to_image_strength=1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                else:
                  pipe_used = "Versatile Variation"
                  images = pipe_versatile_variation(negative_prompt=arg['negative_prompt'], image=init_img, num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              elif prefs['use_SD3'] and status['installed_SD3']:
                pipe_used = "Stable Diffusion 3 Image-to-Image"
                total_steps = int(arg['steps'])
                cross_attention_kwargs = {"joint_attention_kwargs": {"scale": 1.0}} if prefs['use_LoRA_model'] and len(prefs['active_SD3_LoRA_layers']) > 0 else {}
                if prefs['SD3_compel']:
                  prompt_embed, pooled = compel_base(pr)
                  negative_embed, negative_pooled = compel_base(arg['negative_prompt'] if bool(arg['negative_prompt']) else "blurry, ugly")
                  images = pipe_SD3(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, image=init_img, strength= 1 - arg['init_image_strength'], output_type="pil", height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], generator=generator, callback_on_step_end=callback_step).images#[0]
                  del pooled, negative_pooled
                else:
                  images = pipe_SD3(prompt=pr, negative_prompt=arg['negative_prompt'] if bool(arg['negative_prompt']) else "blurry, ugly", image=init_img, strength= 1 - arg['init_image_strength'], output_type="pil", height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], generator=generator, callback_on_step_end=callback_step).images#[0]
                flush()
              elif prefs['use_SDXL'] and status['installed_SDXL']:
                pipe_used = "Stable Diffusion XL Image-to-Image"
                high_noise_frac = prefs['SDXL_high_noise_frac']
                total_steps = int(arg['steps'] * high_noise_frac)
                cross_attention_kwargs = {"cross_attention_kwargs": {"scale": 1.0}} if prefs['use_LoRA_model'] and len(prefs['active_SDXL_LoRA_layers']) > 0 else {}
                if prefs['SDXL_compel']:
                  prompt_embed, pooled = compel_base(pr)
                  negative_embed, negative_pooled = compel_base(arg['negative_prompt'])
                  #[prompt_embed, negative_embed] = compel_base.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])
                  images = pipe_SDXL(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, image=init_img, strength=1 - arg['init_image_strength'], output_type="latent" if high_noise_frac != 1 else "pil", denoising_end=high_noise_frac, num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions, **cross_attention_kwargs, **ip_adapter_arg).images#[0]
                else:
                  if arg['batch_size'] > 1:
                    init_img = [init_img] * arg['batch_size']
                  #image = pipe_SDXL(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength=arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                  images = pipe_SDXL(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength= 1 - arg['init_image_strength'], output_type="latent" if high_noise_frac != 1 else "pil", denoising_end=high_noise_frac, num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions, **cross_attention_kwargs, **ip_adapter_arg).images#[0]
                if high_noise_frac != 1:
                  total_steps = int(arg['steps'] * (1 - high_noise_frac))
                  if prefs['SDXL_compel']:
                    pipe_used += " w/ Compel"
                    prompt_embed, pooled = compel_refiner(pr)
                    negative_embed, negative_pooled = compel_refiner(arg['negative_prompt'])
                    #[prompt_embed, negative_embed] = compel_refiner.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])
                    images = pipe_SDXL_refiner(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, image=images, target_size=(arg['height'], arg['width']), num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions).images
                    del prompt_embed, negative_embed, pooled, negative_pooled
                  else:
                    images = pipe_SDXL_refiner(prompt=pr, negative_prompt=arg['negative_prompt'], image=images, target_size=(arg['height'], arg['width']), num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions).images
                #images = pipe_SDXL(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength=arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                flush()
              elif prefs['use_alt_diffusion'] and status['installed_alt_diffusion']:
                pipe_used = "AltDiffusion Image-to-Image"
                with torch.autocast("cuda"):
                  images = pipe_alt_diffusion_img2img(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step).images
              elif prefs['use_depth2img'] and status['installed_depth2img']:
                pipe_used = "Depth-to-Image"
                images = pipe_depth(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength=1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step).images
              elif prefs['use_inpaint_model'] and status['installed_img2img']:
                pipe_used = "Diffusers Inpaint Image-to-Image"
                white_mask = PILImage.new("RGB", (arg['width'], arg['height']), (255, 255, 255))
                images = pipe_img2img(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, mask_image=white_mask, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **ip_adapter_arg).images
              elif prefs['use_imagic'] and status['installed_imagic']:
                pipe_used = "iMagic Image-to-Image"
                #only one element tensors can be converted to Python scalars
                total_steps = None
                res = pipe_imagic.train(pr, init_img, num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                images = []
                # TODO: alpha= arguments to customize which to make
                total_steps = 0
                res = pipe_imagic(alpha=1, callback=callback_fn, callback_steps=1)
                images.append(res.images[0])
                res = pipe_imagic(alpha=1.5, callback=callback_fn, callback_steps=1)
                images.append(res.images[0])
                res = pipe_imagic(alpha=2, callback=callback_fn, callback_steps=1)
                images.append(res.images[0])
              else:
                #pipe_used = "Long Prompt Weight Image-to-Image"
                #images = pipe.img2img(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                pipe_used = "Stable Diffusion Image-to-Image"
                cross_attention_kwargs = {"cross_attention_kwargs": {"scale": 1.0}} if prefs['use_LoRA_model'] and len(prefs['active_LoRA_layers']) > 0 else {}
                if prefs['SD_compel']:
                  pipe_used += " w/ Compel"
                  prompt_embed = compel_proc.build_conditioning_tensor(pr)
                  negative_embed = compel_proc.build_conditioning_tensor(arg['negative_prompt'] if bool(arg['negative_prompt']) else "blurry")
                  [prompt_embed, negative_embed] = compel_proc.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])
                  images = pipe(prompt_embeds=prompt_embed, negative_prompt_embeds=negative_embed, image=init_img, target_size=(arg['height'], arg['width']), strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **cross_attention_kwargs, **ip_adapter_arg).images
                  del prompt_embed, negative_embed
                else:
                  images = pipe(prompt=pr, negative_prompt=arg['negative_prompt'] if bool(arg['negative_prompt']) else "blurry", image=init_img, target_size=(arg['height'], arg['width']), strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **cross_attention_kwargs, **ip_adapter_arg).images
              clear_last()
              page.auto_scrolling(True)
            elif bool(arg['prompt2']):
              if pipe is None:
                pipe = get_txt2img_pipe()
              pipe_used = "LPW Tween Lerp"
              images_tween = pipe.lerp_between_prompts(pr, arg["prompt2"], length = arg['tweens'], save=False, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator)
              #print(str(images_tween))
              images = images_tween['images']
              #images = pipe(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator)["sample"]
            else:
              if prefs['use_composable'] and status['installed_composable']:
                clear_pipes("composable")
                if pipe_composable is None:
                  prt(Installing("Initializing Composable Text2Image Pipeline..."))
                  pipe_composable = get_composable_pipe()
                  clear_last()
              elif prefs['use_SD3'] and status['installed_SD3']:
                if get_SD3_model(prefs['SD3_model'])['path'] == status['loaded_SD3_model']:
                  clear_pipes("SD3")
                else:
                  clear_pipes()
                if pipe_SD3 is None or status['loaded_SD3'] != "text2image":
                  prt(Installing("Initializing Stable Diffusion 3 Text2Image Pipeline..."))
                  get_SD3_pipe("text2image")
                  clear_last()
                #elif prefs['scheduler_mode'] != status['loaded_scheduler']:
                #  pipe_SD3 = pipeline_scheduler(pipe_SD3)
              elif prefs['use_SDXL'] and status['installed_SDXL']:
                if status['loaded_SDXL'] == "text2image" and get_SDXL_model(prefs['SDXL_model'])['path'] == status['loaded_SDXL_model']:
                  clear_pipes("SDXL")
                else:
                  clear_pipes()
                if pipe_SDXL is None:
                  prt(Installing("Initializing Stable Diffusion XL Text2Image Pipeline..."))
                  get_SDXL_pipe("text2image")
                  clear_last()
                elif prefs['scheduler_mode'] != status['loaded_scheduler']:
                  pipe_SDXL = pipeline_scheduler(pipe_SDXL)
                  pipe_SDXL_refiner = pipeline_scheduler(pipe_SDXL_refiner)
                if prefs['SDXL_model'] == "SDXL-Turbo":
                  pipe_SDXL = pipeline_scheduler(pipe_SDXL, trailing=True)
                if bool(ip_adapter_arg):
                  ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == prefs['ip_adapter_SDXL_model'])
                  pipe_SDXL.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
                  pipe_SDXL.set_ip_adapter_scale(prefs['ip_adapter_strength'])
              elif prefs['use_alt_diffusion'] and status['installed_alt_diffusion']:
                clear_pipes("alt_diffusion")
                if pipe_alt_diffusion is None:
                  prt(Installing("Initializing AltDiffusion Text2Image Pipeline..."))
                  pipe_alt_diffusion = get_alt_diffusion_pipe()
                  clear_last()
              elif prefs['use_SAG'] and status['installed_SAG']:
                clear_pipes("SAG")
                if pipe_SAG is None:
                  prt(Installing("Initializing Self-Attention Guidance Text2Image Pipeline..."))
                  pipe_SAG = get_SAG_pipe()
                  clear_last()
                if bool(ip_adapter_arg):
                  ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == prefs['ip_adapter_model'])
                  pipe_SAG.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
                  pipe_SAG.set_ip_adapter_scale(prefs['ip_adapter_strength'])
              elif prefs['use_attend_and_excite'] and status['installed_attend_and_excite']:
                clear_pipes("attend_and_excite")
                if pipe_attend_and_excite is None:
                  prt(Installing("Initializing Attend and Excite Text2Image Pipeline..."))
                  pipe_attend_and_excite = get_attend_and_excite_pipe()
                  clear_last()
                if bool(ip_adapter_arg):
                  ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == prefs['ip_adapter_model'])
                  pipe_attend_and_excite.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
                  pipe_attend_and_excite.set_ip_adapter_scale(prefs['ip_adapter_strength'])
              elif prefs['use_versatile'] and status['installed_versatile']:
                clear_pipes("versatile_text2img")
                if pipe_versatile_text2img is None:
                  prt(Installing("Initializing Versatile Text2Image Pipeline..."))
                  pipe_versatile_text2img = get_versatile_text2img_pipe()
                  clear_last()
              elif prefs['use_safe'] and status['installed_safe']:
                clear_pipes("safe")
                if pipe_safe is None:
                  prt(Installing("Initializing Safe Stable Diffusion Pipeline..."))
                  pipe_safe = get_safe_pipe()
                  clear_last()
                if bool(ip_adapter_arg):
                  ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == prefs['ip_adapter_model'])
                  pipe_safe.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
                  pipe_safe.set_ip_adapter_scale(prefs['ip_adapter_strength'])
              elif prefs['use_panorama'] and status['installed_panorama']:
                clear_pipes("panorama")
                if pipe_panorama is None:
                  prt(Installing("Initializing Panorama MultiDiffusion Pipeline..."))
                  pipe_panorama = get_panorama_pipe()
                  clear_last()
                if bool(ip_adapter_arg):
                  ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == prefs['ip_adapter_model'])
                  pipe_panorama.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
                  pipe_panorama.set_ip_adapter_scale(prefs['ip_adapter_strength'])
              else:
                clear_pipes("txt2img")
                if pipe is None or status['loaded_task'] != "txt2img" or status['loaded_model'] != get_model(prefs['model_ckpt'])['path']:
                  #prt(Installing("Initializing Long Prompt Weighting Text2Image Pipeline..."))
                  prt(Installing("Initializing Stable Diffusion Text2Image Pipeline..."))
                  pipe = get_SD_pipe(task="txt2img")
                  #pipe = get_txt2img_pipe()
                  clear_last()
                else:
                  pipe = get_SD_pipe(task="txt2img")
                #print(pipe)
                if prefs['model_ckpt'] == "SD-Turbo":
                  pipe = pipeline_scheduler(pipe_SDXL, trailing=True)
                if bool(ip_adapter_arg):
                  ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == prefs['ip_adapter_model'])
                  pipe.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
                  pipe.set_ip_adapter_scale(prefs['ip_adapter_strength'])
              '''with io.StringIO() as buf, redirect_stdout(buf):
                get_text2image(page)
                output = buf.getvalue()
                page.Images.content.controls.append(Text(output.strip())
                page.Images.content.update()
                page.Images.update()
                page.update()'''
              total_steps = arg['steps']
              prt(pb)
              nudge(page.imageColumn, page=page)
              page.auto_scrolling(False)
              if prefs['use_composable'] and status['installed_composable']:
                weights = arg['negative_prompt'] #" 1 | 1"  # Equal weight to each prompt. Can be negative
                if not bool(weights):
                  segments = len(pr.split('|'))
                  weights = '|'.join(['1' * segments])
                pipe_used = "Composable Text-to-Image"
                images = pipe_composable(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], weights=weights, generator=generator, callback=callback_fn, callback_steps=1).images
              elif prefs['use_SD3'] and status['installed_SD3']:
                pipe_used = "Stable Diffusion 3 Text-to-Image"
                total_steps = int(arg['steps'])
                if prefs['SD3_compel']:
                  #print(f"pr:{pr} - neg: {arg['negative_prompt']}")
                  prompt_embed, pooled = compel_base(pr)
                  negative_embed, negative_pooled = compel_base(arg['negative_prompt'] if bool(arg['negative_prompt']) else "blurry, ugly")
                  #[prompt_embed, negative_embed] = compel_base.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])
                  images = pipe_SD3(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, output_type="pil", height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], generator=generator, callback_on_step_end=callback_step).images#[0]
                  del pooled, negative_pooled
                else:
                  images = pipe_SD3(prompt=pr, negative_prompt=arg['negative_prompt'] if bool(arg['negative_prompt']) else "blurry, ugly", output_type="pil", height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], generator=generator, callback_on_step_end=callback_step).images#[0]
                flush()
              elif prefs['use_SDXL'] and status['installed_SDXL']:
                pipe_used = "Stable Diffusion XL Text-to-Image"
                #if arg['batch_size'] > 1:
                #    neg_prompts = [arg['negative_prompt']] * arg['batch_size']
                high_noise_frac = prefs['SDXL_high_noise_frac']
                #TODO: Figure out batch num_images_per_prompt + option to not refine , image[None, :] , num_images_per_prompt=arg['batch_size']
                total_steps = int(arg['steps'] * high_noise_frac)
                cross_attention_kwargs = {"cross_attention_kwargs": {"scale": 1.0}} if prefs['use_LoRA_model'] and len(prefs['active_SDXL_LoRA_layers']) > 0 else {}
                if prefs['SDXL_compel']:
                  #print(f"pr:{pr} - neg: {arg['negative_prompt']}")
                  prompt_embed, pooled = compel_base(pr)
                  negative_embed, negative_pooled = compel_base(arg['negative_prompt'] if bool(arg['negative_prompt']) else "blurry, ugly")
                  #[prompt_embed, negative_embed] = compel_base.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])
                  images = pipe_SDXL(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, output_type="latent" if high_noise_frac != 1 else "pil", denoising_end=high_noise_frac, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions, **cross_attention_kwargs, **ip_adapter_arg).images#[0]
                  del pooled, negative_pooled
                else:
                  images = pipe_SDXL(prompt=pr, negative_prompt=arg['negative_prompt'] if bool(arg['negative_prompt']) else "blurry, ugly", output_type="latent" if high_noise_frac != 1 else "pil", denoising_end=high_noise_frac, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions, **cross_attention_kwargs, **ip_adapter_arg).images#[0]
                if high_noise_frac != 1:
                  total_steps = int(arg['steps'] * (1 - high_noise_frac))
                  if prefs['SDXL_compel']:
                    prompt_embed_refiner, pooled_refiner = compel_refiner(pr)
                    negative_embed_refiner, negative_pooled_refiner = compel_refiner(arg['negative_prompt'] if bool(arg['negative_prompt']) else "blurry")
                    #[prompt_embed_refiner, negative_embed_refiner] = compel_refiner.pad_conditioning_tensors_to_same_length([prompt_embed_refiner, negative_embed_refiner])
                    images = pipe_SDXL_refiner(prompt_embeds=prompt_embed_refiner, pooled_prompt_embeds=pooled_refiner, negative_prompt_embeds=negative_embed_refiner, negative_pooled_prompt_embeds=negative_pooled_refiner, image=images, target_size=(arg['height'], arg['width']), num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions).images
                    #images = pipe_SDXL_refiner(prompt=pr, negative_prompt=arg['negative_prompt'], image=image, num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                    del prompt_embed_refiner, negative_embed_refiner, pooled_refiner, negative_pooled_refiner
                  else:
                    images = pipe_SDXL_refiner(prompt=pr, negative_prompt=arg['negative_prompt' if bool(arg['negative_prompt']) else "blurry"], image=images, target_size=(arg['height'], arg['width']), num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **SDXL_negative_conditions).images
                flush()
              elif prefs['use_alt_diffusion'] and status['installed_alt_diffusion']:
                pipe_used = "AltDiffusion Text-to-Image"
                with torch.autocast("cuda"):
                  images = pipe_alt_diffusion(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step).images
              elif prefs['use_SAG'] and status['installed_SAG']:
                pipe_used = "Self-Attention Guidance Text-to-Image"
                #size = pipe_SAG.unet.config.sample_size * pipe_SAG.vae_scale_factor
                #arg['width'] = size
                #arg['height'] = size
                with torch.autocast("cuda"): #, height=arg['height'], width=arg['width']
                  images = pipe_SAG(prompt=pr, negative_prompt=arg['negative_prompt'], sag_scale=prefs['sag_scale'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1, **ip_adapter_arg).images
              elif prefs['use_attend_and_excite'] and status['installed_attend_and_excite']:
                pipe_used = "Attend and Excite Text-to-Image"
                size = pipe_attend_and_excite.unet.config.sample_size * pipe_attend_and_excite.vae_scale_factor
                arg['width'] = size
                arg['height'] = size
                token_indices = []
                words = []
                ptext = pr[0] if type(pr) == list else pr
                ntext = arg['negative_prompt'][0] if type(arg['negative_prompt']) == list else arg['negative_prompt']
                for ti, w in enumerate(ptext.split()):
                  if w[:1] == '+':
                    token_indices.append(ti + 1)
                    words.append(w[1:])
                    #print(f'indices: {ti + 1} - {w[1:]}')
                  else:
                    words.append(w)
                ptext = ' '.join(words)
                pr = [ptext * arg['batch_size']] if type(pr) == list else ptext
                print(f"token_indices: {token_indices} | ptext: {ptext}")
                if len(token_indices) < 1:
                  token_indices = pipe_attend_and_excite.get_indices(ptext)
                print(f"indices: {token_indices}")
                #, negative_prompt=arg['negative_prompt'], num_images_per_prompt=int(arg['batch_size'])
                #images = pipe_attend_and_excite(prompt=ptext, token_indices=token_indices, max_iter_to_alter=int(prefs['max_iter_to_alter']), height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                images = pipe_attend_and_excite(prompt=ptext, negative_prompt=ntext, token_indices=token_indices, max_iter_to_alter=int(prefs['max_iter_to_alter']), height=arg['height'], width=arg['width'], num_images_per_prompt=int(arg['batch_size']), num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1, **ip_adapter_arg).images
              elif prefs['use_versatile'] and status['installed_versatile']:
                pipe_used = "Versatile Text-to-Image"
                images = pipe_versatile_text2img(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              elif prefs['use_panorama'] and status['installed_panorama']:
                pipe_used = "MultiDiffusion Panorama Text-to-Image"
                arg['width'] = prefs['panorama_width']
                arg['height'] = 512
                images = pipe_panorama(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **cross_attention_kwargs, circular_padding=prefs['panorama_circular_padding'], **ip_adapter_arg).images
              elif prefs['use_safe'] and status['installed_safe']:
                from diffusers.pipelines.stable_diffusion_safe import SafetyConfig
                s = prefs['safety_config']
                safety = SafetyConfig.WEAK if s == 'Weak' else SafetyConfig.MEDIUM if s == 'Medium' else SafetyConfig.STRONG if s == 'Strong' else SafetyConfig.MAX if s == 'Max' else SafetyConfig.STRONG
                pipe_used = f"Safe Diffusion {safety}"
                images = pipe_safe(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1, **safety, **ip_adapter_arg).images
              else:
                pipe_used = "Stable Diffusion Text-to-Image"
                if pipe is None:
                  print("pipe is None")
                cross_attention_kwargs = {"cross_attention_kwargs": {"scale": 1.0}} if prefs['use_LoRA_model'] and len(prefs['active_LoRA_layers']) > 0 else {}
                if prefs['SD_compel']:
                  pipe_used += " w/ Compel"
                  prompt_embed = compel_proc.build_conditioning_tensor(pr)
                  negative_embed = compel_proc.build_conditioning_tensor(arg['negative_prompt'] if bool(arg['negative_prompt']) else "blurry")
                  [prompt_embed, negative_embed] = compel_proc.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])
                  images = pipe(prompt_embeds=prompt_embed, negative_prompt_embeds=negative_embed, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **cross_attention_kwargs, **ip_adapter_arg).images
                  del prompt_embed, negative_embed
                else:
                  images = pipe(prompt=pr, negative_prompt=arg['negative_prompt'] if bool(arg['negative_prompt']) else "blurry", height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback_on_step_end=callback_step, **cross_attention_kwargs, **ip_adapter_arg).images
                #pipe_used = "Long Prompt Weight Text-to-Image"
                #images = pipe.text2img(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              '''if prefs['precision'] == "autocast":
                with autocast("cuda"):
                  images = pipe(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], seed = arg['seed'], generator=generator, callback=callback_fn, callback_steps=1)["sample"]
              else:
                with precision_scope("cuda"):
                  with torch.no_grad():
                    images = pipe(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], seed = arg['seed'], generator=generator, callback=callback_fn, callback_steps=1)["sample"]'''
              clear_last()
              page.auto_scrolling(True)
        except RuntimeError as e:
          clear_last()
          if 'out of memory' in str(e):
            alert_msg(page, f"CRITICAL ERROR: GPU ran out of memory! Flushing memory to save session... Try reducing image size.", content=Text(str(e).strip()))
            clear_pipes()
            pass
          else:
            alert_msg(page, f"RUNTIME ERROR: Unknown error processing image. Check parameters and try again. Restart app if persists.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]), debug_pref=arg)
            pass
        except Exception as e:
          alert_msg(page, f"EXCEPTION ERROR: Unknown error processing image. Check parameters and try again. Restart app if persists.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]), debug_pref=arg)
          abort_run = True
          pass
        finally:
          gc.collect()
          torch.cuda.empty_cache()

      txt2img_output = stable_dir #f'{stable_dir}/stable-diffusion/outputs/txt2img-samples'
      batch_output = prefs['image_output']
      if bool(prefs['batch_folder_name']):
        txt2img_output = os.path.join(stable_dir, prefs['batch_folder_name'])
        batch_output = os.path.join(prefs['image_output'], prefs['batch_folder_name'])
      if not os.path.exists(txt2img_output):
        os.makedirs(txt2img_output)
      if save_to_GDrive or storage_type == "Colab Google Drive":
        if not os.path.exists(batch_output):
          os.makedirs(batch_output)
      elif storage_type == "PyDrive Google Drive": # TODO: I'm not getting the parent folder id right, their docs got confusing
        newFolder = gdrive.CreateFile({'title': prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
        newFolder.Upload()
        batch_output = newFolder
      else:
        if not os.path.exists(batch_output):
          os.makedirs(batch_output)

      filename = format_filename(pr[0] if type(pr) == list else pr)
      if images is None:
        prt(f"ERROR: Problem generating images, check your settings and run above blocks again, or report the error to Skquark if it really seems broken.")
        images = []

      idx = num = 0
      for image in images:
        cur_seed = arg['seed']
        if idx > 0:
          cur_seed += idx
          i_count = f'  ({idx + 1} of {len(images)})  '
          prt(Row([Text(i_count), Text(pr[0] if type(pr) == list else pr, expand=True, weight=FontWeight.BOLD), Text(f'seed: {cur_seed}     ')]))
          #prt(f'{pr[0] if type(pr) == list else pr} - seed:{cur_seed}')
        seed_suffix = "" if not prefs['file_suffix_seed'] else f"-{cur_seed}"
        if prefs['use_imagic'] and status['installed_imagic'] and bool(arg['init_image'] and not bool(arg['mask_image'])):
          if idx == 0: seed_suffix += '-alpha_1'
          if idx == 1: seed_suffix += '-alpha_1_5'
          if idx == 2: seed_suffix += '-alpha_2'
        file_prefix = prefs["file_prefix"]
        if file_prefix == "sd-":
          if 'Diffusion XL' in pipe_used:
            file_prefix = "sdxl-"
          if 'Diffusion 3' in pipe_used:
            file_prefix = "sd3-"
        fname = f'{file_prefix.strip()}{filename}{seed_suffix}'
        image_path = available_file(txt2img_output, fname, idx)
        num = int(image_path.rpartition('-')[2].partition('.')[0])
        #image_path = os.path.join(txt2img_output, f'{fname}-{idx}.png')
        image.save(image_path)
        #print(f'size:{os.path.getsize(f"{fname}-{idx}.png")}')
        if os.path.getsize(image_path) < 2000 or not usable_image: #False: #not sum(image.convert("L").getextrema()) in (0, 2): #image.getbbox():#
          os.remove(os.path.join(txt2img_output, f'{fname}-{num}.png'))
          if strikes >= retry_attempts_if_NSFW:
            if retry_attempts_if_NSFW != 0: prt("Giving up on finding safe image...")
            strikes = 0
            continue
          else: strikes += 1
          new_dream = None
          if isinstance(p, Dream):
            new_dream = p
            new_dream.prompt = pr[0] if type(pr) == list else pr
            new_dream.arg['seed'] = random.randint(0,4294967295)
          else:
            new_dream = Dream(p, arg=dict(seed=random.randint(0,4294967295)))
          updated_prompts.insert(p_idx+1, new_dream)
          prt(f"Filtered NSFW image, retrying prompt with new seed. Attempt {strikes} of {retry_attempts_if_NSFW}...")
          continue
        else: strikes = 0
        #if not prefs['display_upscaled_image'] or not prefs['apply_ESRGAN_upscale']:
          #print(f"Image path:{image_path}")
          #time.sleep(0.4)
          #prt(Row([Img(src=image_path, width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
          #display(image)
        #if bool(batch_folder_name):
        #  fpath = os.path.join(txt2img_output, batch_folder_name, f'{fname}-{idx}.png')
        #fpath = os.path.join(txt2img_output, f'{fname}-{idx}.png')
        #fpath = available_file(txt2img_output, fname, idx)
        fpath = image_path
        if txt2img_output != batch_output:
          new_file = available_file(batch_output, fname, num)
        else:
          new_file = image_path
        #print(f'fpath: {fpath} - idx: {idx}')
        if prefs['centipede_prompts_as_init_images']:
          shutil.copy(fpath, os.path.join(root_dir, 'init_images'))
          last_image = os.path.join(root_dir, 'init_images', f'{fname}-{num}.png')
        page.auto_scrolling(True)
        #if (not prefs['display_upscaled_image'] or not prefs['apply_ESRGAN_upscale']) and prefs['apply_ESRGAN_upscale']:
        #if not prefs['display_upscaled_image'] and prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
          #upscaled_path = new_file #os.path.join(batch_output if save_to_GDrive else txt2img_output, new_file)
          #prt(Row([ImageButton(src=fpath, data=new_file, width=arg['width'], height=arg['height'], subtitle=pr[0] if type(pr) == list else pr, center=True, page=page)], alignment=MainAxisAlignment.CENTER))
          #print(f"Image path:{image_path}")
          #time.sleep(0.2)
          #prt(Row([GestureDetector(content=Img(src_base64=get_base64(fpath), width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True), data=new_file, on_long_press_end=download_image, on_secondary_tap=download_image)], alignment=MainAxisAlignment.CENTER))
          #prt(Row([GestureDetector(content=Img(src=fpath, width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True), data=new_file, on_long_press_end=download_image, on_secondary_tap=download_image)], alignment=MainAxisAlignment.CENTER))
          #prt(ImageButton(src=fpath, width=arg['width'], height=arg['height'], data=new_file, subtitle=pr[0] if type(pr) == list else pr, center=True, page=page))
          #time.sleep(0.3)
          #display(image)
        if prefs['use_upscale'] and status['installed_upscale']:
          clear_pipes(['upscale'])
          if pipe_upscale == None:
            prt(Installing("Initializing Stable Diffusion 2 Upscale Pipeline..."))
            pipe_upscale = get_upscale_pipe()
            clear_last()
          prt(Row([Text("Upscaling 4X"), pb]))
          try:
            output = pipe_upscale(prompt=pr, image=image, guidance_scale=arg['guidance_scale'], generator=generator, noise_level=prefs['upscale_noise_level'], callback=callback_fn, callback_steps=1)
            output.images[0].save(fpath)
            #clear_upscale()
          except Exception as e:
            alert_msg(page, "Error Upscaling Image.  Most likely out of Memory... Reduce image size to less than 512px.", content=Text(e))
            pass
          clear_last()
          #clear_upscale_pipe()
        if prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
          w = int(arg['width'] * prefs["enlarge_scale"])
          h = int(arg['height'] * prefs["enlarge_scale"])
          prt(Row([Text(f'Enlarging {prefs["enlarge_scale"]}X to {w}x{h}')], alignment=MainAxisAlignment.CENTER))
          upscale_image(fpath, fpath, scale=prefs["enlarge_scale"], face_enhance=prefs["face_enhance"])
          clear_last(update=False)

        config_json = arg.copy()
        del config_json['batch_size']
        del config_json['n_iterations']
        del config_json['precision']
        config_json['prompt'] = pr[0] if type(pr) == list else pr
        config_json['sampler'] = prefs['generation_sampler'] if prefs['use_Stability_api'] else prefs['scheduler_mode']
        if bool(prefs['meta_ArtistName']): config_json['artist'] = prefs['meta_ArtistName']
        if bool(prefs['meta_Copyright']): config_json['copyright'] = prefs['meta_Copyright']
        #TODO: if use_LoRA add layers and scale
        if prefs['use_Stability_api']: del config_json['eta']
        del config_json['use_Stability']
        if not bool(config_json['negative_prompt']): del config_json['negative_prompt']
        if not bool(config_json['prompt2']):
          del config_json['prompt2']
          del config_json['tweens']
        if not bool(config_json['init_image']):
          del config_json['init_image']
          del config_json['init_image_strength']
          del config_json['alpha_mask']
        if not bool(config_json['mask_image']):
          del config_json['mask_image']
          del config_json['invert_mask']
        if not bool(config_json['use_clip_guided_model']):
          del config_json["use_clip_guided_model"]
          del config_json["clip_prompt"]
          del config_json["clip_guidance_scale"]
          del config_json["num_cutouts"]
          del config_json["use_cutouts"]
          del config_json["unfreeze_unet"]
          del config_json["unfreeze_vae"]
        else:
          config_json["clip_model_id"] = prefs['clip_model_id']
        if prefs['apply_ESRGAN_upscale']:
          config_json['upscale'] = f"Upscaled {prefs['enlarge_scale']}x with ESRGAN" + (" with GFPGAN Face-Enhance" if prefs['face_enhance'] else "")
        sampler_str = prefs['generation_sampler'] if prefs['use_Stability_api'] else prefs['scheduler_mode']
        config_json['pipeline'] = pipe_used
        config_json['scheduler_mode'] = sampler_str
        config_json['model_path'] = model_path
        if prefs['use_SDXL'] and status['installed_SDXL']:
          config_json['model_path'] = "stabilityai/stable-diffusion-xl-base-1.0"
        if prefs['save_image_metadata']:
          img = PILImage.open(fpath)
          metadata = PngInfo()
          metadata.add_text("artist", prefs['meta_ArtistName'])
          metadata.add_text("copyright", prefs['meta_Copyright'])
          metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {prefs['enlarge_scale']}x with ESRGAN" if prefs['apply_ESRGAN_upscale'] else "")
          metadata.add_text("title", pr[0] if type(pr) == list else pr)
          if prefs['save_config_in_metadata']:
            config = f"prompt: {pr[0] if type(pr) == list else pr}, seed: {cur_seed}, steps: {arg['steps']}, CGS: {arg['guidance_scale']}, iterations: {arg['n_iterations']}" + f", eta: {arg['eta']}" if not prefs['use_Stability_api'] else ""
            config += f", sampler: {sampler_str}"
            if bool(arg['init_image']): config += f", init_image: {arg['init_image']}, init_image_strength: {arg['init_image_strength']}"
            metadata.add_text("config", config)
            #metadata.add_text("prompt", p)
            metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
          img.save(fpath, pnginfo=metadata)

        #new_file = available_file(batch_output if save_to_GDrive else txt2img_output, fname, idx)
        #new_file = fname #.rpartition('.')[0] #f'{file_prefix}{filename}'
        #if os.path.isfile(os.path.join(batch_output if save_to_GDrive else txt2img_output, f'{new_file}-{idx}.png')):
        #  new_file += '-' + random.choice(string.ascii_letters) + random.choice(string.ascii_letters)
        #new_file += f'-{idx}.png'
        if save_to_GDrive:
          shutil.copy(fpath, new_file)#os.path.join(batch_output, new_file))
          #shutil.move(fpath, os.path.join(batch_output, new_file))
        elif storage_type == "PyDrive Google Drive":
          #batch_output
          out_file = gdrive.CreateFile({'title': new_file})
          out_file.SetContentFile(fpath)
          out_file.Upload()
        elif bool(prefs['image_output']):
          if not os.path.exists(new_file):
            shutil.copy(fpath, new_file)#os.path.join(batch_output, new_file))
        if prefs['save_config_json']:
          json_file = os.path.basename(new_file).rpartition('.')[0] + '.json'
          with open(os.path.join(batch_output, json_file), "w") as f:
            json.dump(config_json, f, ensure_ascii=False, indent=4)
          #if save_to_GDrive:
          #shutil.copy(os.path.join(stable_dir, json_file), os.path.join(batch_output, json_file))
          if storage_type == "PyDrive Google Drive":
            out_file = gdrive.CreateFile({'title': json_file})
            out_file.SetContentFile(os.path.join(batch_output, json_file))
            out_file.Upload()
        output_files.append(os.path.join(batch_output if save_to_GDrive else txt2img_output, new_file))
        if (prefs['display_upscaled_image'] and prefs['apply_ESRGAN_upscale']):
          upscaled_path = os.path.join(batch_output if save_to_GDrive else txt2img_output, new_file)
          #time.sleep(0.4)
          #prt(Row([GestureDetector(content=Img(src_base64=get_base64(upscaled_path), width=arg['width'] * float(prefs["enlarge_scale"]), height=arg['height'] * float(prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True), data=upscaled_path, on_long_press_end=download_image, on_secondary_tap=download_image)], alignment=MainAxisAlignment.CENTER))
          #prt(Row([GestureDetector(content=Img(src=upscaled_path, width=arg['width'] * float(prefs["enlarge_scale"]), height=arg['height'] * float(prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True), data=upscaled_path, on_long_press_end=download_image, on_secondary_tap=download_image)], alignment=MainAxisAlignment.CENTER))
          prt(Row([ImageButton(src=upscaled_path, width=arg['width'] * float(prefs["enlarge_scale"]), height=arg['height'] * float(prefs["enlarge_scale"]), data=upscaled_path, subtitle=pr[0] if type(pr) == list else pr, page=page)], alignment=MainAxisAlignment.CENTER))
          #prt(Row([Img(src=upscaled_path, width=arg['width'] * float(prefs["enlarge_scale"]), height=arg['height'] * float(prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
          #prt(Img(src=upscaled_path))
          #upscaled = PILImage.open(os.path.join(batch_output, new_file))
          #display(upscaled)
        #else:
          #time.sleep(0.4)
          #prt(Row([Img(src=new_file, width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        elif not prefs['apply_ESRGAN_upscale'] or not status['installed_ESRGAN']:
          upscaled_path = os.path.join(batch_output if save_to_GDrive else txt2img_output, new_file)
          prt(Row([ImageButton(src=upscaled_path, width=arg['width'], height=arg['height'], data=upscaled_path, subtitle=pr[0] if type(pr) == list else pr, page=page)], alignment=MainAxisAlignment.CENTER))
        elif not prefs['display_upscaled_image'] and prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
          #print(f"Image path:{image_path}")
          upscaled_path = new_file #os.path.join(batch_output if save_to_GDrive else txt2img_output, new_file)
          prt(Row([ImageButton(src=fpath, data=new_file, width=arg['width'], height=arg['height'], subtitle=pr[0] if type(pr) == list else pr, center=True, page=page)], alignment=MainAxisAlignment.CENTER))

        prt(Row([Text(fpath.rpartition(slash)[2])], alignment=MainAxisAlignment.CENTER))
        idx += 1
        if abort_run:
          prt(Text("🛑   Aborting Current Diffusion Run..."))
          abort_run = False
          return
      p_idx += 1
      if abort_run:
        prt(Text("🛑   Aborting Current Diffusion Run..."))
        abort_run = False
        return
    play_snd(Snd.ALERT, page)
  else:
    clear_pipes("interpolation")
    if pipe_interpolation is None:
      pipe_interpolation = get_interpolation_pipe()
    txt2img_output = os.path.join(stable_dir, prefs['batch_folder_name'] if bool(prefs['batch_folder_name']) else 'dreams')
    batch_output = prefs['image_output']
    if not os.path.exists(txt2img_output):
      os.makedirs(txt2img_output)
    #dream_name = prefs['batch_folder_name'] if bool(prefs['batch_folder_name']) else None
    #first = prompts[0]
    arg = args.copy()
    arg['width'] = int(arg['width'])
    arg['height'] = int(arg['height'])
    arg['seed'] = int(arg['seed'])
    arg['guidance_scale'] = float(arg['guidance_scale'])
    arg['steps'] = int(arg['steps'])
    arg['eta'] = float(arg['eta'])
    walk_prompts = []
    walk_seeds = []
    for p in prompts:
      walk_prompts.append(p.prompt)
      if int(p.arg['seed']) < 1 or arg['seed'] is None:
        walk_seeds.append(random.randint(0,4294967295))
      else:
        walk_seeds.append(int(p.arg['seed']))
    img_idx = 0
    from watchdog.observers import Observer
    from watchdog.events import LoggingEventHandler, FileSystemEventHandler
    class Handler(FileSystemEventHandler):
      def __init__(self):
        super().__init__()
      def on_created(self,event):
        nonlocal img_idx
        if event.is_directory:
          return None
        elif event.event_type == 'created':
          page.auto_scrolling(True)
          clear_last()
          #p_count = f'[{img_idx + 1} of {(len(walk_prompts) -1) * int(prefs['num_interpolation_steps'])}]  '
          #prt(Divider(height=6, thickness=2))
          #prt(Row([Text(p_count), Text(walk_prompts[img_idx], expand=True, weight=FontWeight.BOLD), Text(f'seed: {walk_seeds[img_idx]}')]))
          #prt(Row([Img(src=event.src_path, width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
          prt(Row([ImageButton(src=event.src_path, data=event.src_path, width=arg['width'], height=arg['height'], subtitle=f"Frame {img_idx} - {event.src_path}", center=True, page=page)], alignment=MainAxisAlignment.CENTER))
          prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))
          page.update()
          time.sleep(0.2)
          prt(pb)
          page.auto_scrolling(False)
          img_idx += 1
    # TODO: Rename files with to-from prompt text between each
    image_handler = Handler()
    observer = Observer()
    observer.schedule(image_handler, txt2img_output, recursive = True)
    observer.start()
    prt(f"Interpolating latent space between {len(walk_prompts)} prompts with {int(prefs['num_interpolation_steps'])} steps between each.")
    prt(Divider(height=6, thickness=2))
    prt(pb)
    page.auto_scrolling(False)
    #prt(Row([Text(p_count), Text(pr[0] if type(pr) == list else pr, expand=True, weight=FontWeight.BOLD), Text(f'seed: {arg["seed"]}')]))
    images = pipe_interpolation.walk(prompts=walk_prompts, seeds=walk_seeds, num_interpolation_steps=int(prefs['num_interpolation_steps']), batch_size=int(prefs['batch_size']), output_dir=txt2img_output, width=arg['width'], height=arg['height'], guidance_scale=arg['guidance_scale'], num_inference_steps=int(arg['steps']), eta=arg['eta'], callback=callback_fn, callback_steps=1)
    observer.stop()
    clear_last()
    fpath = os.path.dirname(images[0])
    bfolder = fpath.rpartition(slash)[2]
    if prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
      prt('Applying Real-ESRGAN Upscaling to images...')
      upscale_image(images, fpath, scale=prefs["enlarge_scale"], face_enhance=prefs["face_enhance"])
      '''os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
      upload_folder = 'upload'
      result_folder = 'results'
      if os.path.isdir(upload_folder):
          shutil.rmtree(upload_folder)
      if os.path.isdir(result_folder):
          shutil.rmtree(result_folder)
      os.mkdir(upload_folder)
      os.mkdir(result_folder)
      for i in images:
        fname = i.rpartition(slash)[2]
        dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, fname)
        shutil.move(i, dst_path)
      faceenhance = ' --face_enhance' if prefs["face_enhance"] else ''
      run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
      filenames = os.listdir(os.path.join(dist_dir, 'Real-ESRGAN', 'results'))
      for oname in filenames:
        fparts = oname.rpartition('_out')
        fname_clean = fparts[0] + fparts[2]
        opath = os.path.join(fpath, fname_clean)
        shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, oname), opath)
      os.chdir(stable_dir)'''
    os.makedirs(os.path.join(batch_output, bfolder), exist_ok=True)
    imgs = os.listdir(fpath)
    for i in imgs:
      #prt(f'Created {i}')
      #fname = i.rpartition(slash)[2]
      if save_to_GDrive:
        shutil.copy(os.path.join(fpath, i), os.path.join(batch_output, bfolder, i))
      elif storage_type == "PyDrive Google Drive":
        #batch_output
        out_file = gdrive.CreateFile({'title': i})
        out_file.SetContentFile(fpath)
        out_file.Upload()
      elif bool(prefs['image_output']):
        shutil.copy(os.path.join(fpath, i), os.path.join(batch_output, bfolder, i))
    play_snd(Snd.ALERT, page)

def prompt_parse(prompt):
    '''Convert A1111 weights to Compel format'''
    if ')' not in prompt and ']' not in prompt:
        return prompt
    else:
        # Find and replace all instances of the colon format with the desired format
        prompt = re.sub(r'\(([^:]+):([\d.]+)\)', r'(\1)\2', prompt)
        # Find and replace square brackets with round brackets and assign weight
        prompt = re.sub(r'\[([^:\]]+)\]', r'(\1)0.909090909', prompt)
        # Handle the general case of [x:number] and convert it to (x)0.9
        prompt = re.sub(r'\[([^:]+):[\d.]+\]', r'(\1)0.9', prompt)
        return prompt

nspterminology = None

def nsp_parse(prompt):
    import random, os, json
    global nspterminology
    new_prompt = ''
    new_prompts = []
    new_dict = {}
    ptype = type(prompt)
    #if not os.path.exists('./nsp_pantry.json'):
    #    wget('https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.json', f'.{slash}nsp_pantry.json')
    if nspterminology is None:
        response = requests.get("https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.json")
        nspterminology = json.loads(response.content)
    if ptype == dict:
        for pstep, pvalue in prompt.items():
            if type(pvalue) == list:
                for prompt in pvalue:
                    new_prompt = prompt
                    for term in nspterminology:
                        tkey = f'_{term}_'
                        tcount = prompt.count(tkey)
                        for i in range(tcount):
                            new_prompt = new_prompt.replace(tkey, random.choice(nspterminology[term]), 1)
                    new_prompts.append(new_prompt)
                new_dict[pstep] = new_prompts
                new_prompts = []
        return new_dict
    elif ptype == list:
        for pstr in prompt:
            new_prompt = pstr
            for term in nspterminology:
                tkey = f'_{term}_'
                tcount = new_prompt.count(tkey)
                for i in range(tcount):
                    new_prompt = new_prompt.replace(tkey, random.choice(nspterminology[term]), 1)
            new_prompts.append(new_prompt)
            new_prompt = None
        return new_prompts
    elif ptype == str:
        new_prompt = prompt
        for term in nspterminology:
            tkey = f'_{term}_'
            tcount = new_prompt.count(tkey)
            for i in range(tcount):
                new_prompt = new_prompt.replace(tkey, random.choice(nspterminology[term]), 1)
        return new_prompt
    else:
        return

def switchcase(switch, cases, default=None):
    result = cases.get(switch, default)
    return result() if callable(result) else result

class SwitchStr(str):
    def switchcase(self, cases, default=None):
        result = cases.get(self, default)
        return result() if callable(result) else result

def tween_value(start, end, amount):
    values = []
    step = (end - start) / (amount - 1)
    for i in range(amount):
        value = start + i * step
        values.append(value)
    return values

#Code a function in Python programming language named list_variations, which takes a list and returns a set of lists with possible permutations of the list. Example list_variations([1,2,3]) returns [[1,2,3],[1,2],[1,3],[2,3],[1],[2],[3]] */
def list_variations(lst):
    result = []
    for i in range(len(lst)):
        for j in range(i+1, len(lst)+1):
            result.append(lst[i:j])
    return result
#print(str(list_variations([1,2,3])))
def and_list(lst):
  return " and ".join([", ".join(lst[:-1]),lst[-1]] if len(lst) > 2 else lst)

generator_request_modes = ["visually detailed",
  "with long detailed colorful interesting artistic scenic visual descriptions",
  "that is highly detailed, artistically interesting, describes a scene, colorful poetic language, with intricate visual descriptions",
  "that are strange, descriptive, graphically visual, full of interesting subjects described in great detail, painted by an artist",
  "that is technical, wordy, extra detailed, confusingly tangental, colorfully worded, dramatically narrative",
  "that is creative, imaginative, funny, interesting, scenic, dark, witty, visual, unexpected, wild",
  "that includes many subjects with descriptions, color details, artistic expression, point of view",
  "complete sentence using many words to describe a landscape in an epic fantasy genre that includes a lot adjectives",
  "in a way that a person would describe an image separated by commas when necessary. All in lower case. Expand the input below into a more detailed caption without changing the original relative positions or interactions between objects, colors or any other specific attributes if they are disclosed in the original prompt. Clarify positional information, colors, counts of objects, other visual aspects and features. Make sure to include as much detail as possible. Make sure to describe the spatial relationships seen in the image. You can use words like left/right, above/below, front/behind, far/near/adjacent, inside/outside. Make sure to include object interactions like 'a table is in front of the kitchen pot' and 'there are baskets on the table'. Also describe relative sizes of objects seen in the image. Make sure to include counts of prominent objects in the image, especially when there is humans in the image. When its a photograph, include photographic details like bokeh, large field of view etc but dont just say it to say something, do it only when it makes sense. When its art, include details about the style like minimalist, impressionist, oil painting etc. Include world and period knowledge if it makes sense to, like 1950s chevrolet etc."]

def run_prompt_generator(page):
  import random as rnd
  global artists, styles, status, horde_text_models
  if 'GPT' in prefs['prompt_generator']['AI_engine']:
    try:
      import openai
    except:
      page.prompt_generator_list.controls.append(Installing("Installing OpenAI Library..."))
      page.prompt_generator_list.update()
      run_sp("pip install --upgrade openai", realtime=False)
      import openai
      del page.prompt_generator_list.controls[-1]
      page.prompt_generator_list.update()
      pass
    try:
      #openai.api_key = prefs['OpenAI_api_key']
      from openai import OpenAI
      openai_client = OpenAI(api_key=prefs['OpenAI_api_key'])
    except:
      alert_msg(page, "Invalid OpenAI API Key. Change in Settings...")
      return
    status['installed_OpenAI'] = True
  if prefs['prompt_generator']['AI_engine'] == "Google Gemini 1.0":
    if not bool(prefs['PaLM_api_key']):
      alert_msg(page, "You must provide your Google Gemini MakerSuite API key in Settings first")
      return
    try:
      import google.generativeai as genai
      if force_update("generativeai"): raise ModuleNotFoundError("Forcing update")
    except:
      page.prompt_generator_list.controls.append(Installing("Installing Google MakerSuite Library..."))
      page.prompt_generator_list.update()
      run_sp("pip install --upgrade google-generativeai", realtime=False)
      import google.generativeai as genai
      del page.prompt_generator_list.controls[-1]
      page.prompt_generator_list.update()
      pass
    try:
      genai.configure(api_key=prefs['PaLM_api_key'])
    except:
      alert_msg(page, "Invalid Google Gemini API Key. Change in Settings...")
      return
    gemini_model = genai.GenerativeModel(model_name='gemini-pro')
  if prefs['prompt_generator']['AI_engine'] == "Gemini Pro":
    if not bool(prefs['PaLM_api_key']):
      alert_msg(page, "You must provide your Google Gemini MakerSuite API key in Settings first")
      return
    try:
      import vertexai
    except:
      page.prompt_generator_list.controls.append(Installing("Installing Vertex AI Gemini SDK Library..."))
      page.prompt_generator_list.update()
      run_sp("pip install --upgrade google-cloud-aiplatform", realtime=False)
      from vertexai.preview.generative_models import GenerativeModel
      del page.prompt_generator_list.controls[-1]
      page.prompt_generator_list.update()
      pass
    try:
      run_sp("gcloud auth application-default login", realtime=True)
    except:
      alert_msg(page, "Invalid Google Cloud Authentication. Change in Settings...")
      return
  if prefs['prompt_generator']['AI_engine'].startswith("Google Gemini"):
    if not bool(prefs['PaLM_api_key']):
      alert_msg(page, "You must provide your Google Gemini MakerSuite API key in Settings first")
      return
    try:
      import google.generativeai as genai
      if force_update("generativeai"): raise ModuleNotFoundError("Forcing update")
    except:
      page.prompt_generator_list.controls.append(Installing("Installing Google MakerSuite Library..."))
      page.prompt_generator_list.update()
      run_sp("pip install --upgrade google-generativeai", realtime=False)
      import google.generativeai as genai
      del page.prompt_generator_list.controls[-1]
      page.prompt_generator_list.update()
      pass
    try:
      genai.configure(api_key=prefs['PaLM_api_key'])
    except:
      alert_msg(page, "Invalid Google Gemini API Key. Change in Settings...")
      return
    #for model in genai.list_models(): print(model)
    gemini_model_name = 'gemini-1.5-flash-latest' if '1.5 Flash' in prefs['prompt_generator']['AI_engine'] else 'gemini-1.5-pro-latest' if '1.5 Pro' in prefs['prompt_generator']['AI_engine'] else 'gemini-1.0-pro-latest'
    gemini_model = genai.GenerativeModel(model_name=gemini_model_name)
  if "Anthropic Claude 3" in prefs['prompt_generator']['AI_engine']:
    if not bool(prefs['Anthropic_api_key']):
      alert_msg(page, "You must provide your Anthropic.ai Claude API key in Settings first")
      return
    try:
      import anthropic
    except:
      page.prompt_generator_list.controls.append(Installing("Installing Anthropic.ai Claude SDK Library..."))
      page.prompt_generator_list.update()
      run_sp("pip install --upgrade anthropic", realtime=False)
      import anthropic
      del page.prompt_generator_list.controls[-1]
      page.prompt_generator_list.update()
      pass
    os.environ["ANTHROPIC_API_KEY"] = prefs['Anthropic_api_key']
    try:
      anthropic_client = anthropic.Anthropic(api_key=prefs['Anthropic_api_key'])
    except:
      alert_msg(page, "Invalid Anthropic API Authentication. Change in Settings...")
      return
  prompts_gen = []
  prompt_results = []
  subject = ""
  if bool(prefs['prompt_generator']['subject_detail']):
      subject = ", and " + prefs['prompt_generator']['subject_detail']

  def prompt_gen():
    prompt = f'''Write a bullet list of {prefs['prompt_generator']['amount'] if prefs['prompt_generator']['phrase_as_subject'] else (prefs['prompt_generator']['amount'] + 4)} image generation prompts per line about "{prefs['prompt_generator']['phrase']}"{subject}, {generator_request_modes[int(prefs['prompt_generator']['request_mode'])]}, and unique without repetition:

'''
    #print(prompt)
    if not prefs['prompt_generator']['phrase_as_subject']:
      prompt += "\n* "
    else:
      prompt += f"""* A beautiful painting of a serene landscape with a river running through it, lush trees, golden sun illuminating
* Fireflies illuminating autumnal woods, an Autumn in the Brightwood glade, with warm yellow lantern lights
* The Fabric of spacetime continuum over a large cosmological vista, pieces of dark matter, space dust and nebula doted with small dots that seem to form fractal patterns and glowing bright lanterns in distances, also with an stardust effect towards the plane of the galaxy
* Midnight landscape painting of a city under a starry sky, owl in the shaman forest knowing the ways of magic, warm glow over the buildings
* {prefs['prompt_generator']['phrase']}"""
    if prefs['prompt_generator']['AI_engine'] == "OpenAI GPT-3":
      response = openai_client.completions.create(engine="text-davinci-003", prompt=prompt, max_tokens=2400, temperature=prefs['prompt_generator']['AI_temperature'], presence_penalty=1)
      #print(response)
      result = response.choices[0].text.strip()#["choices"][0]["text"].strip()
    elif prefs['prompt_generator']['AI_engine'] == "ChatGPT-3.5 Turbo":
      response = openai_client.chat.completions.create(
        model="gpt-3.5-turbo-0125",
        temperature=prefs['prompt_generator']['AI_temperature'],
        messages=[{"role": "user", "content": prompt}]
      )
      #print(str(response))
      result = response.choices[0].message.content.strip()#["choices"][0]["message"]["content"].strip()
    elif "OpenAI" in prefs['prompt_generator']['AI_engine']:
      gpt_model = OpenAI_models[prefs['prompt_generator']['OpenAI_model']]#"gpt-4-turbo" if "Turbo" in prefs['prompt_generator']['OpenAI_model'] else "gpt-4o" if "4o" in prefs['prompt_generator']['AI_engine'] else "gpt-4"
      response = openai_client.chat.completions.create(
        model=gpt_model,
        temperature=prefs['prompt_generator']['AI_temperature'],
        messages=[{"role": "user", "content": prompt}]
      )
      result = response.choices[0].message.content.strip()#["choices"][0]["message"]["content"].strip()
    elif prefs['prompt_generator']['AI_engine'].startswith("Google Gemini"):
      from google.generativeai.types import HarmCategory, HarmBlockThreshold
      #print(palm.list_models())
      completion = gemini_model.generate_content(prompt, generation_config={
          'temperature': prefs['prompt_generator']['AI_temperature'],
          'max_output_tokens': 1024
          }, safety_settings={
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
      })
      #completion = palm.generate_text(model='models/text-bison-001', prompt=prompt, temperature=prefs['prompt_generator']['AI_temperature'], max_output_tokens=1024)
      try:
        #print(completion.text)
        response = completion.text
      except ValueError:
        try:
          response = completion.candidates[0].content.parts[0].text
        except Exception:
          print(completion.prompt_feedback)
          print(completion.candidates[0].finish_reason)
          print(completion.candidates[0].safety_ratings)
          print(response)
          return
        pass
      #print(str(completion))
      result = response.strip()
    elif prefs['prompt_generator']['AI_engine'] == "Anthropic Claude 3":
      model = "claude-3-sonnet-20240229" if prefs['prompt_generator']['AI_engine'] == "Anthropic Claude 3" else "claude-3-5-sonnet-20240620"
      try:
        response = anthropic_client.messages.create(
          model=model,#"claude-3-sonnet-20240229",#"claude-3-opus-20240229",
          max_tokens=4000,
          temperature=prefs['prompt_generator']['AI_temperature'],
          system=f"Respond with an unordered list of image generation prompts in the amount specified with each line starting with an * asterisk, {generator_request_modes[int(prefs['prompt_generator']['request_mode'])]}, and unique without repetition",
          messages=[
              {"role": "user", "content": prompt}
          ]
        )
      except Exception as e:
        alert_msg(page, "ERROR Running Anthropic API Request...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        del page.prompt_generator_list.controls[-1]
        page.prompt_generator_list.update()
        return
      #completion = palm.generate_text(model='models/text-bison-001', prompt=prompt, temperature=prefs['prompt_generator']['AI_temperature'], max_output_tokens=1024)
      #print(str(response))
      result = response.content[0].text
      print(str(result))
    elif prefs['prompt_generator']['AI_engine'] == "AI-Horde":
      del page.prompt_generator_list.controls[-1]
      page.prompt_generator_list.update()
      result = get_horde_text(page, page.prompt_generator_list, prompt, prefs['prompt_generator']['AIHorde_model'], prefs['prompt_generator']['AI_temperature'])
    elif prefs['prompt_generator']['AI_engine'] == "Perplexity":
      del page.prompt_generator_list.controls[-1]
      page.prompt_generator_list.update()
      result = get_perplexity_text(page, page.prompt_generator_list, prompt, prefs['prompt_generator']['Perplexity_model'], prefs['prompt_generator']['AI_temperature'])

    #if result[-1] == '.': result = result[:-1]
    #print(str(result))
    for p in result.split('\n'):
      pr = p.strip()
      if not bool(pr): continue
      if pr.startswith('#'): continue
      if pr[-1] == '.': pr = pr[:-1]
      if pr[0] == '*': pr = pr[1:].strip()
      if pr[0] == '-': pr = pr[1:].strip()
      elif '.' in pr: # Sometimes got 1. 2.
        pr = pr.partition('.')[2].strip()
      if '"' in pr: pr = pr.replace('"', '')
      if pr.endswith("."):
        pr = pr[:(-1)]
      if '*' in pr:
        pr = pr.replace('*', '')
        #pr = pr.rpartition('*')[2].strip()
      if bool(pr.strip()):
        if pr[0] == ':': pr = pr[1:].strip()
      prompt_results.append(pr)
  #print(f"Request mode influence: {request_modes[prefs['prompt_generator']['request_mode']]}\n")
  page.prompt_generator_list.controls.append(Installing("Requesting Prompts from the AI..."))
  page.prompt_generator_list.update()
  prompt_gen()
  if len(page.prompt_generator_list.controls) > 0:
    del page.prompt_generator_list.controls[-1]
    page.prompt_generator_list.update()
  if len(prompt_results) < prefs['prompt_generator']['amount']:
    additional = prefs['prompt_generator']['amount'] - len(prompt_results)
    print(f"Didn't make enough prompts.. Needed {additional} more.")
  n=1
  for p in prompt_results:
    random_artist=[]
    for a in range(prefs['prompt_generator']['random_artists']):
      random_artist.append(rnd.choice(artists))
    #print(list_variations(random_artist))
    artist = " and ".join([", ".join(random_artist[:-1]),random_artist[-1]] if len(random_artist) > 2 else random_artist)
    random_style = []
    for s in range(prefs['prompt_generator']['random_styles']):
      random_style.append(rnd.choice(styles))
    style = ", ".join(random_style)
    if prefs['prompt_generator']['phrase_as_subject'] and n == 1:
      p = prefs['prompt_generator']['phrase'] + " " + p
    text_prompt = p
    #if prefs['prompt_generator']['random_artists'] > 0 or prefs['prompt_generator']['random_styles'] > 0:
    prompts_gen.append(text_prompt)
    if prefs['prompt_generator']['random_artists'] > 0: text_prompt += f", by {artist}"
    if prefs['prompt_generator']['random_styles'] > 0: text_prompt += f", style of {style}"
    #if prefs['prompt_generator']['random_styles'] != 0 and prefs['prompt_generator']['permutate_artists']:
    #  prompts_gen.append(text_prompt)
    if prefs['prompt_generator']['permutate_artists']:
      for a in list_variations(random_artist):
        prompt_variation = p + f", by {and_list(a)}"
        prompts_gen.append(prompt_variation)
      if prefs['prompt_generator']['random_styles'] > 0:
        prompts_gen.append(p + f", style of {style}")
    #else: prompts_gen.append(text_prompt)
    n += 1
  page.generator.auto_scroll = True
  for item in prompts_gen:
    page.add_to_prompt_generator(item)
    #print(f'   "{item}",')
  nudge(page.generator, page)
  page.generator.auto_scroll = False
  page.generator.update()

remixer_request_modes = [
      "visually detailed wording, flowing sentences, extra long descriptions",
      "that is similar but with more details, themes, imagination, interest, subjects, artistic style, poetry, tone, settings, adjectives, visualizations",
      "that is completely rewritten, inspired by, paints a complete picture of an artistic scene",
      "with detailed colorful interesting artistic scenic visual descriptions, described to a blind person",
      "that is highly detailed, artistically interesting, describes a scene, colorful poetic language, with intricate visual descriptions",
      "that replaces every noun, adjective, verb, pronoun, with related words",
      "that is strange, descriptive, graphically visual, full of interesting subjects described in great detail, painted by an artist",
      "that is highly technical, extremely wordy, extra detailed, confusingly tangental, colorfully worded, dramatically narrative",
      "that is creative, imaginative, funny, interesting, scenic, dark, witty, visual, unexpected, wild",
      "that includes more subjects with descriptions, textured color details, expressive",]
      #"complete sentence using many words to describe a landscape in an epic fantasy genre that includes a lot adjectives",

def run_prompt_remixer(page):
  import random as rnd
  global artists, styles, status
  engine = prefs['prompt_remixer']['AI_engine']
  if 'TextSynth' in engine:
    good_key = True
    try:
      if not bool(prefs['TextSynth_api_key']): good_key = False
    except NameError: good_key = False
    if not good_key:
      alert_msg(page, f"Missing TextSynth api key... Define your key in Settings.")
      return
    else:
      try:
        from textsynthpy import TextSynth, Complete
      except ImportError:
        run_sp("pip install textsynthpy", realtime=False)
        #clear_output()
      finally:
        from textsynthpy import TextSynth, Complete
      if engine =="TextSynth":
        model = prefs['prompt_remixer']['TextSynth_model']
        textsynth_engine = "gptj_6B" if 'GPT-J' in model else "mistral_7B_instruct" if 'Mistral Instruct' in model else "mistral_7B" if 'Mistral' in model else "mixtral_47B_instruct" if 'Mixtral Instruct' in model else "llama2_7B" if 'Llama2 7B' in model else "llama2_70B" if 'Llama2 70B' in model else "mistral_7B"
      else:
        textsynth_engine = "gptj_6B" if 'GPT-J' in engine else "mistral_7B_instruct" if 'Mistral Instruct' in engine else "mistral_7B" if 'Mistral' in engine else "mixtral_47B_instruct" if 'Mixtral Instruct' in engine else "llama2_7B" if 'Llama2 7B' in engine else "llama2_70B" if 'Llama2 70B' in engine else "mistral_7B"
      textsynth = TextSynth(prefs['TextSynth_api_key'], engine=textsynth_engine)
  elif 'GPT' in engine:
    try:
      import openai
    except:
      run_sp("pip install --upgrade openai")
      import openai
      pass
    try:
      #openai.api_key = prefs['OpenAI_api_key']
      from openai import OpenAI
      openai_client = OpenAI(api_key=prefs['OpenAI_api_key'])
    except:
      alert_msg(page, "Invalid OpenAI API Key. Change in Settings...")
      return
    status['installed_OpenAI'] = True
  elif engine.startswith("Google Gemini"):
    if not bool(prefs['PaLM_api_key']):
      alert_msg(page, "You must provide your Google Gemini MakerSuite API key in Settings first")
      return
    try:
      import google.generativeai as genai
      if force_update("generativeai"): raise ModuleNotFoundError("Forcing update")
    except:
      page.prompt_remixer_list.controls.append(Installing("Installing Google MakerSuite Library..."))
      page.prompt_remixer_list.update()
      run_sp("pip install --upgrade google-generativeai", realtime=False)
      import google.generativeai as genai
      del page.prompt_remixer_list.controls[-1]
      page.prompt_remixer_list.update()
      pass
    try:
      genai.configure(api_key=prefs['PaLM_api_key'])
    except:
      alert_msg(page, "Invalid Google Gemini API Key. Change in Settings...")
      return
    gemini_model_name = 'gemini-1.5-flash-latest' if '1.5 Flash' in engine else 'gemini-1.5-pro-latest' if '1.5 Pro' in engine else 'gemini-1.0-pro-latest'
    gemini_model = genai.GenerativeModel(model_name=gemini_model_name)
  elif "Anthropic Claude 3" in engine:
    if not bool(prefs['Anthropic_api_key']):
      alert_msg(page, "You must provide your Anthropic.ai Claude API key in Settings first")
      return
    try:
      import anthropic
    except:
      page.prompt_remixer_list.controls.append(Installing("Installing Anthropic.ai Claude SDK Library..."))
      page.prompt_remixer_list.update()
      run_sp("pip install --upgrade anthropic", realtime=False)
      import anthropic
      del page.prompt_remixer_list.controls[-1]
      page.prompt_remixer_list.update()
      pass
    os.environ["ANTHROPIC_API_KEY"] = prefs['Anthropic_api_key']
    try:
      anthropic_client = anthropic.Anthropic(api_key=prefs['Anthropic_api_key'])
    except:
      alert_msg(page, "Invalid Anthropic API Authentication. Change in Settings...")
      return
  
  prompts_remix = []
  prompt_results = []

  if '_' in prefs['prompt_remixer']['seed_prompt']:
    seed_prompt = nsp_parse(prefs['prompt_remixer']['seed_prompt'])
  else:
    seed_prompt = prefs['prompt_remixer']['seed_prompt']
  if '_' in prefs['prompt_remixer']['optional_about_influencer']:
    optional_about_influencer = nsp_parse(prefs['prompt_remixer']['optional_about_influencer'])
  else:
    optional_about_influencer = prefs['prompt_remixer']['optional_about_influencer']
  about =  f" about {optional_about_influencer}" if bool(optional_about_influencer) else ""
  prompt = f'Write a list of {prefs["prompt_remixer"]["amount"]} remixed variations from the following image generation prompt{about}, "{prefs["prompt_remixer"]["seed_prompt"]}", {remixer_request_modes[int(prefs["prompt_remixer"]["request_mode"])]}, and unique without repetition:\n\n*'
  prompt_results = []

  def prompt_remix():
    if "TextSynth" in engine:
        try:
            response = textsynth.text_complete(prompt=prompt, max_tokens=200, temperature=prefs['prompt_remixer']['AI_temperature'], presence_penalty=1)
            print(f"Response: {response} {type(response)}")
            result = response.text.strip()
        except Exception as e:
            #clear_last()
            alert_msg(page, f"ERROR: Something went wrong with TextSynth...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    elif engine == "OpenAI GPT-3":
      response = openai_client.completions.create(engine="text-davinci-003", prompt=prompt, max_tokens=2400, temperature=prefs["prompt_remixer"]['AI_temperature'], presence_penalty=1)
      #print(response)
      result = response.choices[0].text.strip()
    elif engine == "ChatGPT-3.5 Turbo":
      response = openai_client.chat.completions.create(model="gpt-3.5-turbo-0125", temperature=prefs["prompt_remixer"]['AI_temperature'], messages=[{"role": "user", "content": prompt}])
      #print(str(response))
      result = response.choices[0].message.content.strip()
    elif "GPT" in engine:
      gpt_model = OpenAI_models[prefs['prompt_remixer']['OpenAI_model']]#"gpt-4-turbo" if "Turbo" in engine else "gpt-4o" if "4o" in engine else "gpt-4"
      response = openai_client.chat.completions.create(model=gpt_model, temperature=prefs["prompt_remixer"]['AI_temperature'], messages=[{"role": "user", "content": prompt}])
      result = response.choices[0].message.content.strip()
    elif engine.startswith("Google Gemini"):
      completion = gemini_model.generate_content(prompt, generation_config={
          'temperature': prefs['prompt_remixer']['AI_temperature'],
          'max_output_tokens': 1024
      })
      #completion = palm.generate_text(model='models/text-bison-001', prompt=prompt, temperature=prefs['prompt_remixer']['AI_temperature'], max_output_tokens=1024)
      #print(str(completion.result))
      result = completion.text.strip()
    elif "Anthropic Claude 3" in engine:
      model = "claude-3-sonnet-20240229" if engine == "Anthropic Claude 3" else "claude-3-5-sonnet-20240620"
      try:
        response = anthropic_client.messages.create(
          model=model,#"claude-3-sonnet-20240229",#"claude-3-opus-20240229",
          max_tokens=4000,
          temperature=prefs['prompt_remixer']['AI_temperature'],
          system=f"Respond with an unordered list of remixed variations from the given image generation prompts in the amount specified with each line starting with an * asterisk.",
          messages=[{"role": "user", "content": prompt}]
        )
      except Exception as e:
        alert_msg(page, "ERROR Running Anthropic API Request...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        del page.prompt_remixer_list.controls[-1]
        page.prompt_remixer_list.update()
        return
      result = response.content[0].text
    elif engine == "AI-Horde":
      del page.prompt_remixer_list.controls[-1]
      page.prompt_remixer_list.update()
      result = get_horde_text(page, page.prompt_remixer_list, prompt, prefs['prompt_remixer']['AIHorde_model'], prefs['prompt_remixer']['AI_temperature'])
    elif engine == "Perplexity":
      del page.prompt_remixer_list.controls[-1]
      page.prompt_remixer_list.update()
      result = get_perplexity_text(page, page.prompt_remixer_list, prompt, prefs['prompt_remixer']['Perplexity_model'], prefs['prompt_remixer']['AI_temperature'])

    #if result[-1] == '.': result = result[:-1]
    #print(str(result))
    for p in result.split('\n'):
      pr = p.strip()
      if not bool(pr): continue
      if pr[-1] == '.': pr = pr[:-1]
      if pr[0] == '*': pr = pr[1:].strip()
      elif '.' in pr: # Sometimes got 1. 2.
        pr = pr.partition('.')[2].strip()
      if '*' in pr:
        pr = pr.replace('*', '').strip()
      if pr.endswith(':'): continue
      prompt_results.append(pr)
  page.prompt_remixer_list.controls.append(Text(f"Remixing {seed_prompt}" + (f", about {optional_about_influencer}" if bool(optional_about_influencer) else "") + f"\nRequest mode influence: {remixer_request_modes[int(prefs['prompt_remixer']['request_mode'])]}\n"))
  page.prompt_remixer_list.update()
  #page.add_to_prompt_remixer(f"Remixing {seed_prompt}" + (f", about {optional_about_influencer}" if bool(optional_about_influencer) else "") + f"\nRequest mode influence: {remixer_request_modes[int(prefs['prompt_remixer']['request_mode'])]}\n")
  #print(f"Remixing {seed_prompt}" + (f", about {optional_about_influencer}" if bool(optional_about_influencer) else ""))
  #print(f"Request mode influence: {remixer_request_modes[int(prefs['prompt_remixer']['request_mode'])]}\n")
  page.prompt_remixer_list.controls.append(Installing("Requesting Prompt Remixes..."))
  page.prompt_remixer_list.update()
  prompt_remix()
  if len(page.prompt_remixer_list.controls) > 1:
    del page.prompt_remixer_list.controls[-1]
  del page.prompt_remixer_list.controls[-1]
  page.prompt_remixer_list.update()

  for p in prompt_results:
    random_artist=[]
    for a in range(prefs['prompt_remixer']['random_artists']):
      random_artist.append(rnd.choice(artists))
    #print(list_variations(random_artist))
    artist = " and ".join([", ".join(random_artist[:-1]),random_artist[-1]] if len(random_artist) > 2 else random_artist)
    random_style = []
    for s in range(prefs['prompt_remixer']['random_styles']):
      random_style.append(rnd.choice(styles))
    style = ", ".join(random_style)
    text_prompt = p
    if prefs['prompt_remixer']['random_artists'] > 0: text_prompt += f", by {artist}"
    if prefs['prompt_remixer']['random_styles'] > 0: text_prompt += f", style of {style}"
    if prefs['prompt_remixer']['random_styles'] == 0 and prefs['prompt_remixer']['permutate_artists']:
      prompts_remix.append(text_prompt)
    if prefs['prompt_remixer']['permutate_artists']:
      for a in list_variations(random_artist):
        prompt_variation = p + f", by {and_list(a)}"
        prompts_remix.append(prompt_variation)
      if prefs['prompt_remixer']['random_styles'] > 0:
        prompts_remix.append(p + f", style of {style}")
    else: prompts_remix.append(text_prompt)
  page.remixer.auto_scroll = True
  for item in prompts_remix:
    page.add_to_prompt_remixer(item)
  nudge(page.remixer, page)
  page.remixer.auto_scroll = False

def get_stable_lm(ai_model="StableLM 3b"):
    global pipe_stable_lm, tokenizer_stable_lm
    clear_pipes('stable_lm')
    if pipe_stable_lm != None:
      return pipe_stable_lm
    try:
      import accelerate
    except ModuleNotFoundError:
      run_sp("pip install git+https://github.com/huggingface/accelerate.git", realtime=False)
      import accelerate
      pass
    try:
      #os.environ['LD_LIBRARY_PATH'] += "/usr/lib/wsl/lib:$LD_LIBRARY_PATH"
      import bitsandbytes
    except ModuleNotFoundError:
      run_sp("pip install bitsandbytes", realtime=False)
      import bitsandbytes
      pass
    try:
      import transformers
    except ModuleNotFoundError:
      run_sp("pip install -q transformers==4.21.3 --upgrade --force-reinstall", realtime=False)
      import transformers
      pass
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    if ai_model == "StableLM 3b":
        model_name = "stabilityai/stablelm-tuned-alpha-3b"
    else:
        model_name = "stabilityai/stablelm-base-alpha-7b"
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        pipe_stable_lm = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype="float16",
            load_in_8bit=True,
            device_map="auto",
            offload_folder="./offload",
            cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None
        )
    except Exception as e:
      print(str(e))
      return None
    return pipe_stable_lm

def stable_lm_request(input_sentence, temperature=0.5, max_tokens=2048, top_k=0, top_p=0.9, do_sample=True):
    global pipe_stable_lm, tokenizer_stable_lm
    inputs = tokenizer_stable_lm(input_sentence, return_tensors="pt")
    inputs.to(pipe_stable_lm.device)
    tokens = pipe_stable_lm.generate(
      **inputs,
      max_new_tokens=max_tokens,
      temperature=temperature,
      top_k=top_k,
      top_p=top_p,
      do_sample=do_sample,
      pad_token_id=tokenizer_stable_lm.eos_token_id,
    )
    completion_tokens = tokens[0][inputs['input_ids'].size(1):]
    completion = tokenizer_stable_lm.decode(completion_tokens, skip_special_tokens=True)
    return completion

brainstorm_request_modes = {
    "Brainstorm":"Brainstorm visual ideas for an image prompt about ",
    "Write":"Write an interesting visual scene about ",
    "Rewrite":"Rewrite new variations of ",
    "Edit":"Edit this text to improve details and structure: ",
    "Story":"Write an interesting story with visual details and poetic subjects about ",
    "Description":"Describe in graphic detail ",
    "Picture":"Paint a picture with words about ",
    "Raw Request":"",
}

def run_prompt_brainstormer(page):
    import random as rnd
    global artists, styles, brainstorm_request_modes
    global pipe_stable_lm, tokenizer_stable_lm
    textsynth_engine = "gptj_6B" #param ["gptj_6B", "boris_6B", "fairseq_gpt_13B", "gptneox_20B", "m2m100_1_2B"]
    #markdown HuggingFace Bloom AI Settings
    max_tokens_length = 128 #param {type:'slider', min:1, max:64, step:1}
    seed = int(2222 * prefs['prompt_brainstormer']['AI_temperature']) #param {type:'integer'}
    API_URL = "https://api-inference.huggingface.co/models/bigscience/bloom"
    engine = prefs['prompt_brainstormer']['AI_engine']
    good_key = True
    if 'TextSynth' in engine:
      try:
        if not bool(prefs['TextSynth_api_key']): good_key = False
      except NameError: good_key = False
      if not good_key:
        alert_msg(page, f"Missing TextSynth api key... Define your key in Settings.")
        return
      else:
        try:
          from textsynthpy import TextSynth, Complete
        except ImportError:
          run_sp("pip install textsynthpy", realtime=False)
          #clear_output()
        finally:
          from textsynthpy import TextSynth, Complete
        model = prefs['prompt_brainstormer']['TextSynth_model']
        textsynth_engine = "gptj_6B" if 'GPT-J' in model else "mistral_7B_instruct" if 'Mistral Instruct' in model else "mistral_7B" if 'Mistral' in model else "mixtral_47B_instruct" if 'Mixtral Instruct' in model else "llama2_7B" if 'Llama2 7B' in model else "llama2_70B" if 'Llama2 70B' in model else "mistral_7B"
        #textsynth_engine = "gptj_6B" if 'GPT-J' in engine else "mistral_7B_instruct" if 'Mistral Instruct' in engine else "mistral_7B" if 'Mistral' in engine else "mixtral_47B_instruct" if 'Mixtral Instruct' in engine else "llama2_7B" if 'Llama2 7B' in engine else "llama2_70B" if 'Llama2 70B' in engine else "mistral_7B"
        textsynth = TextSynth(prefs['TextSynth_api_key'], engine=textsynth_engine) # Insert your API key in the previous cell
    elif 'GPT' in engine:
      try:
        if not bool(prefs['OpenAI_api_key']): good_key = False
      except NameError: good_key = False
      if not good_key:
        alert_msg(page, f"Missing OpenAI_api_key... Define your key in Settings.")
        return
      else:
        try:
          import openai
        except ModuleNotFoundError:
          run_sp("pip install --upgrade openai -qq", realtime=False)
          #clear_output()
          pass
        finally:
          import openai
        try:
          from openai import OpenAI
          openai_client = OpenAI(api_key=prefs['OpenAI_api_key'])
        except:
          alert_msg(page, "Invalid OpenAI API Key. Change in Settings...")
          return
    elif prefs['prompt_brainstormer']['AI_engine'] == "HuggingFace Bloom 176B" or prefs['prompt_brainstormer']['AI_engine'] == "HuggingFace Flan-T5 XXL":
      try:
        if not bool(prefs['HuggingFace_api_key']): good_key = False
      except NameError: good_key = False
      if not good_key:
        alert_msg(page, f"Missing HuggingFace_api_key... Define your key in Settings.")
        return
    #ask_OpenAI_instead = False #@param {type:'boolean'}
    elif prefs['prompt_brainstormer']['AI_engine'].startswith("Google Gemini"):
      if not bool(prefs['PaLM_api_key']):
        alert_msg(page, "You must provide your Google Gemini MakerSuite API key in Settings first")
        return
      try:
        import google.generativeai as genai
        if force_update("generativeai"): raise ModuleNotFoundError("Forcing update")
      except:
        page.prompt_brainstormer_list.controls.append(Installing("Installing Google MakerSuite Library..."))
        page.prompt_brainstormer_list.update()
        run_sp("pip install --upgrade google-generativeai", realtime=False)
        import google.generativeai as genai
        del page.prompt_brainstormer_list.controls[-1]
        page.prompt_brainstormer_list.update()
        pass
      try:
        genai.configure(api_key=prefs['PaLM_api_key'])
      except:
        alert_msg(page, "Invalid Google Gemini API Key. Change in Settings...")
        return
      gemini_model_name = 'gemini-1.5-flash-latest' if '1.5 Flash' in prefs['prompt_brainstormer']['AI_engine'] else 'gemini-1.5-pro-latest' if '1.5 Pro' in prefs['prompt_brainstormer']['AI_engine'] else 'gemini-1.0-pro-latest'
      gemini_model = genai.GenerativeModel(model_name='gemini-pro')
    if "Anthropic Claude 3" in prefs['prompt_brainstormer']['AI_engine']:
      if not bool(prefs['Anthropic_api_key']):
        alert_msg(page, "You must provide your Anthropic.ai Claude API key in Settings first")
        return
      try:
        import anthropic
      except:
        page.prompt_brainstormer_list.controls.append(Installing("Installing Anthropic.ai Claude SDK Library..."))
        page.prompt_brainstormer_list.update()
        run_sp("pip install --upgrade anthropic", realtime=False)
        import anthropic
        del page.prompt_brainstormer_list.controls[-1]
        page.prompt_brainstormer_list.update()
        pass
      os.environ["ANTHROPIC_API_KEY"] = prefs['Anthropic_api_key']
      try:
        anthropic_client = anthropic.Anthropic(api_key=prefs['Anthropic_api_key'])
      except:
        alert_msg(page, "Invalid Anthropic API Authentication. Change in Settings...")
        return
    prompt_request_modes = [
        "visually detailed wording, flowing sentences, extra long descriptions",
        "that is similar but with more details, themes, imagination, interest, subjects, artistic style, poetry, tone, settings, adjectives, visualizations",
        "that is completely rewritten, inspired by, paints a complete picture of an artistic seen",
        "with detailed colorful interesting artistic scenic visual descriptions, described to a blind person",
        "that is highly detailed, artistically interesting, describes a scene, colorful poetic language, with intricate visual descriptions",
        "that replaces every noun, adjective, verb, pronoun, with related words",
        "that is strange, descriptive, graphically visual, full of interesting subjects described in great detail, painted by an artist",
        "that is highly technical, extremely wordy, extra detailed, confusingly tangental, colorfully worded, dramatically narrative",
        "that is creative, imaginative, funny, interesting, scenic, dark, witty, visual, unexpected, wild",
        "that includes more subjects with descriptions, textured color details, expressive",]
        #"complete sentence using many words to describe a landscape in an epic fantasy genre that includes a lot adjectives",

    request = f'{brainstorm_request_modes[prefs["prompt_brainstormer"]["request_mode"]]}"{prefs["prompt_brainstormer"]["about_prompt"]}":' if prefs['prompt_brainstormer']['request_mode'] != "Raw Request" else prefs['prompt_brainstormer']['about_prompt']

    def query(payload):
        #print(payload)
        response = requests.request("POST", API_URL, json=payload, headers={"Authorization": f"Bearer {prefs['HuggingFace_api_key']}"})
        #print(response.text)
        return json.loads(response.content.decode("utf-8"))

    def bloom_request(input_sentence):
        parameters = {
            "max_new_tokens": max_tokens_length,
            "do_sample": False,
            "seed": seed,
            "early_stopping": False,
            "length_penalty": 0.0,
            "eos_token_id": None,}
        payload = {"inputs": input_sentence, "parameters": parameters,"options" : {"use_cache": False} }
        data = query(payload)
        if "error" in data:
            return f"\33[31mERROR: {data['error']}\33[0m"

        generation = data[0]["generated_text"].split(input_sentence, 1)[1]
        #return data[0]["generated_text"]
        return generation

    def flan_query(payload):
        #print(payload)
        response = requests.request("POST", "https://api-inference.huggingface.co/models/google/flan-t5-xxl", json=payload, headers={"Authorization": f"Bearer {prefs['HuggingFace_api_key']}"})
        #print(response.text)
        return json.loads(response.content.decode("utf-8"))

    def flan_request(input_sentence):
        parameters = {
            "max_new_tokens": max_tokens_length,
            "do_sample": False,
            "seed": seed,
            "early_stopping": False,
            "length_penalty": 0.0,
            "eos_token_id": None,}
        payload = {"inputs": input_sentence, "parameters": parameters,"options" : {"use_cache": False} }
        data = flan_query(payload)
        if "error" in data:
            return f"\33[31mERROR: {data['error']}\33[0m"

        generation = data[0]["generated_text"].split(input_sentence, 1)[1]
        #return data[0]["generated_text"]
        return generation

    def stable_lm_request(input_sentence):
        global pipe_stable_lm, tokenizer_stable_lm
        inputs = tokenizer_stable_lm(input_sentence, return_tensors="pt")
        inputs.to(pipe_stable_lm.device)
        tokens = pipe_stable_lm.generate(
          **inputs,
          max_new_tokens=2048,
          temperature=prefs['prompt_brainstormer']['AI_temperature'],
          top_k=0,
          top_p=0.9,
          do_sample=True,
          pad_token_id=tokenizer_stable_lm.eos_token_id,
        )
        completion_tokens = tokens[0][inputs['input_ids'].size(1):]
        completion = tokenizer_stable_lm.decode(completion_tokens, skip_special_tokens=True)
        return completion

    def prompt_brainstormer():
      global pipe_stable_lm, tokenizer_stable_lm
      #(prompt=prompt, temperature=AI_temperature, presence_penalty=1, stop= "\n")
      page.prompt_brainstormer_list.controls.append(Installing("Storming the AI's Brain..."))
      page.prompt_brainstormer_list.update()

      if "TextSynth" in prefs['prompt_brainstormer']['AI_engine']:
        response = textsynth.text_complete(prompt=request, max_tokens=200, temperature=prefs['prompt_brainstormer']['AI_temperature'], presence_penalty=1)
        #print(str(response))
        result = response.text.strip()
      elif prefs['prompt_brainstormer']['AI_engine'] == "OpenAI GPT-3":
        response = openai_client.completions.create(engine="text-davinci-003", prompt=request, max_tokens=2400, temperature=prefs['prompt_brainstormer']['AI_temperature'], presence_penalty=1)
        result = response.choices[0].text.strip()
      elif prefs['prompt_brainstormer']['AI_engine'] == "ChatGPT-3.5 Turbo":
        response = openai_client.chat.completions.create(model="gpt-3.5-turbo-0125", temperature=prefs['prompt_brainstormer']['AI_temperature'], messages=[{"role": "user", "content": request}])
        result = response.choices[0].message.content.strip()
      elif "GPT" in prefs['prompt_brainstormer']['AI_engine']:
        gpt_model = OpenAI_models[prefs['prompt_brainstormer']['OpenAI_model']]#"gpt-4-turbo" if "Turbo" in prefs['prompt_brainstormer']['AI_engine'] else "gpt-4o" if "4o" in prefs['prompt_brainstormer']['AI_engine'] else "gpt-4"
        response = openai_client.chat.completions.create(model=gpt_model, temperature=prefs['prompt_brainstormer']['AI_temperature'], messages=[{"role": "user", "content": request}])
        result = response.choices[0].message.content.strip()
      elif prefs['prompt_brainstormer']['AI_engine'] == "HuggingFace Bloom 176B":
        result = bloom_request(request)
      elif prefs['prompt_brainstormer']['AI_engine'] == "HuggingFace Flan-T5":
        result = flan_request(request)
      elif prefs['prompt_brainstormer']['AI_engine'].startswith("Stable"):
        if pipe_stable_lm != None and tokenizer_stable_lm != None:
          page.add_to_prompt_brainstormer(Installing("Installing StableLM-Alpha Pipeline..."))
          pipe_stable_lm = get_stable_lm(prefs['prompt_brainstormer']['AI_engine'])
          del page.prompt_brainstormer_list.controls[-1]
          page.prompt_brainstormer_list.update()
        result = stable_lm_request(request, temperature=prefs['prompt_brainstormer']['AI_temperature'])
      elif prefs['prompt_brainstormer']['AI_engine'].startswith("Google Gemini"):
        completion = gemini_model.generate_content(request, generation_config={
            'temperature': prefs['prompt_brainstormer']['AI_temperature'],
            'max_output_tokens': 1024
        })
        #completion = palm.generate_text(model='models/text-bison-001', prompt=request, temperature=prefs['prompt_brainstormer']['AI_temperature'], max_output_tokens=1024)
        result = completion.text.strip()
      elif "Anthropic Claude 3" in prefs['prompt_brainstormer']['AI_engine']:
        model = "claude-3-sonnet-20240229" if prefs['prompt_brainstormer']['AI_engine'] == "Anthropic Claude 3" else "claude-3-5-sonnet-20240620"
        try:
          response = anthropic_client.messages.create(
            model=model,#"claude-3-sonnet-20240229",#"claude-3-opus-20240229",
            max_tokens=4000,
            temperature=prefs['prompt_brainstormer']['AI_temperature'],
            #system=f"Respond with an unordered list of image generation prompts in the amount specified with each line starting with an * asterisk, {generator_request_modes[int(prefs['prompt_generator']['request_mode'])]}, and unique without repetition",
            messages=[{"role": "user", "content": request}])
        except Exception as e:
          alert_msg(page, "ERROR Running Anthropic API Request...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
          del page.prompt_brainstormer_list.controls[-1]
          page.prompt_brainstormer_list.update()
          return
        result = response.content[0].text.strip()
      elif prefs['prompt_brainstormer']['AI_engine'] == "AI-Horde":
        del page.prompt_brainstormer_list.controls[-1]
        page.prompt_brainstormer_list.update()
        result = get_horde_text(page, page.prompt_brainstormer_list, request, prefs['prompt_brainstormer']['AIHorde_model'], prefs['prompt_brainstormer']['AI_temperature'])
      elif prefs['prompt_brainstormer']['AI_engine'] == "Perplexity":
        del page.prompt_brainstormer_list.controls[-1]
        page.prompt_brainstormer_list.update()
        result = get_perplexity_text(page, page.prompt_brainstormer_list, request, prefs['prompt_brainstormer']['Perplexity_model'], prefs['prompt_brainstormer']['AI_temperature'])

      if len(page.prompt_brainstormer_list.controls) > 1:
        del page.prompt_brainstormer_list.controls[-1]
      page.prompt_brainstormer_list.update()
      if '*' in result:
        result = result.replace('*', '').strip()
      page.add_to_prompt_brainstormer(str(result.strip()) + '\n')
    #print(f"Remixing {seed_prompt}" + (f", about {optional_about_influencer}" if bool(optional_about_influencer) else ""))
    page.brainstormer.auto_scroll = True
    if good_key:
      #print(request)
      prompt_brainstormer()
    nudge(page.brainstormer, page)
    page.brainstormer.auto_scroll = False

def run_prompt_writer(page):
    '''try:
        import nsp_pantry
        from nsp_pantry import nsp_parse
    except ModuleNotFoundError:
        run_sp("wget -qq --show-progress --no-cache --backups=1 https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.py")
        #print(subprocess.run(['wget', '-q', '--show-progress', '--no-cache', '--backups=1', 'https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.py'], stdout=subprocess.PIPE).stdout.decode('utf-8'))
    finally:
        import nsp_pantry
        from nsp_pantry import nsp_parse'''
    import random as rnd
    global artists, styles
    def generate_prompt():
      text_prompts = []
      global art_Subjects, by_Artists, art_Styles
      nsSubjects = nsp_parse(prefs['prompt_writer']['art_Subjects'])
      nsArtists = nsp_parse(prefs['prompt_writer']['by_Artists'])
      nsStyles = nsp_parse(prefs['prompt_writer']['art_Styles'])
      prompt = nsSubjects
      random_artist=[]
      if nsArtists: random_artist.append(nsArtists)
      for a in range(prefs['prompt_writer']['random_artists']):
        random_artist.append(rnd.choice(artists))
      artist = and_list(random_artist)
      #artist = random.choice(artists) + " and " + random.choice(artists)
      random_style = []
      if prefs['prompt_writer']['art_Styles']: random_style.append(nsStyles)
      for s in range(prefs['prompt_writer']['random_styles']):
        random_style.append(rnd.choice(styles))
      style = ", ".join(random_style)
      subject_prompt = prompt
      if len(artist) > 0: prompt += f", by {artist}"
      if len(style) > 0: prompt += f", style of {style}"
      if not prefs['prompt_writer']['permutate_artists']:
        return prompt
      if prefs['prompt_writer']['random_styles'] > 0 and prefs['prompt_writer']['permutate_artists']:
        text_prompts.append(prompt)
      if prefs['prompt_writer']['permutate_artists']:
        for a in list_variations(random_artist):
          prompt_variation = subject_prompt + f", by {and_list(a)}"
          text_prompts.append(prompt_variation)
        if prefs['prompt_writer']['random_styles'] > 0:
          text_prompts.append(subject_prompt + f", style of {style}")
        return text_prompts
      #if mod_Custom and mod_Custom.strip(): prompt += mod_Custom)
      #return prompt
    page.writer.auto_scroll = True
    prompts_writer = []
    for p in range(prefs['prompt_writer']['amount']):
      prompts_writer.append(generate_prompt())
    for item in prompts_writer:
      if type(item) is str:
        page.add_to_prompt_writer(item)
      if type(item) is list:
        for i in item:
          page.add_to_prompt_writer(i)
    nudge(page.writer, page)
    page.writer.auto_scroll = False

def run_magic_prompt(page):
    #import random as rnd
    global artists, styles, magic_prompt_prefs, pipe_gpt2
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.magic_prompt_output.controls.append(line)
      page.magic_prompt_output.update()
    def clear_last(lines=1):
        clear_line(page.magic_prompt_output, lines=lines)
    progress = ProgressBar(bar_height=8)
    prt(Installing("Installing Magic Prompt GPT-2 Pipeline..."))
    try:
        import jinja2
    except:
        run_sp("pip install -q jinja2==3.1.3")
        pass
    try:
        from transformers import pipeline, set_seed
    except:
        run_sp("pip install -qq --upgrade git+https://github.com/huggingface/transformers.git")
        from transformers import pipeline, set_seed
        pass
    try:
        import sentencepiece
    except:
        run_sp("pip install -q sentencepiece")
        pass
    ideas = os.path.join(root_dir, "ideas.txt")
    if not os.path.exists(ideas):
        download_file("https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion/raw/main/ideas.txt")
    prompts_magic = []
    prompt_results = []
    if '_' in magic_prompt_prefs['seed_prompt']:
        seed_prompt = nsp_parse(magic_prompt_prefs['seed_prompt'])
    else:
        seed_prompt = magic_prompt_prefs['seed_prompt']
    clear_pipes("gpt2")
    if pipe_gpt2 == None:
        try:
            pipe_gpt2 = pipeline('text-generation', model='Gustavosta/MagicPrompt-Stable-Diffusion', tokenizer='gpt2')
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Initializing GPT-2 Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
            return
    with open(ideas, "r") as f:
        line = f.readlines()
    clear_last()
    prt("Generating Magic Prompts from your Text Input...")
    prt(progress)

    def generate(starting_text):
        random_seed = get_seed(magic_prompt_prefs['seed'], 100, 1000000)
        set_seed(random_seed)
        if starting_text == "":
            starting_text: str = line[rnd.randrange(0, len(line))].replace("\n", "").lower().capitalize()
            starting_text: str = re.sub(r"[,:\-–.!;?_]", '', starting_text)
        response = pipe_gpt2(starting_text, max_length=(len(starting_text) + rnd.randint(60, 90)), num_return_sequences=int(magic_prompt_prefs['amount']))
        response_list = []
        for x in response:
            resp = x['generated_text'].strip()
            if resp != starting_text and len(resp) > (len(starting_text) + 4) and resp.endswith((":", "-", "—")) is False:
                response_list.append(resp)
        response_end = "\n".join(response_list)
        response_end = re.sub(r"[^ ]+\.[^ ]+", "", response_end)
        response_end = response_end.replace("<", "").replace(">", "")
        if response_end != "":
            return response_end.split("\n")
        else:
            prt("Error Generating Magic Prompt Responses...")
            return []

    #txt = grad.Textbox(lines=1, label="Initial Text", placeholder="English Text here")
    #out = grad.Textbox(lines=4, label="Generated Prompts")
    #examples = []
    #for x in range(8):
    #    examples.append(line[rnd.randrange(0, len(line))].replace("\n", "").lower().capitalize())
    #title = "Stable Diffusion Prompt Generator"
    #description = 'This is a demo of the model series: "MagicPrompt", in this case, aimed at: "Stable Diffusion". To use it, simply submit your text or click on one of the examples. To learn more about the model, [click here](https://huggingface.co/Gustavosta/MagicPrompt-Stable-Diffusion).<br>'
    prompt_results = generate(seed_prompt)
    clear_last()
    clear_last()
    for p in prompt_results:
        random_artist=[]
        for a in range(magic_prompt_prefs['random_artists']):
            random_artist.append(rnd.choice(artists))
        #print(list_variations(random_artist))
        artist = " and ".join([", ".join(random_artist[:-1]),random_artist[-1]] if len(random_artist) > 2 else random_artist)
        random_style = []
        for s in range(magic_prompt_prefs['random_styles']):
            random_style.append(rnd.choice(styles))
        style = ", ".join(random_style)
        text_prompt = p
        prompts_magic.append(text_prompt)
        if magic_prompt_prefs['random_artists'] > 0: text_prompt += f", by {artist}"
        if magic_prompt_prefs['random_styles'] > 0: text_prompt += f", style of {style}"
        #if magic_prompt_prefs['random_styles'] == 0 and magic_prompt_prefs['permutate_artists']:
        #    prompts_magic.append(text_prompt)
        if magic_prompt_prefs['permutate_artists']:
            for a in list_variations(random_artist):
                prompt_variation = p + f", by {and_list(a)}"
                prompts_magic.append(prompt_variation)
            if magic_prompt_prefs['random_styles'] > 0:
                prompts_magic.append(p + f", style of {style}")
        else: prompts_magic.append(text_prompt)
    page.MagicPrompt.auto_scroll = False
    for item in prompts_magic:
        page.add_to_magic_prompt(item)
    nudge(page.MagicPrompt, page)
    page.MagicPrompt.auto_scroll = False
    play_snd(Snd.ALERT, page)

def run_distil_gpt2(page):
    #import random as rnd
    global artists, styles, distil_gpt2_prefs, pipe_distil_gpt2
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.distil_gpt2_output.controls.append(line)
      page.distil_gpt2_output.update()
    def clear_last(lines=1):
      clear_line(page.distil_gpt2_output, lines=lines)
    progress = ProgressBar(bar_height=8)
    installer = Installing("Installing Distil GPT-2 Pipeline...")
    prt(installer)
    try:
        from transformers import GPT2Tokenizer, GPT2LMHeadModel, set_seed
    except:
        installer.status("...installing transformers")
        run_sp("pip install -qq --upgrade git+https://github.com/huggingface/transformers.git")
        from transformers import GPT2Tokenizer, GPT2LMHeadModel, set_seed
        pass
    pip_install("tokenizers", installer=installer)
    prompts_distil = []
    prompt_results = []
    if '_' in distil_gpt2_prefs['seed_prompt']:
        seed_prompt = nsp_parse(distil_gpt2_prefs['seed_prompt'])
    else:
        seed_prompt = distil_gpt2_prefs['seed_prompt']
    clear_pipes("distil_gpt2")
    installer.status("...GPT2Tokenizer distilgpt2")
    tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    if pipe_distil_gpt2 == None:
        try:
            installer.status("...FredZhang7/distilgpt2-stable-diffusion-v2")
            pipe_distil_gpt2 = GPT2LMHeadModel.from_pretrained('FredZhang7/distilgpt2-stable-diffusion-v2')
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Initializing Distil GPT-2 Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]), debug_pref=distil_gpt2_prefs)
            return
    clear_last()
    prt("Generating Distil GPT-2 Results from your Text Input...")
    prt(progress)
    def generate(starting_text):
        random_seed = get_seed(distil_gpt2_prefs['seed'], 100, 1000000)
        set_seed(random_seed)
        input_ids = tokenizer(starting_text, return_tensors='pt').input_ids
        output = pipe_distil_gpt2.generate(input_ids, do_sample=True, temperature=distil_gpt2_prefs['AI_temperature'], top_k=distil_gpt2_prefs['top_k'], max_new_tokens=distil_gpt2_prefs['max_length'], num_return_sequences=distil_gpt2_prefs['amount'], repetition_penalty=distil_gpt2_prefs['repetition_penalty'], penalty_alpha=distil_gpt2_prefs['penalty_alpha'], no_repeat_ngram_size=distil_gpt2_prefs['no_repeat_ngram_size'], early_stopping=True)
        results = []
        for i in range(len(output)):
            results.append(tokenizer.decode(output[i], skip_special_tokens=True))
        return results
    prompt_results = generate(seed_prompt)
    clear_last()
    clear_last()
    for p in prompt_results:
        random_artist=[]
        for a in range(distil_gpt2_prefs['random_artists']):
            random_artist.append(rnd.choice(artists))
        #print(list_variations(random_artist))
        artist = " and ".join([", ".join(random_artist[:-1]),random_artist[-1]] if len(random_artist) > 2 else random_artist)
        random_style = []
        for s in range(distil_gpt2_prefs['random_styles']):
            random_style.append(rnd.choice(styles))
        style = ", ".join(random_style)
        text_prompt = p
        prompts_distil.append(text_prompt)
        if distil_gpt2_prefs['random_artists'] > 0: text_prompt += f", by {artist}"
        if distil_gpt2_prefs['random_styles'] > 0: text_prompt += f", style of {style}"
        #if distil_gpt2_prefs['random_styles'] == 0 and distil_gpt2_prefs['permutate_artists']:
        #    prompts_distil.append(text_prompt)
        if distil_gpt2_prefs['permutate_artists']:
            for a in list_variations(random_artist):
                prompt_variation = p + f", by {and_list(a)}"
                prompts_distil.append(prompt_variation)
            if distil_gpt2_prefs['random_styles'] > 0:
                prompts_distil.append(p + f", style of {style}")
        else: prompts_distil.append(text_prompt)
    for item in prompts_distil:
        page.add_to_distil_gpt2(item)
    play_snd(Snd.ALERT, page)

def run_superprompt(page):
    global artists, styles, superprompt_prefs, pipe_superprompt, tokenizer_superprompt
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.superprompt_output.controls.append(line)
      page.superprompt_output.update()
    def clear_last(lines=1):
      clear_line(page.superprompt_output, lines=lines)
    progress = ProgressBar(bar_height=8)
    installer = Installing("Installing SuperPrompt Pipeline...")
    prt(installer)
    try:
        from transformers import T5Tokenizer, T5ForConditionalGeneration, set_seed
    except:
        installer.status("...installing Transformers")
        run_sp("pip install -qq --upgrade git+https://github.com/huggingface/transformers.git")
        from transformers import T5Tokenizer, T5ForConditionalGeneration, set_seed
        pass
    pip_install("sentencepiece", installer=installer)
    prompts_superprompt = []
    prompt_results = []
    if '_' in superprompt_prefs['seed_prompt']:
        seed_prompt = nsp_parse(superprompt_prefs['seed_prompt'])
    else:
        seed_prompt = superprompt_prefs['seed_prompt']
    clear_pipes("superprompt")
    if pipe_superprompt == None:
        try:
            installer.status("...loading Google Flan-T5 Tokenizer")
            tokenizer_superprompt = T5Tokenizer.from_pretrained("google/flan-t5-small")
            installer.status("...loading roborovski/superprompt-v1 model")
            pipe_superprompt = T5ForConditionalGeneration.from_pretrained("roborovski/superprompt-v1", torch_dtype=torch.float16)
            #pipe_superprompt = GPT2LMHeadModel.from_pretrained('FredZhang7/superpromptgpt2-stable-diffusion-v2')
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Initializing SuperPrompt Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]), debug_pref=superprompt_prefs)
            return
    clear_last()
    prt("Generating SuperPrompt Results from your Text Input...")
    prt(progress)

    def generate(starting_text):
        random_seed = get_seed(superprompt_prefs['seed'], 10, 1000000)
        set_seed(random_seed)
        #input_ids = tokenizer(starting_text, return_tensors='pt').input_ids
        #output = pipe_superprompt.generate(input_ids, do_sample=True, temperature=superprompt_prefs['AI_temperature'], top_k=superprompt_prefs['top_k'], max_new_tokens=superprompt_prefs['max_new_tokens'], num_return_sequences=superprompt_prefs['amount'], repetition_penalty=superprompt_prefs['repetition_penalty'], penalty_alpha=superprompt_prefs['penalty_alpha'], no_repeat_ngram_size=superprompt_prefs['no_repeat_ngram_size'], early_stopping=True)
        input_ids = tokenizer_superprompt(seed_prompt, return_tensors="pt").input_ids.to(torch_device)
        outputs = pipe_superprompt.generate(input_ids, max_new_tokens=superprompt_prefs['max_new_tokens'], num_return_sequences=superprompt_prefs['amount'], repetition_penalty=superprompt_prefs['repetition_penalty'], do_sample=True, temperature=superprompt_prefs['AI_temperature'], top_p=superprompt_prefs['top_p'], top_k=superprompt_prefs['top_k'])
        #better_prompt = tokenizer.decode(outputs[0])
        results = []
        for o in outputs:
            results.append(tokenizer_superprompt.decode(o, skip_special_tokens=True))
        return results
    prompt_results = generate(seed_prompt)
    clear_last(2)
    for p in prompt_results:
        random_artist=[]
        for a in range(superprompt_prefs['random_artists']):
            random_artist.append(rnd.choice(artists))
        #print(list_variations(random_artist))
        artist = " and ".join([", ".join(random_artist[:-1]),random_artist[-1]] if len(random_artist) > 2 else random_artist)
        random_style = []
        for s in range(superprompt_prefs['random_styles']):
            random_style.append(rnd.choice(styles))
        style = ", ".join(random_style)
        text_prompt = p
        prompts_superprompt.append(text_prompt)
        if superprompt_prefs['random_artists'] > 0: text_prompt += f", by {artist}"
        if superprompt_prefs['random_styles'] > 0: text_prompt += f", style of {style}"
        #if superprompt_prefs['random_styles'] == 0 and superprompt_prefs['permutate_artists']:
        #    prompts_superprompt.append(text_prompt)
        if superprompt_prefs['permutate_artists']:
            for a in list_variations(random_artist):
                prompt_variation = p + f", by {and_list(a)}"
                prompts_superprompt.append(prompt_variation)
            if superprompt_prefs['random_styles'] > 0:
                prompts_superprompt.append(p + f", style of {style}")
        else: prompts_superprompt.append(text_prompt)
    for item in prompts_superprompt:
        page.add_to_superprompt(item)
    play_snd(Snd.ALERT, page)

def run_upscaling(page):
    from collections import Counter
    enlarge_scale = ESRGAN_prefs['enlarge_scale']
    face_enhance = ESRGAN_prefs['face_enhance']
    image_path = ESRGAN_prefs['image_path']
    save_to_GDrive = ESRGAN_prefs['save_to_GDrive']
    upload_file = ESRGAN_prefs['upload_file']
    download_locally = ESRGAN_prefs['download_locally']
    upscale_model = ESRGAN_prefs['upscale_model']
    display_image = ESRGAN_prefs['display_image']
    dst_image_path = ESRGAN_prefs['dst_image_path']
    filename_suffix = ESRGAN_prefs['filename_suffix']
    split_image_grid = ESRGAN_prefs['split_image_grid']
    rows = ESRGAN_prefs['rows']
    cols = ESRGAN_prefs['cols']
    ESRGAN_folder = os.path.join(dist_dir, 'Real-ESRGAN')
    installer = Installing("Installing ESRGAN Upscale model...")
    page.add_to_ESRGAN_output(installer)
    if not status['installed_ESRGAN']:
        get_ESRGAN(None, model=upscale_model, installer=installer)
    model_file = os.path.join(ESRGAN_folder, 'experiments', 'pretrained_models', f'{model}.pth')
    if not os.path.isfile(model_file):
        installer.status(f"Downloading {model}")
        get_ESRGAN(None, model=upscale_model, installer=installer)
    def split(im, rows, cols, img_path, should_cleanup=False):
        im_width, im_height = im.size
        row_width = int(im_width / rows)
        row_height = int(im_height / cols)
        n = 0
        for i in range(0, cols):
            for j in range(0, rows):
                box = (j * row_width, i * row_height, j * row_width +
                      row_width, i * row_height + row_height)
                outp = im.crop(box)
                name, ext = os.path.splitext(img_path)
                outp_path = name + "-" + str(n) + ext
                #print("Exporting image tile: " + outp_path)
                outp.save(outp_path)
                n += 1
        if should_cleanup:
            #print("Cleaning up: " + img_path)
            os.remove(img_path)

    os.chdir(ESRGAN_folder)
    upload_folder = 'upload'
    result_folder = 'results'
    if os.path.isdir(upload_folder):
        shutil.rmtree(upload_folder)
    if os.path.isdir(result_folder):
        shutil.rmtree(result_folder)
    os.mkdir(upload_folder)
    os.mkdir(result_folder)

    uploaded = None
    if not upload_file:
      if not image_path:
         alert_msg(page, 'Provide path to image, local or url')
         return
      if '.' in image_path:
        if os.path.exists(image_path):
          uploaded = {image_path: image_path.rpartition(slash)[2]}
        else:
          alert_msg(page, 'File does not exist')
          return
      else:
        if os.path.isdir(image_path):
          uploaded = {}
          for f in os.listdir(image_path):
            uploaded[ os.path.join(image_path, f)] = f
        else:
          alert_msg(page, 'Image Path directory does not exist')
          return
    else:
      uploaded = files.upload()
    page.clear_ESRGAN_output(uploaded)
    page.add_to_ESRGAN_output(Text(f"Upscaling {len(uploaded)} images.."))
    for filename in uploaded.keys():
      if not os.path.isfile(filename):
        #print("Skipping " + filename)
        continue
      fname = filename.rpartition(slash)[2] if slash in filename else filename
      dst_path = os.path.join(upload_folder, fname)
      #print(f'Copy {filename} to {dst_path}')
      shutil.copy(filename, dst_path)
      if split_image_grid:
        img = PILImage.open(dst_path)
        split(img, rows, cols, dst_path, True)
    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
    faceenhance = ' --face_enhance' if face_enhance else ''
    run_sp(f'python inference_realesrgan.py -n {upscale_model} -i {upload_folder} --outscale {enlarge_scale}{faceenhance}', cwd=ESRGAN_folder, realtime=False)
    os.chdir(root_dir)
    if is_Colab:
      from google.colab import files
    if not bool(dst_image_path.strip()):
      if os.path.isdir(image_path):
          dst_image_path = image_path
      else:
          dst_image_path = prefs['image_output'] #os.path.dirname(image_path)
    filenames = os.listdir(os.path.join(dist_dir, 'Real-ESRGAN', 'results'))
    for fname in filenames:
      fparts = fname.rpartition('_out')
      fname_clean = fparts[0] + filename_suffix + fparts[2]
      #print(f'Copying {fname_clean}')
      #if save_to_GDrive:
      if not os.path.isdir(dst_image_path):
        os.makedirs(dst_image_path)
      shutil.copy(os.path.join(dist_dir, 'Real-ESRGAN', 'results', fname), os.path.join(dst_image_path, fname_clean))
      if download_locally:
        files.download(os.path.join(dist_dir, 'Real-ESRGAN', 'results', fname))
      if display_image:
        page.add_to_ESRGAN_output(Image(src=os.path.join(dist_dir, 'Real-ESRGAN', 'results', fname)))
      page.add_to_ESRGAN_output(Row([Text(os.path.join(dst_image_path, fname_clean))], alignment=MainAxisAlignment.CENTER))

def run_retrieve(page):
    upload_file = retrieve_prefs['upload_file']
    image_path = retrieve_prefs['image_path']
    display_full_metadata = retrieve_prefs['display_full_metadata']
    display_image = retrieve_prefs['display_image']
    add_to_prompts = retrieve_prefs['add_to_prompts']

    import json
    if is_Colab:
      from google.colab import files
    def meta_dream(meta):
      if meta is not None and len(meta) > 1:
          print(str(meta))
          arg = {}
          p = ''
          dream = '    Dream('
          if meta.get('title'):
            dream += f'"{meta["title"]}"'
            p = meta["title"]
          if meta.get('prompt'):
            dream += f'"{meta["prompt"]}"'
            p = meta["prompt"]
          if meta.get('config'):
            meta = meta['config']
          if meta.get('prompt'):
            #dream += f'"{meta["prompt"]}"'
            p = meta["prompt"]
          if meta.get('prompt2'):
            dream += f', prompt2="{meta["prompt2"]}"'
            arg["prompt2"] = meta["prompt2"]
          if meta.get('negative_prompt'):
            dream += f', negative="{meta["negative_prompt"]}"'
            arg["negative_prompt"] = meta["negative_prompt"]
          if meta.get('tweens'):
            dream += f', tweens={meta["tweens"]}'
            arg["tweens"] = meta["tweens"]
          if meta.get('width'):
            dream += f', width={meta["width"]}'
            arg["width"] = meta["width"]
          if meta.get('height'):
            dream += f', height={meta["height"]}'
            arg["height"] = meta["height"]
          if meta.get('guidance_scale'):
            dream += f', guidance_scale={meta["guidance_scale"]}'
            arg["guidance_scale"] = meta["guidance_scale"]
          elif meta.get('CGS'):
            dream += f', guidance_scale={meta["CGS"]}'
            arg["guidance_scale"] = meta["CGS"]
          if meta.get('steps'):
            dream += f', steps={meta["steps"]}'
            arg["steps"] = meta["steps"]
          if meta.get('eta'):
            dream += f', eta={meta["eta"]}'
            arg["eta"] = meta["eta"]
          if meta.get('seed'):
            dream += f', seed={meta["seed"]}'
            arg["seed"] = meta["seed"]
          if meta.get('init_image'):
            dream += f', init_image="{meta["init_image"]}"'
            arg["init_image"] = meta["init_image"]
          if meta.get('mask_image'):
            dream += f', mask_image="{meta["mask_image"]}"'
            arg["mask_image"] = meta["mask_image"]
          if meta.get('init_image_strength'):
            dream += f', init_image_strength={meta["init_image_strength"]}'
            arg["init_image_strength"] = meta["init_image_strength"]
          dream += '),'
          page.add_to_retrieve_output(Text(dream, selectable=True))
          if display_full_metadata:
            page.add_to_retrieve_output(Text(str(metadata)))
          if add_to_prompts:
            page.add_to_prompts(p, arg)
      else:
          alert_msg(page, 'Problem reading your config json image meta data.')
          return
    
    def meta_comfy(metac):
      if metac is not None and len(metac) > 1:
          #print(str(meta))
          arg = {}
          #comfyui={"3": {"inputs": {"seed": 1058652716688649, "steps": 50, "cfg": 6.0, "sampler_name": "euler_ancestral", "scheduler": "normal", "denoise": 1.0, "model": ["4", 0], "positive": ["6", 0], "negative": ["7", 0], "latent_image": ["5", 0]}, "class_type": "KSampler"}, "4": {"inputs": {"ckpt_name": "v1-5-pruned-emaonly.ckpt"}, "class_type": "CheckpointLoaderSimple"}, "5": {"inputs": {"width": 960, "height": 512, "batch_size": 4}, "class_type": "EmptyLatentImage"}, "6": {"inputs": {"text": "Large fancy Armenian Church with modern and gothic architecture in a city, high quality Thomas Kinkade, futuristic and complex", "clip": ["4", 1]}, "class_type": "CLIPTextEncode"}, "7": {"inputs": {"text": "text, watermark, russian, russia", "clip": ["4", 1]}, "class_type": "CLIPTextEncode"}, "8": {"inputs": {"samples": ["3", 0], "vae": ["4", 2]}, "class_type": "VAEDecode"}, "9": {"inputs": {"filename_prefix": "ComfyUI", "images": ["8", 0]}, "class_type": "SaveImage"}, "10": {"inputs": {"strength": 1, "noise_augmentation": 0, "conditioning": ["6", 0]}, "class_type": "unCLIPConditioning"}, "11": {"inputs": {"text": ""}, "class_type": "CLIPTextEncode"}, "15": {"inputs": {"style_model_name": null}, "class_type": "StyleModelLoader"}, "16": {"inputs": {"add_noise": "enable", "noise_seed": 262554034916863, "steps": 20, "cfg": 8, "sampler_name": "euler", "scheduler": "karras", "start_at_step": 0, "end_at_step": 10000, "return_with_leftover_noise": "disable"}, "class_type": "KSamplerAdvanced"}, "17": {"inputs": {"model_path": null}, "class_type": "DiffusersLoader"}, "18": {"inputs": {"strength": 1, "noise_augmentation": 0}, "class_type": "unCLIPConditioning"}}
          p = ''
          dream = '    Comfy('
          for k, v in metac.items():
            if 'inputs' in v:
              #print(f"{k}: {v['inputs']}")
              meta = v['inputs']
              if meta.get('add_moise'):
                continue
              if meta.get('title'):
                dream += f'"{meta["title"]}"'
                p = meta["title"]
              if meta.get('prompt'):
                dream += f'"{meta["prompt"]}"'
                p = meta["prompt"]
              if meta.get('config'):
                meta = meta['config']
              if meta.get('prompt'):
                #dream += f'"{meta["prompt"]}"'
                p = meta["prompt"]
              if meta.get('text'):
                if len(meta["text"]) > 8:
                  if not bool(p):
                    dream += f', prompt="{meta["text"]}"'
                    p = meta["text"]
                  else:
                    dream += f', negative="{meta["text"]}"'
                    arg["negative_prompt"] = meta["text"]
              if meta.get('negative_prompt'):
                dream += f', negative="{meta["negative_prompt"]}"'
                arg["negative_prompt"] = meta["negative_prompt"]
              if meta.get('width'):
                dream += f', width={meta["width"]}'
                arg["width"] = meta["width"]
              if meta.get('height'):
                dream += f', height={meta["height"]}'
                arg["height"] = meta["height"]
              if meta.get('guidance_scale'):
                dream += f', guidance_scale={meta["guidance_scale"]}'
                arg["guidance_scale"] = meta["guidance_scale"]
              elif meta.get('cfg'):
                if 'seed' not in meta:
                  continue
                dream += f', guidance_scale={meta["cfg"]}'
                arg["guidance_scale"] = meta["cfg"]
              if meta.get('steps'):
                if 'seed' not in meta:
                  continue
                dream += f', steps={meta["steps"]}'
                arg["steps"] = meta["steps"]
              if meta.get('eta'):
                dream += f', eta={meta["eta"]}'
                arg["eta"] = meta["eta"]
              if meta.get('seed'):
                dream += f', seed={meta["seed"]}'
                arg["seed"] = meta["seed"]
              if meta.get('init_image'):
                dream += f', init_image="{meta["init_image"]}"'
                arg["init_image"] = meta["init_image"]
              if meta.get('mask_image'):
                dream += f', mask_image="{meta["mask_image"]}"'
                arg["mask_image"] = meta["mask_image"]
              if meta.get('init_image_strength'):
                dream += f', init_image_strength={meta["init_image_strength"]}'
                arg["init_image_strength"] = meta["init_image_strength"]
          dream += '),'
          page.add_to_retrieve_output(Text(dream, selectable=True))
          if display_full_metadata:
            page.add_to_retrieve_output(Text(str(metadata)))
          if add_to_prompts:
            page.add_to_prompts(p, arg)
      else:
          alert_msg(page, 'Problem reading your ComfyUI json image meta data.')
          return
    #Todo: Detect Automatic1111 and convert
    def meta_auto1111(metadata):
      if metadata is not None and len(metadata) > 1:
          print(str(meta))
          arg = {}
          #https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/modules/processing.py#L445-L462
          p = ''
          if 'parameters' not in metadata: return
          print(metadata["parameters"])
          p = metadata["parameters"].split("\n")[0]
          arg["negative_prompt"] = metadata["parameters"].split("\n")[1].split(": ")[1]
          steps_index = metadata["parameters"].index("Steps:")
          steps_info = metadata["parameters"][steps_index:].split("\nTemplate")[0]
          steps_list = steps_info.split(", ")
          cfg_index = metadata["parameters"].index("CFG scale:")
          cfg_info = metadata["parameters"][cfg_index:].split("\nTemplate")[0]
          size_index = metadata["parameters"].index("Size:")
          size_info = metadata["parameters"][size_index:].split("\nTemplate")[0]
          if 'x' in size_info:
            size = size_info.split('x')
            dream += f', width={size[0]}'
            arg["width"] = int(size[0])
            dream += f', height={size[2]}'
            arg["height"] = int(size[2])
          dream += '),'
          page.add_to_retrieve_output(Text(dream, selectable=True))
          if display_full_metadata:
            page.add_to_retrieve_output(Text(str(metadata)))
          if add_to_prompts:
            page.add_to_prompts(p, arg)
    uploaded = {}
    if not upload_file:
      if not bool(image_path):
        alert_msg(page, 'Provide path to image, local or url')
        return
      if '.' in image_path:
        if os.path.exists(image_path):
          uploaded = {image_path: image_path.rpartition(slash)[2]}
        else:
          alert_msg(page, 'File does not exist')
          return
      else:
        if os.path.isdir(image_path):
          uploaded = {}
          for f in os.listdir(image_path):
            uploaded[ os.path.join(image_path, f)] = f
        else:
          alert_msg(page, 'The image_path directory does not exist')
          return
    else:
      if not is_Colab:
        uploaded = files.upload()
        alert_msg(page, "Can't upload an image easily from non-Colab systems")
        return
    if len(uploaded) > 1:
      page.add_to_retrieve_output(Text(f"Revealing Dream of {len(uploaded)} images..\n"))
    for filename in uploaded.keys():
      if not os.path.isfile(filename):
        #print("Skipping subfolder " + filename)
        continue
      #print(filename)
      if filename.rpartition('.')[2] == 'json':
        meta = json.load(filename)
        meta_dream(meta)
      elif filename.rpartition('.')[2] == 'png':
        img = PILImage.open(filename)
        metadata = img.info
        if display_image:
          page.add_to_retrieve_output(Img(src=asset_dir(filename), gapless_playback=True))
          #display(img)
        if metadata is None or len(metadata) < 1:
          alert_msg(page, 'Sorry, image has no exif data.')
          return
          #print(metadata)
        else:
          if metadata.get('config_json'):
            json_txt = metadata['config_json']
            #print(json_txt)
            meta = json.loads(json_txt)
            meta_dream(meta)
          elif metadata.get('config'):
            config = metadata['config']
            meta = {}
            key = ""
            val = ""
            if metadata.get('title'):
              meta['prompt'] = metadata['title']
            for col in config.split(':'):
              #print(col.strip())
              if ',' not in col:
                key = col
              else:
                parts = col.rpartition(',')
                val = parts[0].strip()
                if bool(key) and bool(val):
                  meta[key] = val
                  val = ''
                key = parts[2].strip()
          elif metadata.get('prompt'):
            json_txt = metadata['prompt']
            #print(json_txt)
            meta = json.loads(json_txt)
            meta_comfy(meta)
          elif metadata.get('parameters'):
            json_txt = metadata['parameters']
            #print(json_txt)
            meta = json.loads(json_txt)
            meta_auto1111(meta)
          else:
            alert_msg(page, "No Stable Diffusion Deluxe config metadata found inside image.")

def run_initfolder(page):
    prompt_string = initfolder_prefs['prompt_string']
    init_folder = initfolder_prefs['init_folder']
    include_strength = initfolder_prefs['include_strength']
    image_strength = initfolder_prefs['image_strength']
    #init_image='/content/ pic.png', init_image_strength=0.4
    if bool(prompt_string):
      p_str = f'"{prompt_string.strip()}"'
      skip_str = f', init_image_strength={image_strength}' if bool(include_strength) else ''
      if os.path.isdir(init_folder):
        arg = {}
        #print("prompts = [")
        for f in os.listdir(init_folder):
          init_path = os.path.join(init_folder, f)
          if os.path.isdir(init_path): continue
          if f.lower().endswith(('.png', '.jpg', '.jpeg')):
            page.add_to_initfolder_output(Text(f'    Dream({p_str}, init_image="{init_path}"{skip_str}),'))
            if bool(initfolder_prefs['negative_prompt']):
              arg['negative_prompt'] = initfolder_prefs['negative_prompt']
            arg['init_image'] = init_path
            if bool(include_strength):
              arg['init_image_strength'] = image_strength
            page.add_to_prompts(prompt_string, arg)
        if not bool(status['installed_img2img']):
          alert_msg(page, 'Make sure you Install the Image2Image module before running Stable Diffusion on prompts...')
       # print("]")
      else:
        alert_msg(page, 'The init_folder directory does not exist.')
    else: alert_msg(page, 'Your prompt_string is empty. What do you want to apply to images?')

def run_init_video(page):
    prompt = init_video_prefs['prompt']
    video_file = init_video_prefs['video_file']
    file_prefix = init_video_prefs['file_prefix']
    try:
        start_time = float(init_video_prefs['start_time'])
        end_time = float(init_video_prefs['end_time'])
        fps = int(init_video_prefs['fps'])
    except Exception:
        alert_msg(page, "Make sure your Numbers are actual numbers...")
        return
    max_size = init_video_prefs['max_size']
    show_images = init_video_prefs['show_images']
    output_dir = os.path.join(stable_dir, init_video_prefs['batch_folder_name'])
    if not bool(prompt):
        alert_msg(page, "Provide a good prompt to apply to All Frames in List.")
        return
    def clear_last(lines=1):
      clear_line(page.init_video_output, lines=lines)
    page.init_video_output.controls.clear()
    page.init_video_output.update()
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    if video_file.startswith("http"):
        page.add_to_init_video_output(Text("Downloading the Video File..."))
        local = download_file(video_file)
        vname = local.rpartition(slash)
        vout = os.path.join(root_dir, vname)
        shutil.move(local, vout)
        video_file = vout
        clear_last()
    if not bool(video_file):
        alert_msg(page, "Provide a valid Video File to Extract...")
        return
    else:
        if not os.path.exists(video_file):
            alert_msg(page, "The provided Video File doesn't Exist...")
            return

    '''try:
        import ffmpeg
    except Exception:
        run_process("pip install -q ffmpeg", page=page)
        import ffmpeg
        pass
    def convert_video_to_images(video_file, fps, start_time, end_time, output_dir):
        """
        Convert a video file to a sequence of images.
        """
        # Check if the output directory exists, otherwise create it.
        command = ['ffmpeg',
                '-i', video_file,
                #'-r', str(fps),
                #'-ss', str(start_time),
                #'-t', str(end_time),
                '-vf', f'fps={fps}'
                #'-vf', "select='between(t,2,6)+between(t,15,24)'"
                #'-qscale:v', '2',
                '-vsync', "0"
                '-f', 'image2',
                os.path.join(output_dir, '%08d.png')]

        run_process(command, page, print=True)
    try:
        import imageio
    except Exception:
        run_process("pip install -q imageio", page=page)
        import imageio
        pass
    def convert_video_to_images(video_file, fps, start_time, end_time, resolution):
        # create video reader object
        reader = imageio.get_reader(video_file)
        # set the reader parameters
        reader.set_fps(fps)
        reader.set_start_time(start_time)
        reader.set_end_time(end_time)
        # get list of frames
        frames = reader.get_meta_data()['nframes']
        # loop through each frame and save as png with frame number in filename
        for frame in range(frames):
            frame_img = reader.get_data(frame)
            frame_img = frame_img.resize(resolution)
            imageio.imwrite("frame_{}.png".format(frame), frame_img)
        reader.close()'''
    progress = ProgressBar(bar_height=8)
    page.add_to_init_video_output(Installing("Processing Video File..."))
    page.add_to_init_video_output(progress)
    try:
        cap = cv2.VideoCapture(video_file)
    except Exception as e:
        alert_msg(page, "ERROR Reading Video File. May be Incompatible Format...")
        clear_last()
        return
    count = 0
    files = []
    frames = []
    w = h = 0
    cap.set(cv2.CAP_PROP_FPS, fps)
    video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)
    start_frame = int(start_time * fps)
    if end_time == 0 or end_time == 0.0:
        end_frame = int(video_length)
    else:
        end_frame = int(end_time * fps)
    total = end_frame - start_frame

    #print(f"Length: {video_length}, start_frame: {start_frame}, end_frame: {end_frame}")
    for i in range(start_frame, end_frame):
        # Read frame
        cap.set(cv2.CAP_PROP_POS_FRAMES, i)
        success, image = cap.read()

        # Save frame as png
        if success:
            filename = os.path.join(output_dir, f'{file_prefix}{count}.png')
            if w == 0:
                shape = image.shape
                w, h = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)
                clear_last()
                page.add_to_init_video_output(Text(f'Extracting {len(frames)} frames at {w}x{h}, {video_length} seconds long...'))
            image = cv2.resize(image, (w, h), interpolation = cv2.INTER_AREA)
            percent = count / total
            progress.value = percent
            progress.update()
            #print(f"{count / total}% - Saving {filename} - {shape}")
            cv2.imwrite(os.path.join(output_dir, filename), image)
            files.append(filename)
            if show_images:
                page.add_to_init_video_output(Row([Img(src=asset_dir(filename), width=w, height=h, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                #page.add_to_init_video_output(Row([Text(filename)], alignment=MainAxisAlignment.CENTER))
            count += 1
    cap.release()
    if show_images:
        progress.value = 0.0
        progress.update()
    else:
        clear_last()
        clear_last()
    '''
    cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)
    end_time = min(end_time, video_length / fps)
    cap.set(cv2.CAP_PROP_FPS, fps)
    print(f"Length: {video_length}, end_time: {end_time}")

    while cap.isOpened():
        ret, frame = cap.read()
        current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0
        # Break if past end time
        if current_time > end_time and (end_time != 0.0 or end_time == 0):
            break
        frames.append(frame)
        print(f"{count} - ret:{ret}, time: {current_time}")
        count += 1
    for i, frame in enumerate(frames):
        filename = os.path.join(output_dir, f'{file_prefix}{i}.png')
        if w == 0:
            shape = frame.shape
            w, h = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)
            clear_last()
            page.add_to_init_video_output(Text(f'Extracting {len(frames)} frames at {w}x{h}, {video_length} seconds long...'))
        frame = cv2.resize(frame, (w, h), interpolation = cv2.INTER_AREA)
        cv2.imwrite(filename, frame)
        files.append(filename)
        if show_images:
            page.add_to_init_video_output(Row([Img(src=filename, width=w, height=h, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            page.add_to_init_video_output(Row([Text(filename)], alignment=MainAxisAlignment.CENTER))
    cap.release()'''
    if not bool(files):
        alert_msg(page, "ERROR Creating Images from Video...")
        return

    for f in files:
        arg = {}
        if bool(init_video_prefs['negative_prompt']):
            arg['negative_prompt'] = init_video_prefs['negative_prompt']
        arg['init_image'] = f
        arg['width'] = w
        arg['height'] = h
        if bool(init_video_prefs['include_strength']):
            arg['init_image_strength'] = init_video_prefs['image_strength']
        page.add_to_prompts(prompt, arg)
    page.add_to_init_video_output(Text(f'Added {len(files)} Files as Init-Image in Prompts List...', weight=FontWeight.BOLD))
    page.add_to_init_video_output(Text(f'Saved to {output_dir}'))
    play_snd(Snd.DROP, page)

def multiple_of_64(x):
    return multiple_of(x, 64)
def multiple_of_8(x):
    return multiple_of(x, 8)
def multiple_of(x, num):
    return int(round(x/num)*num)
def scale_dimensions(width, height, max=1024, multiple=16):
  max = int(max)
  r_width = width
  r_height = height
  if width < max and height < max:
    if width >= height:
      ratio = max / width
      r_width = max
      r_height = int(height * ratio)
    else:
      ratio = max / height
      r_height = max
      r_width = int(width * ratio)
    width = r_width
    height = r_height
  if width >= height:
    if width > max:
      r_width = max
      r_height = int(height * (max/width))
    else:
      r_width = width
      r_height = height
  else:
    if height > max:
      r_height = max
      r_width = int(width * (max/height))
    else:
      r_width = width
      r_height = height
  return multiple_of(r_width, multiple), multiple_of(r_height, multiple)

def closest_aspect_ratio(width, height):
    aspect_ratio = width / height
    aspect_ratios = ["21:9", "16:9", "3:2", "5:4", "1:1", "4:5", "2:3", "9:16", "9:21"]
    closest_ratio = None
    min_difference = float('inf')
    for ratio_str in aspect_ratios:
        ratio_width, ratio_height = map(int, ratio_str.split(':'))
        ratio_value = ratio_width / ratio_height
        difference = abs(aspect_ratio - ratio_value)
        if difference < min_difference:
            closest_ratio = ratio_str
            min_difference = difference
    return closest_ratio

def run_repainter(page):
    global repaint_prefs, prefs, status, pipe_repaint
    if not check_diffusers(page): return
    if not bool(repaint_prefs['original_image']) or not bool(repaint_prefs['mask_image']):
      alert_msg(page, "You must provide the Original Image and the Mask Image to process...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.RePainter.controls.append(line)
      page.RePainter.update()
    def clear_last(lines=1):
      clear_line(page.RePainter, lines=lines)
    def autoscroll(scroll=True):
      page.RePainter.auto_scroll = scroll
      page.RePainter.update()
    def clear_list():
      page.RePainter.controls = page.RePainter.controls[:1]
    progress = ProgressBar(bar_height=8)
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress
      total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    autoscroll(True)
    clear_list()
    prt(Installing("Installing RePaint Pipeline..."))
    import requests, random
    from io import BytesIO
    from PIL import ImageOps
    if repaint_prefs['original_image'].startswith('http'):
      #response = requests.get(repaint_prefs['original_image'])
      #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
      original_img = PILImage.open(requests.get(repaint_prefs['original_image'], stream=True).raw)
    else:
      if os.path.isfile(repaint_prefs['original_image']):
        original_img = PILImage.open(repaint_prefs['original_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your original_image {repaint_prefs['original_image']}")
        return
    width, height = original_img.size
    width, height = scale_dimensions(width, height, repaint_prefs['max_size'])
    original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
    original_img = ImageOps.exif_transpose(original_img).convert("RGB")
    mask_img = None
    if repaint_prefs['mask_image'].startswith('http'):
      #response = requests.get(repaint_prefs['mask_image'])
      #mask_img = PILImage.open(BytesIO(response.content)).convert("RGB")
      mask_img = PILImage.open(requests.get(repaint_prefs['mask_image'], stream=True).raw)
    else:
      if os.path.isfile(repaint_prefs['mask_image']):
        mask_img = PILImage.open(repaint_prefs['mask_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your mask_image {repaint_prefs['mask_image']}")
        return
    #mask_img = mask_img.convert("L")
    #mask_img = mask_img.convert("1")
    if repaint_prefs['invert_mask']:
       mask_img = ImageOps.invert(mask_img.convert('RGB'))
    mask_img = mask_img.resize((width, height), resample=PILImage.NEAREST)
    mask_img = ImageOps.exif_transpose(mask_img).convert("RGB")
    #print(f'Resize to {width}x{height}')
    clear_pipes('repaint')
    if not status['installed_repaint']:
      get_repaint(page)
      status['installed_repaint'] = True
    if pipe_repaint is None:
      pipe_repaint = get_repaint_pipe()
    clear_last()
    prt("Generating Repaint of your Image...")
    prt(progress)
    autoscroll(False)
    random_seed = int(repaint_prefs['seed']) if int(repaint_prefs['seed']) > 0 else random.randint(0,4294967295)
    generator = torch.Generator(device=torch_device).manual_seed(random_seed)
#Sizes of tensors must match except in dimension 1. Expected size 58 but got size 59 for tensor number 1 in the list.
    try:
      #from IPython.utils.capture import capture_output
      #with capture_output() as captured:
      image = pipe_repaint(image=original_img, mask_image=mask_img, num_inference_steps=repaint_prefs['num_inference_steps'], eta=repaint_prefs['eta'], jump_length=repaint_prefs['jump_length'], jump_n_sample=repaint_prefs['jump_length'], generator=generator, callback=callback_fnc, callback_steps=1).images[0]
      #print(str(captured.stdout))
    except Exception as e:
      clear_last()
      alert_msg(page, f"ERROR: Couldn't Repaint your image for some reason.  Possibly out of memory or something wrong with my code...", content=Text(str(e)))
      return
    fname = repaint_prefs['original_image'].rpartition('.')[0]
    fname = fname.rpartition(slash)[2]
    if prefs['file_suffix_seed']: fname += f"-{random_seed}"
    image_path = available_file(stable_dir, fname, 1)
    image.save(image_path)
    out_path = image_path
    clear_last()
    clear_last()
    autoscroll(True)
    save_metadata(image_path, repaint_prefs, f"Repainter", "repaint", random_seed)
    prt(Row([ImageButton(src=image_path, width=width, height=height, data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
    #prt(Row([Img(src=image_path, width=width, height=height, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
    #TODO: ESRGAN, Metadata & PyDrive
    if storage_type == "Colab Google Drive":
      new_file = available_file(prefs['image_output'], fname, 1)
      out_path = new_file
      shutil.copy(image_path, new_file)
    elif bool(prefs['image_output']):
      new_file = available_file(prefs['image_output'], fname, 1)
      out_path = new_file
      shutil.copy(image_path, new_file)
    prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_image_variation(page):
    global image_variation_prefs, pipe_image_variation
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.ImageVariation.controls.append(line)
      page.ImageVariation.update()
    def clear_last(lines=1):
      clear_line(page.ImageVariation, lines=lines)
    def autoscroll(scroll=True):
      page.ImageVariation.auto_scroll = scroll
      page.ImageVariation.update()
    def clear_list():
      page.ImageVariation.controls = page.ImageVariation.controls[:1]
    progress = ProgressBar(bar_height=8)
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress
      total_steps = image_variation_prefs['num_inference_steps']#len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    autoscroll(True)
    clear_list()
    from io import BytesIO
    from PIL import ImageOps
    from torchvision import transforms
    if image_variation_prefs['init_image'].startswith('http'):
      init_img = PILImage.open(requests.get(image_variation_prefs['init_image'], stream=True).raw)
    else:
      if os.path.isfile(image_variation_prefs['init_image']):
        init_img = PILImage.open(image_variation_prefs['init_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your init_image {image_variation_prefs['init_image']}")
        return
    width, height = init_img.size
    width, height = scale_dimensions(width, height, image_variation_prefs['max_size'], multiple=32)
    '''tform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize(
            (width, height),
            interpolation=transforms.InterpolationMode.BICUBIC,
            antialias=False,
            ),
        transforms.Normalize(
          [0.48145466, 0.4578275, 0.40821073],
          [0.26862954, 0.26130258, 0.27577711]),
    ])
    init_img = tform(init_img).to(torch_device)'''
    init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
    init_img = ImageOps.exif_transpose(init_img).convert("RGB")
    clear_pipes('image_variation')
    if pipe_image_variation == None:
        from diffusers import StableDiffusionImageVariationPipeline
        prt(Installing("Downloading Image Variation Pipeline... See console for progress."))
        model_id = "lambdalabs/sd-image-variations-diffusers"
        pipe_image_variation = StableDiffusionImageVariationPipeline.from_pretrained(model_id, revision="v2.0", safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        pipe_image_variation = pipe_image_variation.to(torch_device)
        pipe_image_variation = pipeline_scheduler(pipe_image_variation)
        #pipe_image_variation = optimize_pipe(pipe_image_variation)
        #pipe_image_variation.set_progress_bar_config(disable=True)
        clear_last()
    s = "s" if image_variation_prefs['num_images'] > 1 else ""
    prt(f"Generating Variation{s} of your Image... ")
    prt(progress)
    autoscroll(False)
    random_seed = get_seed(image_variation_prefs['seed'], 100, 1000000)
    generator = torch.Generator(device=torch_device).manual_seed(random_seed)

    try:
        images = pipe_image_variation(image=init_img, height=height, width=width, num_inference_steps=image_variation_prefs['num_inference_steps'], guidance_scale=image_variation_prefs['guidance_scale'], eta=image_variation_prefs['eta'], num_images_per_prompt=image_variation_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images
    except Exception as e:
        clear_last(2)
        alert_msg(page, "Error running pipeline", content=Text(str(e)), debug_pref=image_variation_prefs)
        return
    clear_last(2)
    autoscroll(True)
    fname = image_variation_prefs['init_image'].rpartition('.')[0]
    fname = fname.rpartition(slash)[2]
    if prefs['file_suffix_seed']: fname += f"-{random_seed}"
    for image in images:
        image_path = available_file(stable_dir, fname, 1)
        image.save(image_path)
        out_path = image_path
        #prt(Row([Img(src=image_path, width=width, height=height, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        prt(Row([ImageButton(src=image_path, width=width, height=height, data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
        #TODO: ESRGAN, Metadata & PyDrive
        if storage_type == "Colab Google Drive":
            new_file = available_file(prefs['image_output'], fname, 1)
            out_path = new_file
            shutil.copy(image_path, new_file)
        elif bool(prefs['image_output']):
            new_file = available_file(prefs['image_output'], fname, 1)
            out_path = new_file
            shutil.copy(image_path, new_file)
        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    page.ImageVariation.auto_scroll = False
    page.ImageVariation.update()
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_background_remover(page):
    global background_remover_prefs, pipe_background_remover
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.BackgroundRemover.controls.append(line)
      page.BackgroundRemover.update()
    def clear_last(lines=1):
      clear_line(page.BackgroundRemover, lines=lines)
    def autoscroll(scroll=True):
      page.BackgroundRemover.auto_scroll = scroll
      page.BackgroundRemover.update()
    def clear_list():
      page.BackgroundRemover.controls = page.BackgroundRemover.controls[:1]
    progress = ProgressBar(bar_height=8)
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress
      total_steps = background_remover_prefs['num_inference_steps']#len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    autoscroll(True)
    clear_list()
    installer = Installing("Downloading Background Remover Pipeline")
    prt(installer)
    try:
        from huggingface_hub import hf_hub_download
    except ModuleNotFoundError:
        installer.status("...HuggingFace Hub")
        run_process("pip install --upgrade huggingface_hub", page=page)
        from huggingface_hub import hf_hub_download
        pass
    try:
        import onnx
    except ImportError as e:
        installer.status("...installing onnx")
        run_sp("pip install -q onnx==1.16.1", realtime=False)#1.14.0
        pass
    try:
        import onnxruntime
    except ImportError as e:
        installer.status("...installing onnxruntime")
        run_sp("pip install -q onnxruntime-gpu==1.15.0", realtime=False)
        pass
    if pipe_background_remover == None:
        installer.status("...downloading model")
        pipe_background_remover = hf_hub_download('nateraw/background-remover-files', 'modnet.onnx', repo_type='dataset')
    from io import BytesIO
    from PIL import ImageOps
    import numpy as np
    import onnxruntime
    import requests
    def get_scale_factor(im_h, im_w, ref_size=512):
        if max(im_h, im_w) < ref_size or min(im_h, im_w) > ref_size:
            if im_w >= im_h:
                im_rh = ref_size
                im_rw = int(im_w / im_h * ref_size)
            elif im_w < im_h:
                im_rw = ref_size
                im_rh = int(im_h / im_w * ref_size)
        else:
            im_rh = im_h
            im_rw = im_w
        im_rw = im_rw - im_rw % 32
        im_rh = im_rh - im_rh % 32
        x_scale_factor = im_rw / im_w
        y_scale_factor = im_rh / im_h
        return x_scale_factor, y_scale_factor
    if background_remover_prefs['init_image'].startswith('http'):
        installer.status("...downloading image")
        init_file = background_remover_prefs['init_image'].rpartition("/")[2].rpartition(".")[0]
        init_img = PILImage.open(requests.get(background_remover_prefs['init_image'], stream=True).raw)
    else:
        if os.path.isfile(background_remover_prefs['init_image']):
            init_file = background_remover_prefs['init_image'].rpartition(slash)[2].rpartition(".")[0]
            init_img = PILImage.open(background_remover_prefs['init_image'])
        else:
            alert_msg(page, f"ERROR: Couldn't find your init_image {background_remover_prefs['init_image']}")
            return
    if bool(background_remover_prefs['output_name']):
        init_file = format_filename(background_remover_prefs['output_name'], force_underscore=True)
    batch_output = os.path.join(stable_dir, background_remover_prefs['batch_folder_name'])
    init_path = os.path.join(batch_output, f"{init_file}.jpg")
    image_path = available_file(batch_output, init_file, 1)
    mask_path = available_file(batch_output, f"{init_file}-mask.png", 1)
    mask_file = available_file(os.path.join(prefs['image_output'], background_remover_prefs['batch_folder_name']), f"{init_file}-mask.png", 1)
    if not os.path.isdir(batch_output):
        os.makedirs(batch_output)
    if not os.path.isdir(os.path.join(prefs['image_output'], background_remover_prefs['batch_folder_name'])):
        os.makedirs(os.path.join(prefs['image_output'], background_remover_prefs['batch_folder_name']))
    init_img.save(init_path)
    clear_pipes('background_remover')
    clear_last()
    prt(f"Generating Foreground of your Image...")
    prt(progress)
    autoscroll(False)
    im = cv2.imread(init_path)
    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)
    if len(im.shape) == 2:
        im = im[:, :, None]
    if im.shape[2] == 1:
        im = np.repeat(im, 3, axis=2)
    elif im.shape[2] == 4:
        im = im[:, :, 0:3]
    im = (im - 127.5) / 127.5
    im_h, im_w, im_c = im.shape
    x, y = get_scale_factor(im_h, im_w)
    im = cv2.resize(im, None, fx=x, fy=y, interpolation=cv2.INTER_AREA)
    im = np.transpose(im)
    im = np.swapaxes(im, 1, 2)
    im = np.expand_dims(im, axis=0).astype('float32')
    session = onnxruntime.InferenceSession(pipe_background_remover, None, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
    input_name = session.get_inputs()[0].name
    output_name = session.get_outputs()[0].name
    result = session.run([output_name], {input_name: im})
    matte = (np.squeeze(result[0]) * 255).astype('uint8')
    matte = cv2.resize(matte, dsize=(im_w, im_h), interpolation=cv2.INTER_AREA)
    cv2.imwrite(mask_path, matte)
    if background_remover_prefs['save_mask']:
        clear_last()
        clear_last()
        prt(Row([ImageButton(src=mask_path, width=im_w, height=im_h, data=mask_file, page=page)], alignment=MainAxisAlignment.CENTER))
    image = PILImage.open(init_path)
    matte = PILImage.open(mask_path)
    image = np.asarray(image)
    if len(image.shape) == 2:
        image = image[:, :, None]
    if image.shape[2] == 1:
        image = np.repeat(image, 3, axis=2)
    elif image.shape[2] == 4:
        image = image[:, :, 0:3]
    b, g, r = cv2.split(image)
    mask = np.asarray(matte)
    a = np.ones(mask.shape, dtype='uint8') * 255
    alpha_im = cv2.merge([b, g, r, a], 4)
    bg = np.zeros(alpha_im.shape)
    new_mask = np.stack([mask, mask, mask, mask], axis=2)
    foreground = np.where(new_mask > background_remover_prefs['threshold'], alpha_im, bg).astype(np.uint8)
    img = PILImage.fromarray(foreground)
    width, height = img.size
    if not background_remover_prefs['save_mask']:
        clear_last()
        clear_last()
    autoscroll(True)
    img.save(image_path)
    out_path = image_path
    #prt(Row([Img(src=image_path, width=width, height=height, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
    prt(Row([ImageButton(src=image_path, width=width, height=height, data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
    #TODO: ESRGAN, Metadata & PyDrive
    if storage_type == "Colab Google Drive":
        new_file = available_file(prefs['image_output'], init_file, 1)
        out_path = new_file
        shutil.copy(image_path, new_file)
        if background_remover_prefs['save_mask']:
            shutil.copy(mask_path, mask_file)
    elif bool(prefs['image_output']):
        new_file = available_file(prefs['image_output'], init_file, 1)
        out_path = new_file
        shutil.copy(image_path, new_file)
        if background_remover_prefs['save_mask']:
            shutil.copy(mask_path, mask_file)
    prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    page.BackgroundRemover.auto_scroll = False
    page.BackgroundRemover.update()
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_horde_worker_regen(page):
    global horde_worker_regen_prefs
    if not bool(prefs['AIHorde_api_key']):
        alert_msg(page, "Provide your AI-Horde API key in Settings tab... Get from aihorde.net/register")
        return
    if not bool(horde_worker_regen_prefs['dreamer_name']):
        alert_msg(page, "Provide a Name for your AI-Horde Dreamer/Worker Instance")
        return
    def prt(line):
        if type(line) == str:
            line = Text(line)
        page.reGen.controls.append(line)
        page.reGen.update()
    def clear_last(lines=1):
        clear_line(page.reGen, lines=lines)
    def autoscroll(scroll=True):
        page.reGen.auto_scroll = scroll
        page.reGen.update()
    page.reGen.controls = page.reGen.controls[:1]
    #autoscroll()
    installer = Installing("Installing Horde Worker reGen Libraries... See console for progress.")
    prt(installer)
    nudge(page.reGen, page)
    horde_worker_reGen_dir = os.path.join(root_dir, "horde-worker-reGen")
    horde_worker_regen_dir = os.path.join(horde_worker_reGen_dir, "horde_worker_regen")
    if os.path.exists(horde_worker_reGen_dir) and force_update("horde-worker"):
        installer.status("...removing old horde-worker-reGen")
        shutil.rmtree(horde_worker_reGen_dir)
    if not os.path.exists(horde_worker_reGen_dir):
        try:
            installer.status("...cloning Haidra-Org/horde-worker-reGen")
            run_sp("git clone https://github.com/Haidra-Org/horde-worker-reGen.git", cwd=root_dir, realtime=False)
            if os.path.exists(os.path.join(horde_worker_reGen_dir, "bridgeData_template.yaml")):
                os.remove(os.path.join(horde_worker_reGen_dir, "bridgeData_template.yaml"))
            installer.status("...installing Horde requirements")
            #run_sp("pip install -r requirements.txt", realtime=True) #pytorch-lightning==1.5.0
            pip_install("horde_sdk~=0.8.3 horde_safety~=0.2.3 hordelib~=2.7.6 horde_model_reference~=0.6.3 python-dotenv|dotenv ruamel.yaml semver python-Levenshtein|Levenshtein pydantic typing_extensions requests StrEnum loguru babel yaml watchdog", installer=installer, upgrade=True)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Horde Worker Requirements:", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    clear_pipes()
    os.environ['AIWORKER_CACHE_HOME'] = os.path.join(horde_worker_reGen_dir, "models")
    installer.status("...preparing yaml")
    max_res = horde_worker_regen_prefs["max_resolution"]
    max_power = 8 if max_res == "512x512" else 18 if max_res == "768x768" else 32 if max_res == "1024x1024" else 50
    models_to_skip = horde_worker_regen_prefs['models_to_skip']
    models_to_skip.append("SDXL_beta::stability.ai#6901")
    blacklist = horde_worker_regen_prefs["blacklist"]
    blacklist = [word.strip() for word in blacklist.split(',')]
    censorlist = horde_worker_regen_prefs["censorlist"]
    censorlist = [word.strip() for word in censorlist.split(',')]
    bridgeData = {
        "horde_url": "https://aihorde.net",
        "api_key": prefs["AIHorde_api_key"],
        "priority_usernames": [],
        "max_threads": horde_worker_regen_prefs['max_threads'],
        "queue_size": horde_worker_regen_prefs['queue_size'],
        "max_batch": horde_worker_regen_prefs["max_batch"],
        "safety_on_gpu": horde_worker_regen_prefs["safety_on_gpu"],
        "require_upfront_kudos": horde_worker_regen_prefs['require_upfront_kudos'],
        # "civitai_api_token": '',
        "dreamer_name": horde_worker_regen_prefs["dreamer_name"],
        "max_power": max_power,
        "blacklist": blacklist,
        "nsfw": horde_worker_regen_prefs["nsfw"],
        "censor_nsfw": horde_worker_regen_prefs["censor_nsfw"],
        "censorlist": censorlist,
        "allow_img2img": horde_worker_regen_prefs["allow_img2img"],
        "allow_painting": horde_worker_regen_prefs["allow_painting"],
        "allow_unsafe_ip": True,
        "allow_post_processing": horde_worker_regen_prefs["allow_post_processing"],
        "allow_controlnet": horde_worker_regen_prefs["allow_controlnet"],
        "allow_lora": horde_worker_regen_prefs["allow_lora"],
        "max_lora_cache_size": 10,
        "dynamic_models": False,
        "number_of_dynamic_models": 0,
        "max_models_to_download": 10,
        "stats_output_frequency": horde_worker_regen_prefs["stats_output_frequency"],
        "cache_home": "./models/",
        "temp_dir": "./tmp",
        # "always_download": True (currently unused in reGen),
        # "disable_terminal_ui": False (currently unused in reGen),
        # "vram_to_leave_free": "80%" (currently unused in reGen),
        # "ram_to_leave_free": "80%" (currently unused in reGen),
        # "disable_disk_cache": False (currently unused in reGen),
        "models_to_load": horde_worker_regen_prefs['models_to_load'],
        "models_to_skip": models_to_skip,
        # "suppress_speed_warnings": False (currently unused in reGen),
        "scribe_name": horde_worker_regen_prefs["scribe_name"],
        "kai_url": "http://localhost:5000",
        "max_length": horde_worker_regen_prefs["max_length"],
        "max_context_length": horde_worker_regen_prefs["max_context_length"],
        "branded_model": True,
        "alchemist_name": horde_worker_regen_prefs["alchemist_name"],
        "forms": ["caption", "nsfw", "interrogation", "post-process"],
    }
    import yaml, subprocess
    bridgeData_yaml_file = os.path.join(horde_worker_reGen_dir, "bridgeData.yaml")
    with open(bridgeData_yaml_file, "w") as outfile:
        yaml.dump(bridgeData, outfile, sort_keys=False)
    if not os.path.exists(bridgeData_yaml_file):
        alert_msg(page, f"Error creating json file {bridgeData_yaml_file}")
        return
    shutil.copy(bridgeData_yaml_file, os.path.join(horde_worker_regen_dir, "bridgeData.yaml"))
    os.chdir(horde_worker_reGen_dir)
    installer.status("...downloading models")
    try:
        run_sp("python convert_config_to_env.py", cwd=horde_worker_reGen_dir, realtime=False)
        console = RunConsole(show_progress=False)
        prt(console)
        console.run_process("python download_models.py", cwd=horde_worker_reGen_dir)
        #run_sp("python download_models.py", cwd=horde_worker_reGen_dir, realtime=True)
        clear_last()
    except Exception as e:
        #clear_last()
        alert_msg(page, "Error Running python download_models.py:", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        pass
    clear_last()
    #autoscroll(True)
    console_column = Column([], spacing=0, scroll=ScrollMode.AUTO, auto_scroll=True)
    def prt_console(line):
        if type(line) == str:
            line = Text(line)
        #if len(console_column.controls) > horde_worker_regen_prefs['log_lines']:
        while len(console_column.controls) >= horde_worker_regen_prefs['log_lines']:
            console_column.controls.pop(0)
        console_column.controls.append(line)
        console_column.update()
    import loguru
    from loguru import logger
    #reGen_logger = logger.bind(context="reGen")
    def display_log(message):
        level = message.record['level'].name.upper()
        if level in horde_worker_regen_prefs['log_levels']:
            try:
                color = colors.GREEN if level == "SUCCESS" else colors.CYAN if level == "INFO" else colors.BLUE if level == "WARNING" else colors.RED if level == "ERROR" else colors.PURPLE
                msg = Tooltip(message=message, content=Row([Text(f"[{level}] ", color=color), Text(message.record['message'], selectable=True)]))
                prt_console(msg)#
            except Exception as e:
                print(f"Error during logging: {e}")
    class CustomSink:
        def __init__(self):
            pass
        def write(self, message):
            display_log(message)
    custom_sink = CustomSink()
    #custom_handler = {"sink": lambda msg: display_log(msg)}
    #custom_handler = lambda message: display_log(message)
    #custom_handler = logger.sink(display_log)
    #logger.configure(handlers=[custom_handler])
    if hasattr(logger, 'sink'):  # Check if sink method exists
        custom_handler = logger.sink(custom_sink)
    else:
        custom_handler = logger.add(custom_sink)
    logger.level("INFO")
    
    log_file = os.path.join(horde_worker_reGen_dir, "logs", "bridge.log")
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    #if not os.path.exists(log_file):
    open(log_file, 'a').close()
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
    class LastLineMonitor(FileSystemEventHandler):
        def __init__(self, filename):
            self.last_line = None
            self.filename = filename
            #self.file_exists = os.path.exists(filename)
        def on_modified(self, event):
            if event.is_directory or event.src_path != self.filename:
                return
            try:
                with open(self.filename, 'r') as f:
                    lines = f.readlines()
                    if lines:
                        if self.last_line is None:
                            self.last_line = lines[-1]
                        else:
                            try:
                                new_lines = lines[lines.index(self.last_line) + 1:]
                                for line in new_lines:
                                    #print(line)
                                    level = line.split('|')[1].strip()# if ' | ' in line else ""
                                    if level.upper() in horde_worker_regen_prefs['log_levels']:
                                        color = colors.GREEN if level == "SUCCESS" else colors.CYAN if level == "INFO" else colors.BLUE if level == "WARNING" else colors.RED if level == "ERROR" else colors.TEAL if level == "DEBUG"  else colors.PURPLE
                                        txt = line.rpartition('- ')[2].strip()# if ' - ' in line else line
                                        msg = Tooltip(message=line.strip(), content=Row([Text(f"[{level}]", color=color), Text(txt, selectable=True)]))
                                        prt_console(msg)#
                                        self.last_line = lines[-1]
                                    #txt = line.rpartition(' - ')[2].strip() if ' - ' in line else line
                                    #prt_console(Markdown(f"[{level}] {txt}", tooltip=line))
                            except Exception as e:
                                print(e)
                                pass
            except FileNotFoundError as e:
                print(e)
                pass
    observer = Observer()
    event_handler = LastLineMonitor(log_file)
    try:
        observer.schedule(event_handler, log_file, recursive=False)
        observer.start()
    except Exception as e:
        print(e)
        pass
    def abort_worker(e):
        nonlocal horde_process
        page.snd_delete.play()
        horde_process.terminate()
        clear_last(2)
        prt("🛑  Terminated reGen Horde Worker Server")
        nudge(page.reGen, page)
        page.snd_error.play()
        try:
            observer.stop()
            observer.join()
        except Exception as e:
            print(e)
            pass
        autoscroll(False)
        os.chdir(root_dir)
        logger.remove(custom_handler)
        #logger.complete()
        return
    header = Header("▶️   Running reGen Horde Worker Server...", actions=[IconButton(icon=icons.CANCEL, tooltip="Abort Horde Worker Server Sharing", on_click=abort_worker)])
    prt(header)
    prt(console_column)
    nudge(page.reGen, page)
    try:
        horde_process = subprocess.Popen(['python', 'run_worker.py'], cwd=horde_worker_reGen_dir)
    except Exception as e:
        clear_last()
        alert_msg(page, "Error Running AI-Horde Worker:", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        pass
    '''try:
        observer.stop()
        observer.join()
        #logger.remove(custom_handler)
        logger.complete()
    except Exception as e:
        print(e)
        pass
    autoscroll(False)'''
    play_snd(Snd.ALERT, page)
    #os.chdir(root_dir)

loaded_blip_diffusion_task = ""
def run_blip_diffusion(page, from_list=False, with_params=False):
    global blip_diffusion_prefs, pipe_blip_diffusion, prefs, loaded_blip_diffusion_task
    if not check_diffusers(page): return
    if int(status['cpu_memory']) <= 10:
      alert_msg(page, f"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run BLIP Diffusion right now. Either Change runtime type to High-RAM mode and restart or use other BLIP Diffusion 2.1 in Extras.")
      return
    blip_diffusion_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            for x in range(blip_diffusion_prefs['num_images']):
                seed = 0 if blip_diffusion_prefs['seed'] <= 0 else blip_diffusion_prefs['seed'] + x
                blip_diffusion_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':blip_diffusion_prefs['init_image'], 'guidance_scale':blip_diffusion_prefs['guidance_scale'], 'steps':blip_diffusion_prefs['steps'], 'width':blip_diffusion_prefs['width'], 'height':blip_diffusion_prefs['height'], 'strength':blip_diffusion_prefs['strength'], 'num_images':blip_diffusion_prefs['num_images'], 'seed':seed})
        else:
            for x in range(p['num_images']):
                seed = 0 if p['seed'] <= 0 else p['seed'] + x
                blip_diffusion_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':p['init_image'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':seed})
    else:
      if not bool(blip_diffusion_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      for x in range(blip_diffusion_prefs['num_images']):
          seed = 0 if blip_diffusion_prefs['seed'] <= 0 else blip_diffusion_prefs['seed'] + x
          blip_diffusion_prompts.append({'prompt': blip_diffusion_prefs['prompt'], 'negative_prompt':blip_diffusion_prefs['negative_prompt'], 'init_image':blip_diffusion_prefs['init_image'], 'guidance_scale':blip_diffusion_prefs['guidance_scale'], 'steps':blip_diffusion_prefs['steps'], 'width':blip_diffusion_prefs['width'], 'height':blip_diffusion_prefs['height'], 'strength':blip_diffusion_prefs['strength'], 'num_images':blip_diffusion_prefs['num_images'], 'seed':seed})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.BLIPDiffusion.controls.append(line)
        if update:
          page.BLIPDiffusion.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.BLIPDiffusion, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.BLIPDiffusion.auto_scroll = scroll
        page.BLIPDiffusion.update()
      else:
        page.BLIPDiffusion.auto_scroll = scroll
        page.BLIPDiffusion.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.BLIPDiffusion.controls = page.BLIPDiffusion.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = blip_diffusion_prefs['steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing BLIP-Diffusion Engine & Model... See console for progress.")
    prt(installer)
    clear_pipes("blip_diffusion")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    cpu_offload = False
    use_controlnet = blip_diffusion_prefs['controlnet_type'] != "None"
    if use_controlnet:
        try:
          from controlnet_aux import CannyDetector
        except ModuleNotFoundError:
          installer.status(f"...installing controlnet-aux")
          run_sp("pip install --upgrade controlnet-aux", realtime=False)
          pass
    task_type = "controlnet" if use_controlnet else "text2img"
    if loaded_blip_diffusion_task != task_type:
        clear_pipes()
    installer.status(f"...blip_diffusion {task_type} Pipeline")
    model_id = "Salesforce/blipdiffusion"
    if pipe_blip_diffusion == None or loaded_blip_diffusion_task != task_type:
        try:
            if task_type == "text2img":
                from diffusers.pipelines import BlipDiffusionPipeline
                model_id = "Salesforce/blipdiffusion"
                pipe_blip_diffusion = BlipDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            elif task_type == "controlnet_canny":
                from diffusers.pipelines import BlipDiffusionControlNetPipeline
                from controlnet_aux import CannyDetector
                model_id = "Salesforce/blipdiffusion-controlnet"
                pipe_blip_diffusion = BlipDiffusionControlNetPipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            if prefs['enable_torch_compile']:
                installer.status(f"...Torch compiling unet")
                pipe_blip_diffusion.unet.to(memory_format=torch.channels_last)
                pipe_blip_diffusion.unet = torch.compile(pipe_blip_diffusion.unet, mode="reduce-overhead", fullgraph=True)
                pipe_blip_diffusion = pipe_blip_diffusion.to("cuda")
            elif cpu_offload:
                pipe_blip_diffusion.enable_model_cpu_offload()
            else:
                pipe_blip_diffusion.to("cuda")
            loaded_blip_diffusion_task = task_type
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing BLIP Diffusion, try running without installing Diffusers first...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]), debug_pref=blip_diffusion_prefs)
            return
    else:
        clear_pipes('blip_diffusion')
    cldm_cond_image = None
    if use_controlnet:
        if not bool(blip_diffusion_prefs['control_image']):
            alert_msg(page, f"ERROR: You must provide a ControlNet Image.")
            return
        fname = blip_diffusion_prefs['control_image'].rpartition(slash)[2]
        if blip_diffusion_prefs['control_image'].startswith('http'):
            installer.status(f"...download control_image")
            control_image = PILImage.open(requests.get(blip_diffusion_prefs['control_image'], stream=True).raw)
        else:
            if os.path.isfile(blip_diffusion_prefs['control_image']):
                control_image = PILImage.open(blip_diffusion_prefs['control_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your control_image {pr['control_image']}")
                return
        control_image = control_image.resize((blip_diffusion_prefs['width'], blip_diffusion_prefs['height']), resample=PILImage.Resampling.LANCZOS)
        control_image = ImageOps.exif_transpose(control_image).convert("RGB")
        if blip_diffusion_prefs['controlnet_type'] == "Canny Edge":
            installer.status(f"...running Canny Edge Detector")
            from controlnet_aux import CannyDetector
            canny = CannyDetector()
            cldm_cond_image = canny(control_image, 30, 70, output_type="pil")
        elif blip_diffusion_prefs['controlnet_type'] == "HED":
            installer.status(f"...running HED Edge Detector")
            from controlnet_aux import HEDdetector
            hed = HEDdetector.from_pretrained("lllyasviel/Annotators")
            cldm_cond_image = hed(control_image)
    clear_last()
    for pr in blip_diffusion_prompts:
        prt("Generating your BLIP-Diffusion Image...")
        prt(progress)
        autoscroll(False)
        init_img = None
        if bool(pr['init_image']):
            fname = pr['init_image'].rpartition(slash)[2]
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        total_steps = pr['steps']
        random_seed = get_seed(pr['seed'])
        generator = torch.Generator(device="cuda").manual_seed(random_seed)
        cond_subject = [x.strip() for x in blip_diffusion_prefs['source_subject_category'].split(',')]
        tgt_subject = [x.strip() for x in blip_diffusion_prefs['target_subject_category'].split(',')]
        try:
            if task_type == "text2img":
                images = pipe_blip_diffusion(
                    prompt=pr['prompt'], neg_prompt=pr['negative_prompt'],
                    source_subject_category=cond_subject, target_subject_category=tgt_subject,
                    reference_image=init_img,
                    #num_images_per_prompt=pr['num_images'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=pr['steps'],
                    guidance_scale=pr['guidance_scale'],
                    prompt_strength=pr['strength'],
                    prompt_reps=blip_diffusion_prefs['prompt_reps'],
                    generator=generator,
                    callback=callback_fnc,
                ).images
            elif task_type == "controlnet":
                images = pipe_blip_diffusion(
                    prompt=pr['prompt'], neg_prompt=pr['negative_prompt'],
                    source_subject_category=cond_subject, target_subject_category=tgt_subject,
                    reference_image=init_img,
                    conditioning_image=cldm_cond_image,
                    #num_images_per_prompt=pr['num_images'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=pr['steps'],
                    guidance_scale=pr['guidance_scale'],
                    prompt_strength=pr['strength'],
                    prompt_reps=blip_diffusion_prefs['prompt_reps'],
                    generator=generator,
                    callback=callback_fnc,
                ).images
        except Exception as e:
            clear_last()
            clear_last()
            alert_msg(page, f"ERROR: Something went wrong generating {task_type} images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]), debug_pref=blip_diffusion_prefs)
            return
        clear_last()
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(blip_diffusion_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, blip_diffusion_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{blip_diffusion_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            if not blip_diffusion_prefs['display_upscaled_image'] or not blip_diffusion_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, blip_diffusion_prefs, f"BLIP-Diffusion {task_type} {'' if blip_diffusion_prefs['controlnet_type'] == 'None' else blip_diffusion_prefs['controlnet_type']}", model_id, random_seed, extra=pr)
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            #if save_to_GDrive:
            batch_output = os.path.join(prefs['image_output'], blip_diffusion_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': blip_diffusion_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder
            #out_path = batch_output# if save_to_GDrive else txt2img_output
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)

            if blip_diffusion_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=blip_diffusion_prefs["enlarge_scale"], face_enhance=blip_diffusion_prefs["face_enhance"])
                image_path = upscaled_path
                save_metadata(image_path, blip_diffusion_prefs, f"BLIP-Diffusion {task_type} {'' if blip_diffusion_prefs['controlnet_type'] == 'None' else blip_diffusion_prefs['controlnet_type']}", model_id, random_seed, extra=pr)
                if blip_diffusion_prefs['display_upscaled_image']:
                    prt(Row([Img(src=asset_dir(upscaled_path), width=pr['width'] * float(blip_diffusion_prefs["enlarge_scale"]), height=pr['height'] * float(blip_diffusion_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], blip_diffusion_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], blip_diffusion_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_anytext(page, from_list=False, with_params=False):
    global anytext_prefs, pipe_anytext, prefs, status
    anytext_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            anytext_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':anytext_prefs['guidance_scale'], 'num_inference_steps':anytext_prefs['num_inference_steps'], 'width':anytext_prefs['width'], 'height':anytext_prefs['height'], 'init_image':anytext_prefs['init_image'], 'mask_image':anytext_prefs['mask_image'], 'init_image_strength':anytext_prefs['init_image_strength'], 'num_images':anytext_prefs['num_images'], 'seed':anytext_prefs['seed']})
        else:
            anytext_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'init_image':p['init_image'], 'mask_image':p['mask_image'], 'init_image_strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(anytext_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      anytext_prompts.append({'prompt': anytext_prefs['prompt'], 'negative_prompt':anytext_prefs['negative_prompt'], 'guidance_scale':anytext_prefs['guidance_scale'], 'num_inference_steps':anytext_prefs['num_inference_steps'], 'width':anytext_prefs['width'], 'height':anytext_prefs['height'], 'init_image':anytext_prefs['init_image'], 'mask_image':anytext_prefs['mask_image'], 'init_image_strength':anytext_prefs['init_image_strength'], 'num_images':anytext_prefs['num_images'], 'seed':anytext_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.AnyText.controls.append(line)
        if update:
          page.AnyText.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.AnyText, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.AnyText.auto_scroll = scroll
        page.AnyText.update()
      else:
        page.AnyText.auto_scroll = scroll
        page.AnyText.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.AnyText.controls = page.AnyText.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = anytext_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing AnyText Engine & Models... See console for progress.")
    prt(installer)
    clear_pipes("anytext")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    anytext_dir = os.path.join(root_dir, "AnyText")
    anytext_d = anytext_dir#os.path.join(anytext_dir, 'anytext')
    anytext_font = os.path.join(anytext_d, 'font')
    anytext_ttf = os.path.join(anytext_font, 'Horison.ttf')
    ttf = os.path.basename(anytext_ttf)
    if not os.path.exists(anytext_dir):
        installer.status(f"...cloning tyxsspa/AnyText.git")
        #run_sp("git clone https://github.com/zgljl2012/AnyText.git", cwd=root_dir)
        run_sp("git clone https://github.com/tyxsspa/AnyText.git", cwd=root_dir)
    if anytext_dir not in sys.path:
        sys.path.append(anytext_dir)
    if bool(anytext_prefs['font_ttf']):
        if '/' in anytext_prefs['font_ttf']:
            ttf = anytext_prefs['font_ttf'].rparition('/')[2]
        elif '\\' in anytext_prefs['font_ttf']:
            ttf = anytext_prefs['font_ttf'].rparition('\\')[2]
        if os.path.isfile(os.path.join(anytext_font, ttf)):
            anytext_ttf = os.path.join(anytext_font, ttf)
        else:
            if anytext_prefs['font_ttf'].startswith("http"):
                ttf_path = download_file(anytext_prefs['font_ttf'], to=anytext_font, ext="ttf")
            else:
                ttf_path = os.path.join(anytext_prefs['font_ttf'])
                if os.path.isfile(ttf_path):
                    shutil.copy(ttf_path, os.path.join(anytext_font, ttf))
                else:
                    prt("Font Path not found...")
                    return
            ttf = os.path.basename(ttf_path)
            anytext_ttf = os.path.join(anytext_font, ttf)
    else:
        anytext_ttf = os.path.join(anytext_font, 'Horison.ttf')
        if not os.path.isfile(anytext_ttf):
            installer.status(f"...downloading Horison.ttf")
            download_file("https://dl.dafont.com/dl/?f=horison", anytext_font, 'horison.zip')
            #run_sp(f"wget https://dl.dafont.com/dl/?f=horison -O {os.path.join(anytext_font, 'horison.zip')}")
            run_sp(f"unzip {os.path.join(anytext_font, 'horison.zip')}", cwd=anytext_font)
            os.remove(os.path.join(anytext_font, 'horison.zip'))
    try:
        import xformers
    except ModuleNotFoundError:
        installer.status("...installing FaceBook's Xformers (slow)")
        run_sp(f"pip install -U xformers=={'0.0.25' if upgrade_torch else '0.0.22.post7'} --index-url https://download.pytorch.org/whl/cu121", realtime=False)
        status['installed_xformers'] = True
        pass
    try:
        import modelscope
    except ModuleNotFoundError:
        import importlib
        installer.status("...installing modelscope")
        run_sp("pip install modelscope", realtime=False)
        installer.status("...upgrading Skquark/modelscope")
        run_sp("pip install -U git+https://github.com/Skquark/modelscope.git", realtime=False)
        import modelscope
        importlib.reload(modelscope)
        pass
    try:
        #import torchmetrics
        from torchmetrics.utilities.imports import _compare_version
    except ModuleNotFoundError:
        import importlib
        installer.status("...installing torchmetrics")
        run_sp("pip install -U torchmetrics==0.11.4", realtime=False)
        import torchmetrics
        importlib.reload(torchmetrics)
        pass
    try:
        import pytorch_lightning
    except ModuleNotFoundError:
        installer.status("...installing pytorch-lightning")
        run_sp("pip install -U pytorch-lightning==1.7.7", realtime=False)
        pass
    pip_install("omegaconf torchmetrics==0.11.4 sentencepiece easydict open-clip-torch|open_clip scikit-image|skimage sacremoses subword_nmt jieba tensorflow fsspec", installer=installer)
    os.chdir(anytext_d)
    from modelscope.pipelines import pipeline
    import cv2, re
    import numpy as np
    def count_lines(prompt):
        prompt = prompt.replace('“', '"')
        prompt = prompt.replace('”', '"')
        p = '"(.*?)"'
        strs = re.findall(p, prompt)
        if len(strs) == 0:
            strs = [' ']
        return len(strs)
    def generate_rectangles(w, h, n, max_trys=200):
        img = np.zeros((h, w, 1), dtype=np.uint8)
        rectangles = []
        attempts = 0
        n_pass = 0
        low_edge = int(max(w, h)*0.3 if n <= 3 else max(w, h)*0.2)  # ~150, ~100
        while attempts < max_trys:
            rect_w = min(np.random.randint(max((w*0.5)//n, low_edge), w), int(w*0.8))
            ratio = np.random.uniform(4, 10)
            rect_h = max(low_edge, int(rect_w/ratio))
            rect_h = min(rect_h, int(h*0.8))
            # gen rotate angle
            rotation_angle = 0
            rand_value = np.random.rand()
            if rand_value < 0.7:
                pass
            elif rand_value < 0.8:
                rotation_angle = np.random.randint(0, 40)
            elif rand_value < 0.9:
                rotation_angle = np.random.randint(140, 180)
            else:
                rotation_angle = np.random.randint(85, 95)
            x = np.random.randint(0, w - rect_w)
            y = np.random.randint(0, h - rect_h)
            rect_pts = cv2.boxPoints(((rect_w/2, rect_h/2), (rect_w, rect_h), rotation_angle))
            rect_pts = np.int32(rect_pts)
            rect_pts += (x, y)
            if np.any(rect_pts < 0) or np.any(rect_pts[:, 0] >= w) or np.any(rect_pts[:, 1] >= h):
                attempts += 1
                continue
            if any(check_overlap_polygon(rect_pts, rp) for rp in rectangles):
                attempts += 1
                continue
            n_pass += 1
            cv2.fillPoly(img, [rect_pts], 255)
            rectangles.append(rect_pts)
            if n_pass == n:
                break
        print("attempts:", attempts)
        if len(rectangles) != n:
            prt(f'Failed in auto generate positions after {attempts} attempts, try again!')
        return img
    def check_overlap_polygon(rect_pts1, rect_pts2):
        poly1 = cv2.convexHull(rect_pts1)
        poly2 = cv2.convexHull(rect_pts2)
        rect1 = cv2.boundingRect(poly1)
        rect2 = cv2.boundingRect(poly2)
        if rect1[0] + rect1[2] >= rect2[0] and rect2[0] + rect2[2] >= rect1[0] and rect1[1] + rect1[3] >= rect2[1] and rect2[1] + rect2[3] >= rect1[1]:
            return True
        return False
    anytext_model = 'damo/cv_anytext_text_generation_editing'
    if pipe_anytext == None:
        installer.status(f"...initializing AnyText Pipeline (slow)")
        try:
            pipe_anytext = pipeline('my-anytext-task', model=anytext_model, model_revision='v1.1.2', use_fp16=not prefs['higher_vram_mode'], use_translator=False, font_path=f'font/{ttf}')
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing AnyText...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]), debug_pref=anytext_prefs)
            return
    clear_last()
    s = "" if len(anytext_prompts) == 1 else "s"
    prt(f"Generating your AnyText Image{s}...")
    for pr in anytext_prompts:
        prt(progress)
        nudge(page.imageColumn if from_list else page.AnyText, page=page)
        autoscroll(False)
        n_lines = count_lines(pr['prompt'])
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(pr['seed'])
        params = {
            "sort_priority": anytext_prefs['sort_priority'],
            "show_debug": True,
            "revise_pos": anytext_prefs['revise_pos'],
            "image_count": pr['num_images'],
            "ddim_steps": pr['num_inference_steps'],
            "image_width": pr['width'],
            "image_height": pr['height'],
            "strength": pr['init_image_strength'],
            "cfg_scale": pr['guidance_scale'],
            "a_prompt": anytext_prefs['a_prompt'],
            "n_prompt": pr['negative_prompt']
        }
        init_img = None
        mask_img = None
        width, height = (pr['width'], pr['height'])
        if bool(pr['init_image']):
            fname = pr['init_image'].rpartition(slash)[2]
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            max_size = max(pr['width'], pr['height'])
            width, height = init_img.size
            width, height = scale_dimensions(width, height, max_size, multiple=64)
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
            init_img = np.array(init_img)
        if bool(pr['mask_image']):
            fname = pr['mask_image'].rpartition(slash)[2]
            if pr['mask_image'].startswith('http'):
                mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['mask_image']):
                    mask_img = PILImage.open(pr['mask_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your mask_image {pr['mask_image']}")
                    return
            max_size = max(pr['width'], pr['height'])
            width, height = mask_img.size
            width, height = scale_dimensions(width, height, max_size, multiple=64)
            mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            mask_img = ImageOps.exif_transpose(mask_img).convert("L")
            alpha_channel = PILImage.new("L", init_img.size, 0)
            alpha_channel.paste(mask_img, (0, 0))
            black_areas = PILImage.new("L", mask_img.size, 0)
            black_areas.paste(mask_img, (0, 0))
            inverted_mask = PILImage.eval(black_areas, lambda x: 255 - x)
            result = PILImage.alpha_composite(init_img, PILImage.new("RGBA", init_img.size, (0, 0, 0, 255)))
            mask_img = result.paste((0, 0, 0, 0), (0, 0), inverted_mask)
            mask_img = np.array(mask_img)
        else:
            mask_img = generate_rectangles(width, height, n_lines, max_trys=500)
            cv2.imwrite(os.path.join(anytext_d, 'pos_imgs.png'), 255-mask_img[..., ::-1])
        input_data = {
            "prompt": pr['prompt'],
            "seed": random_seed,
            "draw_pos": mask_img,
        }
        if bool(pr['init_image']):
            mode = 'text-editing'
            input_data['ori_image'] = init_img
        else:
            mode = 'text-generation'
        try:
            images, rtn_code, rtn_warning, debug_info = pipe_anytext(input_data, mode=mode, **params)
        except Exception as e:
            clear_last(2)
            os.chdir(root_dir)
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]), debug_pref=anytext_prefs)
            return
        if rtn_code == 0:
            clear_last(2)
            alert_msg(page, f"ERROR: {rtn_warning}", content=Text(str(debug_info), selectable=True, max_lines=20))
            print(f"{params}\n{input_data}")
            os.chdir(root_dir)
            return
        if rtn_warning:
            clear_last(2)
            alert_msg(page, f"WARNING: {rtn_warning}", content=Text(str(debug_info), selectable=True, max_lines=20))
            os.chdir(root_dir)
            #return
        #clear_last()
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(anytext_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, anytext_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            os.chdir(root_dir)
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{anytext_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, 1)
            cv2.imwrite(image_path, image[..., ::-1])
            #image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            if not anytext_prefs['display_upscaled_image'] or not anytext_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, anytext_prefs, f"AnyText {mode}", anytext_model, random_seed, extra=pr)
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            batch_output = os.path.join(prefs['image_output'], anytext_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': anytext_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)

            if anytext_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=anytext_prefs["enlarge_scale"], face_enhance=anytext_prefs["face_enhance"])
                image_path = upscaled_path
                os.chdir(stable_dir)
                save_metadata(image_path, anytext_prefs, f"AnyText {mode}", anytext_model, random_seed, extra=pr)
                if anytext_prefs['display_upscaled_image']:
                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(anytext_prefs["enlarge_scale"]), height=pr['height'] * float(anytext_prefs["enlarge_scale"]), data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], anytext_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], anytext_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    os.chdir(root_dir)
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_ip_adapter(page, from_list=False, with_params=False):
    global ip_adapter_prefs, pipe_ip_adapter, prefs, status
    if not check_diffusers(page): return
    ip_adapter_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            ip_adapter_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':ip_adapter_prefs['guidance_scale'], 'num_inference_steps':ip_adapter_prefs['num_inference_steps'], 'width':ip_adapter_prefs['width'], 'height':ip_adapter_prefs['height'], 'init_image':ip_adapter_prefs['init_image'], 'mask_image':ip_adapter_prefs['mask_image'], 'init_image_strength':ip_adapter_prefs['init_image_strength'], 'num_images':ip_adapter_prefs['num_images'], 'seed':ip_adapter_prefs['seed']})
        else:
            ip_adapter_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'init_image':p['init_image'], 'mask_image':p['mask_image'], 'init_image_strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      ip_adapter_prompts.append({'prompt': ip_adapter_prefs['prompt'], 'negative_prompt':ip_adapter_prefs['negative_prompt'], 'guidance_scale':ip_adapter_prefs['guidance_scale'], 'num_inference_steps':ip_adapter_prefs['num_inference_steps'], 'width':ip_adapter_prefs['width'], 'height':ip_adapter_prefs['height'], 'init_image':ip_adapter_prefs['init_image'], 'mask_image':ip_adapter_prefs['mask_image'], 'init_image_strength':ip_adapter_prefs['init_image_strength'], 'num_images':ip_adapter_prefs['num_images'], 'seed':ip_adapter_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.IP_Adapter.controls.append(line)
        if update:
          page.IP_Adapter.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.IP_Adapter, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.IP_Adapter.auto_scroll = scroll
        page.IP_Adapter.update()
      else:
        page.IP_Adapter.auto_scroll = scroll
        page.IP_Adapter.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.IP_Adapter.controls = page.IP_Adapter.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = ip_adapter_prefs['num_inference_steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):
      nonlocal progress
      total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing IP-Adapter Engine & Models... See console for progress.")
    prt(installer)
    use_SDXL = ip_adapter_prefs['use_SDXL']
    if use_SDXL:
        model = get_SDXL_model(prefs['SDXL_model'])
        ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == ip_adapter_prefs['ip_adapter_SDXL_model'])
    else:
        model = get_model(prefs['model_ckpt'])
        ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == ip_adapter_prefs['ip_adapter_model'])
    #use_faceid = 'FaceID' in ip_adapter_model['name'] or 'InstantID' in ip_adapter_model['name']
    model_id = model['path']
    variant = {'variant': model['revision']} if 'revision' in model else {}
    variant = {'variant': model['variant']} if 'variant' in model else variant
    if status['loaded_ip_adapter'] != model_id:
        clear_pipes()
    else:
        clear_pipes("ip_adapter")
    if bool(ip_adapter_prefs['init_image']) and bool(ip_adapter_prefs['mask_image']):
        mode = "inpaint"
    elif bool(ip_adapter_prefs['init_image']):
        mode = "img2img"
    else:
        mode = "txt2img"
    if 'FaceID' in ip_adapter_model['name'] or 'InstantID' in ip_adapter_model['name']:
        use_faceid = True
        mode = "faceid"
        try:
            import insightface
        except Exception:
            pip_install("insightface", installer=installer)
            pass
        finally:
            import insightface
            from insightface.app import FaceAnalysis
        from pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline, draw_kps
        checkpoint_dir = prefs['cache_dir'] if bool(prefs['cache_dir']) else os.path.join(root_dir, 'checkpoints')
        instantid_dir = os.path.join(checkpoint_dir, "models--InstantX--InstantID")
        if not os.path.exists(instantid_dir):
            installer.status("...download InstantID")
            antelopev2_zip = os.path.join(instantid_dir, "antelopev2.zip")
            import gdown
            gdown.download(id="18wEUfMNohBJ4K3Ly5wpTejPfDzp-8fI8", output=antelopev2_zip)
            run_sp(f"unzip {antelopev2_zip} -d {os.path.join(instantid_dir, 'antelopev2')}", realtime=False)
            os.remove(antelopev2_zip)
            from huggingface_hub import hf_hub_download
            hf_hub_download(repo_id="InstantX/InstantID", filename="ControlNetModel/config.json", local_dir=instantid_dir)
            hf_hub_download(repo_id="InstantX/InstantID", filename="ControlNetModel/diffusion_pytorch_model.safetensors", local_dir=instantid_dir)
            hf_hub_download(repo_id="InstantX/InstantID", filename="ip-adapter.bin", local_dir=instantid_dir)
        app = FaceAnalysis(name='antelopev2', root=instantid_dir, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
        app.prepare(ctx_id=0, det_size=(640, 640))
        face_adapter = os.path.join(instantid_dir, 'ip-adapter.bin')
        controlnet_path =  os.path.join(instantid_dir, 'ControlNetModel')
        installer.status("...load IdentityNet")
        from diffusers.models import ControlNetModel
        controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    cpu_offload = ip_adapter_prefs['cpu_offload']
    from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image, AutoPipelineForInpainting
    def change_mode(pipe, mode):
        status['loaded_ip_adapter_mode'] = mode
        if mode == "inpaint":
            return AutoPipelineForInpainting.from_pipe(pipe)
        elif mode == "img2img":
            return AutoPipelineForImage2Image.from_pipe(pipe)
        else:
            return AutoPipelineForText2Image.from_pipe(pipe)
    if pipe_ip_adapter == None:
        installer.status(f"...loading {model_id}")
        try:
            if mode == "inpaint":
                pipe_ip_adapter = AutoPipelineForInpainting.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **variant, **safety)
                status['loaded_ip_adapter_mode'] = "inpaint"
            elif mode == "img2img":
                pipe_ip_adapter = AutoPipelineForImage2Image.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **variant, **safety)
                status['loaded_ip_adapter_mode'] = "img2img"
            elif mode == "faceid":
                if use_SDXL:
                    #from diffusers import StableDiffusionXLInstantIDPipeline
                    from pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline
                    pipe_ip_adapter = StableDiffusionXLInstantIDPipeline.from_pretrained(model_id, controlnet=controlnet, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **variant, **safety)
                else:
                    from diffusers import DiffusionPipeline
                    pipe_ip_adapter = DiffusionPipeline.from_pretrained(model_id, custom_pipeline="ip_adapter_face_id", controlnet=controlnet, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **variant, **safety)
                status['loaded_ip_adapter_mode'] = "faceid"
            else:
                pipe_ip_adapter = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **variant, **safety)
                status['loaded_ip_adapter_mode'] = "txt2img"
            #pipe_ip_adapter = pipeline_scheduler(pipe_ip_adapter)
            if use_SDXL:
                pipe_ip_adapter = optimize_SDXL(pipe_ip_adapter)
            else:
                pipe_ip_adapter = optimize_pipe(pipe_ip_adapter)
            pipe_ip_adapter.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing IP-Adapter...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]), debug_pref=ip_adapter_prefs)
            return
        status['loaded_ip_adapter'] = model_id
    else:
        clear_pipes('ip_adapter')
        if status['loaded_ip_adapter_mode'] != mode:
            try:
                pipe_ip_adapter = change_mode(pipe_ip_adapter, mode)
            except Exception as e:
                clear_last()
                alert_msg(page, f"ERROR in {mode} IP-Adapter AutoPipeline from_pipe...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
        if prefs['scheduler_mode'] != status['loaded_scheduler']:
            pipe_ip_adapter = pipeline_scheduler(pipe_ip_adapter)
    if use_faceid:
        if use_SDXL:
            pipe_ip_adapter.load_ip_adapter_instantid(face_adapter)
        else:
            pipe_ip_adapter.load_ip_adapter_face_id(ip_adapter_model['path'], weight_name=ip_adapter_model['weight_name'])
    else:
        pipe_ip_adapter.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
    pipe_ip_adapter.set_ip_adapter_scale(ip_adapter_prefs['ip_adapter_strength'])
    if ip_adapter_prefs['ip_adapter_image'].startswith('http'):
        ip_adapter_image = PILImage.open(requests.get(ip_adapter_prefs['ip_adapter_image'], stream=True).raw)
    else:
        if os.path.isfile(ip_adapter_prefs['ip_adapter_image']):
            ip_adapter_image = PILImage.open(ip_adapter_prefs['ip_adapter_image'])
        else:
            alert_msg(page, f"ERROR: Couldn't find your ip_adapter_image {ip_adapter_prefs['ip_adapter_image']}")
            return
    #init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
    ip_adapter_image = ImageOps.exif_transpose(ip_adapter_image).convert("RGB")
    if use_faceid:
        face_info = app.get(cv2.cvtColor(np.array(ip_adapter_image), cv2.COLOR_RGB2BGR))
        if use_SDXL:
            face_info = sorted(face_info, key=lambda x:(x['bbox'][2]-x['bbox'][0])*x['bbox'][3]-x['bbox'][1])[-1]  # only use the maximum face
            face_emb = face_info['embedding']
            face_kps = draw_kps(ip_adapter_image, face_info['kps'])
        else:
            face_emb = torch.from_numpy(face_info[0].normed_embedding).unsqueeze(0)
    clear_last()
    s = "" if len(ip_adapter_prompts) == 1 else "s"
    prt(f"Generating your IP-Adapter Image{s}...")
    for pr in ip_adapter_prompts:
        prt(progress)
        autoscroll(False)
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(pr['seed'])
        generator = torch.Generator(device="cuda").manual_seed(random_seed)
        init_img = None
        mask_img = None
        if bool(pr['init_image']):
            fname = pr['init_image'].rpartition(slash)[2]
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        if bool(pr['mask_image']):
            fname = pr['mask_image'].rpartition(slash)[2]
            if pr['mask_image'].startswith('http'):
                mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['mask_image']):
                    mask_img = PILImage.open(pr['mask_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your mask_image {pr['mask_image']}")
                    return
            mask_img = mask_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
            mask_img = ImageOps.exif_transpose(mask_img).convert("RGB")
        mode = "inpaint" if init_img != None and mask_img != None else "img2img" if init_img != None else "txt2img"
        if use_faceid:
            mode = "faceid"
        try:
            if status['loaded_ip_adapter_mode'] != mode:
                pipe_ip_adapter = change_mode(pipe_ip_adapter, mode)
            if mode == "inpaint":
                images = pipe_ip_adapter(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    num_images_per_prompt=pr['num_images'],
                    #height=pr['height'],
                    #width=pr['width'],
                    num_inference_steps=pr['num_inference_steps'],
                    guidance_scale=pr['guidance_scale'],
                    ip_adapter_image=ip_adapter_image,
                    init_image=init_img,
                    mask_image=mask_img,
                    init_image_strength=pr['init_image_strength'],
                    generator=generator,
                    callback_on_step_end=callback_fnc,
                ).images
            elif mode == "img2img":
                images = pipe_ip_adapter(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    num_images_per_prompt=pr['num_images'],
                    #height=pr['height'],
                    #width=pr['width'],
                    num_inference_steps=pr['num_inference_steps'],
                    guidance_scale=pr['guidance_scale'],
                    ip_adapter_image=ip_adapter_image,
                    init_image=init_img,
                    init_image_strength=pr['init_image_strength'],
                    generator=generator,
                    callback_on_step_end=callback_fnc,
                ).images
            elif mode == "faceid":
                if use_SDXL:
                    images = pipe_ip_adapter(
                        prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                        num_images_per_prompt=pr['num_images'],
                        height=pr['height'],
                        width=pr['width'],
                        image_embeds=face_emb,
                        image=face_kps,
                        controlnet_conditioning_scale=pr['init_image_strength'],
                        num_inference_steps=pr['num_inference_steps'],
                        guidance_scale=pr['guidance_scale'],
                        #ip_adapter_image=ip_adapter_image,
                        generator=generator,
                        callback_on_step_end=callback_fnc,
                    ).images
                else:
                    images = pipe_ip_adapter(
                        prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                        num_images_per_prompt=pr['num_images'],
                        height=pr['height'],
                        width=pr['width'],
                        image_embeds=face_emb,
                        num_inference_steps=pr['num_inference_steps'],
                        guidance_scale=pr['guidance_scale'],
                        generator=generator,
                        callback_on_step_end=callback_fnc,
                    ).images
            else:
                images = pipe_ip_adapter(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    num_images_per_prompt=pr['num_images'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=pr['num_inference_steps'],
                    guidance_scale=pr['guidance_scale'],
                    ip_adapter_image=ip_adapter_image,
                    generator=generator,
                    callback_on_step_end=callback_fnc,
                ).images
        except Exception as e:
            clear_last()
            clear_last()
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        #clear_last()
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(ip_adapter_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, ip_adapter_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{ip_adapter_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            if not ip_adapter_prefs['display_upscaled_image'] or not ip_adapter_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, ip_adapter_prefs, f"IP-Adapter", model_id, random_seed, scheduler=True, extra=pr)
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            batch_output = os.path.join(prefs['image_output'], ip_adapter_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': ip_adapter_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)

            if ip_adapter_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=ip_adapter_prefs["enlarge_scale"], face_enhance=ip_adapter_prefs["face_enhance"])
                image_path = upscaled_path
                save_metadata(image_path, ip_adapter_prefs, f"IP-Adapter", model_id, random_seed, scheduler=True, extra=pr)
                if ip_adapter_prefs['display_upscaled_image']:
                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(ip_adapter_prefs["enlarge_scale"]), height=pr['height'] * float(ip_adapter_prefs["enlarge_scale"]), data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], ip_adapter_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], ip_adapter_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_hd_painter(page, from_list=False, with_params=False):
    global hd_painter_prefs, pipe_hd_painter, prefs, status
    if not check_diffusers(page): return
    if not bool(hd_painter_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
    hd_painter_prompts = []
    if from_list:
        if len(prompts) < 1:
            alert_msg(page, "You need to add Prompts to your List first... ")
            return
        for p in prompts:
            if with_params:
                hd_painter_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':hd_painter_prefs['guidance_scale'], 'num_inference_steps':hd_painter_prefs['num_inference_steps'], 'width':hd_painter_prefs['width'], 'height':hd_painter_prefs['height'], 'init_image':hd_painter_prefs['init_image'], 'mask_image':hd_painter_prefs['mask_image'], 'init_image_strength':hd_painter_prefs['init_image_strength'], 'num_images':hd_painter_prefs['num_images'], 'seed':hd_painter_prefs['seed']})
            else:
                hd_painter_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'init_image':p['init_image'], 'mask_image':p['mask_image'], 'init_image_strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
        if not bool(hd_painter_prefs['init_image']) or not bool(hd_painter_prefs['mask_image']):
            alert_msg(page, "You must provide an initial image and mask image to process your image generation...")
            return
        hd_painter_prompts.append({'prompt': hd_painter_prefs['prompt'], 'negative_prompt':hd_painter_prefs['negative_prompt'], 'guidance_scale':hd_painter_prefs['guidance_scale'], 'num_inference_steps':hd_painter_prefs['num_inference_steps'], 'width':hd_painter_prefs['width'], 'height':hd_painter_prefs['height'], 'init_image':hd_painter_prefs['init_image'], 'mask_image':hd_painter_prefs['mask_image'], 'init_image_strength':hd_painter_prefs['init_image_strength'], 'num_images':hd_painter_prefs['num_images'], 'seed':hd_painter_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.HD_Painter.controls.append(line)
        if update:
          page.HD_Painter.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.HD_Painter, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.HD_Painter.auto_scroll = scroll
        page.HD_Painter.update()
      else:
        page.HD_Painter.auto_scroll = scroll
        page.HD_Painter.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.HD_Painter.controls = page.HD_Painter.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = hd_painter_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    def callback_fn(pipe, step, timestep, callback_kwargs):
      callback_fnc.has_been_called = True
      nonlocal progress
      total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing HD-Painter Engine & Models... See console for progress.")
    prt(installer)
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    cpu_offload = hd_painter_prefs['cpu_offload']
    hd_painter_model = hd_painter_prefs['hd_painter_model'] if hd_painter_prefs['hd_painter_model'] != "Custom" else hd_painter_prefs['hd_painter_custom_model'] #"hd_painter/hd_painter-512" if hd_painter_prefs['hd_painter_model'] == "hd_painter-512" else "hd_painter/hd_painter-256" if hd_painter_prefs['hd_painter_model'] == "hd_painter-256" else hd_painter_prefs['hd_painter_custom_model']
    if 'loaded_hd_painter' not in status: status['loaded_hd_painter'] = ""
    if hd_painter_model != status['loaded_hd_painter']:
        clear_pipes()
    else:
        clear_pipes("hd_painter")
    #from optimum.intel import OVLatentConsistencyModelPipeline
    #pipe = OVLatentConsistencyModelPipeline.from_pretrained("rupeshs/HD_Painter-dreamshaper-v7-openvino-int8", ov_config={"CACHE_DIR": ""})
    mem_kwargs = {} if prefs['higher_vram_mode'] else {'variant': "fp16", 'torch_dtype': torch.float16}
    from diffusers import DiffusionPipeline, DDIMScheduler
    if pipe_hd_painter == None:
        installer.status(f"...initialize HD-Painter Pipeline")
        try:
            pipe_hd_painter = DiffusionPipeline.from_pretrained(hd_painter_model, custom_pipeline="hd_painter", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **mem_kwargs)
            #if prefs['enable_torch_compile']:
            #    installer.status(f"...Torch compiling transformer")
            #    pipe_hd_painter.transformer = torch.compile(pipe_hd_painter.transformer, mode="reduce-overhead", fullgraph=True)
            #    pipe_hd_painter = pipe_hd_painter.to(torch_device)
            #elif cpu_offload:
            #    pipe_hd_painter.enable_model_cpu_offload()
            #else:
            pipe_hd_painter.to(torch_device)
            pipe_hd_painter.scheduler = DDIMScheduler.from_config(pipe_hd_painter.scheduler.config)
            #pipe_hd_painter = pipe_scheduler(pipe_hd_painter)
            pipe_hd_painter.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing HD-Painter...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        status['loaded_hd_painter'] = hd_painter_model
    else:
        clear_pipes('hd_painter')
    ip_adapter_arg = {}
    if hd_painter_prefs['use_ip_adapter']:
        installer.status(f"...initialize IP-Adapter")
        ip_adapter_img = None
        if hd_painter_prefs['ip_adapter_image'].startswith('http'):
            i_response = requests.get(hd_painter_prefs['ip_adapter_image'])
            ip_adapter_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
        else:
            if os.path.isfile(hd_painter_prefs['ip_adapter_image']):
                ip_adapter_img = PILImage.open(hd_painter_prefs['ip_adapter_image'])
            else:
                clear_last()
                prt(f"ERROR: Couldn't find your ip_adapter_image {hd_painter_prefs['ip_adapter_image']}")
        if bool(ip_adapter_img):
            ip_adapter_arg['ip_adapter_image'] = ip_adapter_img
        if bool(ip_adapter_arg):
            ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == hd_painter_prefs['ip_adapter_model'])
            pipe_hd_painter.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
            pipe_hd_painter.set_ip_adapter_scale(hd_painter_prefs['ip_adapter_strength'])
    clear_last()
    s = "" if len(hd_painter_prompts) == 1 else "s"
    prt(f"Generating your HD-Painter Image{s}...")
    for pr in hd_painter_prompts:
        prt(progress)
        autoscroll(False)
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(int(pr['seed']))
        generator = torch.manual_seed(random_seed)
        init_img = None
        mask_img = None
        if bool(pr['init_image']):
            fname = pr['init_image'].rpartition(slash)[2]
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            width, height = init_img.size
            width, height = scale_dimensions(width, height, hd_painter_prefs['max_size'], multiple=32)
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        if bool(pr['mask_image']):
            fname = pr['mask_image'].rpartition(slash)[2]
            if pr['mask_image'].startswith('http'):
                mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['mask_image']):
                    mask_img = PILImage.open(pr['mask_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your mask_image {pr['mask_image']}")
                    return
            width, height = mask_img.size
            width, height = scale_dimensions(width, height, hd_painter_prefs['max_size'], multiple=32)
            mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            mask_img = ImageOps.exif_transpose(init_img).convert("RGB")
        try:
            images = pipe_hd_painter(
                prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                image=init_img,
                mask=mask_img,
                strength=pr['init_image_strength'],
                num_images_per_prompt=pr['num_images'],
                num_inference_steps=pr['num_inference_steps'],
                guidance_scale=pr['guidance_scale'],
                eta=hd_painter_prefs['eta'],
                use_rasg=hd_painter_prefs['use_rasg'],
                use_painta=hd_painter_prefs['use_painta'],
                generator=generator,
                #painta_scale_factors=[2, 4],  # 16 x 16 and 32 x 32
                #rasg_scale_factor=4,  # 16x16 by default
                #list_of_painta_layer_names=None,
                #list_of_rasg_layer_names=None,
                callback_on_step_end=callback_fn,
                **ip_adapter_arg,
            ).images
        except Exception as e:
            clear_last(2)
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        #clear_last()
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(hd_painter_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, hd_painter_prefs['batch_folder_name'])
        makedir(txt2img_output)
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        for idx, image in range(images):
            fname = format_filename(pr['prompt'])
            #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{hd_painter_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            batch_output = os.path.join(prefs['image_output'], hd_painter_prefs['batch_folder_name'])
            makedir(batch_output)
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': hd_painter_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)
            if hd_painter_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=hd_painter_prefs["enlarge_scale"], face_enhance=hd_painter_prefs["face_enhance"])
                image_path = upscaled_path
                os.chdir(stable_dir)
                save_metadata(image_path, hd_painter_prefs, f"HD-Painter", hd_painter_model, random_seed, extra=pr)
                if hd_painter_prefs['display_upscaled_image']:
                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(hd_painter_prefs["enlarge_scale"]), height=pr['height'] * float(hd_painter_prefs["enlarge_scale"]), data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if not hd_painter_prefs['display_upscaled_image'] or not hd_painter_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, hd_painter_prefs, f"HD-Painter", hd_painter_model, random_seed, extra=pr)
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], hd_painter_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], hd_painter_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_reference(page, from_list=False):
    global reference_prefs, pipe_reference
    if not check_diffusers(page): return
    if not reference_prefs['reference_attn'] and not reference_prefs['reference_adain']:
      alert_msg(page, "Reference Attention and/or Reference Adain must be set to True... ")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.Reference.controls.append(line)
        if update:
          page.Reference.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.Reference, lines=lines)
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.Reference.controls = page.Reference.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.Reference.auto_scroll = scroll
        page.Reference.update()
      else:
        page.Reference.auto_scroll = scroll
        page.Reference.update()
    progress = ProgressBar(bar_height=8)
    total_steps = reference_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    reference_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        reference = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'ref_image': p['init_image'] if bool(p['init_image']) else reference_prefs['ref_image'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps': p['steps'], 'width': p['width'], 'height': p['height'], 'seed': p['seed']}
        reference_prompts.append(reference)
    else:
        if not bool(reference_prefs['prompt']):
            alert_msg(page, "You need to add a Text Prompt first... ")
            return
        reference = {'prompt':reference_prefs['prompt'], 'negative_prompt': reference_prefs['negative_prompt'], 'ref_image': reference_prefs['ref_image'], 'guidance_scale':reference_prefs['guidance_scale'], 'num_inference_steps': reference_prefs['num_inference_steps'], 'width': reference_prefs['width'], 'height': reference_prefs['height'], 'seed': reference_prefs['seed']}
        reference_prompts.append(reference)
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    else:
      clear_list()
    autoscroll(True)
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    if reference_prefs['use_SDXL']:
        model = "stabilityai/stable-diffusion-xl-base-1.0"
    else:
        model = get_model(prefs['model_ckpt'])['path']
    if reference_prefs['last_model'] == model:
        clear_pipes('reference')
    else:
        clear_pipes()
        reference_prefs['last_model'] = model
    #torch.cuda.empty_cache()
    #torch.cuda.reset_max_memory_allocated()
    #torch.cuda.reset_peak_memory_stats()
    if pipe_reference == None:
        prt(Installing(f"Installing Reference Pipeline with {model} Model... "))
        diffusers_dir = os.path.join(root_dir, "diffusers")
        if not os.path.exists(diffusers_dir):
          run_process("git clone https://github.com/Skquark/diffusers.git", realtime=False, cwd=root_dir)
        if os.path.join(diffusers_dir, "examples", "community") not in sys.path:
          sys.path.append(os.path.join(diffusers_dir, "examples", "community"))
        try:
            if reference_prefs['use_SDXL']:
                from stable_diffusion_xl_reference import StableDiffusionXLReferencePipeline
                pipe_reference = StableDiffusionXLReferencePipeline.from_pretrained(model, torch_dtype=torch.float16, use_safetensors=True, variant="fp16", safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                pipe_reference = pipeline_scheduler(pipe_reference)
                pipe_reference = optimize_SDXL(pipe_reference, freeu=False)
                pipe_reference.set_progress_bar_config(disable=True)
            else:
                from stable_diffusion_reference import StableDiffusionReferencePipeline
                pipe_reference = StableDiffusionReferencePipeline.from_pretrained(model, torch_dtype=torch.float16, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                #pipe_reference.to(torch_device)
                pipe_reference = pipeline_scheduler(pipe_reference)
                pipe_reference = optimize_pipe(pipe_reference, freeu=False)
                pipe_reference.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Reference Pipeline", content=Text(str(e)))
            return
        #pipe_reference.set_progress_bar_config(disable=True)
        clear_last()
    else:
        pipe_reference = pipeline_scheduler(pipe_reference)
    s = "s" if reference_prefs['num_images'] > 1 or reference_prefs['batch_size'] > 1 else ""
    prt(f"Generating Reference{s} of your Image...")
    batch_output = os.path.join(stable_dir, reference_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], reference_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for pr in reference_prompts:
        if pr['ref_image'].startswith('http'):
            init_img = PILImage.open(requests.get(pr['ref_image'], stream=True).raw)
        else:
            if os.path.isfile(pr['ref_image']):
                init_img = PILImage.open(pr['ref_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your ref_image {pr['ref_image']}")
                return
        width, height = init_img.size
        width, height = scale_dimensions(width, height, reference_prefs['max_size'])
        init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)
        init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        width = pr['width']
        height = pr['height']
        total_steps = pr['num_inference_steps']
        for num in range(reference_prefs['num_images']):
            prt(progress)
            autoscroll(False)
            random_seed = get_seed(pr['seed'])
            #generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            try:
                images = pipe_reference(ref_image=init_img, prompt=pr['prompt'], negative_prompt=pr['negative_prompt'], num_inference_steps=pr['num_inference_steps'], attention_auto_machine_weight=reference_prefs['attention_auto_machine_weight'], gn_auto_machine_weight=reference_prefs['gn_auto_machine_weight'], style_fidelity=reference_prefs['style_fidelity'], reference_attn=reference_prefs['reference_attn'], reference_adain=reference_prefs['reference_adain'], guidance_scale=pr['guidance_scale'], width=width, height=height, num_images_per_prompt=reference_prefs['batch_size'], callback=callback_fnc, callback_steps=1).images
            except Exception as e:
                clear_last()
                alert_msg(page, "Error running Reference Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]), debug_pref=reference_prefs)
                return
            autoscroll(True)
            clear_last()
            fname = format_filename(pr['prompt'])
            for image in images:
                if prefs['file_suffix_seed']: fname += f"-{random_seed}"
                #for image in images:
                image_path = available_file(os.path.join(stable_dir, reference_prefs['batch_folder_name']), fname, num)
                unscaled_path = image_path
                output_file = image_path.rpartition(slash)[2]
                image.save(image_path)
                out_path = os.path.dirname(image_path)
                upscaled_path = os.path.join(out_path, output_file)
                if not reference_prefs['display_upscaled_image'] or not reference_prefs['apply_ESRGAN_upscale']:
                    save_metadata(image_path, reference_prefs, f"Reference", model, random_seed, extra=pr)
                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if reference_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    upscale_image(image_path, upscaled_path, scale=reference_prefs["enlarge_scale"])
                    image_path = upscaled_path
                    save_metadata(image_path, reference_prefs, f"Reference", model, random_seed, extra=pr)
                    if reference_prefs['display_upscaled_image']:
                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(reference_prefs["enlarge_scale"]), height=height * float(reference_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], reference_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], reference_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_controlnet_qr(page, from_list=False):
    global controlnet_qr_prefs, pipe_controlnet_qr, pipe_controlnet
    if not check_diffusers(page): return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.ControlNetQR.controls.append(line)
        if update:
          page.ControlNetQR.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.ControlNetQR, lines=lines)
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.ControlNetQR.controls = page.ControlNetQR.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.ControlNetQR.auto_scroll = scroll
        page.ControlNetQR.update()
      else:
        page.ControlNetQR.auto_scroll = scroll
        page.ControlNetQR.update()
    progress = ProgressBar(bar_height=8)
    total_steps = controlnet_qr_prefs['num_inference_steps'] * 2
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    controlnet_qr_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        controlnet_qr = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'init_image': p['init_image'] if bool(p['init_image']) else controlnet_qr_prefs['init_image'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps': p['steps'], 'width': p['max_size'], 'height': p['max_size'], 'strength':p['init_image_strength'], 'seed': p['seed']}
        controlnet_qr_prompts.append(controlnet_qr)
    else:
        if not bool(controlnet_qr_prefs['prompt']):
            alert_msg(page, "You need to add a Text Prompt first... ")
            return
        controlnet_qr = {'prompt':controlnet_qr_prefs['prompt'], 'negative_prompt': controlnet_qr_prefs['negative_prompt'], 'init_image': controlnet_qr_prefs['init_image'], 'guidance_scale':controlnet_qr_prefs['guidance_scale'], 'num_inference_steps': controlnet_qr_prefs['num_inference_steps'], 'width': controlnet_qr_prefs['max_size'], 'height': controlnet_qr_prefs['max_size'], 'strength': controlnet_qr_prefs['strength'], 'seed': controlnet_qr_prefs['seed']}
        controlnet_qr_prompts.append(controlnet_qr)
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    else:
      clear_list()
    autoscroll(True)
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    try:
        import qrcode
    except ImportError:
        prt(Installing(f"Installing QRCode Library... "))
        run_sp("pip install qrcode", realtime=False)
        import qrcode
        clear_last()
        pass
    use_SDXL = 'SDXL' in controlnet_qr_prefs['controlnet_version']
    if not use_SDXL:
        if '2.1' in controlnet_qr_prefs['controlnet_version']:
            sd_model = "stabilityai/stable-diffusion-2-1"
        else:
            sd_model = "runwayml/stable-diffusion-v1-5"
        #sd_model = get_model(prefs['model_ckpt'])['path']
        controlnet_model = "DionTimmer/controlnet_qrcode-control_v1p_sd15" if controlnet_qr_prefs['controlnet_version'].endswith("1.5") else "DionTimmer/controlnet_qrcode-control_v11p_sd21"
        if 'Monster' in controlnet_qr_prefs['controlnet_version']: #"monster-labs/control_v1p_sd15_qrcode_monster"
            controlnet_model = "prantik-s/monster_qrcode_v2"
        elif 'Pattern' in controlnet_qr_prefs['controlnet_version']:
            controlnet_model = "Nacholmo/controlnet-qr-pattern-v2"
            sd_model = "Nacholmo/Counterfeit-V2.5-vae-swapped"
    else:
        sd_model = get_SDXL_model(prefs['SDXL_model'])['path']
        if 'Monster' in controlnet_qr_prefs['controlnet_version']:
            controlnet_model = "monster-labs/control_v1p_sdxl_qrcode_monster"
        elif 'Pattern' in controlnet_qr_prefs['controlnet_version']:
            if 'LLLite' not in controlnet_qr_prefs['controlnet_version']:
                controlnet_model = "Nacholmo/controlnet-qr-pattern-sdxl"
            else:
                controlnet_model = "Nacholmo/qr-pattern-sdxl-ControlNet-LLLite"
        elif 'Canny' in controlnet_qr_prefs['controlnet_version']:
            controlnet_model = "xinsir/controlnet-canny-sdxl-1.0"
        elif 'Scribble' in controlnet_qr_prefs['controlnet_version']:
            controlnet_model = "xinsir/controlnet-scribble-sdxl-1.0"
    #monster-labs/control_v1p_sdxl_qrcode_monster
    use_ip_adapter = controlnet_qr_prefs['use_ip_adapter']
    if use_ip_adapter:
        if use_SDXL:
            ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == controlnet_qr_prefs['ip_adapter_SDXL_model'])
        else:
            ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == controlnet_qr_prefs['ip_adapter_model'])
    else:
        ip_adapter_model = None
    if controlnet_qr_prefs['last_model'] == sd_model and controlnet_qr_prefs['last_controlnet_model'] == controlnet_model and pipe_controlnet_qr != None:
        clear_pipes('controlnet_qr')
    else:
        clear_pipes()
        controlnet_qr_prefs['last_model'] = sd_model
        controlnet_qr_prefs['last_controlnet_model'] = controlnet_model
    #torch.cuda.empty_cache()
    #torch.cuda.reset_max_memory_allocated()
    #torch.cuda.reset_peak_memory_stats()
    if pipe_controlnet_qr == None:
        installer = Installing(f"Installing ControlNet QRCode Pipeline with {controlnet_qr_prefs['controlnet_version']} Model... ")
        prt(installer)
        try:
            if not use_SDXL:
                from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel
                installer.status(f"...get {controlnet_model}")
                pipe_controlnet = ControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.float16)
                installer.status(f"...get {sd_model}")
                pipe_controlnet_qr = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(sd_model, controlnet=pipe_controlnet, torch_dtype=torch.float16, variant='fp16', safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                installer.status(f"...optimizing pipe")
                pipe_controlnet_qr = pipeline_scheduler(pipe_controlnet_qr)
                pipe_controlnet_qr = optimize_pipe(pipe_controlnet_qr)
                pipe_controlnet_qr.set_progress_bar_config(disable=True)
            else:
                from diffusers import StableDiffusionXLControlNetImg2ImgPipeline, ControlNetModel, AutoencoderKL
                installer.status(f"...get {controlnet_model}")
                pipe_controlnet = ControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.float16)
                installer.status(f"...get sdxl-vae-fp16-fix vae")
                vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16).to("cuda")
                installer.status(f"...get {sd_model}")
                pipe_controlnet_qr = StableDiffusionXLControlNetImg2ImgPipeline.from_pretrained(sd_model, controlnet=pipe_controlnet, vae=vae, variant="fp16", use_safetensors=True, torch_dtype=torch.float16, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                installer.status(f"...optimizing pipe")
                pipe_controlnet_qr = pipeline_scheduler(pipe_controlnet_qr)
                pipe_controlnet_qr = optimize_SDXL(pipe_controlnet_qr)
                pipe_controlnet_qr.set_progress_bar_config(disable=True)
                
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing ControlNet QRCode Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]), debug_pref=controlnet_qr_prefs)
            return
        clear_last()
    else:
        pipe_controlnet_qr = pipeline_scheduler(pipe_controlnet_qr)
    if use_ip_adapter:
        pipe_controlnet_qr.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
        pipe_controlnet_qr.set_ip_adapter_scale(controlnet_qr_prefs['ip_adapter_strength'])
        if controlnet_qr_prefs['ip_adapter_image'].startswith('http'):
            ip_adapter_image = PILImage.open(requests.get(controlnet_qr_prefs['ip_adapter_image'], stream=True).raw)
        else:
            if os.path.isfile(controlnet_qr_prefs['ip_adapter_image']):
                ip_adapter_image = PILImage.open(controlnet_qr_prefs['ip_adapter_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your ip_adapter_image {controlnet_qr_prefs['ip_adapter_image']}")
                return
        ip_adapter_image = ImageOps.exif_transpose(ip_adapter_image).convert("RGB")
        status['loaded_ip_adapter'] = ip_adapter_model
        ip_adapter_args = {'ip_adapter_image': ip_adapter_image}
    else:
        status['loaded_ip_adapter'] = ""
        ip_adapter_args = {}
    s = "s" if controlnet_qr_prefs['num_images'] > 1 or controlnet_qr_prefs['batch_size'] > 1 else ""
    prt(f"Generating ControlNet QR{s} of your Image...")
    batch_output = os.path.join(stable_dir, controlnet_qr_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
        os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], controlnet_qr_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
        os.makedirs(batch_output)
    batch_size = controlnet_qr_prefs['batch_size']
    mode = controlnet_qr_prefs['selected_mode'] if '"' not in controlnet_qr_prefs['selected_mode'] else controlnet_qr_prefs['selected_mode'].split('"')[1]
    if mode == "image":#controlnet_qr_prefs['use_image']:
        if not bool(controlnet_qr_prefs['ref_image']):
            alert_msg(page, f"ERROR: If using your own QR image, you must provide it.")
            return
        if controlnet_qr_prefs['ref_image'].startswith('http'):
            qrcode_image = PILImage.open(requests.get(controlnet_qr_prefs['ref_image'], stream=True).raw)
        else:
            if os.path.isfile(pr['ref_image']):
                qrcode_image = PILImage.open(controlnet_qr_prefs['ref_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your ref_image {controlnet_qr_prefs['ref_image']}")
                return
    else:
        if not bool(controlnet_qr_prefs['qr_content']):
            alert_msg(page, f"ERROR: If Generating QR Code, you must provide content like a URL.")
            return
        prt("Generating QR Code from content...")
        qr = qrcode.QRCode(
            version=1,
            error_correction=qrcode.constants.ERROR_CORRECT_H,
            box_size=10,
            border=controlnet_qr_prefs['border_thickness'],
        )
        qr.add_data(controlnet_qr_prefs['qr_content'])
        qr.make(fit=True)
        qrcode_image = qr.make_image(fill_color="black", back_color="white")
        clear_last()
    width, height = qrcode_image.size
    width, height = scale_dimensions(width, height, controlnet_qr_prefs['max_size'], multiple=32)
    qrcode_image = qrcode_image.resize((width, height), resample=PILImage.Resampling.LANCZOS)

    for pr in controlnet_qr_prompts:
        if bool(pr['init_image']):
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            #width = pr['width']
            #height = pr['height']
        else:
            init_img = qrcode_image
            pr['strength'] = 0.99
        total_steps = pr['num_inference_steps'] * 2
        for num in range(controlnet_qr_prefs['num_images']):
            prt(progress)
            prt(Container(content=None))
            clear_last()
            autoscroll(False)
            random_seed = get_seed(int(pr['seed']) + num)
            generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            try:
                images = pipe_controlnet_qr(prompt=[pr['prompt']] * batch_size, negative_prompt=[pr['negative_prompt']] * batch_size, image=[init_img] * batch_size, control_image=qrcode_image, num_inference_steps=pr['num_inference_steps'], guidance_scale=pr['guidance_scale'], controlnet_conditioning_scale=float(controlnet_qr_prefs['conditioning_scale']), control_guidance_start=controlnet_qr_prefs['control_guidance_start'], control_guidance_end=controlnet_qr_prefs['control_guidance_end'], width=width, height=height, num_images_per_prompt=controlnet_qr_prefs['batch_size'], strength=pr['strength'], generator=generator, callback=callback_fnc, callback_steps=1, **ip_adapter_args).images
            except Exception as e:
                clear_last()
                alert_msg(page, "Error running ControlNet-QRCode Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]), debug_pref=controlnet_qr_prefs)
                return
            autoscroll(True)
            clear_last()
            fname = format_filename(pr['prompt'])
            i = 0
            for image in images:
                if prefs['file_suffix_seed']: fname += f"-{random_seed}"
                #for image in images:
                image_path = available_file(os.path.join(stable_dir, controlnet_qr_prefs['batch_folder_name']), fname, num)
                unscaled_path = image_path
                output_file = image_path.rpartition(slash)[2]
                image.save(image_path)
                out_path = os.path.dirname(image_path)
                upscaled_path = os.path.join(out_path, output_file)
                if not controlnet_qr_prefs['display_upscaled_image'] or not controlnet_qr_prefs['apply_ESRGAN_upscale']:
                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if controlnet_qr_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    upscale_image(image_path, upscaled_path, scale=controlnet_qr_prefs["enlarge_scale"])
                    image_path = upscaled_path
                    if controlnet_qr_prefs['display_upscaled_image']:
                        time.sleep(0.6)
                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(controlnet_qr_prefs["enlarge_scale"]), height=height * float(controlnet_qr_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if prefs['save_image_metadata']:
                    img = PILImage.open(image_path)
                    metadata = PngInfo()
                    metadata.add_text("artist", prefs['meta_ArtistName'])
                    metadata.add_text("copyright", prefs['meta_Copyright'])
                    metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {controlnet_qr_prefs['enlarge_scale']}x with ESRGAN" if controlnet_qr_prefs['apply_ESRGAN_upscale'] else "")
                    metadata.add_text("pipeline", "ControlNet-QRCode")
                    if prefs['save_config_in_metadata']:
                      metadata.add_text("title", pr['prompt'])
                      config_json = controlnet_qr_prefs.copy()
                      config_json['controlnet_model_path'] = controlnet_model
                      config_json['model_path'] = sd_model
                      config_json['seed'] = random_seed + i
                      del config_json['num_images'], config_json['batch_size']
                      del config_json['display_upscaled_image']
                      del config_json['batch_folder_name']
                      del config_json['max_size']
                      del config_json['last_model']
                      del config_json['last_controlnet_model']
                      if not config_json['apply_ESRGAN_upscale']:
                        del config_json['enlarge_scale']
                        del config_json['apply_ESRGAN_upscale']
                      metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                    img.save(image_path, pnginfo=metadata)
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], controlnet_qr_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], controlnet_qr_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                time.sleep(0.2)
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
                i += 1
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_controlnet_segment(page, from_list=False):
    global controlnet_segment_prefs, pipe_controlnet_segment
    if not check_diffusers(page): return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.ControlNetSegmentAnything.controls.append(line)
        if update:
          page.ControlNetSegmentAnything.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.ControlNetSegmentAnything, lines=lines)
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.ControlNetSegmentAnything.controls = page.ControlNetSegmentAnything.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.ControlNetSegmentAnything.auto_scroll = scroll
        page.ControlNetSegmentAnything.update()
      else:
        page.ControlNetSegmentAnything.auto_scroll = scroll
        page.ControlNetSegmentAnything.update()
    progress = ProgressBar(bar_height=8)
    total_steps = controlnet_segment_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    controlnet_segment_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        controlnet_segment = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'ref_image': p['init_image'] if bool(p['init_image']) else controlnet_segment_prefs['ref_image'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps': p['steps'], 'width': p['width'], 'height': p['height'], 'seed': p['seed']}
        controlnet_segment_prompts.append(controlnet_segment)
    else:
        if not bool(controlnet_segment_prefs['prompt']):
            alert_msg(page, "You need to add a Text Prompt first... ")
            return
        controlnet_segment = {'prompt':controlnet_segment_prefs['prompt'], 'negative_prompt': controlnet_segment_prefs['negative_prompt'], 'ref_image': controlnet_segment_prefs['ref_image'], 'guidance_scale':controlnet_segment_prefs['guidance_scale'], 'num_inference_steps': controlnet_segment_prefs['num_inference_steps'], 'width': controlnet_segment_prefs['width'], 'height': controlnet_segment_prefs['height'], 'seed': controlnet_segment_prefs['seed']}
        controlnet_segment_prompts.append(controlnet_segment)
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    else:
      clear_list()
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    model = "runwayml/stable-diffusion-v1-5"#get_model(prefs['model_ckpt'])['path']
    model_name = "1.5"#get_model(prefs['model_ckpt'])['name']
    controlnet_model = "mfidabel/controlnet-segment-anything"
    autoscroll(True)
    installer = Installing(f"Installing ControlNet Segment-Anything Pipeline with {model} Model... ")
    prt(installer)
    clear_pipes('controlnet_segment')
    #torch.cuda.empty_cache()
    #torch.cuda.reset_max_memory_allocated()
    #torch.cuda.reset_peak_memory_stats()
    try:
        from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
    except ModuleNotFoundError:
        installer.status("...facebookresearch/segment-anything")
        run_sp("pip install git+https://github.com/facebookresearch/segment-anything.git", realtime=False)
        from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
        pass
    SAM_URL = "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
    SAM_dir = os.path.join(root_dir, "sam_vit_h_4b8939.pth")
    if not os.path.exists(SAM_dir):
        installer.status("...Downloading SAM Weights")
        r = requests.get(SAM_URL, allow_redirects=True)
        installer.status("...Writing SAM Weights")
        with open(SAM_dir, "wb") as sam_weights:
            sam_weights.write(r.content)
        del r

    gc.collect()
    sam = sam_model_registry["vit_h"](checkpoint=SAM_dir).to(torch_device)
    mask_generator = SamAutomaticMaskGenerator(sam)
    gc.collect()

    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
    #prt(installer)
    if pipe_controlnet_segment == None:
        try:
            installer.status(f"...ControlNet Model {controlnet_model}")
            pipe_controlnet = ControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.float16).to(torch_device)
            installer.status(f"...SD Model {model_name}")
            pipe_controlnet_segment = StableDiffusionControlNetPipeline.from_pretrained(model, controlnet=pipe_controlnet, torch_dtype=torch.float16, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            pipe_controlnet_segment = pipeline_scheduler(pipe_controlnet_segment)
            pipe_controlnet_segment = optimize_pipe(pipe_controlnet_segment)
            pipe_controlnet_segment.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing ControlNet Segment-Anything Pipeline", content=Text(str(e)))
            return
        #pipe_controlnet_segment.set_progress_bar_config(disable=True)
    else:
        pipe_controlnet_segment = pipeline_scheduler(pipe_controlnet_segment)

    def show_anns(anns):
        if len(anns) == 0:
            return
        sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
        h, w =  anns[0]['segmentation'].shape
        final_img = PILImage.fromarray(np.zeros((h, w, 3), dtype=np.uint8), mode="RGB")
        for ann in sorted_anns:
            m = ann['segmentation']
            img = np.empty((m.shape[0], m.shape[1], 3), dtype=np.uint8)
            for i in range(3):
                img[:,:,i] = np.random.randint(255, dtype=np.uint8)
            final_img.paste(PILImage.fromarray(img, mode="RGB"), (0, 0), PILImage.fromarray(np.uint8(m*255)))
        return final_img

    clear_last()
    s = "s" if controlnet_segment_prefs['num_images'] > 1 or controlnet_segment_prefs['batch_size'] > 1 else ""
    prt(f"Generating ControlNet Segment-Anything{s} of your Image...")
    batch_output = os.path.join(stable_dir, controlnet_segment_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], controlnet_segment_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for pr in controlnet_segment_prompts:
        if pr['ref_image'].startswith('http'):
            init_img = PILImage.open(requests.get(pr['ref_image'], stream=True).raw)
        else:
            if os.path.isfile(pr['ref_image']):
                init_img = PILImage.open(pr['ref_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your ref_image {pr['ref_image']}")
                return
        width, height = init_img.size
        width, height = scale_dimensions(width, height, controlnet_segment_prefs['max_size'], multiple=32)
        init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)
        init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        #width = pr['width']
        #height = pr['height']
        total_steps = pr['num_inference_steps']
        batch_size = controlnet_segment_prefs['batch_size']
        fname = format_filename(pr['prompt'])
        segmented_image = os.path.join(stable_dir, controlnet_segment_prefs['batch_folder_name'], f"{fname}-segmented.png")
        for num in range(controlnet_segment_prefs['num_images']):
            prt("Creating Segmented Mask Image...")
            random_seed = get_seed(int(pr['seed']) + num)
            generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            np.random.seed(int(random_seed))
            init_img_np = np.asarray(init_img)
            masks = mask_generator.generate(init_img_np)
            segmented_map = show_anns(masks)
            segmented_map.save(segmented_image)
            clear_last()#src_base64=pil_to_base64(segmented_map)
            prt(Row([Img(src=asset_dir(segmented_image), width=width, height=height, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            del masks
            flush()
            prt(progress)
            autoscroll(False)
            #segmented_map = segment_image(init_img, random_seed)
            #yield segmented_map, [PILImage.fromarray(np.zeros((width, height, 3), dtype=np.uint8))] * batch_size]
            try:
                images = pipe_controlnet_segment([pr['prompt']] * batch_size, [segmented_map] * batch_size, negative_prompt=[pr['negative_prompt']] * batch_size, num_inference_steps=pr['num_inference_steps'], guidance_scale=pr['guidance_scale'], width=width, height=height, generator=generator, callback=callback_fnc, callback_steps=1).images
            except Exception as e:
                clear_last()
                alert_msg(page, "Error running ControlNet Segment-Anything Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
                return
            autoscroll(True)
            clear_last()
            for image in images:
                if prefs['file_suffix_seed']: fname += f"-{random_seed}"
                #for image in images:
                image_path = available_file(os.path.join(stable_dir, controlnet_segment_prefs['batch_folder_name']), fname, num)
                unscaled_path = image_path
                output_file = image_path.rpartition(slash)[2]
                image.save(image_path)
                out_path = os.path.dirname(image_path)
                upscaled_path = os.path.join(out_path, output_file)
                if not controlnet_segment_prefs['display_upscaled_image'] or not controlnet_segment_prefs['apply_ESRGAN_upscale']:
                    save_metadata(image_path, controlnet_segment_prefs, f"ControlNetSegmentAnything", model, random_seed, extra=pr)
                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if controlnet_segment_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    upscale_image(image_path, upscaled_path, scale=controlnet_segment_prefs["enlarge_scale"])
                    image_path = upscaled_path
                    save_metadata(image_path, controlnet_segment_prefs, f"ControlNetSegmentAnything", model, random_seed, extra=pr)
                    if controlnet_segment_prefs['display_upscaled_image']:
                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(controlnet_segment_prefs["enlarge_scale"]), height=height * float(controlnet_segment_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], controlnet_segment_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], controlnet_segment_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_EDICT(page):
    global EDICT_prefs, prefs, status, pipe_EDICT, text_encoder_EDICT
    if not check_diffusers(page): return
    if not bool(EDICT_prefs['init_image']):
      alert_msg(page, "You must provide the Original Image and the Mask Image to process...")
      return
    if not bool(EDICT_prefs['base_prompt']) or not bool(EDICT_prefs['target_prompt']):
      alert_msg(page, "You must provide a base prompt describing image and target prompt to process...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.EDICT.controls.append(line)
      page.EDICT.update()
    def clear_last(lines=1):
      clear_line(page.EDICT, lines=lines)
    def autoscroll(scroll=True):
      page.EDICT.auto_scroll = scroll
      page.EDICT.update()
    def clear_list():
      page.EDICT.controls = page.EDICT.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = EDICT_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    def center_crop_resize(im):
        width, height = im.size
        d = min(width, height)
        left = (width - d) / 2
        upper = (height - d) / 2
        right = (width + d) / 2
        lower = (height + d) / 2
        return im.crop((left, upper, right, lower)).resize((EDICT_prefs['max_size'], EDICT_prefs['max_size']))
    autoscroll(True)
    clear_list()
    prt(Installing("Installing EDICT Editor Pipeline..."))
    import requests, random
    from io import BytesIO
    from PIL import ImageOps
    from PIL.PngImagePlugin import PngInfo
    if EDICT_prefs['init_image'].startswith('http'):
      #response = requests.get(EDICT_prefs['init_image'])
      #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
      original_img = PILImage.open(requests.get(EDICT_prefs['init_image'], stream=True).raw)
    else:
      if os.path.isfile(EDICT_prefs['init_image']):
        original_img = PILImage.open(EDICT_prefs['init_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your init_image {EDICT_prefs['init_image']}")
        return
    #width, height = original_img.size
    #width, height = scale_dimensions(width, height, EDICT_prefs['max_size'])
    original_img = center_crop_resize(original_img)
    clear_pipes('EDICT')
    torch.cuda.empty_cache()
    #torch.cuda.reset_max_memory_allocated()
    torch.cuda.reset_peak_memory_stats()
    torch_dtype = torch.float16
    model_id = get_model(prefs['model_ckpt'])['path']
    if pipe_EDICT is None:
        from diffusers import DiffusionPipeline, DDIMScheduler
        from transformers import CLIPTextModel
        try:
            scheduler = DDIMScheduler(
                num_train_timesteps=1000,
                beta_start=0.00085,
                beta_end=0.012,
                beta_schedule="scaled_linear",
                set_alpha_to_one=False,
                clip_sample=False,
            )
            text_encoder_EDICT = CLIPTextModel.from_pretrained(pretrained_model_name_or_path="openai/clip-vit-large-patch14", torch_dtype=torch_dtype)
            pipe_EDICT = DiffusionPipeline.from_pretrained(
                pretrained_model_name_or_path=model_id,
                custom_pipeline="edict_pipeline",
                revision="fp16",
                scheduler=scheduler,
                text_encoder=text_encoder_EDICT,
                leapfrog_steps=True,
                torch_dtype=torch_dtype,
                cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
            ).to(torch_device)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Couldn't Initialize EDICT Pipeline for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    clear_last()
    prt("Generating EDICT Edit of your Image...")
    prt(progress)
    autoscroll(False)
    batch_output = os.path.join(stable_dir, EDICT_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], EDICT_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    random_seed = int(EDICT_prefs['seed']) if int(EDICT_prefs['seed']) > 0 else random.randint(0,4294967295)
    for i in range(EDICT_prefs['num_images']):
        generator = torch.Generator(device="cpu").manual_seed(random_seed)
        #generator = torch.manual_seed(random_seed)
        try:
            images = pipe_EDICT(base_prompt=EDICT_prefs['base_prompt'], target_prompt=EDICT_prefs['target_prompt'], image=original_img, num_inference_steps=EDICT_prefs['num_inference_steps'], strength=EDICT_prefs['strength'], guidance_scale=EDICT_prefs['guidance_scale'], generator=generator)#.images
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Couldn't EDICT Edit your image for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        clear_last()
        clear_last()
        filename = format_filename(EDICT_prefs['target_prompt'])
        #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
        num = 0
        for image in images:
            random_seed += num
            fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
            image_path = available_file(os.path.join(stable_dir, EDICT_prefs['batch_folder_name']), fname, i)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            image.save(image_path)
            width, height = image.size
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)
            if not EDICT_prefs['display_upscaled_image'] or not EDICT_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, EDICT_prefs, f"EDICT Editor", model_id, random_seed)
                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if EDICT_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=EDICT_prefs["enlarge_scale"])
                image_path = upscaled_path
                save_metadata(image_path, EDICT_prefs, f"EDICT Editor", model_id, random_seed)
                if EDICT_prefs['display_upscaled_image']:
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(EDICT_prefs["enlarge_scale"]), height=height * float(EDICT_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], EDICT_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], EDICT_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
            num += 1
        random_seed += 1
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_DiffEdit(page):
    global DiffEdit_prefs, prefs, status, pipe_DiffEdit, text_encoder_DiffEdit
    if not check_diffusers(page): return
    if not bool(DiffEdit_prefs['init_image']):
      alert_msg(page, "You must provide the Original Image and the Mask Image to process...")
      return
    if not bool(DiffEdit_prefs['target_prompt']):
      alert_msg(page, "You must provide a target prompt to process...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.DiffEdit.controls.append(line)
      page.DiffEdit.update()
    def clear_last(lines=1):
      clear_line(page.DiffEdit, lines=lines)
    def autoscroll(scroll=True):
      page.DiffEdit.auto_scroll = scroll
      page.DiffEdit.update()
    def clear_list():
      page.DiffEdit.controls = page.DiffEdit.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = DiffEdit_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    def center_crop_resize(im):
        width, height = im.size
        d = min(width, height)
        left = (width - d) / 2
        upper = (height - d) / 2
        right = (width + d) / 2
        lower = (height + d) / 2
        return im.crop((left, upper, right, lower)).resize((DiffEdit_prefs['max_size'], DiffEdit_prefs['max_size']))
    autoscroll(True)
    clear_list()
    source_prompt = DiffEdit_prefs['source_prompt']
    target_prompt = DiffEdit_prefs['target_prompt']
    prt(Installing(f'Installing DiffEdit Pipeline{" and Caption Generator" if not bool(source_prompt) else ""}...'))
    clear_pipes('DiffEdit')
    import requests, random
    from io import BytesIO
    from PIL import ImageOps
    from PIL.PngImagePlugin import PngInfo
    if DiffEdit_prefs['init_image'].startswith('http'):
      #response = requests.get(DiffEdit_prefs['init_image'])
      #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
      original_img = PILImage.open(requests.get(DiffEdit_prefs['init_image'], stream=True).raw)
    else:
      if os.path.isfile(DiffEdit_prefs['init_image']):
        original_img = PILImage.open(DiffEdit_prefs['init_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your init_image {DiffEdit_prefs['init_image']}")
        return
    #width, height = original_img.size
    #width, height = scale_dimensions(width, height, DiffEdit_prefs['max_size'])
    original_img = center_crop_resize(original_img)

    @torch.no_grad()
    def generate_caption(images, caption_generator, caption_processor):
        text = "a photograph of"
        inputs = caption_processor(images, text, return_tensors="pt").to(device="cuda", dtype=caption_generator.dtype)
        caption_generator.to("cuda")
        outputs = caption_generator.generate(**inputs, max_new_tokens=128)
        # offload caption generator
        caption_generator.to("cpu")
        caption = caption_processor.batch_decode(outputs, skip_special_tokens=True)[0]
        print(f"Caption: {caption}")
        del inputs
        del caption_generator
        return caption
    if not bool(source_prompt):
        prt("Generating Caption from Image with Blip...")
        from transformers import BlipForConditionalGeneration, BlipProcessor
        captioner_id = "Salesforce/blip-image-captioning-base"
        processor = BlipProcessor.from_pretrained(captioner_id)
        blip_model = BlipForConditionalGeneration.from_pretrained(captioner_id, torch_dtype=torch.float16, low_cpu_mem_usage=True)
        source_prompt = generate_caption(original_img, blip_model, processor)
        del processor
        del blip_model
        clear_last()

    flush()
    torch_dtype = torch.float16
    model_id = get_model(prefs['model_ckpt'])['path']
    if pipe_DiffEdit is None:
        from diffusers import DDIMScheduler, DDIMInverseScheduler, StableDiffusionDiffEditPipeline
        try:
            pipe_DiffEdit = StableDiffusionDiffEditPipeline.from_pretrained(
                model_id,
                torch_dtype=torch.float16,
                cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
                safety_checker=None,
            )
            pipe_DiffEdit.scheduler = DDIMScheduler.from_config(pipe_DiffEdit.scheduler.config)
            #pipe_DiffEdit.scheduler = pipeline_scheduler(pipe_DiffEdit)
            pipe_DiffEdit.inverse_scheduler = DDIMInverseScheduler.from_config(pipe_DiffEdit.scheduler.config)
            pipe_DiffEdit.enable_model_cpu_offload()
            pipe_DiffEdit.enable_vae_slicing()
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Couldn't Initialize DiffEdit Pipeline for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    clear_last()
    prt("Generating DiffEdit of your Image...")
    prt(progress)
    autoscroll(False)
    batch_output = os.path.join(stable_dir, DiffEdit_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], DiffEdit_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    random_seed = int(DiffEdit_prefs['seed']) if int(DiffEdit_prefs['seed']) > 0 else random.randint(0,4294967295)

    for i in range(DiffEdit_prefs['num_images']):
        generator = torch.manual_seed(random_seed)
        #generator = torch.manual_seed(random_seed)
        try:
            mask_image = pipe_DiffEdit.generate_mask(
                image=original_img,
                source_prompt=source_prompt,
                target_prompt=target_prompt,
                generator=generator,
            )
            inv_latents = pipe_DiffEdit.invert(prompt=source_prompt, image=original_img, generator=generator).latents
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Couldn't generate mask for DiffEdit your image for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        try:
            images = pipe_DiffEdit(prompt=target_prompt, mask_image=mask_image, image_latents=inv_latents, negative_prompt=source_prompt, num_inference_steps=DiffEdit_prefs['num_inference_steps'], inpaint_strength=DiffEdit_prefs['strength'], guidance_scale=DiffEdit_prefs['guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).images
            #images = pipe_DiffEdit(source_prompt=source_prompt, target_prompt=target_prompt, mask_image=mask_image, image_latents=inv_latents, num_inference_steps=DiffEdit_prefs['num_inference_steps'], inpaint_strength=DiffEdit_prefs['strength'], guidance_scale=DiffEdit_prefs['guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).images
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Couldn't generate DiffEdit your image for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        clear_last()
        clear_last()
        filename = format_filename(DiffEdit_prefs['target_prompt'])
        #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
        num = 0
        for image in images:
            random_seed += num
            fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
            image_path = available_file(os.path.join(stable_dir, DiffEdit_prefs['batch_folder_name']), fname, i)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            image.save(image_path)
            width, height = image.size
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)
            if not DiffEdit_prefs['display_upscaled_image'] or not DiffEdit_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, DiffEdit_prefs, f"DiffEdit", model_id, random_seed)
                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if DiffEdit_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=DiffEdit_prefs["enlarge_scale"])
                image_path = upscaled_path
                save_metadata(image_path, DiffEdit_prefs, f"DiffEdit", model_id, random_seed)
                if DiffEdit_prefs['display_upscaled_image']:
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(DiffEdit_prefs["enlarge_scale"]), height=height * float(DiffEdit_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            #TODO: PyDrive
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], DiffEdit_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], DiffEdit_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
            num += 1
        random_seed += 1
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_null_text(page):
    global null_text_prefs, prefs, status, pipe_null_text, text_encoder_null_text
    if not check_diffusers(page): return
    if not bool(null_text_prefs['init_image']):
      alert_msg(page, "You must provide the Original Image and the Mask Image to process...")
      return
    if not bool(null_text_prefs['base_prompt']) or not bool(null_text_prefs['target_prompt']):
      alert_msg(page, "You must provide a base prompt describing image and target prompt to process...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.Null_Text.controls.append(line)
      page.Null_Text.update()
    def clear_last(lines=1):
      clear_line(page.Null_Text, lines=lines)
    def autoscroll(scroll=True):
      page.Null_Text.auto_scroll = scroll
      page.Null_Text.update()
    def clear_list():
      page.Null_Text.controls = page.Null_Text.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = null_text_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    def center_crop_resize(im):
        width, height = im.size
        d = min(width, height)
        left = (width - d) / 2
        upper = (height - d) / 2
        right = (width + d) / 2
        lower = (height + d) / 2
        return im.crop((left, upper, right, lower)).resize((null_text_prefs['max_size'], null_text_prefs['max_size']))
    autoscroll(True)
    clear_list()
    prt(Installing("Installing Null-Text Inversion Pipeline... See console for progress."))
    import requests, random
    from io import BytesIO
    from PIL import ImageOps
    from PIL.PngImagePlugin import PngInfo
    diffusers_dir = os.path.join(root_dir, "diffusers")
    if not os.path.exists(diffusers_dir) or force_update("diffusers_dir"):
      os.chdir(root_dir)
      #installer.status("...clone diffusers")
      run_process("git clone https://github.com/Skquark/diffusers.git", realtime=False, cwd=root_dir)
    if str(os.path.join(diffusers_dir, "examples")) not in sys.path:
      sys.path.append(os.path.join(diffusers_dir, "examples"))
      sys.path.append(os.path.join(diffusers_dir, "examples", "community"))
    if null_text_prefs['init_image'].startswith('http'):
      #response = requests.get(null_text_prefs['init_image'])
      #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
      original_img = PILImage.open(requests.get(null_text_prefs['init_image'], stream=True).raw)
    else:
      if os.path.isfile(null_text_prefs['init_image']):
        original_img = PILImage.open(null_text_prefs['init_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your init_image {null_text_prefs['init_image']}")
        return
    width, height = original_img.size
    width, height = scale_dimensions(width, height, null_text_prefs['max_size'])
    original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
    input_image = os.path.join(uploads_dir, os.path.basename(null_text_prefs['init_image']))
    original_img.save(input_image)
    #original_img = center_crop_resize(original_img)
    clear_pipes('null_text')
    torch_dtype = torch.float32
    #model_id = get_model(prefs['model_ckpt'])['path']
    model_id = "runwayml/stable-diffusion-v1-5"
    if pipe_null_text is None:
        from diffusers.schedulers import DDIMScheduler
        #from examples.community.pipeline_null_text_inversion import NullTextPipeline
        from pipeline_null_text_inversion import NullTextPipeline
        try:
            scheduler = DDIMScheduler(num_train_timesteps=null_text_prefs['num_train_timesteps'], beta_start=0.00085, beta_end=0.0120, beta_schedule="scaled_linear", steps_offset=1, clip_sample=False)
            pipe_null_text = NullTextPipeline.from_pretrained(model_id, scheduler = scheduler, torch_dtype=torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None).to(torch_device)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Couldn't Initialize null_text Pipeline for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    clear_last()
    prt("Generating Null-Text Inversion of your Image... See console for progress.")
    prt(progress)
    autoscroll(False)
    batch_output = os.path.join(stable_dir, null_text_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], null_text_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    random_seed = int(null_text_prefs['seed']) if int(null_text_prefs['seed']) > 0 else random.randint(0,4294967295)
    steps = null_text_prefs['num_inference_steps']
    for i in range(null_text_prefs['num_images']):
        generator = torch.Generator(device=torch_device).manual_seed(random_seed)
        #generator = torch.manual_seed(random_seed)
        try:
            inverted_latent, uncond = pipe_null_text.invert(input_image, null_text_prefs['base_prompt'], num_inner_steps=null_text_prefs['num_inner_steps'], early_stop_epsilon= 1e-5, num_inference_steps = steps)
            images = pipe_null_text(null_text_prefs['target_prompt'], uncond, inverted_latent, guidance_scale=null_text_prefs['guidance_scale'], num_inference_steps=steps).images #.images[0].save(input_image+".output.jpg"), generator=generator
            print(f"images type: {type(images)} {images} \n0: {images[0]}")
            #images = pipe_null_text(base_prompt=null_text_prefs['base_prompt'], target_prompt=null_text_prefs['target_prompt'], image=original_img, num_inference_steps=null_text_prefs['num_inference_steps'], strength=null_text_prefs['strength'], guidance_scale=null_text_prefs['guidance_scale'], generator=generator)#.images
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Couldn't null_text Edit your image for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        clear_last()
        clear_last()
        filename = format_filename(null_text_prefs['target_prompt'])
        #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
        num = 0
        for image in images:
            random_seed += num
            fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
            image_path = available_file(os.path.join(stable_dir, null_text_prefs['batch_folder_name']), fname, i)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            #if isinstance(image, str):
            #  print(image)
            #  image = PILImage.open(image)
            image.save(image_path)
            width, height = image.size
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)
            if not null_text_prefs['display_upscaled_image'] or not null_text_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, null_text_prefs, f"Null-Text Inversion", model_id, random_seed)
                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if null_text_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=null_text_prefs["enlarge_scale"])
                image_path = upscaled_path
                save_metadata(image_path, null_text_prefs, f"Null-Text Inversion", model_id, random_seed)
                if null_text_prefs['display_upscaled_image']:
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(null_text_prefs["enlarge_scale"]), height=height * float(null_text_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            #TODO: PyDrive
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], null_text_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], null_text_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            time.sleep(0.2)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
            num += 1
        random_seed += 1
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_CLIPstyler(page):
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.CLIPstyler.controls.append(line)
      page.CLIPstyler.update()
    def clear_last(lines=1):
      clear_line(page.CLIPstyler, lines=lines)
    def autoscroll(scroll=True):
      page.CLIPstyler.auto_scroll = scroll
      page.CLIPstyler.update()
    def clear_list():
      page.CLIPstyler.controls = page.CLIPstyler.controls[:1]
    clipstyler_dir = os.path.join(root_dir, "CLIPstyler")
    if not os.path.exists(clipstyler_dir):
          os.mkdir(clipstyler_dir)
    if CLIPstyler_prefs['original_image'].startswith('http'):
        import requests
        from io import BytesIO
        response = requests.get(CLIPstyler_prefs['original_image'])
        fpath = os.path.join(clipstyler_dir, CLIPstyler_prefs['original_image'].rpartition(slash)[2])
        original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
        #width, height = original_img.size
        #width, height = scale_dimensions(width, height)
        original_img = original_img.resize((CLIPstyler_prefs['width'], CLIPstyler_prefs['height']), resample=PILImage.Resampling.LANCZOS).convert("RGB")
        original_img.save(fpath)
        CLIPstyler_prefs['image_dir'] = fpath
    elif os.path.isfile(CLIPstyler_prefs['original_image']):
        fpath = os.path.join(clipstyler_dir, CLIPstyler_prefs['original_image'].rpartition(slash)[2])
        original_img = PILImage.open(CLIPstyler_prefs['original_image'])
        #width, height = original_img.size
        #width, height = scale_dimensions(width, height)
        original_img = original_img.resize((CLIPstyler_prefs['width'], CLIPstyler_prefs['height']), resample=PILImage.Resampling.LANCZOS).convert("RGB")
        original_img.save(fpath)
        CLIPstyler_prefs['image_dir'] = fpath
    else:
        alert_msg(page, "Couldn't find a valid File, Path or URL...")
        return
    progress = ProgressBar(bar_height=8)
    autoscroll(True)
    clear_list()
    prt(Installing("Downloading CLIP-Styler Packages..."))
    run_process("pip install ftfy regex tqdm", realtime=False, page=page)
    run_sp("pip install git+https://github.com/openai/CLIP.git", realtime=False)
    #os.chdir(clipstyler_dir)
    os.chdir(root_dir)
    run_sp("pip install git+https://github.com/cyclomon/CLIPstyler.git", realtime=True)
    #!git clone https://github.com/cyclomon/CLIPstyler/
    run_sp(f"git clone https://github.com/cyclomon/CLIPstyler/ {clipstyler_dir}", realtime=True)
    os.chdir(root_dir)
    #run_process(f"git clone https://github.com/paper11667/CLIPstyler/ {clipstyler_dir}", realtime=False, page=page)
    if clipstyler_dir not in sys.path:
        sys.path.append(clipstyler_dir)

    import torch.nn
    import torch.optim as optim
    from torchvision import transforms, models
    import StyleNet
    import utils
    import clip
    import torch.nn.functional as F
    from template import imagenet_templates
    from torchvision import utils as vutils
    import argparse
    from torchvision.transforms.functional import adjust_contrast
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    VGG = models.vgg19(pretrained=True).features
    VGG.to(device)
    save_dir = stable_dir
    if bool(CLIPstyler_prefs['batch_folder_name']):
        save_dir = os.path.join(stable_dir, CLIPstyler_prefs['batch_folder_name'])
    new_file = format_filename(CLIPstyler_prefs["prompt_text"])
    images = []
    for parameter in VGG.parameters():
        parameter.requires_grad_(False)

    def img_denormalize(image):
        mean=torch.tensor([0.485, 0.456, 0.406]).to(device)
        std=torch.tensor([0.229, 0.224, 0.225]).to(device)
        mean = mean.view(1,-1,1,1)
        std = std.view(1,-1,1,1)
        image = image*std +mean
        return image

    def img_normalize(image):
        mean=torch.tensor([0.485, 0.456, 0.406]).to(device)
        std=torch.tensor([0.229, 0.224, 0.225]).to(device)
        mean = mean.view(1,-1,1,1)
        std = std.view(1,-1,1,1)
        image = (image-mean)/std
        return image

    def clip_normalize(image,device):
        image = F.interpolate(image,size=224,mode='bicubic')
        mean=torch.tensor([0.48145466, 0.4578275, 0.40821073]).to(device)
        std=torch.tensor([0.26862954, 0.26130258, 0.27577711]).to(device)
        mean = mean.view(1,-1,1,1)
        std = std.view(1,-1,1,1)
        image = (image-mean)/std
        return image

    def get_image_prior_losses(inputs_jit):
        diff1 = inputs_jit[:, :, :, :-1] - inputs_jit[:, :, :, 1:]
        diff2 = inputs_jit[:, :, :-1, :] - inputs_jit[:, :, 1:, :]
        diff3 = inputs_jit[:, :, 1:, :-1] - inputs_jit[:, :, :-1, 1:]
        diff4 = inputs_jit[:, :, :-1, :-1] - inputs_jit[:, :, 1:, 1:]
        loss_var_l2 = torch.norm(diff1) + torch.norm(diff2) + torch.norm(diff3) + torch.norm(diff4)
        return loss_var_l2

    from argparse import Namespace
    source = CLIPstyler_prefs['source']

    training_args = {
        "lambda_tv": 2e-3,
        "lambda_patch": 9000,
        "lambda_dir": 500,
        "lambda_c": 150,
        "crop_size": CLIPstyler_prefs['crop_size'],
        "num_crops":CLIPstyler_prefs['num_crops'],
        "img_height":CLIPstyler_prefs['height'],
        "img_width":CLIPstyler_prefs['width'],
        "max_step":CLIPstyler_prefs['training_iterations'],
        "lr":5e-4,
        "thresh":0.7,
        "content_path":CLIPstyler_prefs['image_dir'],
        "text":CLIPstyler_prefs['prompt_text']
    }

    style_args = Namespace(**training_args)

    def compose_text_with_templates(text: str, templates=imagenet_templates) -> list:
        return [template.format(text) for template in templates]

    content_path = style_args.content_path
    content_image = utils.load_image2(content_path, img_height=style_args.img_height,img_width =style_args.img_width)
    content_image = content_image.to(device)
    content_features = utils.get_features(img_normalize(content_image), VGG)
    target = content_image.clone().requires_grad_(True).to(device)
    style_net = StyleNet.UNet()
    style_net.to(device)

    style_weights = {'conv1_1': 0.1,
                    'conv2_1': 0.2,
                    'conv3_1': 0.4,
                    'conv4_1': 0.8,
                    'conv5_1': 1.6}
    clear_last()
    prt("Generating Stylized Image from your source... Check console output for progress.")
    prt(progress)
    autoscroll(False)

    content_weight = style_args.lambda_c
    show_every = 20
    optimizer = optim.Adam(style_net.parameters(), lr=style_args.lr)
    s_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)
    steps = style_args.max_step
    content_loss_epoch = []
    style_loss_epoch = []
    total_loss_epoch = []
    output_image = content_image
    m_cont = torch.mean(content_image,dim=(2,3),keepdim=False).squeeze(0)
    m_cont = [m_cont[0].item(),m_cont[1].item(),m_cont[2].item()]
    cropper = transforms.Compose([transforms.RandomCrop(style_args.crop_size)])
    augment = transforms.Compose([
        transforms.RandomPerspective(fill=0, p=1,distortion_scale=0.5),
        transforms.Resize(224)
    ])

    clip_model, preprocess = clip.load('ViT-B/32', device, jit=False)
    prompt = style_args.text

    with torch.no_grad():
        template_text = compose_text_with_templates(prompt, imagenet_templates)
        tokens = clip.tokenize(template_text).to(device)
        text_features = clip_model.encode_text(tokens).dcrop_sizech()
        text_features = text_features.mean(axis=0, keepdim=True)
        text_features /= text_features.norm(dim=-1, keepdim=True)
        template_source = compose_text_with_templates(source, imagenet_templates)
        tokens_source = clip.tokenize(template_source).to(device)
        text_source = clip_model.encode_text(tokens_source).dcrop_sizech()
        text_source = text_source.mean(axis=0, keepdim=True)
        text_source /= text_source.norm(dim=-1, keepdim=True)
        source_features = clip_model.encode_image(clip_normalize(content_image,device))
        source_features /= (source_features.clone().norm(dim=-1, keepdim=True))


    num_crops = style_args.num_crops
    for epoch in range(0, steps+1):
        s_scheduler.step()
        target = style_net(content_image,use_sigmoid=True).to(device)
        target.requires_grad_(True)
        target_features = utils.get_features(img_normalize(target), VGG)
        content_loss = 0
        content_loss += torch.mean((target_features['conv4_2'] - content_features['conv4_2']) ** 2)
        content_loss += torch.mean((target_features['conv5_2'] - content_features['conv5_2']) ** 2)
        loss_patch=0
        img_proc =[]
        for n in range(num_crops):
            target_crop = cropper(target)
            target_crop = augment(target_crop)
            img_proc.append(target_crop)
        img_proc = torch.cat(img_proc,dim=0)
        img_aug = img_proc
        image_features = clip_model.encode_image(clip_normalize(img_aug,device))
        image_features /= (image_features.clone().norm(dim=-1, keepdim=True))
        img_direction = (image_features-source_features)
        img_direction /= img_direction.clone().norm(dim=-1, keepdim=True)
        text_direction = (text_features-text_source).repeat(image_features.size(0),1)
        text_direction /= text_direction.norm(dim=-1, keepdim=True)
        loss_temp = (1- torch.cosine_similarity(img_direction, text_direction, dim=1))
        loss_temp[loss_temp<style_args.thresh] =0
        loss_patch+=loss_temp.mean()
        glob_features = clip_model.encode_image(clip_normalize(target,device))
        glob_features /= (glob_features.clone().norm(dim=-1, keepdim=True))
        glob_direction = (glob_features-source_features)
        glob_direction /= glob_direction.clone().norm(dim=-1, keepdim=True)
        loss_glob = (1- torch.cosine_similarity(glob_direction, text_direction, dim=1)).mean()
        reg_tv = style_args.lambda_tv*get_image_prior_losses(target)
        total_loss = style_args.lambda_patch*loss_patch + content_weight * content_loss+ reg_tv+ style_args.lambda_dir*loss_glob
        total_loss_epoch.append(total_loss)
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        autoscroll(True)
        if epoch % show_every == 0:
            prt("After %d iters:" % epoch)
            prt('  Total loss: ', total_loss.item())
            prt('  Content loss: ', content_loss.item())
            prt('  patch loss: ', loss_patch.item())
            prt('  dir loss: ', loss_glob.item())
            prt('  TV loss: ', reg_tv.item())

        if epoch % show_every == 0:
            output_image = target.clone()
            output_image = torch.clamp(output_image,0,1)
            output_image = adjust_contrast(output_image,1.5)
            img = utils.im_convert2(output_image)
            save_file = available_file(save_dir, new_file, 1)
            img.save(save_file)
            prt(Row([ImageButton(src=save_file, width=CLIPstyler_prefs['width'], height=CLIPstyler_prefs['height'], subtitle=save_file, show_subtitle=True, page=page)], alignment=MainAxisAlignment.CENTER))
            #prt(Row([Img(src=save_file, width=CLIPstyler_prefs['width'], height=CLIPstyler_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            #prt(Row([Text(save_file)], alignment=MainAxisAlignment.CENTER))
            images.append(save_file)
            #plt.imshow(utils.im_convert2(output_image))
            #plt.show()
        progress.value = (epoch) / steps
        progress.tooltip = f'[{(epoch)} / {steps}]'
        progress.update()
    #clear_last()
    # TODO: ESRGAN and copy to GDrive and Metadata
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_semantic(page):
    global semantic_prefs, prefs, status, pipe_semantic, model_path
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.SemanticGuidance.controls.append(line)
      page.SemanticGuidance.update()
    def clear_last(lines=1):
      clear_line(page.SemanticGuidance, lines=lines)
    def autoscroll(scroll=True):
      page.SemanticGuidance.auto_scroll = scroll
      page.SemanticGuidance.update()
    def clear_list():
      page.SemanticGuidance.controls = page.SemanticGuidance.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = semantic_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    prt(Installing("Installing Semantic Guidance Pipeline..."))

    clear_pipes('semantic')
    if pipe_semantic is None:
        from diffusers import SemanticStableDiffusionPipeline
        pipe_semantic = SemanticStableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
        pipe_semantic = pipeline_scheduler(pipe_semantic)
        pipe_semantic = optimize_pipe(pipe_semantic, vae_slicing=False, freeu=False)
        pipe_semantic.set_progress_bar_config(disable=True)
    else:
        pipe_semantic = pipeline_scheduler(pipe_semantic)
    clear_last()
    prt("Generating Semantic Guidance of your Image...")
    prt(progress)
    autoscroll(False)
    batch_output = os.path.join(stable_dir, semantic_prefs['batch_folder_name'])
    makedir(batch_output)
    batch_output = os.path.join(prefs['image_output'], semantic_prefs['batch_folder_name'])
    makedir(batch_output)
    random_seed = get_seed(semantic_prefs['seed'])
    generator = torch.Generator(device=torch_device).manual_seed(random_seed)
    #generator = torch.manual_seed(random_seed)
    width = semantic_prefs['width']
    height = semantic_prefs['height']
    editing_prompt = []
    edit_warmup_steps = []
    edit_guidance_scale = []
    edit_threshold = []
    edit_weights = []
    reverse_editing_direction = []
    for ep in semantic_prefs['editing_prompts']:
        editing_prompt.append(ep['editing_prompt'])
        edit_warmup_steps.append(int(ep['edit_warmup_steps']))
        edit_guidance_scale.append(ep['edit_guidance_scale'])
        edit_threshold.append(ep['edit_threshold'])
        edit_weights.append(ep['edit_weights'])
        reverse_editing_direction.append(ep['reverse_editing_direction'])
    try:
      #print(f"prompt={semantic_prefs['prompt']}, negative_prompt={semantic_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={semantic_prefs['edit_momentum_scale']}, edit_mom_beta={semantic_prefs['edit_mom_beta']}, num_inference_steps={semantic_prefs['num_inference_steps']}, eta={semantic_prefs['eta']}, guidance_scale={semantic_prefs['guidance_scale']}")
      images = pipe_semantic(prompt=semantic_prefs['prompt'], negative_prompt=semantic_prefs['negative_prompt'], editing_prompt=editing_prompt, edit_warmup_steps=edit_warmup_steps, edit_guidance_scale=edit_guidance_scale, edit_threshold=edit_threshold, edit_weights=edit_weights, reverse_editing_direction=reverse_editing_direction, edit_momentum_scale=semantic_prefs['edit_momentum_scale'], edit_mom_beta=semantic_prefs['edit_mom_beta'], num_inference_steps=semantic_prefs['num_inference_steps'], eta=semantic_prefs['eta'], guidance_scale=semantic_prefs['guidance_scale'], width=width, height=height, num_images_per_prompt=semantic_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images
    except Exception as e:
      clear_last()
      alert_msg(page, f"ERROR: Couldn't Semantic Guidance your image for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
      return
    clear_last()
    clear_last()
    filename = f"{prefs['file_prefix']}{format_filename(semantic_prefs['prompt'])}"
    filename = filename[:int(prefs['file_max_length'])]
    #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
    autoscroll(True)
    num = 0
    for image in images:
        random_seed += num
        fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
        image_path = available_file(os.path.join(stable_dir, semantic_prefs['batch_folder_name']), fname, num)
        unscaled_path = image_path
        output_file = image_path.rpartition(slash)[2]
        image.save(image_path)
        out_path = os.path.dirname(image_path)
        upscaled_path = os.path.join(out_path, output_file)
        if not semantic_prefs['display_upscaled_image'] or not semantic_prefs['apply_ESRGAN_upscale']:
            save_metadata(image_path, semantic_prefs, f"Semantic Guidance", model_path, random_seed, scheduler=True)
            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
        if semantic_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            upscale_image(image_path, upscaled_path, scale=semantic_prefs["enlarge_scale"])
            image_path = upscaled_path
            save_metadata(image_path, semantic_prefs, f"Semantic Guidance", model_path, random_seed, scheduler=True)
            if semantic_prefs['display_upscaled_image']:
                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(semantic_prefs["enlarge_scale"]), height=height * float(semantic_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        #TODO: PyDrive
        if storage_type == "Colab Google Drive":
            new_file = available_file(os.path.join(prefs['image_output'], semantic_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        elif bool(prefs['image_output']):
            new_file = available_file(os.path.join(prefs['image_output'], semantic_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
        num += 1
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_demofusion(page, from_list=False, with_params=False):
    global demofusion_prefs, pipe_demofusion, prefs
    if not check_diffusers(page): return
    demofusion_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            demofusion_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':p['init_image'] if bool(p['init_image']) else demofusion_prefs['init_image'], 'guidance_scale':demofusion_prefs['guidance_scale'], 'steps':demofusion_prefs['steps'], 'width':demofusion_prefs['width'], 'height':demofusion_prefs['height'], 'num_images':demofusion_prefs['num_images'], 'seed':demofusion_prefs['seed']})
        else:
            demofusion_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':p['init_image'] if bool(p['init_image']) else demofusion_prefs['init_image'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(demofusion_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      demofusion_prompts.append({'prompt': demofusion_prefs['prompt'], 'negative_prompt':demofusion_prefs['negative_prompt'], 'init_image':demofusion_prefs['init_image'], 'guidance_scale':demofusion_prefs['guidance_scale'], 'steps':demofusion_prefs['steps'], 'width':demofusion_prefs['width'], 'height':demofusion_prefs['height'], 'num_images':demofusion_prefs['num_images'], 'seed':demofusion_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.DemoFusion.controls.append(line)
        if update:
          page.DemoFusion.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.DemoFusion, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
      else:
        page.DemoFusion.auto_scroll = scroll
        page.DemoFusion.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.DemoFusion.controls = page.DemoFusion.controls[:1]
    #progress = ProgressBar(bar_height=8)
    
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing DemoFusion Engine & Models... See console for progress.")
    prt(installer)
    clear_pipes("demofusion")
    #from pipeline_demofusion_sdxl import DemoFusionSDXLPipeline
    #model_ckpt = get_SDXL_model(prefs['SDXL_model'])['path']
    model_ckpt = "stabilityai/stable-diffusion-xl-base-1.0"
    if pipe_demofusion == None:
        #clear_pipes('demofusion')
        try:
            from diffusers import DiffusionPipeline, AutoencoderKL
            #vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)
            pipe_demofusion = DiffusionPipeline.from_pretrained(model_ckpt, custom_pipeline="pipeline_demofusion_sdxl", custom_revision="main", torch_dtype=torch.float16, variant="fp16", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            if prefs['enable_torch_compile']:
                installer.status(f"...Torch compiling unet")
                #pipe_demofusion.unet.to(memory_format=torch.channels_last)
                pipe_demofusion.unet = torch.compile(pipe_demofusion.unet, mode="reduce-overhead", fullgraph=True)
                pipe_demofusion = pipe_demofusion.to("cuda")
            elif demofusion_prefs['cpu_offload']:
                pipe_demofusion.enable_model_cpu_offload()
            else:
                pipe_demofusion = pipe_demofusion.to("cuda")
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing DemoFusion...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    else:
        clear_pipes('demofusion')
    clear_last()
    txt2img_output = os.path.join(stable_dir, demofusion_prefs['batch_folder_name'])
    makedir(txt2img_output)
    batch_output = os.path.join(prefs['image_output'], demofusion_prefs['batch_folder_name'])
    makedir(batch_output)
    s = "" if len(demofusion_prompts) == 1 or pr['num_images'] == 1 else "s"
    progress = Progress(f"Generating your DemoFusion Image{s}...")
    total_steps = demofusion_prefs['steps']
    phase = 0
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
        callback_fnc.has_been_called = True
        nonlocal progress, total_steps, phase
        if step == 0:
            phase += 1
        stage = f'...Phase {phase} {"Decoding" if (step + 1) == total_steps else "Denoising"}'
        #total_steps = len(latents)
        percent = (step +1)/ total_steps
        progress.progress.value = percent
        progress.progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
        progress.progress.update()
        progress.status(stage)
    def load_and_process_image(pil_image, w=1024, h=1024):
        from torchvision import transforms
        transform = transforms.Compose([transforms.Resize((w, h)), transforms.ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])
        image = transform(pil_image)
        image = image.unsqueeze(0).half()
        return image
    for pr in demofusion_prompts:
        phase = 0
        prt(progress)
        autoscroll(False)
        image_args = {}
        if bool(pr['init_image']):
            from PIL import ImageOps
            fname = pr['init_image'].rpartition(slash)[2]
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
            image_lr = load_and_process_image(init_img, pr['width'], pr['height']).to('cuda')
            image_args = {'image_lr': image_lr}
        total_steps = pr['steps']
        random_seed = get_seed(pr['seed'])
        generator = torch.Generator(device="cuda").manual_seed(random_seed)
        try:
            images = pipe_demofusion(
                prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                sigma=demofusion_prefs['sigma'],
                stride=demofusion_prefs['stride'],
                view_batch_size=demofusion_prefs['view_batch_size'],
                cosine_scale_1=demofusion_prefs['cosine_scale_1'],
                cosine_scale_2=demofusion_prefs['cosine_scale_2'],
                cosine_scale_3=demofusion_prefs['cosine_scale_3'],
                multi_decoder=demofusion_prefs['multi_decoder'],
                show_image=demofusion_prefs['show_image'],
                num_images_per_prompt=pr['num_images'],
                height=pr['height'],
                width=pr['width'],
                num_inference_steps=pr['steps'],
                guidance_scale=pr['guidance_scale'],
                generator=generator,
                callback=callback_fnc,
                **image_args,
            )
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        clear_last()
        autoscroll(True)
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{demofusion_prefs["file_prefix"]}{fname}{seed_suffix}'
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            if not demofusion_prefs['display_upscaled_image'] or not demofusion_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, demofusion_prefs, f"DemoFusion", model_ckpt, random_seed, extra=pr)
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': demofusion_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)

            if demofusion_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=demofusion_prefs["enlarge_scale"], faceenhance=demofusion_prefs["face_enhance"])
                image_path = upscaled_path
                save_metadata(image_path, demofusion_prefs, f"DemoFusion", model_ckpt, random_seed, extra=pr)
                if demofusion_prefs['display_upscaled_image']:
                    prt(Row([Img(src=asset_dir(upscaled_path), width=pr['width'] * float(demofusion_prefs["enlarge_scale"]), height=pr['height'] * float(demofusion_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "Colab Google Drive":
                new_file = available_file(batch_output, fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(batch_output, fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_image2text(page):
    global fuyu_tokenizer, fuyu_model, fuyu_processor, moondream2_tokenizer, moondream2_model
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.image2text_output.controls.append(line)
      page.image2text_output.update()
    def clear_last(lines=1):
      clear_line(page.image2text_output, lines=lines)
    if image2text_prefs['use_AIHorde'] and not bool(prefs['AIHorde_api_key']):
      alert_msg(page, "To use AIHorde API service, you must provide an API key in settings")
      return
    progress = ProgressBar(bar_height=8)
    if len(image2text_prefs['images']) < 1:
      alert_msg(page, "You must add one or more files to interrogate first... ")
      return
    if image2text_prefs['method'] == "Fuyu-8B":
        if not check_diffusers(page): return
        installer = Installing("Downloading Fuyu-8B Image2Text Model...")
        prt(installer)
        clear_pipes("fuyu")
        from transformers import AutoTokenizer, FuyuForCausalLM, FuyuImageProcessor, FuyuProcessor
        import torch
        dtype = torch.bfloat16
        model_id = "adept/fuyu-8b"
        if fuyu_tokenizer == None:
            installer.status("...adept/fuyu-8b Tokenizer")
            fuyu_tokenizer = AutoTokenizer.from_pretrained(model_id)
            installer.status("...loading model (see console)")
            fuyu_model = FuyuForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype=dtype, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            installer.status("...FuyuImageProcessor")
            fuyu_processor = FuyuProcessor(image_processor=FuyuImageProcessor(), tokenizer=fuyu_tokenizer)
        folder_path = image2text_prefs['folder_path']
        prompt_mode = "What is happening in this image?" if 'Detailed' in image2text_prefs['fuyu_mode'] else "Generate a coco-style caption with art style.\n" if 'Simple' in image2text_prefs['fuyu_mode'] else image2text_prefs['question']
        i2t_prompts = []
        clear_last()
        for file in image2text_prefs['images']:
            prt(f"Interrogating Images to Describe Prompt...")
            prt(progress)
            image = PILImage.open(os.path.join(folder_path, file)).convert('RGB')
            try:
                model_inputs = fuyu_processor(text=prompt_mode, images=[image])
                model_inputs = {k: v.to(dtype=dtype if torch.is_floating_point(v) else v.dtype, device=torch_device) for k,v in model_inputs.items()}
                generation_output = fuyu_model.generate(**model_inputs, max_new_tokens=60)
                prompt_len = model_inputs["input_ids"].shape[-1]
                prompt = fuyu_tokenizer.decode(generation_output[0][prompt_len:], skip_special_tokens=True).strip()
            except Exception as e:
                clear_last()
                alert_msg(page, f"ERROR: Couldn't run Fuyu generate for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            clear_last()
            clear_last()
            i2t_prompts.append(prompt)
            page.add_to_image2text(prompt)
    elif image2text_prefs['method'] == "Moondream 2":
        if not check_diffusers(page): return
        installer = Installing("Downloading Moondream 2 Image2Text Model...")
        prt(installer)
        clear_pipes("moondream2")
        pip_install("timm einops", installer=installer)
        from transformers import AutoModelForCausalLM, AutoTokenizer
        model_id = "vikhyatk/moondream2"
        revision = "2024-03-06"
        if moondream2_tokenizer == None:
            installer.status("...loading model (see console)")
            moondream2_model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, revision=revision, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            installer.status("...vikhyatk/moondream2 Tokenizer")
            moondream2_tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)
        folder_path = image2text_prefs['folder_path']
        prompt_mode = "What is happening in this image? Describe it in visual details, artistic style, related artist names, colors and composition." if 'Detailed' in image2text_prefs['openai_mode'] else "Generate an image prompt with art styles, Poetic Captions, flowing adjectives, and detailed captions." if 'Poetic' in image2text_prefs['openai_mode'] else "Generate a technical detailed caption, describing all subjects, adjectives, styles, observations, colors and technical details to recreate." if 'Technical' in image2text_prefs['openai_mode'] else "Generate a coco-style caption with art style.\n" if 'Simple' in image2text_prefs['openai_mode'] else "Describe the style of this art, with a list of all the known artists it resembles and artistic styles it uses, then lay out the image composition with nouns and descriptive adjectives." if 'Artistic' in image2text_prefs['openai_mode'] else image2text_prefs['question']
        i2t_prompts = []
        clear_last()
        for file in image2text_prefs['images']:
            prt(f"Interrogating Images to Describe Prompt...")
            prt(progress)
            image = PILImage.open(os.path.join(folder_path, file)).convert('RGB')
            try:
                enc_image = moondream2_model.encode_image(image)
                prompt = moondream2_model.answer_question(enc_image, prompt_mode, moondream2_tokenizer).strip()
            except Exception as e:
                clear_last()
                alert_msg(page, f"ERROR: Couldn't run Moondream 2 generate for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            clear_last()
            clear_last()
            i2t_prompts.append(prompt)
            page.add_to_image2text(prompt)
    elif image2text_prefs['method'] == "Google Gemini Pro":
        installer = Installing("Installing Google MakerSuite Library......")
        prt(installer)
        if not bool(prefs['PaLM_api_key']):
          alert_msg(page, "You must provide your Google Gemini MakerSuite API key in Settings first")
          return
        try:
          import google.generativeai as genai
          if force_update("generativeai"): raise ModuleNotFoundError("Forcing update")
        except:
          run_sp("pip install --upgrade google-generativeai", realtime=False)
          import google.generativeai as genai
          pass
        try:
          genai.configure(api_key=prefs['PaLM_api_key'])
        except:
          alert_msg(page, "Invalid Google Gemini API Key. Change in Settings...")
          return
        gemini_model = genai.GenerativeModel(model_name='gemini-pro-vision')
        folder_path = image2text_prefs['folder_path']
        prompt_mode = "What is happening in this image? Describe it in visual details, artistic style, related artist names, colors and composition." if 'Detailed' in image2text_prefs['gemini_mode'] else "Generate an image prompt with art styles, Poetic Captions, flowing adjectives, and detailed captions." if 'Poetic' in image2text_prefs['gemini_mode'] else "Generate a technical detailed caption, describing all subjects, adjectives, styles, observations, colors and technical details to recreate." if 'Technical' in image2text_prefs['gemini_mode'] else "Generate a coco-style caption with art style.\n" if 'Simple' in image2text_prefs['gemini_mode'] else "Describe the style of this art, with a list of all the known artists it resembles and artistic styles it uses, then lay out the image composition with nouns and descriptive adjectives." if 'Artistic' in image2text_prefs['gemini_mode'] else image2text_prefs['question']
        i2t_prompts = []
        clear_last()
        for file in image2text_prefs['images']:
            prt(f"Interrogating Images to Describe Prompt...")
            prt(progress)
            image = PILImage.open(os.path.join(folder_path, file)).convert('RGB')
            try:
                response = gemini_model.generate_content([prompt_mode, image], stream=True, generation_config={'max_output_tokens': 1024})
                response.resolve()
                prompt = response.text
                prompt = prompt.replace('*', '').replace('\n', ' ').strip()
            except Exception as e:
                clear_last()
                alert_msg(page, f"ERROR: Couldn't run Google Gemini Pro Vision request for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            clear_last()
            clear_last()
            i2t_prompts.append(prompt)
            page.add_to_image2text(prompt)
    elif "OpenAI GPT-4" in image2text_prefs['method']:
        installer = Installing("Installing OpenAI API Library...")
        prt(installer)
        gpt_model = "gpt-4-vision-preview" if image2text_prefs['method'] == "OpenAI GPT-4 Vision" else "gpt-4o"
        if not bool(prefs['OpenAI_api_key']):
          alert_msg(page, "You must provide your OpenAI API key in Settings first")
          return
        try:
          import openai
          if force_update("openai"): raise ModuleNotFoundError("Forcing update")
        except ModuleNotFoundError:
          run_sp("pip install --upgrade openai -qq", realtime=False)
          pass
        finally:
          import openai
          import requests
        try:
          from openai import OpenAI
          openai_client = OpenAI(api_key=prefs['OpenAI_api_key'])
        except:
          alert_msg(page, "Invalid OpenAI API Key. Change in Settings...")
          return
        def encode_image(image_path):
            import base64
            with open(image_path, "rb") as image_file:
              return base64.b64encode(image_file.read()).decode('utf-8')
        folder_path = image2text_prefs['folder_path']
        prompt_mode = "What is happening in this image? Describe it in visual details, artistic style, related artist names, colors and composition." if 'Detailed' in image2text_prefs['openai_mode'] else "Generate an image prompt with art styles, Poetic Captions, flowing adjectives, and detailed captions." if 'Poetic' in image2text_prefs['openai_mode'] else "Generate a technical detailed caption, describing all subjects, adjectives, styles, observations, colors and technical details to recreate." if 'Technical' in image2text_prefs['openai_mode'] else "Generate a coco-style caption with art style.\n" if 'Simple' in image2text_prefs['openai_mode'] else "Describe the style of this art, with a list of all the known artists it resembles and artistic styles it uses, then lay out the image composition with nouns and descriptive adjectives." if 'Artistic' in image2text_prefs['openai_mode'] else image2text_prefs['question']
        i2t_prompts = []
        clear_last()
        for file in image2text_prefs['images']:
            prt(f"Interrogating Images to Describe Prompt...")
            prt(progress)
            image = encode_image(os.path.join(folder_path, file))
            headers = {"Content-Type": "application/json", "Authorization": f"Bearer {prefs['OpenAI_api_key']}"}
            payload = {
              "model": gpt_model,
              "messages": [
                {
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": prompt_mode
                    },
                    {
                      "type": "image_url",
                      "image_url": {
                        "url": f"data:image/jpeg;base64,{image}"
                      }
                    }
                  ]
                }
              ],
              "max_tokens": 1024
            }
            try:
                response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload)
                prompt = dict(response.json())
                prompt = prompt['choices'][0]['message']['content']
                prompt = prompt.replace('*', '').replace('\n', ' ').strip()
            except Exception as e:
                clear_last()
                alert_msg(page, f"ERROR: Couldn't run Google Gemini Pro Vision request for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            clear_last()
            clear_last()
            i2t_prompts.append(prompt)
            page.add_to_image2text(prompt)
    elif "Anthropic Claude 3" in image2text_prefs['method']:
        installer = Installing("Installing Anthropic.ai Claude SDK Library......")
        prt(installer)
        model = "claude-3-sonnet-20240229" if image2text_prefs['method'] == "Anthropic Claude 3 Vision" else "claude-3-5-sonnet-20240620"
        if not bool(prefs['Anthropic_api_key']):
          alert_msg(page, "You must provide your Anthropic.ai Claude API key in Settings first")
          return
        try:
          import anthropic
        except:
          run_sp("pip install --upgrade anthropic", realtime=False)
          import anthropic
          pass
        os.environ["ANTHROPIC_API_KEY"] = prefs['Anthropic_api_key']
        try:
          anthropic_client = anthropic.Anthropic(api_key=prefs['Anthropic_api_key'])
        except:
          alert_msg(page, "Invalid Anthropic API Authentication. Change in Settings...")
          return
        def encode_image(image_path):
            import base64
            with open(image_path, "rb") as image_file:
              return base64.b64encode(image_file.read()).decode('utf-8')
        folder_path = image2text_prefs['folder_path']
        prompt_mode = "What is happening in this image? Describe it in visual details, artistic style, related artist names, colors and composition." if 'Detailed' in image2text_prefs['openai_mode'] else "Generate an image prompt with art styles, Poetic Captions, flowing adjectives, and detailed captions." if 'Poetic' in image2text_prefs['openai_mode'] else "Generate a technical detailed caption, describing all subjects, adjectives, styles, observations, colors and technical details to recreate." if 'Technical' in image2text_prefs['openai_mode'] else "Generate a coco-style caption with art style.\n" if 'Simple' in image2text_prefs['openai_mode'] else "Describe the style of this art, with a list of all the known artists it resembles and artistic styles it uses, then lay out the image composition with nouns and descriptive adjectives." if 'Artistic' in image2text_prefs['openai_mode'] else image2text_prefs['question']
        i2t_prompts = []
        clear_last()
        for file in image2text_prefs['images']:
            prt(f"Interrogating Images to Describe Prompt...")
            prt(progress)
            image = encode_image(os.path.join(folder_path, file))
            file_extension = file.split(".")[-1]
            image_media_type = {"jpg": "image/jpeg", "jpeg": "image/jpeg", "png": "image/png", "gif": "image/gif", "bmp": "image/bmp", "tiff": "image/tiff", "webp": "image/webp", "svg": "image/svg+xml", "ico": "image/x-icon"}.get(file_extension.lower(), "application/octet-stream")
            #image_media_type = "image/jpeg" if file.endswith("jpg") else "image/png"
            try:
                message = anthropic_client.messages.create(
                    model=model,#"claude-3-sonnet-20240229",#"claude-3-opus-20240229",
                    max_tokens=1024,
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "image",
                                    "source": {
                                        "type": "base64",
                                        "media_type": image_media_type,
                                        "data": image,
                                    },
                                },
                                {
                                    "type": "text",
                                    "text": prompt_mode
                                }
                            ],
                        }
                    ],
                )
            except Exception as e:
                alert_msg(page, "ERROR Running Anthropic API Request...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                clear_last()
                return
            #print(message)
            prompt = message.content[0].text
            prompt = prompt.replace('*', '').replace('\n', ' ').strip()
            clear_last()
            clear_last()
            i2t_prompts.append(prompt)
            page.add_to_image2text(prompt)
    elif image2text_prefs['method'] == "BLIP-Interrogation":
        installer = Installing("Downloading Image2Text CLIP-Interrogator Blips...")
        prt(installer)
        '''try:
            if transformers.__version__ != "4.21.3": # Diffusers conflict
              run_process("pip uninstall -y transformers", realtime=False)
        except Exception:
            pass
        run_process("pip install -q transformers==4.21.3 --upgrade --force-reinstall", realtime=False)'''
        #run_sp("pip install --upgrade transformers==4.21.2", realtime=False)
        run_process("pip install ftfy regex tqdm timm fairscale requests", realtime=False)
        import importlib
        importlib.reload(transformers)
        run_process("pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip", realtime=False)
        run_process("pip install -e git+https://github.com/pharmapsychotic/BLIP.git@lib#egg=blip", realtime=False)
        run_process("pip clone https://github.com/pharmapsychotic/clip-interrogator.git", realtime=False)
            #['pip', 'install', 'ftfy', 'gradio', 'regex', 'tqdm', 'transformers==4.21.2', 'timm', 'fairscale', 'requests'],
        #    pass
        # Have to force downgrade of transformers because error with cache_dir, but should upgrade after run
        run_process("pip install clip-interrogator", realtime=False)

        '''def setup():
            install_cmds = [
                ['pip', 'install', 'ftfy', 'gradio', 'regex', 'tqdm', 'transformers==4.21.2', 'timm', 'fairscale', 'requests'],
                ['pip', 'install', 'git+https://github.com/openai/CLIP.git@main#egg=clip'],
                ['pip', 'install', 'git+https://github.com/pharmapsychotic/BLIP.git@lib#egg=blip'],
                ['git', 'clone', 'https://github.com/pharmapsychotic/clip-interrogator.git'],
                ['pip', 'install', 'clip-interrogator'],
            ]
            for cmd in install_cmds:
                print(subprocess.run(cmd, stdout=subprocess.PIPE).stdout.decode('utf-8'))
        setup()'''
        #run_sp("pip install git+https://github.com/openai/CLIP.git", realtime=False)
        import argparse, sys, time
        if 'src/blip' not in sys.path:
            sys.path.append('src/blip')
            sys.path.append('src/clip')
            sys.path.append('clip-interrogator')
        import clip
        import torch
        from clip_interrogator import Interrogator, Config
        clear_last()
        prt("Interrogating Images to Describe Prompt... Check console output for progress.")
        prt(progress)
        try:
            ci = Interrogator(Config())
        except Exception as e:
            clear_last()
            alert_msg(page, "ERROR: Problem running Interrogator. Try running before installing Diffusers...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
            return
        def inference(image, mode):
            nonlocal ci
            image = image.convert('RGB')
            if mode == 'best':
                return ci.interrogate(image)
            elif mode == 'classic':
                return ci.interrogate_classic(image)
            else:
                return ci.interrogate_fast(image)
        folder_path = image2text_prefs['folder_path']
        mode = image2text_prefs['mode'].lower() #'best' #param ["best","classic", "fast"]
        #files = [f for f in os.listdir(folder_path) if f.endswith('.jpg') or  f.endswith('.png')] if os.path.exists(folder_path) else []
        files = image2text_prefs['images']
        clear_last()
        i2t_prompts = []
        for file in files:
            image = PILImage.open(os.path.join(folder_path, file)).convert('RGB')
            prompt = inference(image, mode)
            i2t_prompts.append(prompt)
            page.add_to_image2text(prompt)
            #thumb = image.copy()
            #thumb.thumbnail([256, 256])
            #display(thumb)
            #print(prompt)
        if image2text_prefs['save_csv']:
            if len(i2t_prompts):
                import csv
                csv_path = os.path.join(folder_path, 'img2txt_prompts.csv')
                with open(csv_path, 'w', encoding='utf-8', newline='') as f:
                    w = csv.writer(f, quoting=csv.QUOTE_MINIMAL)
                    w.writerow(['image', 'prompt'])
                    for file, prompt in zip(files, i2t_prompts):
                        w.writerow([file, prompt])

                prt(f"\n\n\nGenerated {len(i2t_prompts)} and saved to {csv_path}, enjoy!")
            else:
                prt(f"Sorry, we couldn't find any images in {folder_path}")
        run_process("pip uninstall -y git+https://github.com/pharmapsychotic/BLIP.git@lib#egg=blip", realtime=False)
        run_process("pip uninstall -y clip-interrogator", realtime=False)
        run_process("pip uninstall -y transformers", realtime=False)
        run_process("pip install --upgrade transformers", realtime=False)
        clear_last()
    elif image2text_prefs['method'] == "AIHorde Crowdsourced":
        if not status['installed_AIHorde']:
          prt(Installing("Installing AIHorde API Library..."))
          get_AIHorde(page)
          clear_last()
        import requests
        from io import BytesIO

        api_host = 'https://aihorde.net/api'
        api_check_url = f"{api_host}/v2/generate/check/"
        api_get_result_url = f"{api_host}/v2/interrogate/status/"
        interrogate_url = f"{api_host}/v2/interrogate/async"
        mode = image2text_prefs['mode'] #'best' #param ["best","classic", "fast"]
        headers = {
            #'Content-Type': 'application/json',
            #'Accept': 'application/json',
            'apikey': prefs['AIHorde_api_key'],
        }
        payload = {
            "forms": [{"name": "caption"}],
            "slow_workers": False if mode == "fast" else True,
        }
        def get_captions(filename):
          with open(filename, 'r') as f:
            lines = f.readlines()
          captions = []
          for line in lines:
            if 'caption result:' in line:
              captions.append(line.split('caption result: ')[1])
          return captions
        def interrogate_line(j, cat):
          l = []
          for t in j[cat]:
            l.append(f"{t['text']}:{round(t['confidence'], 1)}")
          return f"**{cat.capitalize()}:** {', '.join(l)}\n  \n  "
        def get_interrogation(j):
          attr = ""
          for t in j.keys():
            attr += interrogate_line(j, t)
          return attr
        def get_interrogation_cat(j, cat):
          l = []
          for t in j[cat]:
            if t['confidence'] > 3:
              l.append(t['text'])
          if cat == "artist" or cat == "artists":
            return and_list(l)
          else:
            return ', '.join(l)
        def get_interrogation_prompt(j):
          attrs = {}
          for t in j.keys():
            attrs[t] = get_interrogation_cat(j, t)
          full = f", by {attrs['artist' if 'artist' in attrs else 'artists']}, as {attrs['medium' if 'medium' in attrs else 'mediums']}, style of {attrs['flavors' if 'flavors' in attrs else 'flavor']}, {attrs['tags' if 'tags' in attrs else 'tag']}, {attrs['movement' if 'movement' in attrs else 'movements']}, technique of {attrs['techniques' if 'techniques' in attrs else 'technique']}, trending on {attrs['trending' if 'trending' in attrs else 'sites']}"
          return full
        import yaml
        AI_Horde = os.path.join(root_dir, "AI-Horde-CLI")
        cli_response = os.path.join(AI_Horde, "cliRequests.log")
        alchemy_yml = os.path.join(AI_Horde, "cliRequestsData_Alchemy.yml")
        interrogation_txt = os.path.join(AI_Horde, "cliRequestsData_Alchemy.yml_interrogation.txt")
        def make_yml(request):
            alchemy = f'''
filename: "horde_alchemy"
api_key: "{prefs["AIHorde_api_key"]}"
submit_dict:
    trusted_workers: {image2text_prefs["trusted_workers"]}
    slow_workers: {image2text_prefs["slow_workers"]}
    forms:
        - name: "{request}"'''

            al = yaml.safe_load(alchemy)

            with open(alchemy_yml, 'w') as file:
                yaml.dump(al, file)
        if image2text_prefs["request_mode"] == "Caption" or image2text_prefs["request_mode"] == "Interrogation":
            make_yml(image2text_prefs["request_mode"].lower())
        elif image2text_prefs["request_mode"] == "Full Prompt":
            make_yml("caption")
        #print(open('names.yaml').read())
        folder_path = image2text_prefs['folder_path']
        #files = [f for f in image2text_prefs['images'] if f.endswith('.jpg') or  f.endswith('.png')] else []
        #files = [f for f in os.listdir(folder_path) if f.endswith('.jpg') or  f.endswith('.png')] if os.path.exists(folder_path) else []
        #print(f"Files: {len(files)}")
        i2t_prompts = []
        for file in image2text_prefs['images']:
            prt(f"Getting {image2text_prefs['request_mode']} with the Horde to Describe Image...")
            prt(progress)
            stats = Text("Stable Horde API Interrogation ")
            #prt(stats)
            img_file = os.path.join(folder_path, file)
            try:
                run_process(f'python cli_request_alchemy.py --api_key {prefs["AIHorde_api_key"]} --file "{alchemy_yml}" --source_image "{img_file}"', cwd=AI_Horde, realtime=True)
                clear_last()
                clear_last()
                if image2text_prefs["request_mode"] == "Caption":
                    captions = get_captions(cli_response)
                    for r in captions:
                      prompt = to_title(r.strip(), sentence=True)
                      i2t_prompts.append(prompt)
                      page.add_to_image2text(prompt)
                    new_log = available_file(AI_Horde, "cliRequests", 0, ext="log")
                    shutil.move(cli_response, new_log)
                elif image2text_prefs["request_mode"] == "Interrogation":
                    with open(interrogation_txt) as json_file:
                      interrogation = json.load(json_file)
                    prt(Markdown(get_interrogation(interrogation), selectable=True))
                    new_interrogation = available_file(AI_Horde, "cliRequestsData_Alchemy.yml_interrogation", 0, ext="txt")
                    shutil.move(interrogation_txt, new_interrogation)
                    os.remove(cli_response)
                elif image2text_prefs["request_mode"] == "Full Prompt":
                    captions = get_captions(cli_response)
                    make_yml("interrogation")
                    prt(f"Getting Interrogation with the Horde to Describe Image...")
                    prt(progress)
                    run_process(f'python cli_request_alchemy.py --api_key {prefs["AIHorde_api_key"]} --file "{alchemy_yml}" --source_image "{img_file}"', cwd=AI_Horde, realtime=True)
                    clear_last()
                    clear_last()
                    with open(interrogation_txt) as json_file:
                      interrogation = json.load(json_file)
                    prt(Markdown(get_interrogation(interrogation), selectable=True))
                    new_interrogation = available_file(AI_Horde, "cliRequestsData_Alchemy.yml_interrogation", 0, ext="txt")
                    shutil.move(interrogation_txt, new_interrogation)
                    styles = get_interrogation_prompt(interrogation)
                    for r in captions:
                      prompt = to_title(r.strip(), sentence=True)
                      i2t_prompts.append(f"{prompt}{styles}")
                      page.add_to_image2text(f"{prompt}{styles}")
                    new_log = available_file(AI_Horde, "cliRequests", 0, ext="log")
                    shutil.move(cli_response, new_log)
            except Exception as e:
                clear_last()
                alert_msg(page, f"ERROR Interrogating Image...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            ''' API Method, want to go back to if I can upload source_image
            image = PILImage.open(os.path.join(folder_path, file)).convert('RGB')
            buff = io.BytesIO()
            image.save(buff, format="PNG")
            buff.seek(0)
            img_str = io.BufferedReader(buff).read()
            payload['source_image'] = img_str.decode()

            response = requests.post(interrogate_url, headers=headers, json=json.dumps(payload))
            if response != None:
              if response.status_code != 202:
                if response.status_code == 400:
                  alert_msg(page, "Stable Horde-API ERROR: Validation Error...", content=Text(str(response.text)))
                  return
                else:
                  prt(Text(f"Stable Horde-API ERROR {response.status_code}: " + str(response.text), selectable=True))
                  print(payload)
                  continue
            artifacts = json.loads(response.content)
            q_id = artifacts['id']
            print(f"ID: {q_id}")
            elapsed_seconds = 0
            try:
              while True:
                check_response = requests.get(api_check_url + q_id)
                check = json.loads(check_response.content)
                div = check['wait_time'] + elapsed_seconds
                if div == 0: continue
                try:
                  percentage = (1 - check['wait_time'] / div)
                except Exception:
                  continue
                pb.value = percentage
                pb.update()
                status_txt = f"Stable Horde API Interrogation - Queued Position: {check['queue_position']} - Waiting: {check['waiting']} - Wait Time: {check['wait_time']}"
                stats.value = status_txt #str(check)
                stats.update()
                if bool(check['done']):
                  break
                time.sleep(1)
                elapsed_seconds += 1
            except Exception as e:
              alert_msg(page, f"EXCEPTION ERROR: Unknown error processing image. Check parameters and try again. Restart app if persists.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
              return
            get_response = requests.get(api_get_result_url + q_id)
            final_results = json.loads(get_response.content)
            clear_last()
            clear_last()
            prt(str(final_results))
            for r in final_results['forms']:
              prompt = r['result']['*']
              i2t_prompts.append(prompt)
              page.add_to_image2text(prompt)'''
    play_snd(Snd.ALERT, page)

def run_BLIP2_image2text(page):
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.BLIP2_image2text_output.controls.append(line)
      page.BLIP2_image2text_output.update()
    def clear_last(lines=1):
      clear_line(page.BLIP2_image2text_output, lines=lines)
    progress = ProgressBar(bar_height=8)
    prt(Installing("Installing BLIP2 Image2Text from Salesforce LAVIS..."))

    try:
        import lavis
        #from lavis.models import load_model_and_preprocess
    except ModuleNotFoundError:
        try:
            #run_process("pip install clip-salesforce-lavis", page=page, show=True)
            run_sp("pip install -q salesforce-lavis", realtime=True)
        except Exception as e:
            clear_last()
            alert_msg(page, "ERROR Installing salesforce-lavis. Try running before installing Diffusers...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
            return
        pass
    finally:
        from lavis.models import load_model_and_preprocess
    import requests
    device = torch.device(torch_device)
    clear_last()
    prt(Installing("Downloading BLIP2 Image2Text from Salesforce LAVIS... It's huge, see console for progress."))
    model_name = "blip2_t5"
    if '.' in BLIP2_image2text_prefs['model_type']:
      model_name = "blip2_opt"
    if BLIP2_image2text_prefs['model_type'] == "base":
      model_name = "img2prompt_vqa"
    try:
        model, vis_processors, _ = load_model_and_preprocess(
            name=model_name, model_type=BLIP2_image2text_prefs['model_type'], is_eval=True, device=device)
    except Exception as e:
        clear_last()
        alert_msg(page, "ERROR: Problem running Interrogator. Try running before installing Diffusers...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
        return
    vis_keys = vis_processors.keys()
    print(str(vis_keys))
    prt("  Examining Images to Describe Prompt... Check console output for progress, first run is slow.")
    prt(progress)
    folder_path = BLIP2_image2text_prefs['folder_path']
    files = [f for f in os.listdir(folder_path) if f.endswith('.jpg') or  f.endswith('.png')] if os.path.exists(folder_path) else []
    clear_last()
    BLIP2_i2t_prompts = []
    try:
        for file in files:
            img = PILImage.open(os.path.join(folder_path, file)).convert('RGB')
            image = vis_processors["eval"](img).unsqueeze(0).to(device)
            if BLIP2_image2text_prefs['num_captions'] == 1:
                prompt = model.generate({"image": image})
            else:
                prompt = model.generate({"image": image}, use_nucleus_sampling=True, num_captions=BLIP2_image2text_prefs['num_captions'])
            print(prompt)
            if isinstance(prompt, str):
                BLIP2_i2t_prompts.append(prompt)
                page.add_to_BLIP2_image2text(prompt)
            else:
                for p in prompt:
                    BLIP2_i2t_prompts.append(p)
                    page.add_to_BLIP2_image2text(p)
            if bool(BLIP2_image2text_prefs['question_prompt']):
                question = f"Question: {BLIP2_image2text_prefs['question_prompt']} Answer:"
                answer = model.generate({"image": image, "prompt": question})
                a = answer.rpartition(':')[2].strip()
                prt(f"Answer: {a}")
    except Exception as e:
        clear_last()
        alert_msg(page, "ERROR: Problem Generating from Model. Probably out of memory...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
        return
    clear_last()
    play_snd(Snd.ALERT, page)

def run_dance_diffusion(page):
    if not check_diffusers(page): return
    global dance_pipe, dance_prefs
    if dance_prefs['dance_model'] == 'Community' or dance_prefs['dance_model'] == 'Custom':
      alert_msg(page, "Custom Community Checkpoints are not functional yet, working on it so check back later... ")
      return
    from diffusers import DanceDiffusionPipeline
    import scipy.io.wavfile, random
    try:
      import gdown
    except ImportError:
      run_sp("pip -q install gdown==4.7.3")
    finally:
      import gdown
    #import sys
    #sys.path.append('drive/gdrive/MyDrive/NotebookDatasets/CMVRLG')
    #print(dir(os))
    #print(dir(os.path))
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.dance_output.controls.append(line)
      page.dance_output.update()
    def clear_last(lines=1):
      clear_line(page.dance_output, lines=lines)
    def play_audio(e):
      e.control.data.play()
    progress = ProgressBar(bar_height=8)
    prt(Installing("Downloading Dance Diffusion Models"))
    diffusers_dir = os.path.join(root_dir, "diffusers")
    if not os.path.exists(diffusers_dir):
      run_process("git clone https://github.com/Skquark/diffusers.git", realtime=False, cwd=root_dir)
    dance_model_file = f"harmonai/{dance_prefs['dance_model']}"
    if dance_prefs['dance_model'] == 'Community':
      models_path = os.path.join(root_dir, 'dancediffusion_models')
      os.makedirs(models_path, exist_ok=True)
      for c in community_dance_diffusion_models:
        if c['name'] == dance_prefs['community_model']:
          community = c
      model_out = os.path.join(models_path, format_filename(community['name'], use_dash=True))
      if not os.path.exists(model_out):
        prt("Converting Checkpoint to Diffusers...")
        os.makedirs(model_out, exist_ok=True)
        if bool(community['download']):
          dance_model_file = os.path.join(models_path, community['ckpt'])
          if community['download'].startswith('https://drive'):
            gdown.download(url=community['download'], output=dance_model_file, quiet=True)
          elif community['download'].startswith('http'):
            local = download_file(community['download'])
            print(f"Download {community['download']} local:{local}")
            shutil.move(local, os.path.join(models_path, community['ckpt']))
          else:
            dance_model_file = community['download']
          sample_generator = os.path.join(root_dir, 'sample-generator')
          if not os.path.exists(sample_generator):
            run_process("git clone https://github.com/harmonai-org/sample-generator", page=page, cwd=root_dir)
          import sys
          sys.path.insert(1, os.path.join(sample_generator, 'audio_diffusion'))
          run_sp(f"pip install {sample_generator}", cwd=sample_generator, realtime=False)
          v_diffusion_pytorch = os.path.join(root_dir, 'v-diffusion-pytorch')
          if not os.path.exists(v_diffusion_pytorch):
            run_sp("git clone --recursive https://github.com/crowsonkb/v-diffusion-pytorch", cwd=root_dir, realtime=False)
          run_sp(f"pip install {v_diffusion_pytorch}", cwd=v_diffusion_pytorch, realtime=False)
          run_sp(f"python {os.path.join(diffusers_dir, 'scripts', 'convert_dance_diffusion_to_diffusers.py')} --model_path {dance_model_file} --checkpoint_path {model_out}", cwd=os.path.join(sample_generator, 'audio_diffusion'))#os.path.join(diffusers_dir, 'scripts'))
          clear_last()
          dance_model_file = model_out
          #run_sp(f'gdown {community['download']} {dance_model_file}')
          #run_sp(f"wget {community['download']} -O {models_path}")
      else:
        dance_model_file = model_out
    if dance_prefs['dance_model'] == 'Custom':
      models_path = os.path.join(root_dir, 'dancediffusion_models')
      os.makedirs(models_path, exist_ok=True)
      if bool(dance_prefs['custom_model']):
        if dance_prefs['custom_model'].startswith('https://drive'):
          dance_model_file = os.path.join(models_path, "custom_dance.ckpt")
          gdown.download(url=dance_prefs['custom_model'], output=dance_model_file, quiet=True)
        elif dance_prefs['custom_model'].startswith('http'):
          fname = dance_prefs['custom_model'].rpartition('/')[2]
          local = download_file(dance_prefs['custom_model'])
          dance_model_file = os.path.join(models_path, fname)
          print(f"Download {dance_prefs['custom_model']} local:{local}")
          shutil.move(local, dance_model_file)
        else:
          dance_model_file = dance_prefs['custom_model']
    if dance_prefs['train_custom']:
      dance_audio = os.path.join(root_dir, 'dance-audio')
      sample_generator = os.path.join(root_dir, 'sample-generator')
      if not os.path.exists(sample_generator):
        run_process("git clone https://github.com/harmonai-org/sample-generator", page=page, cwd=root_dir)
      run_process(f"pip install {sample_generator}", page=page, cwd=root_dir, show=True)
      v_diffusion_pytorch = os.path.join(root_dir, 'v-diffusion-pytorch')
      if not os.path.exists(v_diffusion_pytorch):
        run_sp("git clone --recursive https://github.com/crowsonkb/v-diffusion-pytorch", cwd=root_dir, realtime=False)
      run_sp(f"pip install {v_diffusion_pytorch}", cwd=v_diffusion_pytorch, realtime=False)
      run_cmd = "python " + os.path.join(sample_generator, 'train_uncond.py')
      custom_name = format_filename(dance_prefs['custom_name'], use_dash=True)
      output_dir = os.path.join(dance_audio, custom_name)
      output_dir = output_dir.replace(" ", r"\ ")
      random_crop_str = f"--random-crop True" if dance_prefs['random_crop'] else ""
      run_cmd += f''' --ckpt-path {dance_model_file}\
          --name {custom_name}\
          --training-dir {dance_audio}\
          --sample-size {dance_prefs['sammple_size']}\
          --accum-batches {dance_prefs['accumulate_batches']}\
          --sample-rate {dance_prefs['sample_rate']}\
          --batch-size {dance_prefs['finetune_batch_size']}\
          --demo-every {dance_prefs['demo_every']}\
          --checkpoint-every {dance_prefs['checkpoint_every']}\
          --num-workers 2\
          --num-gpus 1\
          {random_crop_str}\
          --save-path {output_dir}'''
      prt(" Training with " + run_cmd)
      prt(progress)
      try:
        run_process(run_cmd, page=page, cwd=sample_generator, show=True)
      except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR: CUDA Out of Memory, or something else. Try changing parameters and try again...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
        with torch.no_grad():
          torch.cuda.empty_cache()
        return
      if dance_prefs['save_model']:
        print("Upload to HuggingFace (todo)")
      clear_last()
      clear_last()
      #dance_model_file = os.path.join(output_dir, custom_name + '.ckpt')
    try:
      dance_pipe = DanceDiffusionPipeline.from_pretrained(dance_model_file, torch_dtype=torch.float16, device_map="auto")
      dance_pipe = dance_pipe.to(torch_device)
    except Exception as e:
      clear_last()
      alert_msg(page, f"ERROR: Problem getting DanceDiffusion Pipeline. Try changing parameters and try again...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
      return
    dance_pipe.set_progress_bar_config(disable=True)
    random_seed = int(dance_prefs['seed']) if int(dance_prefs['seed']) > 0 else random.randint(0,4294967295)
    dance_generator = torch.Generator(device=torch_device).manual_seed(random_seed)
    clear_last()
    pb.width=(page.width if page.web else page.window.width) - 50
    prt(pb)
    if prefs['higher_vram_mode']:
      output = dance_pipe(generator=dance_generator, batch_size=int(dance_prefs['batch_size']), num_inference_steps=int(dance_prefs['inference_steps']), audio_length_in_s=float(dance_prefs['audio_length_in_s']))
    else:
      output = dance_pipe(generator=dance_generator, batch_size=int(dance_prefs['batch_size']), num_inference_steps=int(dance_prefs['inference_steps']), audio_length_in_s=float(dance_prefs['audio_length_in_s'])) #, torch_dtype=torch.float16
    #, callback=callback_fn, callback_steps=1)
    audio = output.audios
    audio_slice = audio[0, -3:, -3:]
    clear_last()
    #prt(f'audio: {type(audio[0])}, audio_slice: {type(audio_slice)}, len:{len(audio)}')
    #audio_slice.tofile("/content/dance-test.wav")
    audio_name = f"dance-{dance_prefs['dance_model']}" + (f"-{random_seed}" if prefs['file_suffix_seed'] else '')
    audio_local = os.path.join(root_dir, "audio_out")
    audio_out = audio_local
    os.makedirs(audio_local, exist_ok=True)
    if storage_type == "Colab Google Drive":
      audio_out = prefs['image_output'].rpartition(slash)[0] + slash + 'audio_out'
      os.makedirs(audio_out, exist_ok=True)
    i = 0
    for a in audio:
      fname = available_file(audio_local, audio_name, i, ext="wav")
      scipy.io.wavfile.write(fname, dance_pipe.unet.sample_rate, a.transpose())
      os.path.abspath(fname)
      #a_out = Audio(src=fname, autoplay=False)
      #page.overlay.append(a_out)
      #page.update()
      display_name = fname
      #a.tofile(f"/content/dance-{i}.wav")
      if storage_type == "Colab Google Drive":
        audio_save = available_file(audio_out, audio_name, i, ext='wav')
        shutil.copy(fname, audio_save)
        display_name = audio_save
      prt(AudioPlayer(src=fname, display=display_name, data=audio_save, page=page))
      #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
      i += 1
    play_snd(Snd.ALERT, page)

def run_audio_diffusion(page):
    global audio_diffusion_prefs, pipe_audio_diffusion, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.audio_diffusion_output.controls.append(line)
      page.audio_diffusion_output.update()
    def clear_last(lines=1):
      clear_line(page.audio_diffusion_output, lines=lines)
    #if not bool(audio_diffusion_prefs['text']):
    #  alert_msg(page, "Provide Text for the AI to create the sound of...")
    #  return
    #prt(AudioPlayer(asset_dir(os.path.join(root_dir, 'audio_out', 'Experiments', 'tts-Shimmer-Youve_got_to_check_out_Stable_Diffusion_Deluxe_Its_got_almost_every_AI_tool_out_there_with_a_beautiful_UI_thats_so_easy_to_use-0.mp3')), 'tts-Shimmer-Youve_got_to_check_out_Stable_Diffusion_Deluxe_Its_got_almost_every_AI_tool_out_there_with_a_beautiful_UI_thats_so_easy_to_use-0.mp3', page=page))
    #return #- Was testing Spectrogram
    if not check_diffusers(page): return
    total_steps = audio_diffusion_prefs['steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    progress = ProgressBar(bar_height=8)
    installer = Installing("Downloading Audio Diffusion Pipeline...")
    prt(installer)
    audio_diffusion_dir = os.path.join(root_dir, "audio_diffusion")

    try:
        import mel
    except ModuleNotFoundError:
        installer.status("...installing mel")
        try:
            run_process("pip install -q mel", page=page, show=True, print=True)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing AudioDiffusion requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
            return
        pass
    finally:
        import mel
    import scipy.io.wavfile
    from diffusers import DiffusionPipeline, DDIMScheduler
    model_id = audio_diffusion_prefs['audio_model']
    if audio_diffusion_prefs['loaded_model'] != model_id:
      clear_pipes()
    else:
      clear_pipes('audio_diffusion')
    if pipe_audio_diffusion == None:
      installer.status("...initializing audio_diffusion pipe")
      try:
          # TODO: Switch DDPM
        a_scheduler = DDIMScheduler()
        pipe_audio_diffusion = DiffusionPipeline.from_pretrained(model_id, scheduler=a_scheduler, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        pipe_audio_diffusion = pipe_audio_diffusion.to(torch_device)
        pipe_audio_diffusion.set_progress_bar_config(disable=True)
        audio_diffusion_prefs['loaded_model'] = model_id
      except Exception as e:
        clear_last()
        alert_msg(page, "Error setting up Audio Diffusion Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    init = audio_diffusion_prefs['audio_file']
    if init.startswith('http'):
        installer.status("...downloading audio file")
        init_audio = download_file(init)
    else:
        if os.path.isfile(init):
            init_audio = init
        else:
            init_audio = None
    clear_last()
    prt(Text("  Generating Audio Diffusion Sounds...", weight=FontWeight.BOLD))
    prt(progress)
    random_seed = get_seed(audio_diffusion_prefs['seed'])
    generator = torch.Generator(device=torch_device).manual_seed(random_seed)
    try:
        output = pipe_audio_diffusion(audio_file=init_audio, slice=audio_diffusion_prefs['slice'], steps=audio_diffusion_prefs['steps'], start_step=audio_diffusion_prefs['start_step'], mask_start_secs=audio_diffusion_prefs['mask_start_secs'], mask_end_secs=audio_diffusion_prefs['mask_end_secs'], eta=audio_diffusion_prefs['eta'], batch_size=int(audio_diffusion_prefs['batch_size']), generator=generator)#, callback=callback_fnc)
    except Exception as e:
        clear_last()
        alert_msg(page, "Error Generating Audio Diffusion Output", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    images = output.images
    audios = output.audios
    sample_rate = pipe_audio_diffusion.mel.get_sample_rate()
    save_dir = os.path.join(root_dir, 'audio_out', audio_diffusion_prefs['batch_folder_name'])
    if not os.path.exists(save_dir):
      os.makedirs(save_dir, exist_ok=True)
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    if bool(audio_diffusion_prefs['batch_folder_name']):
      audio_out = os.path.join(audio_out, audio_diffusion_prefs['batch_folder_name'])
    os.makedirs(audio_out, exist_ok=True)
    #voice_dirs = os.listdir(os.path.join(root_dir, "audio_diffusion-tts", 'audio_diffusion', 'voices'))
    #print(str(voice_dirs))
    clear_last()
    clear_last()
    a_name = audio_diffusion_prefs['audio_name']
    if not bool(a_name):
        if bool(init_audio):
            a_name = init_audio.rpartition(slash)[2].rpartition('.')[0]
        else:
            a_name = f"audio_diffusion-{model_id.rpartition('/')[2]}"
    fname = format_filename(a_name, force_underscore=True)
    if fname[-1] == '.': fname = fname[:-1]
    file_prefix = audio_diffusion_prefs['file_prefix']
    audio_name = f'{file_prefix}{fname}'
    audio_name = audio_name[:int(prefs['file_max_length'])]
    for image in images:
        iname = available_file(save_dir, audio_name, 0)
        image.save(iname)
        out_path = iname
        prt(Row([Img(src=asset_dir(iname), fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        if storage_type == "Colab Google Drive":
            new_file = available_file(prefs['image_output'], fname, 0)
            out_path = new_file
            shutil.copy(iname, new_file)
        elif bool(prefs['image_output']):
            new_file = available_file(prefs['image_output'], fname, 0)
            out_path = new_file
            shutil.copy(iname, new_file)
        #prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    for a in audios:
        aname = available_file(save_dir, audio_name, 0, ext="wav")
        #for i in range(waveform.shape[0]):
        #    sf.write(aname, waveform[i, 0], samplerate=sample_rate)
        #torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)
        #IPython.display.Audio('generated.wav')
        scipy.io.wavfile.write(aname, sample_rate, a.transpose())
        #a_out = Audio(src=aname, autoplay=False)
        #page.overlay.append(a_out)
        #page.update()
        display_name = aname
        #a.tofile(f"/content/dance-{i}.wav")
        if storage_type == "Colab Google Drive":
            audio_save = available_file(audio_out, audio_name, 0, ext='wav')
            shutil.copy(aname, audio_save)
        elif bool(prefs['image_output']):
            audio_save = available_file(audio_out, audio_name, 0, ext='wav')
            shutil.copy(aname, audio_save)
        else: audio_save = aname
        display_name = audio_save
        prt(AudioPlayer(src=aname, display=audio_save, data=audio_save, page=page))
        #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
    play_snd(Snd.ALERT, page)

def run_music_gen(page):
    global music_gen_prefs, pipe_music_gen, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.music_gen_output.controls.append(line)
      page.music_gen_output.update()
    def clear_last(lines=1):
      clear_line(page.music_gen_output, lines=lines)
    def play_audio(e):
      e.control.data.play()
    if not bool(music_gen_prefs['prompt']):
      alert_msg(page, "Provide Text for the AI to create the sound of...")
      return
    '''total_steps = music_gen_prefs['steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()'''
    progress = ProgressBar(bar_height=8)
    installer = Installing("Downloading MusicGen Pipeline...")
    prt(installer)
    music_gen_dir = os.path.join(root_dir, "music_gen")
    pip_install("sentencepiece ffmpeg librosa torchlibrosa", installer=installer)
    try:
        import audiocraft
        #from audiocraft.models import musicgen
    except ModuleNotFoundError:
        installer.status("...facebookresearch/audiocraft")
        #run_sp("pip install -U audiocraft", realtime=True)
        run_sp("pip install -U git+https://github.com/facebookresearch/audiocraft#egg=audiocraft", realtime=False)
        #run_sp("pip install -U git+https://github.com/Oncorporation/audiocraft#egg=audiocraft", realtime=False)
        pass
    finally:
        from audiocraft.models import musicgen
        from audiocraft.data.audio import audio_write
        from audiocraft.data.audio_utils import apply_fade, apply_tafade
        from audiocraft.utils.extend import generate_music_segments, add_settings_to_image, INTERRUPTING
    text = music_gen_prefs['prompt']
    init = music_gen_prefs['audio_file']
    model_id = music_gen_prefs['audio_model'] if not bool(init) else "melody"
    if music_gen_prefs['loaded_model'] != model_id:
        installer.status("...clear_pipes")
        clear_pipes()
    else:
      clear_pipes('music_gen')
    if pipe_music_gen == None:
        installer.status(f"...MusicGen pretrained {model_id}")
        try:
            pipe_music_gen = musicgen.MusicGen.get_pretrained(model_id, device='cuda')
            music_gen_prefs['loaded_model'] = model_id
        except Exception as e:
            clear_last()
            alert_msg(page, "Error setting up MusicGen Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    clear_last()
    def get_melody(melody_filepath):
        import librosa
        audio_data= list(librosa.load(melody_filepath, sr=None))
        audio_data[0], audio_data[1] = audio_data[1], audio_data[0]
        melody = tuple(audio_data)
        return melody
    for num in range(music_gen_prefs['num_samples']):
        if init.startswith('http'):
            installer.status("...download audio_file")
            melody = download_file(init)
        else:
            if os.path.isfile(init):
                melody = init
            else:
                melody = None
        if melody is not None:
            melody = get_melody(melody)
        duration = music_gen_prefs['duration']
        overlap = music_gen_prefs['overlap']
        dimension = music_gen_prefs['dimension']
        seed = music_gen_prefs['seed']
        output = None
        first_chunk = None
        total_samples = duration * 50 + 3
        segment_duration = duration
        initial_duration = duration
        output_segments = []
        gen_status = Text("  Generating MusicGen Sounds...", weight=FontWeight.BOLD)
        prt(gen_status)
        prt(progress)
        if seed <= 0:
            random_seed = rnd.randint(0, 0xffff_ffff_ffff)
        else:
            random_seed = seed + num
        torch.manual_seed(random_seed)
        #if melody != None and duration > 30:
        #    duration = 30
        chunk = 0
        while duration > 0:
            chunk += 1
            if not output_segments: # first pass of long or short song
                if segment_duration > pipe_music_gen.lm.cfg.dataset.segment_duration:
                    segment_duration = pipe_music_gen.lm.cfg.dataset.segment_duration
                else:
                    segment_duration = duration
            else: # next pass of long song
                if duration + overlap < pipe_music_gen.lm.cfg.dataset.segment_duration:
                    segment_duration = duration + overlap
                else:
                    segment_duration = pipe_music_gen.lm.cfg.dataset.segment_duration
            gen_status.value = f"  Generating MusicGen... Segment duration: {segment_duration}, Remaining duration: {duration}, overlap: {overlap}, chunk: {chunk}"
            page.music_gen_output.update()
            #print(f'Segment duration: {segment_duration}, duration: {duration}, overlap: {overlap}')
            pipe_music_gen.set_generation_params(
                use_sampling=music_gen_prefs['use_sampling'],
                two_step_cfg=music_gen_prefs['two_step_cfg'],
                top_k=music_gen_prefs['top_k'],
                top_p=music_gen_prefs['top_p'],
                temperature=music_gen_prefs['temperature'],
                cfg_coef=music_gen_prefs['guidance'],
                duration=segment_duration,
                rep_penalty=0.5
            )
            try:
                if melody:
                    if duration > pipe_music_gen.lm.cfg.dataset.segment_duration:
                        output_segments, duration = generate_music_segments(text, melody, random_seed, pipe_music_gen, duration, overlap, pipe_music_gen.lm.cfg.dataset.segment_duration, harmony_only=music_gen_prefs['harmony_only'])
                    else:
                        # pure original code
                        sr, melody = melody[0], torch.from_numpy(melody[1]).to(pipe_music_gen.device).float().t().unsqueeze(0)
                        print(melody.shape)
                        if melody.dim() == 2:
                            melody = melody[None]
                        melody = melody[..., :int(sr * pipe_music_gen.lm.cfg.dataset.segment_duration)]
                        output = pipe_music_gen.generate_with_chroma(
                            descriptions=[text],
                            melody_wavs=melody,
                            melody_sample_rate=sr,
                            progress=False
                        )
                    # All output_segments are populated, so we can break the loop or set duration to 0
                    break
                    '''import torchaudio
                    sr, melody = melody[0], torch.from_numpy(melody[1]).to(pipe_music_gen.device).float().t().unsqueeze(0)
                    melody, sr = torchaudio.load(init_audio)
                    #sr = 32000
                    #melody = torch.from_numpy(init_audio).to(pipe_music_gen.device).float().t().unsqueeze(0)
                    #if melody.dim() == 2:
                    #    melody = melody[None]
                    melody = melody[None].expand(3, -1, -1)
                    #melody = melody[..., :int(sr * pipe_music_gen.lm.cfg.dataset.segment_duration)]
                    output = pipe_music_gen.generate_with_chroma(
                        descriptions=[music_gen_prefs['prompt'] * music_gen_prefs['batch_size']],
                        melody_wavs=melody,
                        melody_sample_rate=sr,
                        progress=True
                    )
                    duration -= segment_duration'''
                else:
                    if not output_segments:
                        next_segment = pipe_music_gen.generate(descriptions=[text], progress=False)
                        duration -= segment_duration
                    else:
                        last_chunk = output_segments[-1][:, :, -overlap*pipe_music_gen.sample_rate:]
                        next_segment = pipe_music_gen.generate_continuation(last_chunk, pipe_music_gen.sample_rate, descriptions=[text], progress=True)
                        duration -= segment_duration - overlap
                    output_segments.append(next_segment)
                    '''if output is None:
                        next_segment = pipe_music_gen.generate(descriptions=[music_gen_prefs['prompt'] * music_gen_prefs['batch_size']], progress=True)
                                                      #progress=updateProgress)
                        duration -= segment_duration
                    else:
                        if first_chunk is None and pipe_music_gen.name == "melody" and music_gen_prefs['recondition']:
                            first_chunk = output[:, :, :pipe_music_gen.lm.cfg.dataset.segment_duration*pipe_music_gen.sample_rate]
                        last_chunk = output[:, :, -overlap*pipe_music_gen.sample_rate:]
                        next_segment = pipe_music_gen.generate_continuation(last_chunk,
                            pipe_music_gen.sample_rate, descriptions=[[music_gen_prefs['prompt'] * music_gen_prefs['batch_size']]], progress=True) #, melody_wavs=(first_chunk), resample=False
                        duration -= segment_duration - overlap'''
                    #output = pipe_music_gen.generate(descriptions=[music_gen_prefs['prompt'] * music_gen_prefs['batch_size']], progress=True)
                #if output is None:
                #    output = next_segment
                #else:
                #    output = torch.cat([output[:, :, :-overlap*pipe_music_gen.sample_rate], next_segment], 2)
            except Exception as e:
                clear_last()
                alert_msg(page, "Error Generating Music Output", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            #output = output.detach().cpu().float()[0]
        if output_segments:
            try:
                output = output_segments[0]
                for i in range(1, len(output_segments)):
                    if overlap > 0:
                        overlap_samples = overlap * pipe_music_gen.sample_rate
                        overlapping_output_fadeout = output[:, :, -overlap_samples:]
                        #overlapping_output_fadeout = apply_fade(overlapping_output_fadeout,sample_rate=pipe_music_gen.sample_rate,duration=overlap,out=True,start=True, curve_end=0.0, current_device=pipe_music_gen.device)
                        overlapping_output_fadeout = apply_tafade(overlapping_output_fadeout,sample_rate=pipe_music_gen.sample_rate,duration=overlap,out=True,start=True,shape="linear")
                        overlapping_output_fadein = output_segments[i][:, :, :overlap_samples]
                        #overlapping_output_fadein = apply_fade(overlapping_output_fadein,sample_rate=pipe_music_gen.sample_rate,duration=overlap,out=False,start=False, curve_start=0.0, current_device=pipe_music_gen.device)
                        overlapping_output_fadein = apply_tafade(overlapping_output_fadein,sample_rate=pipe_music_gen.sample_rate,duration=overlap,out=False,start=False, shape="linear")
                        overlapping_output = torch.cat([overlapping_output_fadeout[:, :, :-(overlap_samples // 2)], overlapping_output_fadein],dim=2)
                        gen_status.value = f"  Saving MusicGen... Overlap size Fade:{overlapping_output.size()}\n output: {output.size()}\n segment: {output_segments[i].size()}"
                        page.music_gen_output.update()
                        #print(f" Overlap size Fade:{overlapping_output.size()}\n output: {output.size()}\n segment: {output_segments[i].size()}")
                        output = torch.cat([output[:, :, :-overlap_samples], overlapping_output, output_segments[i][:, :, overlap_samples:]], dim=dimension)
                    else:
                        output = torch.cat([output, output_segments[i]], dim=dimension)
                output = output.detach().cpu().float()[0]
            except Exception as e:
                alert_msg(page, f"Error combining segments: {e}. Using the first segment only.")
                output = output_segments[0].detach().cpu().float()[0]
        else:
            try:
                output = output.detach().cpu().float()[0]
            except Exception as e:
                clear_last()
                alert_msg(page, "Error Saving Music Output", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return

        #sample_rate = pipe_music_gen.mel.get_sample_rate()
        save_dir = os.path.join(root_dir, 'audio_out', music_gen_prefs['batch_folder_name'])
        if not os.path.exists(save_dir):
            os.makedirs(save_dir, exist_ok=True)
        audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
        if bool(music_gen_prefs['batch_folder_name']):
            audio_out = os.path.join(audio_out, music_gen_prefs['batch_folder_name'])
        os.makedirs(audio_out, exist_ok=True)
        #voice_dirs = os.listdir(os.path.join(root_dir, "music_gen-tts", 'music_gen', 'voices'))
        #print(str(voice_dirs))
        clear_last()
        clear_last()
        a_name = music_gen_prefs['audio_name']
        if not bool(a_name):
            a_name = music_gen_prefs['prompt']
        fname = format_filename(a_name, force_underscore=True)
        if fname[-1] == '.': fname = fname[:-1]
        audio_name = f'{music_gen_prefs["file_prefix"]}{fname}'
        audio_name = audio_name[:int(prefs['file_max_length'])]
        #audios = output.audios
        idx = 0
        for wav in output:
            aname = available_file(save_dir, audio_name, num + idx, ext="wav")
            with open(aname, "wb") as file:
                audio_write(file.name, wav.cpu(), pipe_music_gen.sample_rate, strategy="loudness", loudness_compressor=True, loudness_headroom_db=18, add_suffix=False, channels=2)
                #waveform_video = gr.make_waveform(file.name)
            #for i in range(waveform.shape[0]):
            #    sf.write(aname, waveform[i, 0], samplerate=sample_rate)
            #torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)
            #IPython.display.Audio('generated.wav')
            #scipy.io.wavfile.write(aname, sample_rate, a.transpose())
            #a_out = Audio(src=aname, autoplay=False)
            #page.overlay.append(a_out)
            #page.update()
            display_name = aname
            #a.tofile(f"/content/dance-{i}.wav")
            if storage_type == "Colab Google Drive":
                audio_save = available_file(audio_out, audio_name, num + idx, ext='wav')
                shutil.copy(aname, audio_save)
            elif bool(prefs['image_output']):
                audio_save = available_file(audio_out, audio_name, num + idx, ext='wav')
                shutil.copy(aname, audio_save)
            else: audio_save = aname
            display_name = audio_save
            prt(AudioPlayer(src=aname, display=audio_save, data=audio_out, page=page))
            #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
            idx += 1
        output = None
        first_chunk = None
        output_segments = []
    flush()
    torch.cuda.ipc_collect()
    play_snd(Snd.ALERT, page)

# https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb

def run_dreambooth(page):
    global dreambooth_prefs, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.dreambooth_output.controls.append(line)
      page.dreambooth_output.update()
    def clear_last(lines=1):
      clear_line(page.dreambooth_output, lines=lines)
    if not check_diffusers(page): return
    save_path = os.path.join(root_dir, "my_concept")
    error = False
    if not os.path.exists(save_path):
      error = True
    elif len(os.listdir(save_path)) == 0:
      error = True
    if len(page.db_file_list.controls) == 0:
      error = True
    if error:
      alert_msg(page, "Couldn't find a list of images to train concept. Add image files to the list...")
      return
    prt(Installing("Downloading DreamBooth Conceptualizers"))
    diffusers_dir = os.path.join(root_dir, "diffusers")
    if not os.path.exists(diffusers_dir):
      os.chdir(root_dir)
      run_process("git clone https://github.com/Skquark/diffusers.git", realtime=False, cwd=root_dir)
    os.chdir(diffusers_dir)
    #run_process('pip install -e ".[training]"', cwd=diffusers_dir, realtime=False)
    run_process('pip install "git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]"', cwd=root_dir, realtime=False)
    dreambooth_dir = os.path.join(diffusers_dir, "examples", "dreambooth")
    os.chdir(dreambooth_dir)
    run_process("pip install -r requirements.txt", cwd=dreambooth_dir, realtime=False)
    try:
      #os.environ['LD_LIBRARY_PATH'] += "/usr/lib/wsl/lib:$LD_LIBRARY_PATH"
      import bitsandbytes
    except ModuleNotFoundError:
      run_sp("pip install bitsandbytes", realtime=False)
      import bitsandbytes
      pass
    #from accelerate.utils import write_basic_config
    #write_basic_config()
    import argparse
    from io import BytesIO
    #save_path = "./my_concept"
    #if not os.path.exists(save_path):
    #  os.mkdir(save_path)

    clear_pipes()
    clear_last()
    num_new_images = None

    from argparse import Namespace
    dreambooth_args = Namespace(
        pretrained_model_name_or_path=model_path,
        resolution=dreambooth_prefs['max_size'],
        center_crop=True,
        instance_data_dir=save_path,
        prompt=dreambooth_prefs['instance_prompt'].strip(),
        learning_rate=dreambooth_prefs['learning_rate'],#5e-06,
        max_train_steps=dreambooth_prefs['max_train_steps'],#450,
        train_batch_size=dreambooth_prefs['train_batch_size'],
        gradient_accumulation_steps=2,
        max_grad_norm=1.0,
        #mixed_precision="no", # set to "fp16" for mixed-precision training.
        gradient_checkpointing=True, # set this to True to lower the memory usage.
        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes
        enable_xformers_memory_efficient_attention = status['installed_xformers'],
        seed=dreambooth_prefs['seed'],#3434554,
        with_prior_preservation=dreambooth_prefs['prior_preservation'],
        prior_loss_weight=dreambooth_prefs['prior_loss_weight'],
        sample_batch_size=dreambooth_prefs['sample_batch_size'],
        class_data_dir=dreambooth_prefs['prior_preservation_class_folder'],
        class_prompt=dreambooth_prefs['prior_preservation_class_prompt'],
        num_class_images=dreambooth_prefs['num_class_images'],
        output_dir=os.path.join(root_dir, "dreambooth-concept"),
    )
    if not os.path.exists(dreambooth_args.output_dir): os.mkdir(dreambooth_args.output_dir)
    arg_str = "accelerate launch ./train_dreambooth.py"
    for k, v in vars(dreambooth_args).items():
      if isinstance(v, str):
        if ' ' in v:
          v = f'"{v}"'
      if isinstance(v, bool):
        if bool(v):
          arg_str += f" --{k}"
      else:
        arg_str += f" --{k}={v}"
    prt("***** Running training *****")
    #if num_new_images != None: prt(f"  Number of class images to sample: {num_new_images}.")
    #prt(f"  Instantaneous batch size per device = {dreambooth_args.train_batch_size}")
    #prt(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    #prt(f"  Gradient Accumulation steps = {dreambooth_args.gradient_accumulation_steps}")
    #prt(f"  Total optimization steps = {dreambooth_args.max_train_steps}")
    prt(arg_str)
    progress = ProgressBar(bar_height=8)
    prt(progress)
    try:
      run_process(arg_str, page=page, cwd=dreambooth_dir)
      #run_sp(arg_str, cwd=dreambooth_dir)
    except Exception as e:
      clear_last()
      alert_msg(page, f"ERROR: CUDA Ran Out of Memory. Try reducing parameters and try again...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
      with torch.no_grad():
        torch.cuda.empty_cache()
      return
    clear_last()
    name_of_your_concept = dreambooth_prefs['name_of_your_concept']
    if(dreambooth_prefs['save_concept']):
      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd
      from huggingface_hub import create_repo
      from diffusers import StableDiffusionPipeline
      api = HfApi()
      your_username = api.whoami()["name"]
      dreambooth_pipe = StableDiffusionPipeline.from_pretrained(
        dreambooth_args.output_dir,
        torch_dtype=torch.float16,
      ).to("cuda")
      os.makedirs("fp16_model",exist_ok=True)
      dreambooth_pipe.save_pretrained("fp16_model")
      hf_token = prefs['HuggingFace_api_key']
      if(dreambooth_prefs['where_to_save_concept'] == "Public Library"):
        repo_id = f"sd-dreambooth-library/{format_filename(name_of_your_concept, use_dash=True)}"
        #Join the Concepts Library organization if you aren't part of it already
        run_sp(f"curl -X POST -H 'Authorization: Bearer '{hf_token} -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-dreambooth-library/share/SSeOwppVCscfTEzFGQaqpfcjukVeNrKNHX", realtime=False)
        #!curl -X POST -H 'Authorization: Bearer '$hf_token -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-dreambooth-library/share/SSeOwppVCscfTEzFGQaqpfcjukVeNrKNHX
      else:
        repo_id = f"{your_username}/{format_filename(name_of_your_concept, use_dash=True)}"
      output_dir = dreambooth_args.output_dir
      if(not prefs['HuggingFace_api_key']):
        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();
      else:
        hf_token = prefs['HuggingFace_api_key']

      images_upload = os.listdir(save_path)
      image_string = ""
      #repo_id = f"sd-dreambooth-library/{slugify(name_of_your_concept)}"
      for i, image in enumerate(images_upload):
          image_string = f'''{image_string}![image {i}](https://huggingface.co/{repo_id}/resolve/main/concept_images/{image})
'''
      description = dreambooth_prefs['readme_description']
      if bool(description.strip()):
        description = dreambooth_prefs['readme_description'] + '\n\n'
      readme_text = f'''---
license: mit
---
### {name_of_your_concept} on Stable Diffusion via Dreambooth using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
#### model by {api.whoami()["name"]}
This your the Stable Diffusion model fine-tuned the {name_of_your_concept} concept taught to Stable Diffusion with Dreambooth.
It can be used by modifying the `instance_prompt`: **{dreambooth_prefs['instance_prompt']}**

{description}You can also train your own concepts and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).
And you can run your new concept via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-concepts)

Here are the images used for training this concept:
{image_string}
'''
      #Save the readme to a file
      readme_file = open("README.md", "w")
      readme_file.write(readme_text)
      readme_file.close()
      #Save the token identifier to a file
      text_file = open("token_identifier.txt", "w")
      text_file.write(dreambooth_prefs['instance_prompt'])
      text_file.close()
      operations = [
        CommitOperationAdd(path_in_repo="token_identifier.txt", path_or_fileobj="token_identifier.txt"),
        CommitOperationAdd(path_in_repo="README.md", path_or_fileobj="README.md"),
      ]
      print(repo_id)
      print(readme_text)
      try:
        create_repo(repo_id, private=True, token=hf_token)
      except Exception as e:
        alert_msg(page, f"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
        return
      api.create_commit(repo_id=repo_id, operations=operations, commit_message=f"Upload the concept {name_of_your_concept} embeds and token",token=hf_token)
      api.upload_folder(folder_path="fp16_model", path_in_repo="", repo_id=repo_id,token=hf_token)
      api.upload_folder(folder_path=save_path, path_in_repo="concept_images", repo_id=repo_id, token=hf_token)
      prefs['custom_model'] = repo_id
      prefs['custom_models'].append({'name': name_of_your_concept, 'path':repo_id})
      page.custom_model.value = repo_id
      try:
        page.custom_model.update()
      except Exception: pass
      prt(Markdown(f"## Your concept was saved successfully to _{repo_id}_.<br>[Click here to access it](https://huggingface.co/{repo_id}) and go to _Installers->Model Checkpoint->Custom Model Path_ to use. Include Token in prompts.", on_tap_link=lambda e: e.page.launch_url(e.data)))
    play_snd(Snd.ALERT, page)

def run_textualinversion(page):
    global textualinversion_prefs, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.textualinversion_output.controls.append(line)
      page.textualinversion_output.update()
    def clear_last(lines=1):
      clear_line(page.textualinversion_output, lines=lines)
    if not check_diffusers(page): return
    save_path = os.path.join(root_dir, "my_concept")
    error = False
    if not os.path.exists(save_path):
      error = True
    elif len(os.listdir(save_path)) == 0:
      error = True
    if len(page.ti_file_list.controls) == 0:
      error = True
    if error:
      alert_msg(page, "Couldn't find a list of images to train concept. Add image files to the list...")
      return
    page.textualinversion_output.controls.clear()
    page.textualinversion_output.update()
    prt(Installing("Downloading Textual-Inversion Training Models"))

    placeholder_token = textualinversion_prefs['placeholder_token'].strip()
    if not placeholder_token.startswith('<'): placeholder_token = '<' + placeholder_token
    if not placeholder_token.endswith('>'): placeholder_token = placeholder_token + '>'
    initializer_token = textualinversion_prefs['initializer_token'].strip()
    if bool(initializer_token):
      if not initializer_token.startswith('<'): initializer_token = '<' + initializer_token
      if not initializer_token.endswith('>'): initializer_token = initializer_token + '>'
    diffusers_dir = os.path.join(root_dir, "diffusers")
    if not os.path.exists(diffusers_dir):
      os.chdir(root_dir)
      run_process("git clone https://github.com/Skquark/diffusers.git", realtime=False, cwd=root_dir)
    run_process('pip install git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]', cwd=root_dir, realtime=False)
    os.chdir(diffusers_dir)
    run_sp('pip install -e ".[training]"', cwd=diffusers_dir, realtime=False)
    textualinversion_dir = os.path.join(diffusers_dir, "examples", "textual_inversion")
    #textualinversion_dir = os.path.join(diffusers_dir, "examples", "text_to_image")
    os.chdir(textualinversion_dir)
    run_sp("pip install -r requirements.txt", cwd=textualinversion_dir, realtime=False)
    run_process("pip install -qq bitsandbytes", page=page)
    run_sp("accelerate config default", realtime=False)
    #from accelerate.utils import write_basic_config
    #write_basic_config()
    import argparse
    from io import BytesIO

    clear_pipes()
    clear_last()
    #num_new_images = None
    random_seed = get_seed(textualinversion_prefs['seed'])
    name_of_your_model = textualinversion_prefs['name_of_your_model']
    from argparse import Namespace
    textualinversion_args = Namespace(
        pretrained_model_name_or_path=model_path if not textualinversion_prefs['use_SDXL'] else "stabilityai/stable-diffusion-xl-base-1.0",
        resolution=textualinversion_prefs['resolution'],
        center_crop=True,
        #train_data_dir=save_path,
        #caption_column=textualinversion_prefs['instance_prompt'].strip(),
        train_data_dir=save_path,
        validation_prompt=textualinversion_prefs['validation_prompt'].strip(),
        placeholder_token=placeholder_token,
        initializer_token=initializer_token,
        learnable_property=textualinversion_prefs['what_to_teach'],
        learning_rate=textualinversion_prefs['learning_rate'],#5e-06,'
        lr_scheduler=textualinversion_prefs['lr_scheduler'],
        lr_warmup_steps=textualinversion_prefs['lr_warmup_steps'],
        scale_lr=textualinversion_prefs['scale_lr'],
        max_train_steps=textualinversion_prefs['max_train_steps'],#450,
        train_batch_size=textualinversion_prefs['train_batch_size'],
        checkpointing_steps=textualinversion_prefs['checkpointing_steps'],
        gradient_accumulation_steps=textualinversion_prefs['gradient_accumulation_steps'],
        validation_steps=textualinversion_prefs['validation_steps'],
        num_vectors=textualinversion_prefs['num_vectors'],
        #mixed_precision="no", # set to "fp16" for mixed-precision training.
        gradient_checkpointing=True, # set this to True to lower the memory usage.
        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes
        enable_xformers_memory_efficient_attention = status['installed_xformers'],
        seed=random_seed,
        #with_prior_preservation=textualinversion_prefs['prior_preservation'],
        #prior_loss_weight=textualinversion_prefs['prior_loss_weight'],
        #sample_batch_size=textualinversion_prefs['sample_batch_size'],
        #class_data_dir=textualinversion_prefs['class_data_dir'],
        #class_prompt=textualinversion_prefs['class_prompt'],
        num_class_images=textualinversion_prefs['num_class_images'],
        output_dir=os.path.join(save_path, format_filename(textualinversion_prefs['name_of_your_concept'], use_dash=True)),
    )
    output_dir = textualinversion_args.output_dir
    if textualinversion_prefs['use_SDXL']:
        arg_str = "accelerate launch textual_inversion_sdxl.py"
    else:
        arg_str = "accelerate launch textual_inversion.py"
    #arg_str = 'accelerate --mixed_precision="fp16" launch train_text_to_image_lora.py'
    for k, v in vars(textualinversion_args).items():
      if isinstance(v, str):
        if ' ' in v:
          v = f'"{v}"'
      if isinstance(v, bool):
        if bool(v):
          arg_str += f" --{k}"
      else:
        arg_str += f" --{k}={v}"
    prt(Text("*** Running Training *** See Console for Progress", weight=FontWeight.BOLD))
    #if num_new_images != None: prt(f"  Number of class images to sample: {num_new_images}.")
    #prt(f"  Instantaneous batch size per device = {textualinversion_args.train_batch_size}")
    #prt(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    #prt(f"  Gradient Accumulation steps = {textualinversion_args.gradient_accumulation_steps}")
    #prt(f"  Total optimization steps = {textualinversion_args.max_train_steps}")
    prt(arg_str)
    progress = ProgressBar(bar_height=8)
    prt(progress)
    if(textualinversion_prefs['save_model']):
      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd
      from huggingface_hub import Repository, create_repo, whoami
      #from diffusers import StableDiffusionPipeline
      api = HfApi()
      your_username = api.whoami()["name"]
      #textualinversion_pipe = StableDiffusionPipeline.from_pretrained(
      #  textualinversion_args.output_dir,
      #  torch_dtype=torch.float16,
      #).to("cuda")
      #os.makedirs("fp16_model",exist_ok=True)
      #textualinversion_pipe.save_pretrained("fp16_model")
      hf_token = prefs['HuggingFace_api_key']
      private = False if textualinversion_prefs['where_to_save_model'] == "Public HuggingFace" else True
      repo_id = f"{your_username}/{format_filename(name_of_your_model, use_dash=True)}"
      output_dir = textualinversion_args.output_dir
      if(not prefs['HuggingFace_api_key']):
        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();
      else:
        hf_token = prefs['HuggingFace_api_key']
      try:
        create_repo(repo_id, private=private, exist_ok=True, token=hf_token)
        repo = Repository(output_dir, clone_from=repo_id, token=hf_token)
      except Exception as e:
        alert_msg(page, f"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
        return
    else:
      if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)

    try:
      os.chdir(textualinversion_dir)
      #!accelerate $arg_str # type: ignore
      os.system(arg_str)
      #run_sp(arg_str, cwd=textualinversion_dir, realtime=True)
      #run_sp(arg_str, cwd=textualinversion_dir)
    except Exception as e:
      clear_last()
      alert_msg(page, f"ERROR: Out of Memory (or something else). Try reducing parameters and try again...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
      with torch.no_grad():
        torch.cuda.empty_cache()
      return
    clear_last()

    #title Save your newly created concept to the [library of concepts](https://huggingface.co/sd-concepts-library)?
    save_concept_to_public_library = textualinversion_prefs['save_concept']
    name_of_your_concept = textualinversion_prefs['name_of_your_concept']
    # `hf_token_write`: leave blank if you logged in with a token with `write access` in the [Initial Setup](#scrollTo=KbzZ9xe6dWwf). If not, [go to your tokens settings and create a write access token](https://huggingface.co/settings/tokens)
    hf_token_write = prefs['HuggingFace_api_key']

    if(save_concept_to_public_library):
        from huggingface_hub import HfApi, HfFolder, CommitOperationAdd
        from huggingface_hub import create_repo
        api = HfApi()
        your_username = api.whoami()["name"]
        repo_id = f"sd-concepts-library/{format_filename(name_of_your_concept, use_dash=True)}"
        #output_dir = textualinversion_prefs["output_dir"]
        if(not hf_token_write):
            with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();
        else:
            hf_token = hf_token_write
        if(textualinversion_prefs['where_to_save_concept'] == "Public Library"):
            #Join the Concepts Library organization if you aren't part of it already
            run_sp(f"curl -X POST -H 'Authorization: Bearer '{hf_token} -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-concepts-library/share/VcLXJtzwwxnHYCkNMLpSJCdnNFZHQwWywv", realtime=False)
            # curl -X POST -H 'Authorization: Bearer '$hf_token -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-concepts-library/share/VcLXJtzwwxnHYCkNMLpSJCdnNFZHQwWywv
        else:
            repo_id = f"{your_username}/{format_filename(name_of_your_concept, use_dash=True)}"
        images_upload = os.listdir(save_path)
        image_string = ""
        repo_id = f"sd-concepts-library/{format_filename(name_of_your_concept, use_dash=True)}"
        for i, image in enumerate(images_upload):
            image_string = f'''{image_string}![{placeholder_token} {i}](https://huggingface.co/{repo_id}/resolve/main/concept_images/{image})
        '''
        if(textualinversion_prefs['what_to_teach'] == "style"):
            what_to_teach_article = f"a `{textualinversion_prefs['what_to_teach']}`"
        else:
            what_to_teach_article = f"an `{textualinversion_prefs['what_to_teach']}`"
        description = textualinversion_prefs['readme_description']
        if bool(description.strip()):
            description = textualinversion_prefs['readme_description'] + '\n\n'
        readme_text = f'''---
license: creativeml-openrail-m
base_model: {model_path if not textualinversion_prefs['use_SDXL'] else "stabilityai/stable-diffusion-xl-base-1.0"}
tags:
- stable-diffusion
- stable-diffusion-diffusers
- text-to-image
- diffusers
- textual_inversion
inference: true
---
# Textual inversion text2image fine-tuning - {repo_id}
### {name_of_your_concept} by {your_username} using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
This is the `{placeholder_token}` concept taught to Stable Diffusion via Textual Inversion. You can load this concept into the [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb) notebook. You can also train your own concepts and load them into the concept libraries there too, or using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb).

{description}Here is the new concept you will be able to use as {what_to_teach_article}:
{image_string}
'''
        #Save the readme to a file
        readme_file = open("README.md", "w")
        readme_file.write(readme_text)
        readme_file.close()
        #Save the token identifier to a file
        text_file = open("token_identifier.txt", "w")
        text_file.write(placeholder_token)
        text_file.close()
        #Save the type of teached thing to a file
        type_file = open("type_of_concept.txt","w")
        type_file.write(textualinversion_prefs['what_to_teach'])
        type_file.close()
        operations = [
            CommitOperationAdd(path_in_repo="learned_embeds.bin", path_or_fileobj=f"{output_dir}/learned_embeds.bin"),
            CommitOperationAdd(path_in_repo="token_identifier.txt", path_or_fileobj="token_identifier.txt"),
            CommitOperationAdd(path_in_repo="type_of_concept.txt", path_or_fileobj="type_of_concept.txt"),
            CommitOperationAdd(path_in_repo="README.md", path_or_fileobj="README.md"),
        ]
        try:
          create_repo(repo_id, private=True, token=hf_token)
        except Exception as e:
          alert_msg(page, f"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
          return
        api = HfApi()
        api.create_commit(
            repo_id=repo_id,
            operations=operations,
            commit_message=f"Upload the concept {name_of_your_concept} embeds and token",
            token=hf_token
        )
        api.upload_folder(
            folder_path=save_path,
            path_in_repo="concept_images",
            repo_id=repo_id,
            token=hf_token
        )
        prefs['custom_model'] = repo_id
        prefs['custom_models'].append({'name': name_of_your_concept, 'path':repo_id})
        page.custom_model.value = repo_id
        try:
          page.custom_model.update()
        except Exception: pass
        prt(Markdown(f"## Your concept was saved successfully to _{repo_id}_.<br>[Click here to access it](https://huggingface.co/{repo_id}) and go to _Installers->Model Checkpoint->Custom Model Path_ to use. Include Token to your Prompt text.", on_tap_link=lambda e: e.page.launch_url(e.data)))
    play_snd(Snd.ALERT, page)


def run_LoRA_dreambooth(page):
    global LoRA_dreambooth_prefs, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.LoRA_dreambooth_output.controls.append(line)
      page.LoRA_dreambooth_output.update()
    def clear_last(lines=1):
      clear_line(page.LoRA_dreambooth_output, lines=lines)
    if not check_diffusers(page): return
    save_path = os.path.join(root_dir, "my_model")
    error = False
    if not os.path.exists(save_path):
      error = True
    elif len(os.listdir(save_path)) == 0:
      error = True
    if len(page.lora_dreambooth_file_list.controls) == 0:
      error = True
    if error:
      alert_msg(page, "Couldn't find a list of images to train model. Add image files to the list...")
      return
    page.LoRA_dreambooth_output.controls.clear()
    page.LoRA_dreambooth_output.update()
    prt(Installing("Downloading LoRA DreamBooth Conceptualizers"))
    diffusers_dir = os.path.join(root_dir, "diffusers")
    if not os.path.exists(diffusers_dir):
      os.chdir(root_dir)
      run_process("git clone https://github.com/Skquark/diffusers.git", realtime=False, cwd=root_dir)
    run_process('pip install git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]', cwd=root_dir, realtime=False)
    os.chdir(diffusers_dir)
    run_sp('pip install -e ".[training]"', cwd=diffusers_dir, realtime=False)
    LoRA_dreambooth_dir = os.path.join(diffusers_dir, "examples", "dreambooth")
    #LoRA_dreambooth_dir = os.path.join(diffusers_dir, "examples", "text_to_image")
    os.chdir(LoRA_dreambooth_dir)
    run_sp("pip install -r requirements.txt", cwd=LoRA_dreambooth_dir, realtime=False)
    run_process("pip install -qq bitsandbytes", page=page)
    run_sp("accelerate config default", realtime=False)
    #from accelerate.utils import write_basic_config
    #write_basic_config()
    import argparse
    from io import BytesIO
    #save_path = "./my_model"
    #if not os.path.exists(save_path):
    #  os.mkdir(save_path)

    clear_pipes()
    clear_last()
    #num_new_images = None
    random_seed = get_seed(LoRA_dreambooth_prefs['seed'])
    name_of_your_model = LoRA_dreambooth_prefs['name_of_your_model']
    from argparse import Namespace
    LoRA_dreambooth_args = Namespace(
        pretrained_model_name_or_path=model_path if not LoRA_dreambooth_prefs['use_SDXL'] else "stabilityai/stable-diffusion-xl-base-1.0",
        resolution=LoRA_dreambooth_prefs['resolution'],
        center_crop=True,
        mixed_precision="fp16",
        #train_data_dir=save_path,
        #caption_column=LoRA_dreambooth_prefs['instance_prompt'].strip(),
        instance_data_dir=save_path,
        prompt=LoRA_dreambooth_prefs['instance_prompt'].strip(),
        learning_rate=LoRA_dreambooth_prefs['learning_rate'],#5e-06,'
        lr_scheduler=LoRA_dreambooth_prefs['lr_scheduler'],
        lr_warmup_steps=LoRA_dreambooth_prefs['lr_warmup_steps'],
        lr_num_cycles=LoRA_dreambooth_prefs['lr_num_cycles'],
        lr_power=LoRA_dreambooth_prefs['lr_power'],
        scale_lr=LoRA_dreambooth_prefs['scale_lr'],
        max_train_steps=LoRA_dreambooth_prefs['max_train_steps'],#450,
        train_batch_size=LoRA_dreambooth_prefs['train_batch_size'],
        checkpointing_steps=LoRA_dreambooth_prefs['checkpointing_steps'],
        gradient_accumulation_steps=LoRA_dreambooth_prefs['gradient_accumulation_steps'],
        max_grad_norm=1.0,
        #mixed_precision="no", # set to "fp16" for mixed-precision training.
        gradient_checkpointing=True, # set this to True to lower the memory usage.
        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes
        enable_xformers_memory_efficient_attention = status['installed_xformers'],
        seed=random_seed,
        with_prior_preservation=LoRA_dreambooth_prefs['prior_preservation'],
        prior_loss_weight=LoRA_dreambooth_prefs['prior_loss_weight'],
        rank=4,#LoRA_dreambooth_prefs['rank'], TODO: "The dimension of the LoRA update matrices."
        sample_batch_size=LoRA_dreambooth_prefs['sample_batch_size'],
        #class_data_dir=LoRA_dreambooth_prefs['class_data_dir'],
        #class_prompt=LoRA_dreambooth_prefs['class_prompt'],
        num_class_images=LoRA_dreambooth_prefs['num_class_images'],
        output_dir=os.path.join(root_dir, "LoRA-model", format_filename(LoRA_dreambooth_prefs['name_of_your_model'], use_dash=True)),
    )
    output_dir = LoRA_dreambooth_args.output_dir
    if not os.path.exists(os.path.join(root_dir, "LoRA-model")): os.makedirs(os.path.join(root_dir, "LoRA-model"))
    if LoRA_dreambooth_prefs['use_SDXL']:
      arg_str = "launch train_dreambooth_lora_sdxl.py"
    else:
      arg_str = "launch train_dreambooth_lora.py"
    #arg_str = 'accelerate --mixed_precision="fp16" launch train_text_to_image_lora.py'
    for k, v in vars(LoRA_dreambooth_args).items():
      if isinstance(v, str):
        if ' ' in v:
          v = f'"{v}"'
      if isinstance(v, bool):
        if bool(v):
          arg_str += f" --{k}"
      else:
        arg_str += f" --{k}={v}"
    prt(Text("*** Running training ***", weight=FontWeight.BOLD))
    #if num_new_images != None: prt(f"  Number of class images to sample: {num_new_images}.")
    #prt(f"  Instantaneous batch size per device = {LoRA_dreambooth_args.train_batch_size}")
    #prt(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    #prt(f"  Gradient Accumulation steps = {LoRA_dreambooth_args.gradient_accumulation_steps}")
    #prt(f"  Total optimization steps = {LoRA_dreambooth_args.max_train_steps}")
    prt(arg_str)
    progress = ProgressBar(bar_height=8)
    prt(progress)
    if(LoRA_dreambooth_prefs['save_model']):
      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd
      from huggingface_hub import Repository, create_repo, whoami
      #from diffusers import StableDiffusionPipeline
      api = HfApi()
      your_username = api.whoami()["name"]
      #LoRA_dreambooth_pipe = StableDiffusionPipeline.from_pretrained(
      #  LoRA_dreambooth_args.output_dir,
      #  torch_dtype=torch.float16,
      #).to("cuda")
      #os.makedirs("fp16_model",exist_ok=True)
      #LoRA_dreambooth_pipe.save_pretrained("fp16_model")
      hf_token = prefs['HuggingFace_api_key']
      private = False if LoRA_dreambooth_prefs['where_to_save_model'] == "Public HuggingFace" else True
      repo_id = f"{your_username}/{format_filename(name_of_your_model, use_dash=True)}"
      output_dir = LoRA_dreambooth_args.output_dir
      if(not prefs['HuggingFace_api_key']):
        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();
      else:
        hf_token = prefs['HuggingFace_api_key']
      try:
        create_repo(repo_id, private=private, exist_ok=True, token=hf_token)
        repo = Repository(output_dir, clone_from=repo_id, token=hf_token)
      except Exception as e:
        alert_msg(page, f"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
        return
    else:
      if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)

    try:
      #%cd $LoRA_dreambooth_dir # type: ignore
      os.chdir(LoRA_dreambooth_dir)
      #!accelerate $arg_str # type: ignore
      os.system("accelerate" + arg_str)
      #run_sp(arg_str, cwd=LoRA_dreambooth_dir, realtime=True)
      #run_sp(arg_str, cwd=LoRA_dreambooth_dir)
    except Exception as e:
      clear_last()
      alert_msg(page, f"ERROR: Out of Memory (or something else). Try reducing parameters and try again...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
      with torch.no_grad():
        torch.cuda.empty_cache()
      return
    clear_last()
    if(LoRA_dreambooth_prefs['save_model']):
      model_images = os.path.join(output_dir, 'model_images')
      if not os.path.exists(model_images): os.makedirs(model_images, exist_ok=True)
      images_upload = os.listdir(save_path)
      image_string = ""
      #repo_id = f"sd-LoRA-library/{slugify(name_of_your_model)}"
      for i, image in enumerate(images_upload):
          img_name = f"image_{i}.png"
          shutil.copy(os.path.join(save_path, image), os.path.join(model_images, image))
          #image.save(os.path.join(repo_folder, f"image_{i}.png"))
          #img_str += f"![img_{i}](./image_{i}.png)\n"
          image_string = f'''{image_string}![img_{i}-{image}](https://huggingface.co/{repo_id}/resolve/main/model_images/{image})
'''
      description = LoRA_dreambooth_prefs['readme_description']
      if bool(description.strip()):
        description = LoRA_dreambooth_prefs['readme_description'] + '\n\n'
      readme_text = f'''---
license: mit
---
### {name_of_your_model} on Stable Diffusion via LoRA Dreambooth using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
#### model by {api.whoami()["name"]}
This is a model fine-tuned on the {model_path} model taught to Stable Diffusion with LoRA.
It can be used by modifying the `instance_prompt`: **{LoRA_dreambooth_prefs['instance_prompt']}**

{description}You can also train your own models and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).
And you can run your new model via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-models)

Here are the images used for training this model:
{image_string}
'''
      #Save the readme to a file
      #readme_file = open(os.path.join(output_dir, "README.md"), "w")
      #readme_file.write(readme_text)
      #readme_file.close()
      yaml = f"""
---
license: creativeml-openrail-m
base_model: {model_path}
tags:
- stable-diffusion
- stable-diffusion-diffusers
- stable-diffusion-deluxe
- text-to-image
- diffusers
- lora
inference: true
---
      """
      model_card = f"""
# LoRA DreamBooth - {name_of_your_model}
These are LoRA adaption weights for {name_of_your_model}. The weights were trained on {LoRA_dreambooth_args.instance_prompt} using [DreamBooth](https://dreambooth.github.io/).\n
### {repo_id} on Stable Diffusion via LoRA Dreambooth using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
#### Model by {api.whoami()["name"]}

{description}You can also train your own models and upload them to the library by using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb) or [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).

Images used for training this model:
{image_string}
"""
      readme_file = open(os.path.join(output_dir, "README.md"), "w")
      readme_file.write(yaml + model_card)#(readme_text)
      readme_file.close()
      #with open(os.path.join(output_dir, "README.md"), "w") as f:
      #    f.write(yaml + model_card)
      #Save the token identifier to a file
      #text_file = open("token_identifier.txt", "w")
      #text_file.write(LoRA_dreambooth_prefs['instance_prompt'])
      #text_file.close()
      #operations = [
        #CommitOperationAdd(path_in_repo="token_identifier.txt", path_or_fileobj="token_identifier.txt"),
        #CommitOperationAdd(path_in_repo="README.md", path_or_fileobj="README.md"),
      #]
      print(repo_id)
      print(model_card)


      with open(os.path.join(output_dir, ".gitignore"), "w+") as gitignore:
        if "step_*" not in gitignore:
            gitignore.write("step_*\n")
        if "epoch_*" not in gitignore:
            gitignore.write("epoch_*\n")
      try:
        #api.upload_folder(folder_path=output_dir, path_in_repo="", repo_id=repo_id, token=hf_token)
        #api.upload_folder(folder_path=save_path, path_in_repo="model_images", repo_id=repo_id, token=hf_token)
        #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f"Upload the model {name_of_your_model} embeds and token",token=hf_token)
        repo.push_to_hub(commit_message=f"Upload the LoRA model {name_of_your_model} embeds and weights", blocking=False, auto_lfs_prune=True)
      except Exception as e:
        alert_msg(page, f"ERROR Pushing {name_of_your_model} Repository {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
        return
      #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f"Upload the model {name_of_your_model} embeds and token",token=hf_token)
      #api.upload_folder(folder_path="fp16_model", path_in_repo="", repo_id=repo_id,token=hf_token)
      prefs['LoRA_dreambooth_model'] = name_of_your_model
      prefs['custom_models'].append({'name': name_of_your_model, 'path':repo_id})
      page.LoRA_dreambooth_model.options.insert(0, dropdown.Option(name_of_your_model))
      page.LoRA_dreambooth_model.value = name_of_your_model
      page.LoRA_dreambooth_model.update()
      save_settings_file(page)
      prt(Markdown(f"## Your model was saved successfully to _{repo_id}_.\n[Click here to access it](https://huggingface.co/{repo_id}). Use it in _Parameters->Use LaRA Model_ dropdown on top of any other Model loaded.", on_tap_link=lambda e: e.page.launch_url(e.data)))
    play_snd(Snd.ALERT, page)


def run_LoRA(page):
    global LoRA_prefs, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.LoRA_output.controls.append(line)
      page.LoRA_output.update()
    def clear_last(lines=1):
      clear_line(page.LoRA_output, lines=lines)
    if not check_diffusers(page): return
    save_path = os.path.join(root_dir, "my_model")
    error = False
    if not os.path.exists(save_path):
      error = True
    elif len(os.listdir(save_path)) == 0:
      error = True
    if len(page.lora_file_list.controls) == 0:
      error = True
    if error:
      alert_msg(page, "Couldn't find a list of images to train model. Add image files to the list...")
      return
    page.LoRA_output.controls.clear()
    page.LoRA_output.update()
    installer = Installing("Downloading LoRA Conceptualizers")
    prt(installer)
    diffusers_dir = os.path.join(root_dir, "diffusers")
    if not os.path.exists(diffusers_dir):
      os.chdir(root_dir)
      installer.status("...clone diffusers")
      run_process("git clone https://github.com/Skquark/diffusers.git", realtime=False, cwd=root_dir)
    run_process('pip install git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]', cwd=root_dir, realtime=False)
    os.chdir(diffusers_dir)
    installer.status("...install training")
    run_sp('pip install -e ".[training]"', cwd=diffusers_dir, realtime=False)
    #LoRA_dir = os.path.join(diffusers_dir, "examples", "dreambooth")
    LoRA_dir = os.path.join(diffusers_dir, "examples", "text_to_image")
    os.chdir(LoRA_dir)
    installer.status("...installing requirements")
    run_sp("pip install -r requirements.txt", cwd=LoRA_dir, realtime=False)
    pip_install("bitsandbytes", installer=installer)
    installer.status("...accelerate config")
    run_sp("accelerate config default", realtime=False)
    #from accelerate.utils import write_basic_config
    #write_basic_config()
    import argparse
    from io import BytesIO
    from huggingface_hub import HfApi, HfFolder, CommitOperationAdd
    from huggingface_hub import Repository, create_repo, whoami
    #from diffusers import StableDiffusionPipeline
    api = HfApi()
    your_username = api.whoami()["name"]
    hf_token = prefs['HuggingFace_api_key']
    metadata_jsonl = []
    for fl in page.lora_file_list.controls:
        f = fl.title.value
        fn = f.rpartition(slash)[2]
        text = fl.subtitle.value
        metadata_jsonl.append({'file_name':fn, 'text':text})
    with open(os.path.join(save_path, "metadata.jsonl"), "w") as f:
        for meta in metadata_jsonl:
          print(json.dumps(meta), file=f)
        #json.dump(metadata_jsonl, f, ensure_ascii=False, indent=4)
    clear_pipes()
    clear_last()
    #num_new_images = None
    random_seed = get_seed(LoRA_prefs['seed'])
    name_of_your_model = LoRA_prefs['name_of_your_model']
    repo_id = f"{your_username}/{format_filename(name_of_your_model, use_dash=True)}"
    from argparse import Namespace
    #--lr_num_cycles=1 --lr_power=1 --prior_loss_weight=1.0 --sample_batch_size=4 --num_class_images=100
    LoRA_args = Namespace(
        pretrained_model_name_or_path=model_path,
        #dataset_name=repo_id,
        train_data_dir=save_path,
        resolution=LoRA_prefs['resolution'],
        center_crop=True,
        image_column="image",
        caption_column="text",
        #caption_column=LoRA_prefs['instance_prompt'].strip(),
        #instance_data_dir=save_path,
        validation_prompt=LoRA_prefs['validation_prompt'].strip(),
        num_validation_images = LoRA_prefs['num_validation_images'],
        validation_epochs=LoRA_prefs['validation_epochs'],
        learning_rate=LoRA_prefs['learning_rate'],#5e-06,'
        lr_scheduler=LoRA_prefs['lr_scheduler'],
        lr_warmup_steps=LoRA_prefs['lr_warmup_steps'],
        #lr_num_cycles=LoRA_prefs['lr_num_cycles'],
        #lr_power=LoRA_prefs['lr_power'],
        scale_lr=LoRA_prefs['scale_lr'],
        max_train_steps=LoRA_prefs['max_train_steps'],#450,
        train_batch_size=LoRA_prefs['train_batch_size'],
        checkpointing_steps=LoRA_prefs['checkpointing_steps'],
        gradient_accumulation_steps=LoRA_prefs['gradient_accumulation_steps'],
        max_grad_norm=1.0,
        mixed_precision="fp16", # set to "fp16" for mixed-precision training.
        gradient_checkpointing=LoRA_prefs['gradient_checkpointing'], # set this to True to lower the memory usage.
        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes
        enable_xformers_memory_efficient_attention = status['installed_xformers'],
        seed=random_seed,
        with_prior_preservation=LoRA_prefs['prior_preservation'],
        #prior_loss_weight=LoRA_prefs['prior_loss_weight'],
        #sample_batch_size=LoRA_prefs['sample_batch_size'],
        #class_data_dir=LoRA_prefs['class_data_dir'],
        #class_prompt=LoRA_prefs['class_prompt'],
        #num_class_images=LoRA_prefs['num_class_images'],
        #dream_training=LoRA_prefs['dream_training'],
        #dream_detail_preservation=LoRA_prefs['dream_detail_preservation'],
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        hub_model_id=repo_id,
        output_dir=os.path.join(root_dir, "LoRA-model", format_filename(LoRA_prefs['name_of_your_model'], use_dash=True)),
    )
    output_dir = LoRA_args.output_dir
    if not os.path.exists(os.path.join(root_dir, "LoRA-model")): os.makedirs(os.path.join(root_dir, "LoRA-model"))
    #arg_str = "accelerate launch train_dreambooth_lora.py"
    arg_str = 'launch train_text_to_image_lora.py'
    for k, v in vars(LoRA_args).items():
      if isinstance(v, str):
        if ' ' in v:
          v = f'"{v}"'
      if isinstance(v, bool) or v == None:
        if bool(v):
          arg_str += f" --{k}"
      else:
        arg_str += f" --{k}={v}"
    prt(Text("*** Running training ***", weight=FontWeight.BOLD))
    #if num_new_images != None: prt(f"  Number of class images to sample: {num_new_images}.")
    #prt(f"  Instantaneous batch size per device = {LoRA_args.train_batch_size}")
    #prt(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    #prt(f"  Gradient Accumulation steps = {LoRA_args.gradient_accumulation_steps}")
    #prt(f"  Total optimization steps = {LoRA_args.max_train_steps}")
    prt(arg_str)
    progress = ProgressBar(bar_height=8)
    prt(progress)
    if(LoRA_prefs['save_model']):
      private = False if LoRA_prefs['where_to_save_model'] == "Public HuggingFace" else True
      output_dir = LoRA_args.output_dir
      if(not prefs['HuggingFace_api_key']):
        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();
      else:
        hf_token = prefs['HuggingFace_api_key']
      try:
        create_repo(repo_id, private=private, exist_ok=True, token=hf_token)
        repo = Repository(output_dir, clone_from=repo_id, token=hf_token)
      except Exception as e:
        alert_msg(page, f"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
        return
    else:
      if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)

    try:
      #run_sp("accelerate " + arg_str, cwd=LoRA_dir, realtime=True)
      #run_sp(arg_str, cwd=LoRA_dir)
      #%cd $LoRA_dir # type: ignore
      #!accelerate $arg_str # type: ignore
      os.chdir(LoRA_dir)
      os.system("accelerate" + arg_str)
    except Exception as e:
      clear_last()
      alert_msg(page, f"ERROR: Out of Memory (or something else). Try reducing parameters and try again...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
      with torch.no_grad():
        torch.cuda.empty_cache()
      return
    clear_last()
    if(LoRA_prefs['save_model']):
      model_images = os.path.join(output_dir, 'model_images')
      if not os.path.exists(model_images): os.makedirs(model_images, exist_ok=True)
      images_upload = os.listdir(save_path)
      image_string = ""
      #repo_id = f"sd-LoRA-library/{slugify(name_of_your_model)}"
      for i, image in enumerate(images_upload):
          if image.endswith("jsonl"): continue
          img_name = f"image_{i}.png"
          shutil.copy(os.path.join(save_path, image), os.path.join(model_images, image))
          #image.save(os.path.join(repo_folder, f"image_{i}.png"))
          #img_str += f"![img_{i}](./image_{i}.png)\n"
          image_string = f'''{image_string}![img_{i}-{image}](https://huggingface.co/{repo_id}/resolve/main/model_images/{image})
'''
      shutil.copy(os.path.join(save_path, "metadata.jsonl"), os.path.join(model_images, "metadata.jsonl"))
      description = LoRA_prefs['readme_description']
      if bool(description.strip()):
        description = LoRA_prefs['readme_description'] + '\n\n'
      readme_text = f'''---
license: mit
---
### {name_of_your_model} on Stable Diffusion via LoRA Dreambooth using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
#### model by {api.whoami()["name"]}
This your the Stable Diffusion model fine-tuned the {name_of_your_model} model taught to Stable Diffusion with LoRA.
It can be used by modifying the `validation_prompt`: **{LoRA_prefs['validation_prompt']}**

{description}You can also train your own models and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).
And you can run your new model via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-models)

Here are the images used for training this model:
{image_string}
'''
      #Save the readme to a file
      #readme_file = open(os.path.join(output_dir, "README.md"), "w")
      #readme_file.write(readme_text)
      #readme_file.close()
      yaml = f"""
---
license: creativeml-openrail-m
base_model: {model_path}
tags:
- stable-diffusion
- stable-diffusion-diffusers
- stable-diffusion-deluxe
- text-to-image
- diffusers
- lora
inference: true
---
      """
      model_card = f"""
# LoRA Model - {name_of_your_model}
These are LoRA adaption weights for {model_path}. The weights were validated with {LoRA_args.validation_prompt} using [DreamBooth](https://dreambooth.github.io/).\n
### {repo_id} on Stable Diffusion via LoRA using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
#### Model by {your_username}

{description}You can also train your own models and upload them to the library by using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb) or [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).

Images used for training this model:
{image_string}
"""
      readme_file = open(os.path.join(output_dir, "README.md"), "w")
      readme_file.write(yaml + model_card)#(readme_text)
      readme_file.close()
      #with open(os.path.join(output_dir, "README.md"), "w") as f:
      #    f.write(yaml + model_card)
      #Save the token identifier to a file
      #text_file = open("token_identifier.txt", "w")
      #text_file.write(LoRA_prefs['instance_prompt'])
      #text_file.close()
      #operations = [
        #CommitOperationAdd(path_in_repo="token_identifier.txt", path_or_fileobj="token_identifier.txt"),
        #CommitOperationAdd(path_in_repo="README.md", path_or_fileobj="README.md"),
      #]
      print(repo_id)
      print(model_card)

      with open(os.path.join(output_dir, ".gitignore"), "w+") as gitignore:
        if "step_*" not in gitignore:
            gitignore.write("step_*\n")
        if "epoch_*" not in gitignore:
            gitignore.write("epoch_*\n")
      try:
        #api.upload_folder(folder_path=output_dir, path_in_repo="", repo_id=repo_id, token=hf_token)
        #api.upload_folder(folder_path=save_path, path_in_repo="model_images", repo_id=repo_id, token=hf_token)
        #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f"Upload the model {name_of_your_model} embeds and token",token=hf_token)
        repo.push_to_hub(commit_message=f"Upload the LoRA model {name_of_your_model} embeds and weights", blocking=False, auto_lfs_prune=True)
      except Exception as e:
        alert_msg(page, f"ERROR Pushing {name_of_your_model} Repository {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
        return
      #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f"Upload the model {name_of_your_model} embeds and token",token=hf_token)
      #api.upload_folder(folder_path="fp16_model", path_in_repo="", repo_id=repo_id,token=hf_token)
      prefs['LoRA_model'] = name_of_your_model
      prefs['custom_LoRA_models'].append({'name': name_of_your_model, 'path':repo_id})
      page.LoRA_model.options.insert(0, dropdown.Option(name_of_your_model))
      page.LoRA_model.value = name_of_your_model
      page.LoRA_model.update()
      save_settings_file(page)
      prt(Markdown(f"## Your model was saved successfully to _{repo_id}_.\n[Click here to access it](https://huggingface.co/{repo_id}). Use it in _Parameters->Use LaRA Model_ dropdown on top of any other Model loaded.", on_tap_link=lambda e: e.page.launch_url(e.data)))
    play_snd(Snd.ALERT, page)


def run_converter(page):
    global converter_prefs, prefs
    #https://colab.research.google.com/github/camenduru/converter-colab/blob/main/converter_colab.ipynb
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.converter_output.controls.append(line)
      page.converter_output.update()
    def clear_last(lines=1):
      clear_line(page.converter_output, lines=lines)
    if not check_diffusers(page): return
    model_name = converter_prefs['model_name']
    model_path = converter_prefs['model_path']
    if not bool(converter_prefs['model_name']):
      alert_msg(page, "Provide a name to call the converted custom model")
      return
    if not bool(model_path):
      alert_msg(page, "Provide the path to the custom model to convert")
      return
    model_file = format_filename(model_name, use_dash=True)
    if converter_prefs['from_format'] == 'ckpt':
      model_file += '.ckpt'
    page.converter_output.controls.clear()
    installer = Installing("Initializing Converter Tools...")
    prt(installer)
    custom_models = os.path.join(root_dir, 'custom_models',)
    custom_path = os.path.join(custom_models, format_filename(model_name, use_dash=True))
    checkpoint_file = os.path.join(custom_models, model_file)
    makedir(custom_path)
    pip_install("omegaconf gdown==4.7.3", installer=installer)
    installer.status("...get Diffusers")
    diffusers_dir = os.path.join(root_dir, "diffusers")
    if not os.path.exists(diffusers_dir):
      run_process("git clone https://github.com/Skquark/diffusers.git", realtime=False, cwd=root_dir)
    run_process('pip install "git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]"', cwd=root_dir, realtime=False)
    scripts_dir = os.path.join(diffusers_dir, "scripts")
    progress = ProgressBar(bar_height=8)

    installer.status("...downloading")
    if model_path.startswith('https://drive'):
      import gdown
      gdown.download(url=model_path, output=checkpoint_file, quiet=True)
    elif model_path.startswith('http'):
      local = download_file(model_path)
      print(f"Download {model_path} local:{local}")
      shutil.move(local, checkpoint_file)
    elif os.path.isfile(model_path):
      shutil.copy(model_path, checkpoint_file)
    elif os.path.isdir(model_path):
      if os.path.exists(custom_path):
        shutil.rmtree(custom_path)
      shutil.copytree(model_path, custom_path, dirs_exist_ok=True)
      checkpoint_file = custom_path
    elif '/' in model_path and not model_path.startswith('/'):
      checkpoint_file = model_path # hopefully Huggingface
    else:
      alert_msg(page, f"Couldn't recognize source model file path {model_path}.")
      return
    clear_last()
    prt(Text(f'Converting {model_file} to {converter_prefs["to_format"]}...', weight=FontWeight.BOLD))
    prt(progress)

    if converter_prefs['from_format'] == "lora_safetensors":
      run_cmd = f"python {os.path.join(scripts_dir, 'convert_lora_safetensor_to_diffusers.py')}"
    else:
      run_cmd = f"python {os.path.join(scripts_dir, 'convert_original_stable_diffusion_to_diffusers.py')}"
    if converter_prefs['from_format'] == "safetensors":
      run_cmd += f' --from_safetensors'
    if converter_prefs['from_format'] == "controlnet":
      run_cmd += f' --controlnet'
    if converter_prefs['from_format'] == "lora_safetensors" and bool(converter_prefs['base_model']):
      run_cmd += f' --base_model {converter_prefs["base_model"]}'
    if converter_prefs['from_format'] == "ckpt" or converter_prefs['from_format'] == "safetensors" or converter_prefs['from_format'] == "lora_safetensors":
      run_cmd += f' --checkpoint_path {checkpoint_file}'
    if converter_prefs['to_format'] == "safetensors":
      run_cmd += f' --to_safetensors'
    #if status['installed_xformers']:
    if converter_prefs['model_type'] == "SD v1.x text2image" and converter_prefs['from_format'] != "lora_safetensors":
      run_cmd += f' --image_size 512'
    elif converter_prefs['model_type'] == "SD v2.x text2image" and converter_prefs['from_format'] != "lora_safetensors":
      run_cmd += f' --image_size 768'
      run_cmd += f' --upcast_attention'
      run_cmd += f' --prediction_type v_prediction'
    run_cmd += f' --scheduler_type {converter_prefs["scheduler_type"]}'
    if converter_prefs['half_percision']:
      run_cmd += f' --half'
    run_cmd += f' --dump_path {custom_path}'
    #TODO: Add this to UI to use
    #if bool(converter_prefs['vae_path']): #"Set to a path, hub id to an already converted vae to not convert it again."
    #  run_cmd += f' --vae_path {converter_prefs["vae_path"]}'
    if converter_prefs['from_format'] == converter_prefs["to_format"]:
      out_file = os.path.join(custom_path, f"{format_filename(model_name, use_dash=True)}.{converter_prefs['to_format']}")
      print(f"From and To Formats are the same. Using file without conversion to {out_file}")
      shutil.move(checkpoint_file, out_file)
      custom_path = out_file
    else:
      prt(f"Running {run_cmd}")
      try:
        run_sp(run_cmd, cwd=scripts_dir, realtime=True)
        #run_process(run_cmd, page=page, cwd=scripts_dir, show=True)
      except Exception as e:
        clear_last()
        alert_msg(page, "Error Running convert_original_stable_diffusion_to_diffusers", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
      if len(os.listdir(custom_path)) == 0:
        prt(f"Problem converting your model. Check console and source checkpoint and try again...")
        os.rmdir(custom_path)
        return
      clear_last()
    clear_last()
    clear_last()
    prt(f"Done Converting... Saved locally at {custom_path}")
    if converter_prefs['load_custom_model']:
      prefs['custom_model'] = custom_path
      prefs['custom_models'].append({'name': model_name, 'path':custom_path})
    if(converter_prefs['save_model']):
      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd
      from huggingface_hub import model_info, create_repo, create_branch, upload_folder
      from huggingface_hub.utils import RepositoryNotFoundError, revisionNotFoundError
      from diffusers import StableDiffusionPipeline
      api = HfApi()
      your_username = api.whoami()["name"]
      '''dreambooth_pipe = StableDiffusionPipeline.from_pretrained(
        custom_path,
        torch_dtype=torch.float16,
      ).to("cuda")
      os.makedirs("fp16_model",exist_ok=True)
      dreambooth_pipe.save_pretrained("fp16_model")'''
      hf_token = prefs['HuggingFace_api_key']
      private = True
      if(converter_prefs['where_to_save_model'] == "Public Library"):
        private = False
      #  repo_id = f"sd-dreambooth-library/{slugify(model_name)}"
      if '/' in model_path and not model_path.startswith('/'):
        repo_id = model_path
      else:
        repo_id = f"{your_username}/{format_filename(model_name, use_dash=True)}"
      #output_dir = dreambooth_args.output_dir
      if(not bool(prefs['HuggingFace_api_key'])):
        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();
      else:
        hf_token = prefs['HuggingFace_api_key']

      description = converter_prefs['readme_description']
      if bool(description.strip()):
        description = converter_prefs['readme_description'] + '\n\n'
      readme_text = f'''---
license: mit
---
### {model_name} model on Stable Diffusion using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
#### model by {api.whoami()["name"]}

{description}
You can also train your own models and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).
And you can run your new concept via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-concepts)

    '''
      #Save the readme to a file
      readme_file = open("README.md", "w")
      readme_file.write(readme_text)
      readme_file.close()
      #Save the token identifier to a file
      '''text_file = open("token_identifier.txt", "w")
      text_file.write(dreambooth_prefs['instance_prompt'])
      text_file.close()'''
      operations = [
        #CommitOperationAdd(path_in_repo="token_identifier.txt", path_or_fileobj="token_identifier.txt"),
        CommitOperationAdd(path_in_repo="README.md", path_or_fileobj="README.md"),
      ]
      print(repo_id)
      print(readme_text)
      try:
          repo_exists = True
          r_info = model_info(repo_id, token=hf_token)
      except RepositoryNotFoundError:
          repo_exists = False
          pass
      if not repo_exists:
        try:
          create_repo(repo_id, private=private, token=hf_token)
        except Exception as e:
          alert_msg(page, f"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
          return
      branch = f"{converter_prefs['from_format']}-to-{converter_prefs['to_format']}"
      try:
          branch_exists = True
          b_info = model_info(repo_id, revision=branch, token=hf_token)
      except revisionNotFoundError:
          branch_exists = False
      finally:
          if branch_exists:
              print(b_info)
          else:
              create_branch(repo_id, branch=branch, token=hf_token)
      api.create_commit(repo_id=repo_id, operations=operations, commit_message=f"Upload the converted model {model_name} embeds",token=hf_token)
      api.upload_folder(folder_path=custom_path, path_in_repo="", revision=branch, repo_id=repo_id, commit_message=f"Upload the converted model {model_name} embeds", token=hf_token)
      #api.upload_folder(folder_path="fp16_model", path_in_repo="", repo_id=repo_id,token=hf_token)
      #api.upload_folder(folder_path=save_path, path_in_repo="concept_images", repo_id=repo_id, token=hf_token)
      prefs['custom_model'] = repo_id
      prefs['custom_models'].append({'name': model_name, 'path':repo_id})
      page.custom_model.value = repo_id
      try:
        page.custom_model.update()
      except Exception: pass
      prt(Markdown(f"## Your model was saved successfully to _{repo_id}_.<br>[Click here to access it](https://huggingface.co/{repo_id}) and go to _Installers->Model Checkpoint->Custom Model Path_ to use. Include Token in prompts.", on_tap_link=lambda e: e.page.launch_url(e.data)))

    def push_ckpt(model_to, token, branch):
      try:
          repo_exists = True
          r_info = model_info(model_to, token=token)
      except RepositoryNotFoundError:
          repo_exists = False
      finally:
          if repo_exists:
              print(r_info)
          else:
              create_repo(model_to, private=True, token=token)
      try:
          branch_exists = True
          b_info = model_info(model_to, revision=branch, token=token)
      except revisionNotFoundError:
          branch_exists = False
      finally:
          if branch_exists:
              print(b_info)
          else:
              create_branch(model_to, branch=branch, token=token)
      upload_folder(folder_path="ckpt", path_in_repo="", revision=branch, repo_id=model_to, commit_message=f"ckpt", token=token)
      return "push ckpt done!"
    play_snd(Snd.ALERT, page)

def run_checkpoint_merger(page):
    global checkpoint_merger_prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.checkpoint_merger_output.controls.append(line)
      page.checkpoint_merger_output.update()
    def clear_last(lines=1):
      clear_line(page.checkpoint_merger_output, lines=lines)
    if not check_diffusers(page): return
    if len(checkpoint_merger_prefs['pretrained_models']) < 2:
        alert_msg(page, "Select 2 or more compatible checkpoint models to the list before running...")
        return
    prt(Installing("Downloading Required Models and Merging..."))
    try:
        from diffusers import DiffusionPipeline
        model_path = checkpoint_merger_prefs['pretrained_models'][0]
        checkpoint_merger_pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="checkpoint_merger.py", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        merged_pipe = checkpoint_merger_pipe.merge(checkpoint_merger_prefs['pretrained_models'], interp = checkpoint_merger_prefs['interp'] if checkpoint_merger_prefs['interp'] != "weighted_sum" else None, alpha = checkpoint_merger_prefs['alpha'], force = checkpoint_merger_prefs['force'], cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        merged_pipe.to(torch_device)
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR: Problem running Merger. Check parameters and try again...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
        with torch.no_grad():
            torch.cuda.empty_cache()
        return
    model_name = format_filename(checkpoint_merger_prefs['name_of_your_model'], force_underscores=True)
    output_dir = os.path.join(root_dir, 'my_models', model_name)
    repo_id = f"{prefs['HuggingFace_username']}/{model_name}"
    if bool(checkpoint_merger_prefs['validation_prompt']):
        prt("Generating Test Validation Image...")
        try:
            image = merged_pipe(checkpoint_merger_prefs['validation_prompt']).images[0]
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Problem creating image with merged_pipe...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
            with torch.no_grad():
                torch.cuda.empty_cache()
            return
        fname = format_filename(checkpoint_merger_prefs['validation_prompt'])
        fpath = available_file(stable_dir, fname, 0)
        image.save(fpath)
        clear_last()
        prt(Img(src=asset_dir(fpath)))
    if checkpoint_merger_prefs['save_model']:
        private = False if checkpoint_merger_prefs['where_to_save_model'] == "Public HuggingFace" else True
        from huggingface_hub import HfFolder, create_repo, Repository
        if(not prefs['HuggingFace_api_key']):
            with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();
        else:
            hf_token = prefs['HuggingFace_api_key']
        try:
            create_repo(repo_id, private=private, exist_ok=True, token=hf_token)
            repo = Repository(output_dir, clone_from=repo_id, token=hf_token)
        except Exception as e:
            alert_msg(page, f"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
            return
    else:
        if os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
    try:
        merged_pipe.save_pretrained(output_dir)
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR: Issue saving pretrained.  Check parameters and try again...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
        with torch.no_grad():
            torch.cuda.empty_cache()
        return
    del checkpoint_merger_pipe
    del merged_pipe
    if checkpoint_merger_prefs['save_model']:
        description = checkpoint_merger_prefs['readme_description']
        if bool(description.strip()):
            description = checkpoint_merger_prefs['readme_description'] + '\n\n'
        models_string = ""
        for m in checkpoint_merger_prefs['pretrained_models']:
            models_string += f"* [{m}](https://huggingface.co/{m})\n"
        yaml = f"""
---
license: creativeml-openrail-m
base_model: {model_path}
tags:
- stable-diffusion
- stable-diffusion-diffusers
- stable-diffusion-deluxe
- text-to-image
- diffusers
- lora
- merge
inference: true
---
"""
        model_card = f"""
# Merged Checkpoint Model - {checkpoint_merger_prefs['name_of_your_model']}
These are fine-tuned combined weights of {' + '.join(checkpoint_merger_prefs['pretrained_models'])}.\n
### {repo_id} on Stable Diffusion via Custom Checkpoint using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
#### Model by {prefs['HuggingFace_username']}
{description}

Checkpoints used for training this model:
{models_string}
Alpha Interpolation: {checkpoint_merger_prefs['alpha']}
Interpolation Method: {checkpoint_merger_prefs['interp']}
"""
        readme_file = open(os.path.join(output_dir, "README.md"), "w")
        readme_file.write(yaml + model_card)#(readme_text)
        readme_file.close()
        print(repo_id)
        print(model_card)
        with open(os.path.join(output_dir, ".gitignore"), "w+") as gitignore:
            if "step_*" not in gitignore:
                gitignore.write("step_*\n")
            if "epoch_*" not in gitignore:
                gitignore.write("epoch_*\n")
        try:
            #api.upload_folder(folder_path=output_dir, path_in_repo="", repo_id=repo_id, token=hf_token)
            #api.upload_folder(folder_path=save_path, path_in_repo="model_images", repo_id=repo_id, token=hf_token)
            #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f"Upload the model {name_of_your_model} embeds and token",token=hf_token)
            repo.push_to_hub(commit_message=f"Upload the Merged model {model_name} embeds and weights", blocking=False, auto_lfs_prune=True)
        except Exception as e:
            alert_msg(page, f"ERROR Pushing {model_name} Repository {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
            return
        prt(Markdown(f"## Your model was saved successfully to _{repo_id}_.\n[Click here to access it](https://huggingface.co/{repo_id}). Use it in _Installers->Diffusers Custom Model_ dropdown.", on_tap_link=lambda e: e.page.launch_url(e.data)))

    play_snd(Snd.ALERT, page)

def run_tortoise_tts(page):
    #https://github.com/neonbjb/tortoise-tts
    global tortoise_prefs, pipe_tortoise_tts, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.tortoise_output.controls.append(line)
      page.tortoise_output.update()
    def clear_last(lines=1):
      clear_line(page.tortoise_output, lines=lines)
    def play_audio(e):
      e.control.data.play()
    if tortoise_prefs['train_custom'] and not bool(tortoise_prefs['custom_voice_name']):
      alert_msg(page, "Provide a Custom Voice Name when training your audio files.")
      return
    if not bool(tortoise_prefs['text']):
      alert_msg(page, "Provide Text for the AI voice to read...")
      return
    progress = ProgressBar(bar_height=8)
    installer = Installing("Downloading Tortoise-TTS Packages...")
    prt(installer)
    tortoise_dir = os.path.join(root_dir, "tortoise-tts")
    voice_dir = os.path.join(tortoise_dir, 'tortoise', 'voices')
    if not os.path.isdir(tortoise_dir):
      installer.status("...cloning jnorberg/toroise-tts")
      os.chdir(root_dir)
      run_process("git clone https://github.com/jnordberg/tortoise-tts.git", page=page)
    os.chdir(tortoise_dir)
    pip_install("ffmpeg pydub", installer=installer)
    try:
      from tortoise.api import TextToSpeech
    except Exception:
      installer.status("...installing all requirements")
      try:
        run_process("pip install -r requirements.txt", page=page, cwd=tortoise_dir)
        run_process("python setup.py install", page=page, cwd=tortoise_dir)
      except Exception as e:
        clear_last()
        alert_msg(page, "Error Installing Tortoise TextToSpeech requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
        return
      pass
    import torch
    import torchaudio
    import torch.nn as nn
    import torch.nn.functional as F
    from tortoise.api import TextToSpeech
    from tortoise.utils.audio import load_audio, load_voice, load_voices
    import pydub
    clear_pipes('tortoise_tts')
    # This will download all the models used by Tortoise from the HuggingFace hub.
    if pipe_tortoise_tts == None:
      installer.status("...initialize TextToSpeech pipe")
      try:
        pipe_tortoise_tts = TextToSpeech()
      except Exception as e:
        clear_last()
        alert_msg(page, "Error downloading Tortoise TextToSpeech package", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    clear_last()
    prt(Text("  Generating Tortoise Text-to-Speech... (slow, but wins the race)", weight=FontWeight.BOLD))
    prt(progress)
    save_dir = os.path.join(root_dir, 'audio_out', tortoise_prefs['batch_folder_name'])
    if not os.path.exists(save_dir):
      os.makedirs(save_dir, exist_ok=True)
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    custom_voices = os.path.join(audio_out, 'custom_voices')
    if bool(tortoise_prefs['batch_folder_name']):
      audio_out = os.path.join(audio_out, tortoise_prefs['batch_folder_name'])
    os.makedirs(audio_out, exist_ok=True)
    #voice_dirs = os.listdir(os.path.join(root_dir, "tortoise-tts", 'tortoise', 'voices'))
    #print(str(voice_dirs))
    fname = format_filename(tortoise_prefs['text'])
    if fname[-1] == '.': fname = fname[:-1]
    file_prefix = tortoise_prefs['file_prefix']
    tortoise_custom_voices = prefs['tortoise_custom_voices']
    tortoise_prefs['voice'] = []
    if tortoise_prefs['train_custom']:
        if len(tortoise_prefs['custom_wavs']) <2:
          alert_msg(page, "To train a custom voice, provide at least 2 audio files to mimic.")
          return
        CUSTOM_VOICE_NAME = format_filename(tortoise_prefs['custom_voice_name'])
        custom_voice_folder = os.path.join(root_dir, "tortoise-tts", 'tortoise', 'voices', CUSTOM_VOICE_NAME)
        os.makedirs(custom_voice_folder, exist_ok=True)
        for i, f in enumerate(tortoise_prefs['custom_wavs']):
            if f.lower().endswith('mp3'):
              sound = pydub.AudioSpegment.from_mp3(f)
              sound.export(os.path.join(custom_voice_folder, f'{i+1}.wav'), format="wav", bitrate="22050")
            elif f.lower().endswith('wav'):
              sound = pydub.AudioSpegment.from_wav(f)
              sound.export(os.path.join(custom_voice_folder, f'{i+1}.wav'), format="wav", bitrate="22050")
            else:
              alert_msg(f"Unknown file type {f.rpartition('.')[2]}... Use only .wav and .mp3 audio clips.")
              return
              #shutil.copy(f, os.path.join(custom_voice_folder, f'{i+1}.wav'))
        #for i, file_data in enumerate(files.upload().values()):
        #    with open(os.path.join(custom_voice_folder, f'{i}.wav'), 'wb') as f:
        #        f.write(file_data)
        if not CUSTOM_VOICE_NAME in tortoise_prefs['voice']:
          #tortoise_prefs['voice'].append(CUSTOM_VOICE_NAME)
          page.tortoise_voices.controls.append(Checkbox(label=CUSTOM_VOICE_NAME, value=True, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
          page.tortoise_voices.update()
          output_voice_folder = os.path.join(custom_voices, CUSTOM_VOICE_NAME)
          if os.path.exists(output_voice_folder):
            shutil.rmtree(output_voice_folder)
            print(f'Output Voice Folder already existed: {output_voice_folder}... Deleting to remake.')
          #os.makedirs(output_voice_folder, exist_ok=False)
          shutil.copytree(custom_voice_folder, output_voice_folder)
          prefs['tortoise_custom_voices'].append({'name':CUSTOM_VOICE_NAME, 'folder': output_voice_folder})
          save_settings_file(page)
    for v in page.tortoise_voices.controls:
        if v.value == True:
          tortoise_prefs['voice'].append(v.label)
          if not os.path.exists(os.path.join(voice_dir, v.label)):
            for custom in tortoise_custom_voices:
              if custom['name'] == v.label:
                if os.path.exists(custom['folder']):
                  #os.makedirs(os.path.join(voice_dir, custom['name'], exist_ok=True))
                  shutil.copytree(custom['folder'], os.path.join(voice_dir, custom['name']))
                else:
                  print(f"Couldn't find custom folder {custom['folder']}")
    # Load it and send it through Tortoise.
    voice = tortoise_prefs['voice']
    v_str = voice if isinstance(voice, str) else '+'.join(voice) if isinstance(voice, list) else ''
    if len(voice) == 0: v_str = 'random'
    audio_name = f'{file_prefix}{v_str}-{fname}'
    audio_name = audio_name[:int(prefs['file_max_length'])]
    fname = available_file(save_dir, audio_name, 0, ext="wav")
    #print(str(voice))
    #print(fname)
    if len(voice) == 0:
        voice_samples = conditioning_latents = None
    elif len(voice) == 1:
        voice_samples, conditioning_latents = load_voice(voice[0])
    else:
        voice_samples, conditioning_latents = load_voices(voice)
    gen = pipe_tortoise_tts.tts_with_preset(tortoise_prefs['text'], voice_samples=voice_samples, conditioning_latents=conditioning_latents, preset=tortoise_prefs['preset'])
    torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)
    #IPython.display.Audio('generated.wav')
    clear_last()
    clear_last()
    #a_out = Audio(src=fname, autoplay=False)
    #page.overlay.append(a_out)
    #page.update()
    display_name = fname
    #a.tofile(f"/content/dance-{i}.wav")
    if storage_type == "Colab Google Drive":
      audio_save = available_file(audio_out, audio_name, 0, ext='wav')
      shutil.copy(fname, audio_save)
      display_name = audio_save
    prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))
    #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
    play_snd(Snd.ALERT, page)

def run_openai_tts(page):
    global openai_tts_prefs, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.openai_tts_output.controls.append(line)
      page.openai_tts_output.update()
    def clear_last(lines=1):
      clear_line(page.openai_tts_output, lines=lines)
    if not bool(openai_tts_prefs['text']):
      alert_msg(page, "Provide Text for the AI voice to read...")
      return
    progress = ProgressBar(bar_height=8)
    installer = Installing("Installing OpenAI API Package...")
    prt(installer)
    #pip_install("ffmpeg pydub", installer=installer)
    try:
        import openai
        if version.parse(openai.__version__).base_version < version.parse("1.12.0"):
            installer.status("...uninstalling old openai")
            run_process("pip uninstall -y openai", realtime=False)
            raise ModuleNotFoundError("Forcing update")
        if force_update("openai"): raise ModuleNotFoundError("Forcing update")
    except:
        installer.status("...installing openai")
        run_process("pip install -q --upgrade openai", realtime=False)
        clear_last()
        import openai
        pass
    try:
        #openai.api_key = prefs['OpenAI_api_key']
        from openai import OpenAI
        client = OpenAI(api_key=prefs['OpenAI_api_key'])
    except Exception as e:
        alert_msg(page, f"Seems like your OpenAI API Key is Invalid. Check it again...", content=Text(str(e)))
        return
    clear_last()
    prt(Text("  Generating OpenAI Text-to-Speech...", weight=FontWeight.BOLD))
    prt(progress)
    save_dir = os.path.join(root_dir, 'audio_out', openai_tts_prefs['batch_folder_name'])
    if not os.path.exists(save_dir):
      os.makedirs(save_dir, exist_ok=True)
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    if bool(openai_tts_prefs['batch_folder_name']):
      audio_out = os.path.join(audio_out, openai_tts_prefs['batch_folder_name'])
    os.makedirs(audio_out, exist_ok=True)
    #voice_dirs = os.listdir(os.path.join(root_dir, "openai_tts-tts", 'openai_tts', 'voices'))
    #print(str(voice_dirs))
    fname = format_filename(openai_tts_prefs['text'])
    if fname[-1] == '.': fname = fname[:-1]
    file_prefix = openai_tts_prefs['file_prefix']
    voice = openai_tts_prefs['voice'].lower()
    format = openai_tts_prefs['format']
    audio_name = f'{file_prefix}{openai_tts_prefs["voice"]}-{fname}'
    audio_name = audio_name[:int(prefs['file_max_length'])]
    fname = available_file(save_dir, audio_name, 0, ext=format)
    try:
        response = client.audio.speech.create(
            model=openai_tts_prefs['preset'],
            voice=voice,
            input=openai_tts_prefs['text'],
            response_format=format,
            speed=float(openai_tts_prefs['speed']),
        )
        response.stream_to_file(fname)#with_streaming_response
    except Exception as e:
        clear_last()
        clear_last()
        alert_msg(page, f"ERROR: Couldn't run Text-To-Speech on your text for some reason...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    clear_last()
    clear_last()
    display_name = fname
    if storage_type == "Colab Google Drive":
      audio_save = available_file(audio_out, audio_name, 0, ext=format)
      shutil.copy(fname, audio_save)
      display_name = audio_save
    prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))
    play_snd(Snd.ALERT, page)

def run_audio_ldm(page):
    global audioLDM_prefs, pipe_audio_ldm, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.audioLDM_output.controls.append(line)
      page.audioLDM_output.update()
    def clear_last(lines=1):
      clear_line(page.audioLDM_output, lines=lines)
    def play_audio(e):
      e.control.data.play()
    if not bool(audioLDM_prefs['text']):
      alert_msg(page, "Provide Text for the AI to create the sound of...")
      return
    if not check_diffusers(page): return
    progress = ProgressBar(bar_height=8)
    installer = Installing("Downloading Audio LDM Packages...", )
    prt(installer)
    audioLDM_dir = os.path.join(root_dir, "audioldm-text-to-audio-generation")
    #voice_dir = os.path.join(audioLDM_dir, 'audioldm', 'voices')
    if not os.path.isdir(audioLDM_dir):
      os.chdir(root_dir)
      run_process("git clone https://huggingface.co/spaces/haoheliu/audioldm-text-to-audio-generation", page=page)
    os.chdir(audioLDM_dir)
    import sys
    if os.path.join(audioLDM_dir, 'audioldm') not in sys.path:
        sys.path.append(os.path.join(audioLDM_dir, 'audioldm'))
    try:
        from audioldm import text_to_audio, build_model
    except Exception:
        pip_install("einops pyyaml soundfile librosa torchlibrosa pandas gradio", installer=installer, q=True)
    finally:
        from audioldm import text_to_audio, build_model
    import soundfile as sf
    model_id="cvssp/audioldm-s-full-v2"
    clear_pipes('audio_ldm')
    # This will download all the models used by Audio LDM from the HuggingFace hub.
    if pipe_audio_ldm == None:
      try:
        pipe_audio_ldm = build_model()
      except Exception as e:
        clear_last()
        alert_msg(page, "Error downloading Audio LDM package", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    clear_last()
    prt(Text("  Generating AudioLDM Sounds...", weight=FontWeight.BOLD))
    prt(progress)
    random_seed = get_seed(audioLDM_prefs['seed'])
    try:
      waveform = text_to_audio(pipe_audio_ldm, audioLDM_prefs['text'], random_seed, duration=audioLDM_prefs['duration'], guidance_scale=audioLDM_prefs['guidance_scale'], n_candidate_gen_per_text=int(audioLDM_prefs['n_candidates']))
    except Exception as e:
      clear_last()
      alert_msg(page, "Error generating text_to_audio waveform...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
      return
    save_dir = os.path.join(root_dir, 'audio_out', audioLDM_prefs['batch_folder_name'])
    makedir(save_dir)
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    if bool(audioLDM_prefs['batch_folder_name']):
      audio_out = os.path.join(audio_out, audioLDM_prefs['batch_folder_name'])
    makedir(audio_out)
    #voice_dirs = os.listdir(os.path.join(root_dir, "audioldm-tts", 'audioldm', 'voices'))
    #print(str(voice_dirs))
    fname = format_filename(audioLDM_prefs['text'])
    if fname[-1] == '.': fname = fname[:-1]
    file_prefix = audioLDM_prefs['file_prefix']
    audio_name = f'{file_prefix}-{fname}'
    audio_name = audio_name[:int(prefs['file_max_length'])]
    fname = available_file(save_dir, audio_name, 0, ext="wav")
    for i in range(waveform.shape[0]):
        sf.write(fname, waveform[i, 0], samplerate=16000)
    #torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)
    #IPython.display.Audio('generated.wav')
    clear_last(2)
    display_name = fname
    #a.tofile(f"/content/dance-{i}.wav")
    if storage_type == "Colab Google Drive":
      audio_save = available_file(audio_out, audio_name, 0, ext='wav')
      shutil.copy(fname, audio_save)
      display_name = audio_save
    #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
    prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))
    nudge(page.audioLDM, page)
    play_snd(Snd.ALERT, page)

def run_audio_ldm2(page):
    global audioLDM2_prefs, pipe_audio_ldm2, prefs, status
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.audioLDM2_output.controls.append(line)
      page.audioLDM2_output.update()
    def clear_last(lines=1):
      clear_line(page.audioLDM2_output, lines=lines)
    if not bool(audioLDM2_prefs['text']):
      alert_msg(page, "Provide Text for the AI to create the sound of...")
      return
    if not check_diffusers(page): return
    progress = ProgressBar(bar_height=8)
    total_steps = audioLDM2_prefs['steps']# * audioLDM2_prefs['batch_size']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    installer = Installing("Loading Audio LDM-2 Packages...")
    prt(installer)
    pip_install("soundfile pandas scipy", installer=installer)
    import soundfile as sf
    import scipy
    if audioLDM2_prefs['save_mp3']:
        get_ffmpeg(installer)
        pip_install("pydub", q=True, installer=installer)
        import ffmpeg, pydub
    from diffusers import AudioLDM2Pipeline
    model_id = audioLDM2_prefs['model_name']
    if 'loaded_ldm2' not in status:
        status['loaded_ldm2'] = ""
    if status['loaded_ldm2'] == model_id:
        clear_pipes('audio_ldm2')
    else:
        clear_pipes()
    # This will download all the models used by Audio LDM from the HuggingFace hub.
    if pipe_audio_ldm2 == None:
      try:
        installer.status("...loading pipeline")
        pipe_audio_ldm2 = AudioLDM2Pipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)#build_model(model_name=model_id)
        pipe_audio_ldm2 = pipeline_scheduler(pipe_audio_ldm2)
        status['loaded_scheduler'] = prefs['scheduler_mode']
        #pipe_audio_ldm2.scheduler = LMSDiscreteScheduler.from_config(pipe_audio_ldm2.scheduler.config)
        if prefs['enable_torch_compile']:
            installer.status("...torch compile")
            pipe_audio_ldm2 = torch.compile(pipe_audio_ldm2)
            #torch.set_float32_matmul_precision("high")
        pipe_audio_ldm2 = pipe_audio_ldm2.to(torch_device)
        pipe_audio_ldm2.set_progress_bar_config(disable=True)
        status['loaded_ldm2'] = model_id
      except Exception as e:
        clear_last()
        alert_msg(page, "Error downloading Audio LDM 2 model", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        installer.status(f"...scheduler {prefs['scheduler_mode']}")
        pipe_audio_ldm2 = pipeline_scheduler(pipe_audio_ldm2)
      
    clear_last()
    prt(Text("  Generating AudioLDM-2 Sounds...", weight=FontWeight.BOLD))
    prt(progress)
    random_seed = get_seed(audioLDM2_prefs['seed'])
    generator = torch.Generator("cuda").manual_seed(random_seed)
    extra_args = {'transcription': audioLDM2_prefs['transcription'], 'max_new_tokens': 512} if 'speech' in model_id else {}
    try:
      audios = pipe_audio_ldm2(audioLDM2_prefs['text'],
          negative_prompt=audioLDM2_prefs['negative_prompt'],
          num_inference_steps=audioLDM2_prefs['steps'],
          guidance_scale=audioLDM2_prefs['guidance_scale'],
          audio_length_in_s=audioLDM2_prefs['duration'],
          num_waveforms_per_prompt=audioLDM2_prefs['batch_size'],
          generator=generator,
          callback=callback_fnc,
          **extra_args,
      ).audios
      #waveform = text_to_audio(pipe_audio_ldm2, audioLDM2_prefs['text'], random_seed, duration=10, guidance_scale=audioLDM2_prefs['guidance_scale'], n_candidate_gen_per_text=int(audioLDM2_prefs['n_candidates']), batchsize=int(audioLDM2_prefs['batch_size']), transcript=audioLDM2_prefs['transcription'] if 'speech' in model_id else "")
    except Exception as e:
      clear_last()
      alert_msg(page, "Error generating AudioLDM2 waveform...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
      return
    save_dir = os.path.join(root_dir, 'audio_out', audioLDM2_prefs['batch_folder_name'])
    makedir(save_dir)
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    if bool(audioLDM2_prefs['batch_folder_name']):
      audio_out = os.path.join(audio_out, audioLDM2_prefs['batch_folder_name'])
    makedir(audio_out)
    #voice_dirs = os.listdir(os.path.join(root_dir, "audioldm2-tts", 'audioldm2', 'voices'))
    #print(str(voice_dirs))
    # waveform = [(16000, np.random.randn(16000)), (16000, np.random.randn(16000))]
    clear_last()
    clear_last()
    num = 0
    for audio in audios:
        fname = format_filename(audioLDM2_prefs['text'])
        if fname[-1] == '.': fname = fname[:-1]
        file_prefix = audioLDM2_prefs['file_prefix']
        audio_name = f'{file_prefix}-{fname}'
        audio_name = audio_name[:int(prefs['file_max_length'])]
        audio_metadata = {
          "sample_rate": 16000,
          "artist": prefs['meta_ArtistName'],
          "copyright": prefs['meta_Copyright'],
          "software": "Stable Diffusion Deluxe",
        }
        if prefs['save_config_in_metadata']:
          config_json = audioLDM2_prefs.copy()
          del config_json['batch_size']
          del config_json['n_candidates']
          del config_json['file_prefix']
          config_json['seed'] = random_seed + num
          config_json['sceduler'] = status['loaded_scheduler']
          audio_metadata["config"] = config_json
        fname = available_file(save_dir, audio_name, 0, ext="wav")
        scipy.io.wavfile.write(fname, data=audio, rate=16000)#, audio_metadata
        #for i in range(waveform.shape[0]):
        #    sf.write(fname, waveform[i, 0], samplerate=16000)
        #torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)
        if audioLDM2_prefs['save_mp3']:
          wav_file = pydub.AudioSegment.from_wav(fname)
          #tags = pydub.utils.mediainfo(wav_file).get('TAG', {})
          mp3_name = available_file(audio_out, audio_name, 0, ext="mp3")
          mp3_file = wav_file.export(mp3_name, format="mp3", tags=audio_metadata)
          os.remove(fname)
          fname = mp3_name
          display_name = fname
        else:
        #if storage_type == "Colab Google Drive":
          audio_save = available_file(audio_out, audio_name, 0, ext='wav')
          shutil.move(fname, audio_save)
          fname = audio_save
          display_name = audio_save
        #display_name = fname
        #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
        prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))
        num += 1
    play_snd(Snd.ALERT, page)

def run_music_lang(page):
    global musiclang_prefs, pipe_music_lang, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.MusicLang.controls.append(line)
      page.MusicLang.update()
    def clear_last(lines=1):
      clear_line(page.MusicLang, lines=lines)
    #if not bool(musiclang_prefs['midi_file']):
    #  alert_msg(page, "Provide a MIDI file for the AI to create the music of...")
    #  return
    progress = ProgressBar(bar_height=8)
    installer = Installing("Downloading MusicLang Packages...", )
    prt(installer)
    pip_install("musiclang_predict==1.1.5 pyFluidSynth midi2audio fluidsynth", installer=installer, q=True)
    get_ffmpeg(installer)
    from musiclang_predict import MusicLangPredictor
    from musiclang import Score
    from midi2audio import FluidSynth
    import ffmpeg
    model_id="musiclang/musiclang-v2"
    clear_pipes('music_lang')
    if pipe_music_lang == None:
        installer.status("...initializing MusicLang model...")
        try:
            pipe_music_lang = MusicLangPredictor(model_id)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error downloading MusicLang package", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
            return
    fluidR3 = os.path.join(uploads_dir, "FluidR3_GM.sf2")
    if not os.path.isfile(fluidR3):
        installer.status("...downloading FluidR3_GM.sf2")
        download_file("https://musical-artifacts.com/artifacts/738/FluidR3_GM.sf2", to=uploads_dir)
    random_seed = get_seed(musiclang_prefs['seed'])
    #time_signature = (4, 4)
    midi_file = musiclang_prefs['midi_file'].strip()
    bar_range = musiclang_prefs['bar_range']
    chord_progression = musiclang_prefs['chord_progression']
    nb_tokens = musiclang_prefs['nb_tokens']
    temperature = musiclang_prefs['temperature']
    top_p = musiclang_prefs['top_p']
    if bool(midi_file):
        # Load the MIDI file and use it as the score prompt
        filepath = midi_file
        start_bar, end_bar = map(int, bar_range.split("-"))
        score = Score.from_midi(filepath, chord_range=(start_bar, end_bar))
        tempo = score.config['tempo'] # Use the tempo from the MIDI file and change input
        time_signature = score.config['time_signature']
        time_signature = (time_signature[1], time_signature[2])
        tempo_message = f"Warning : real tempo of file is : {int(tempo)} BPM."  # Update message based on MIDI file
        print(tempo_message)
    else:
        time_sig = musiclang_prefs['time_signature']
        bad_time = False
        if '/' not in time_sig:
            bad_time = True
        try:
            beat, note = time_sig.split('/')
            beat = int(beat.strip())
            note = int(note.strip())
            time_signature = (beat, note)
        except Exception:
            bad_time = True
            pass
        if bad_time:
            alert_msg(page, "Invalid Time Signature. Please use the format like 4/4 or 3/4 or 7/8")
            return
        tempo = int(musiclang_prefs['tempo'])
        score = None  # Default score is None if no MIDI file is uploaded
    clear_last()
    prt(Text("  Generating MusicLang Sounds...", weight=FontWeight.BOLD))
    prt(progress)
    try:
        if chord_progression.strip() == "" and score is None:
            # Generate without specific chord progression or MIDI prompt
            generated_score = pipe_music_lang.predict(
                nb_tokens=int(nb_tokens),
                temperature=float(temperature),
                topp=top_p,
                rng_seed=random_seed
            )
        elif score is not None and chord_progression.strip() == "":
            # Generate using the uploaded MIDI file as a prompt
            generated_score = pipe_music_lang.predict(
                score=score,  # Use the uploaded MIDI as the score prompt
                nb_tokens=int(nb_tokens),
                temperature=float(temperature),
                topp=top_p,
                rng_seed=random_seed
            )
        else:
            # Generate with specific chord progression
            generated_score = pipe_music_lang.predict_chords(
                chord_progression,
                score=score,  # Use the uploaded MIDI as the score prompt
                time_signature=time_signature,
                temperature=temperature,
                topp=top_p,
                rng_seed=random_seed
            )
    except Exception as e:
        clear_last()
        alert_msg(page, "Error generating MIDI waveform...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    chord_repr = generated_score.to_chord_repr()
    save_dir = os.path.join(root_dir, 'audio_out', musiclang_prefs['batch_folder_name'])
    makedir(save_dir)
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    if bool(musiclang_prefs['batch_folder_name']):
      audio_out = os.path.join(audio_out, musiclang_prefs['batch_folder_name'])
    makedir(audio_out)
    #voice_dirs = os.listdir(os.path.join(root_dir, "audioldm-tts", 'audioldm', 'voices'))
    #print(str(voice_dirs))
    fname = format_filename(musiclang_prefs['audio_name'])
    if fname[-1] == '.': fname = fname[:-1]
    file_prefix = musiclang_prefs['file_prefix']
    audio_name = f'{file_prefix}-{fname}'
    audio_name = audio_name[:int(prefs['file_max_length'])]
    midi_path = available_file(audio_out, audio_name, 0, no_num=True, ext="mid")
    wav_path = available_file(audio_out, audio_name, 0, no_num=True, ext="wav")
    mp3_path = available_file(audio_out, audio_name, 0, no_num=True, ext="mp3")
    generated_score.to_midi(midi_path, tempo=tempo, time_signature=time_signature)
    FluidSynth(fluidR3).midi_to_audio(midi_path, wav_path)
    #run_sp(f'ffmpeg -i {wav_path} -acodec libmp3lame -y -loglevel quiet -stats {mp3_path}')
    ffmpeg.input(wav_path).output(mp3_path, acodec='libmp3lame', overwrite_output=True, quiet=True).run()
    os.remove(wav_path)
    if isinstance(str, chord_repr):
        print(str(chord_repr))
    clear_last(2)
    display_name = mp3_path
    #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
    prt(Markdown(f"Saved MIDI File [{midi_path}]({filepath_to_url(midi_path)})", on_tap_link=lambda e: e.page.launch_url(e.data)))
    prt(AudioPlayer(src=mp3_path, display=display_name, data=display_name, page=page))
    nudge(page.MusicLang, page)
    play_snd(Snd.ALERT, page)


def run_zeta_editing(page):
    global zeta_editing_prefs, pipe_audio_ldm2, prefs, status
    if not check_diffusers(page): return
    if not bool(zeta_editing_prefs['audio_file']):
      alert_msg(page, "You must provide the Source Audio File to process...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.ZetaEditing.controls.append(line)
      page.ZetaEditing.update()
    def clear_last(lines=1):
      clear_line(page.ZetaEditing, lines=lines)
    if not bool(zeta_editing_prefs['source_prompt']):
      alert_msg(page, "Provide Text for the AI to create the sound of...")
      return
    if not check_diffusers(page): return
    page.ZetaEditing.controls = page.ZetaEditing.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = zeta_editing_prefs['steps']# * zeta_editing_prefs['batch_size']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    installer = Installing("Loading Zeta Editing LDM-2 Packages...")
    prt(installer)
    audio_editing_dir = os.path.join(root_dir, 'audioEditing')
    if not os.path.exists(audio_editing_dir):
        run_sp("git clone https://huggingface.co/spaces/hilamanor/audioEditing", cwd=root_dir)
    if audio_editing_dir not in sys.path:
        sys.path.append(audio_editing_dir)
    pip_install("tqdm soundfile progressbar einops scipy librosa==0.9.2", installer=installer)
    if zeta_editing_prefs['save_mp3']:
        get_ffmpeg(installer)
        pip_install("pydub", q=True, installer=installer)
        import ffmpeg, pydub
    #from diffusers import AudioLDM2Pipeline
    import torchaudio
    from torch import inference_mode
    from models import load_model
    import utils
    #import gradio as gr
    from inversion_utils import inversion_forward_process, inversion_reverse_process
    model_id = zeta_editing_prefs['model_name']
    if 'loaded_ldm2' not in status:
        status['loaded_ldm2'] = ""
    if status['loaded_ldm2'] == model_id:
        clear_pipes('audio_ldm2')
    else:
        clear_pipes()
    # This will download all the models used by Audio LDM from the HuggingFace hub.
    if pipe_audio_ldm2 == None:
      try:
        installer.status("...loading pipeline")
        pipe_audio_ldm2 = load_model(model_id=model_id, device=torch_device)
        #AudioLDM2Pipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)#build_model(model_name=model_id)
        #pipe_audio_ldm2 = pipeline_scheduler(pipe_audio_ldm2)
        #status['loaded_scheduler'] = prefs['scheduler_mode']
        #pipe_audio_ldm2.scheduler = LMSDiscreteScheduler.from_config(pipe_audio_ldm2.scheduler.config)
        #pipe_audio_ldm2 = pipe_audio_ldm2.to(torch_device)
        #pipe_audio_ldm2.set_progress_bar_config(disable=True)
        status['loaded_ldm2'] = model_id
      except Exception as e:
        clear_last()
        alert_msg(page, f"Error downloading {model_id} model", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    def invert(ldm_stable, x0, prompt_src, num_diffusion_steps, cfg_scale_src):  # , ldm_stable):
        pipe_audio_ldm2.model.scheduler.set_timesteps(num_diffusion_steps, device=torch_device)
        with inference_mode():
            w0 = ldm_stable.vae_encode(x0)
        # find Zs and wts - forward process
        _, zs, wts = inversion_forward_process(ldm_stable, w0, etas=1,
                                              prompts=[prompt_src],
                                              cfg_scales=[cfg_scale_src],
                                              prog_bar=True,
                                              num_inference_steps=num_diffusion_steps,
                                              numerical_fix=True)
        return zs, wts
    init = zeta_editing_prefs['audio_file']
    if init.startswith('http'):
        installer.status("...downloading audio file")
        init_audio = download_file(init)
    else:
        if os.path.isfile(init):
            init_audio = init
        else:
            init_audio = None
    if not init_audio:
        alert_msg(page, "Error Downloading the Source Audio File to process...")
        return
    clear_last()
    save_dir = os.path.join(root_dir, 'audio_out', zeta_editing_prefs['batch_folder_name'])
    makedir(save_dir)
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    if bool(zeta_editing_prefs['batch_folder_name']):
        audio_out = os.path.join(audio_out, zeta_editing_prefs['batch_folder_name'])
    makedir(audio_out)
    x0 = utils.load_audio(init_audio, pipe_audio_ldm2.get_fn_STFT(), device=torch_device)
    for num in range(zeta_editing_prefs['batch_size']):
        do_inversion = True
        prt(Text("  Generating ZETA Editing Sounds... See console for progress.", weight=FontWeight.BOLD))
        prt(progress)
        random_seed = get_seed(zeta_editing_prefs['seed'])
        random_seed += num
        torch.manual_seed(random_seed)
        if do_inversion:  # always re-run inversion
            zs_tensor, wts_tensor = invert(ldm_stable=pipe_audio_ldm2, x0=x0, prompt_src=zeta_editing_prefs['source_prompt'],
                                          num_diffusion_steps=zeta_editing_prefs['steps'],
                                          cfg_scale_src=zeta_editing_prefs['source_guidance_scale'])
            #wts = gr.State(value=wts_tensor)
            #zs = gr.State(value=zs_tensor)
            #saved_inv_model = model_id
            do_inversion = False
        try:
            tstart = torch.tensor(zeta_editing_prefs['t_start'], dtype=torch.int)
            skip = zeta_editing_prefs['steps'] - tstart
            w0, _ = inversion_reverse_process(pipe_audio_ldm2, xT=wts_tensor, skips=zeta_editing_prefs['steps'] - skip,
                                              etas=1., prompts=[zeta_editing_prefs['target_prompt']],
                                              neg_prompts=[zeta_editing_prefs['negative_prompt']], cfg_scales=[zeta_editing_prefs['guidance_scale']],
                                              prog_bar=True,
                                              zs=zs_tensor[:int(zeta_editing_prefs['steps'] - skip)])
            with inference_mode():
                x0_dec = pipe_audio_ldm2.vae_decode(w0)
            if x0_dec.dim() < 4:
                x0_dec = x0_dec[None, :, :, :]
            with torch.no_grad():
                audio = pipe_audio_ldm2.decode_to_mel(x0_dec)
            #waveform = text_to_audio(pipe_audio_ldm2, zeta_editing_prefs['text'], random_seed, duration=10, guidance_scale=zeta_editing_prefs['guidance_scale'], n_candidate_gen_per_text=int(zeta_editing_prefs['n_candidates']), batchsize=int(zeta_editing_prefs['batch_size']), transcript=zeta_editing_prefs['transcription'] if 'speech' in model_id else "")
        except Exception as e:
            clear_last()
            alert_msg(page, "Error generating ZETA waveform...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
            return
        clear_last()
        clear_last()
        fname = format_filename(zeta_editing_prefs['target_prompt'])
        if fname[-1] == '.': fname = fname[:-1]
        file_prefix = zeta_editing_prefs['file_prefix']
        audio_name = f'{file_prefix}-{fname}'
        audio_name = audio_name[:int(prefs['file_max_length'])]
        audio_metadata = {
          "sample_rate": 16000,
          "artist": prefs['meta_ArtistName'],
          "copyright": prefs['meta_Copyright'],
          "software": "Stable Diffusion Deluxe",
          "website": "https://DiffusionDeluxe.com",
        }
        if prefs['save_config_in_metadata']:
            config_json = zeta_editing_prefs.copy()
            del config_json['file_prefix']
            config_json['seed'] = random_seed + num
            #config_json['sceduler'] = status['loaded_scheduler']
            audio_metadata["config"] = config_json
        fname = available_file(save_dir, audio_name, num, no_num=True, ext="wav")
        torchaudio.save(fname, audio, sample_rate=16000)
        if zeta_editing_prefs['save_mp3']:
            wav_file = pydub.AudioSegment.from_wav(fname)
            #tags = pydub.utils.mediainfo(wav_file).get('TAG', {})
            mp3_name = available_file(audio_out, audio_name, num, no_num=True, ext="mp3")
            mp3_file = wav_file.export(mp3_name, format="mp3", tags=audio_metadata)
            os.remove(fname)
            fname = mp3_name
            display_name = fname
        else:
            audio_save = available_file(audio_out, audio_name, num, ext='wav')
            shutil.move(fname, audio_save)
            fname = audio_save
            display_name = audio_save
          #display_name = fname
        #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
        prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))
    play_snd(Snd.ALERT, page)

def run_music_ldm(page):
    global musicLDM_prefs, pipe_music_ldm, prefs, status
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.musicLDM_output.controls.append(line)
      page.musicLDM_output.update()
    def clear_last(lines=1):
      clear_line(page.musicLDM_output, lines=lines)
    if not bool(musicLDM_prefs['text']):
      alert_msg(page, "Provide Text for the AI to create the music of...")
      return
    if not check_diffusers(page): return
    progress = ProgressBar(bar_height=8)
    total_steps = musicLDM_prefs['steps']# * musicLDM_prefs['batch_size']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    installer = Installing("Loading Music LDM Packages...")
    prt(installer)
    pip_install("soundfile pandas scipy", q=True, installer=installer)
    if musicLDM_prefs['save_mp3']:
        pip_install("ffmpeg pydub", q=True, installer=installer)
        import pydub, ffmpeg
    import scipy
    from diffusers import MusicLDMPipeline, PNDMScheduler
    model_id = musicLDM_prefs['model_name']
    if 'loaded_ldm' not in status:
        status['loaded_ldm'] = ""
    if status['loaded_ldm'] == model_id:
        clear_pipes('music_ldm')
    else:
        clear_pipes()
    if pipe_music_ldm == None:
      try:
        installer.status("...loading pipeline")
        scheduler = PNDMScheduler(skip_prk_steps=True)
        pipe_music_ldm = MusicLDMPipeline.from_pretrained(model_id, torch_dtype=torch.float16, scheduler=scheduler, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)#build_model(model_name=model_id)
        #pipe_music_ldm = pipeline_scheduler(pipe_music_ldm, big_3=True)
        #status['loaded_scheduler'] = prefs['scheduler_mode']
        #pipe_music_ldm.scheduler = LMSDiscreteScheduler.from_config(pipe_music_ldm.scheduler.config)
        if prefs['enable_torch_compile']:
            installer.status("...torch compile")
            pipe_music_ldm = torch.compile(pipe_music_ldm)
            #torch.set_float32_matmul_precision("high")
        pipe_music_ldm = pipe_music_ldm.to(torch_device)
        pipe_music_ldm.set_progress_bar_config(disable=True)
        status['loaded_ldm'] = model_id
      except Exception as e:
        clear_last()
        alert_msg(page, "Error downloading MusicLDM model", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    #elif prefs['scheduler_mode'] != status['loaded_scheduler']:
    #    installer.status(f"...scheduler {prefs['scheduler_mode']}")
    #    pipe_music_ldm = pipeline_scheduler(pipe_music_ldm)
      
    clear_last()
    prt(Text("  Generating MusicLDM Songs...", weight=FontWeight.BOLD))
    prt(progress)
    random_seed = get_seed(musicLDM_prefs['seed'])
    generator = torch.Generator("cuda").manual_seed(random_seed)
    try:
      audios = pipe_music_ldm(musicLDM_prefs['text'],
          negative_prompt=musicLDM_prefs['negative_prompt'],
          num_inference_steps=musicLDM_prefs['steps'],
          guidance_scale=musicLDM_prefs['guidance_scale'],
          audio_length_in_s=musicLDM_prefs['duration'],
          num_waveforms_per_prompt=musicLDM_prefs['batch_size'],
          generator=generator,
          callback=callback_fnc,
      ).audios
      #waveform = text_to_audio(pipe_music_ldm, musicLDM_prefs['text'], random_seed, duration=10, guidance_scale=musicLDM_prefs['guidance_scale'], n_candidate_gen_per_text=int(musicLDM_prefs['n_candidates']), batchsize=int(musicLDM_prefs['batch_size']), transcript=musicLDM_prefs['transcription'] if 'speech' in model_id else "")
    except Exception as e:
      clear_last()
      alert_msg(page, "Error generating MusicLDM waveform...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
      return
    save_dir = os.path.join(root_dir, 'audio_out', musicLDM_prefs['batch_folder_name'])
    if not os.path.exists(save_dir):
      os.makedirs(save_dir, exist_ok=True)
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    if bool(musicLDM_prefs['batch_folder_name']):
      audio_out = os.path.join(audio_out, musicLDM_prefs['batch_folder_name'])
    os.makedirs(audio_out, exist_ok=True)
    #voice_dirs = os.listdir(os.path.join(root_dir, "musicldm-tts", 'musicldm', 'voices'))
    #print(str(voice_dirs))
    # waveform = [(16000, np.random.randn(16000)), (16000, np.random.randn(16000))]
    clear_last()
    clear_last()
    num = 0
    for audio in audios:
        fname = format_filename(musicLDM_prefs['text'])
        if fname[-1] == '.': fname = fname[:-1]
        file_prefix = musicLDM_prefs['file_prefix']
        audio_name = f'{file_prefix}-{fname}'
        audio_name = audio_name[:int(prefs['file_max_length'])]
        audio_metadata = {
          "sample_rate": 16000,
          "artist": prefs['meta_ArtistName'],
          "copyright": prefs['meta_Copyright'],
          "software": "Stable Diffusion Deluxe",
        }
        if prefs['save_config_in_metadata']:
          config_json = musicLDM_prefs.copy()
          del config_json['batch_size']
          del config_json['n_candidates']
          del config_json['file_prefix']
          config_json['seed'] = random_seed + num
          config_json['sceduler'] = status['loaded_scheduler']
          audio_metadata["config"] = config_json
        fname = available_file(save_dir, audio_name, 0, ext="wav")
        scipy.io.wavfile.write(fname, data=audio, rate=16000)#, audio_metadata
        #for i in range(waveform.shape[0]):
        #    sf.write(fname, waveform[i, 0], samplerate=16000)
        #torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)
        if musicLDM_prefs['save_mp3']:
          wav_file = pydub.AudioSegment.from_wav(fname)
          #tags = pydub.utils.mediainfo(wav_file).get('TAG', {})
          mp3_name = available_file(audio_out, audio_name, 0, ext="mp3")
          mp3_file = wav_file.export(mp3_name, format="mp3", tags=audio_metadata)
          os.remove(fname)
          fname = mp3_name
          display_name = fname
        else:
        #if storage_type == "Colab Google Drive":
          audio_save = available_file(audio_out, audio_name, 0, ext='wav')
          shutil.move(fname, audio_save)
          fname = audio_save
          display_name = audio_save
        #display_name = fname
        #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
        prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))
        num += 1
    play_snd(Snd.ALERT, page)

def run_stable_audio(page):
    global stable_audio_prefs, pipe_stable_audio, config_stable_audio, status
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.stable_audio_output.controls.append(line)
      page.stable_audio_output.update()
    def clear_last(lines=1):
      clear_line(page.stable_audio_output, lines=lines)
    if not bool(stable_audio_prefs['text']):
      alert_msg(page, "Provide Text for the AI to create the sound of...")
      return
    progress = ProgressBar(bar_height=8)
    installer = Installing("Loading Stable Audio Tools...")
    prt(installer)
    pip_install("stable-audio-tools einops scipy", installer=installer)
    import torchaudio
    from einops import rearrange
    from stable_audio_tools import get_pretrained_model
    from stable_audio_tools.inference.generation import generate_diffusion_cond
    if stable_audio_prefs['save_mp3']:
        pip_install("pydub", q=True, installer=installer)
        import pydub
    else:
        pip_install("soundfile", q=True, installer=installer)
        import soundfile as sf
    model_id = stable_audio_prefs['model_name'] if stable_audio_prefs['model_name'] != "Custom" else stable_audio_prefs['custom_model']
    clear_pipes('stable_audio')
    if pipe_stable_audio == None:
      try:
        installer.status("...loading model")
        pipe_stable_audio, config_stable_audio = get_pretrained_model(model_id)
        pipe_stable_audio = pipe_stable_audio.to(torch_device)
      except Exception as e:
        clear_last()
        alert_msg(page, "Error downloading Stable Audio model", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    clear_last()
    def progress_callback(callback_info):
        nonlocal progress
        current_step = callback_info["i"]
        progress.value = current_step / stable_audio_prefs['steps']
        progress.update()
    random_seed = get_seed(stable_audio_prefs['seed'])
    for n in range(stable_audio_prefs['batch_size']):
        prt(Text("  Generating Stable Audio...", weight=FontWeight.BOLD))
        prt(progress)
        sample_rate = config_stable_audio["sample_rate"]
        sample_size = config_stable_audio["sample_size"]
        conditioning = [{
            "prompt": stable_audio_prefs['text'],
            "seconds_start": 0,
            "seconds_total": stable_audio_prefs['duration']
        }]
        if bool(stable_audio_prefs['negative_prompt']):
            negative_conditioning = [{"prompt": stable_audio_prefs['negative_prompt'], "seconds_start": 0, "seconds_total": stable_audio_prefs['duration']}]
        else:
            negative_conditioning = None
        try:
            output = generate_diffusion_cond(
                model_id,
                steps=stable_audio_prefs['steps'],
                #batch_size=stable_audio_prefs['batch_size'],
                seed=random_seed + n,
                cfg_scale=stable_audio_prefs['guidance_scale'],
                conditioning=conditioning,
                negative_conditioning=negative_conditioning,
                sample_size=sample_size,
                sigma_min=stable_audio_prefs['sigma_min'],
                sigma_max=stable_audio_prefs['sigma_max'],
                sampler_type=stable_audio_prefs['sampler'],#"dpmpp-3m-sde",
                callback=progress_callback,
                device=torch_device,
            )
            progress.value = None
        except Exception as e:
            clear_last()
            alert_msg(page, "Error generating StableAudio waveform...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
            return
        audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
        if bool(stable_audio_prefs['batch_folder_name']):
          audio_out = os.path.join(audio_out, stable_audio_prefs['batch_folder_name'])
        makedir(audio_out)
        clear_last(2)
        # TODO: Figure out batch file seperation
        output = rearrange(output, "b d n -> d (b n)")
        output = output.to(torch.float32).div(torch.max(torch.abs(output))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()
        max_length = sample_rate * stable_audio_prefs['duration']
        if output.shape[1] > max_length:
            output = output[:, :max_length]
        fname = format_filename(stable_audio_prefs['text'])
        if fname[-1] == '.': fname = fname[:-1]
        file_prefix = stable_audio_prefs['file_prefix']
        audio_name = f'{file_prefix}-{fname}'
        audio_name = audio_name[:int(prefs['file_max_length'])]
        audio_metadata = {
            "sample_rate": sample_rate,
            "sample_size": sample_size,
            "artist": prefs['meta_ArtistName'],
            "copyright": prefs['meta_Copyright'],
            "software": "Stable Diffusion Deluxe",
        }
        if prefs['save_config_in_metadata']:
            config_json = stable_audio_prefs.copy()
            del config_json['batch_size']
            del config_json['file_prefix']
            config_json['seed'] = random_seed + n
            audio_metadata["config"] = config_json
        fname = available_file(audio_out, audio_name, n, ext="wav")
        torchaudio.save(fname, output, sample_rate)
        if stable_audio_prefs['save_mp3']:
            wav_file = pydub.AudioSegment.from_wav(fname)
            #tags = pydub.utils.mediainfo(wav_file).get('TAG', {})
            mp3_name = available_file(audio_out, audio_name, n, ext="mp3")
            mp3_file = wav_file.export(mp3_name, format="mp3", tags=audio_metadata)
            os.remove(fname)
            fname = mp3_name
            display_name = fname
        else:
            import json
            audio_data, _ = sf.read(fname)
            metadata_comment = json.dumps(audio_metadata)
            sf.write(fname, audio_data, sample_rate, format='WAV', subtype='PCM_16', **metadata_comment)
            display_name = audio_out
        prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))
    play_snd(Snd.ALERT, page)

def run_bark(page):
    global bark_prefs, pipe_bark, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.bark_output.controls.append(line)
      page.bark_output.update()
    def clear_last(lines=1):
      clear_line(page.bark_output, lines=lines)
    def play_audio(e):
      e.control.data.play()
    if not bool(bark_prefs['text']):
      alert_msg(page, "Provide Text for the AI to create the sound of...")
      return
    progress = ProgressBar(bar_height=8)
    installer = Installing("Downloading Bark Packages...")
    prt(installer)
    pip_install("scipy", installer=installer)
    from scipy.io.wavfile import write as write_wav
    import sys
    if os.path.join(root_dir, 'audioldm') not in sys.path:
        sys.path.append(os.path.join(root_dir, 'audioldm'))
    try:
        from bark import SAMPLE_RATE, generate_audio, preload_models
        if force_update("bark"): raise ImportError("Forcing update")
    except Exception:
        installer.status("...installing suno-ai/bark")
        try:
            run_process("pip install git+https://github.com/suno-ai/bark.git", page=page)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Bark requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
            return
        pass
    finally:
        from bark import SAMPLE_RATE, generate_audio, preload_models
    if bark_prefs['use_bettertransformer']:
      #https://colab.research.google.com/drive/1XO0RhINg4ZZCdJJmPeJ9lOQs98skJ8h_
        try:
            from optimum.bettertransformer import BetterTransformer
        except ModuleNotFoundError:
            installer.status("...installing Optimum BetterTransformer")
            run_process("pip install --upgrade git+https://github.com/huggingface/optimum.git", page=page)
            from optimum.bettertransformer import BetterTransformer
            pass
        from transformers import BarkModel, set_seed, AutoProcessor
        installer.status("...initializing suno/bark model")
        bark_model = BarkModel.from_pretrained("suno/bark", torch_dtype=torch.float16).to(torch_device)
        installer.status("...initializing suno/bark processor")
        processor = AutoProcessor.from_pretrained("suno/bark")
        installer.status("...initializing BetterTransformer model")
        bark_model = BetterTransformer.transform(bark_model, keep_original_model=False)
    import soundfile as sf
    clear_pipes()
    if not bark_prefs['use_bettertransformer']:
      preload_models()
    clear_last()
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    if bool(bark_prefs['batch_folder_name']):
      audio_out = os.path.join(audio_out, bark_prefs['batch_folder_name'])
    os.makedirs(audio_out, exist_ok=True)
    history_prompt = bark_prefs['acoustic_prompt']
    if history_prompt == "Unconditional":
        history_prompt = None
    if history_prompt == "Anouncer":
        history_prompt = "announcer"
    for i in range(bark_prefs['n_iterations']):
        prt(Text("  Generating Bark Audio...", weight=FontWeight.BOLD))
        prt(progress)
        try:
            if not bark_prefs['use_bettertransformer']:
                audio_array = generate_audio(bark_prefs['text'], history_prompt=history_prompt, text_temp=bark_prefs['text_temp'], waveform_temp=bark_prefs['waveform_temp'])
            else:
                set_seed(0)
                inputs = processor(bark_prefs['text'], voice_preset=history_prompt).to(torch_device)
                audio_array = bark_model.generate(**inputs, do_sample = True, fine_temperature=bark_prefs['text_temp'], coarse_temperature=bark_prefs['waveform_temp'])
                audio_array = audio_array.cpu().numpy().squeeze()
                sample_rate = bark_model.generation_config.sample_rate
        except Exception as e:
            clear_last()
            alert_msg(page, "Error generating Bark waveform...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
            return
        fname = format_filename(bark_prefs['text'])
        if fname[-1] == '.': fname = fname[:-1]
        file_prefix = bark_prefs['file_prefix']
        audio_name = f'{file_prefix}-{fname}'
        audio_name = audio_name[:int(prefs['file_max_length'])]
        fname = available_file(audio_out, audio_name, i, ext="wav")
        write_wav(fname, SAMPLE_RATE if not bark_prefs['use_bettertransformer'] else sample_rate, audio_array)
        clear_last()
        clear_last()
        #a_out = Audio(src=fname, autoplay=False)
        #page.overlay.append(a_out)
        #page.update()
        display_name = fname
        prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))
        #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
    play_snd(Snd.ALERT, page)

def run_riffusion(page):
    global riffusion_prefs, pipe_riffusion, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.riffusion_output.controls.append(line)
      page.riffusion_output.update()
    def clear_last(lines=1):
      clear_line(page.riffusion_output, lines=lines)
    def play_audio(e):
      e.control.data.play()
    if not bool(riffusion_prefs['prompt']):
      alert_msg(page, "Provide Text for the AI to create the sound of...")
      return
    if not check_diffusers(page): return
    progress = ProgressBar(bar_height=8)
    installer = Installing("Downloading Riffusion Packages...")
    prt(installer)
    riffusion_dir = os.path.join(root_dir, "riffusion-inference")
    if not os.path.isdir(riffusion_dir):
      installer.status("...cloning riffusion-inference")
      os.chdir(root_dir) # -b v0.3.0
      run_process("git clone https://github.com/hmartiro/riffusion-inference", page=page)
    os.chdir(riffusion_dir)
    import sys
    if os.path.join(riffusion_dir, 'riffusion') not in sys.path:
        sys.path.append(os.path.join(riffusion_dir, 'riffusion'))
    try:
        from riffusion.spectrogram_image_converter import SpectrogramImageConverter
    except Exception:
        try:
            installer.status("...installing requirements")
            run_process("pip install -r requirements.txt", page=page, cwd=riffusion_dir)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Riffusion requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
            return
        pass
    finally:
        from riffusion.spectrogram_image_converter import SpectrogramImageConverter
        from riffusion.spectrogram_params import SpectrogramParams
        try:
            from riffusion.audio import spectrogram_from_waveform
        except Exception:
            print("Still can't find riffusion.audio import spectrogram_from_waveform")
            pass
        #from IPython.display import Audio
        from scipy.io import wavfile
    def image_from_spectrogram(spectrogram: np.ndarray, max_volume: float = 50, power_for_image: float = 0.25) -> PILImage.Image:
        data = np.power(spectrogram, power_for_image)
        data = data * 255 / max_volume
        data = 255 - data
        image = PILImage.fromarray(data.astype(np.uint8))
        image = image.transpose(PILImage.FLIP_TOP_BOTTOM)
        image = image.convert("RGB")
        return image
    model_id="riffusion/riffusion-model-v1"
    save_dir = os.path.join(root_dir, 'audio_out', riffusion_prefs['batch_folder_name'])
    if not os.path.exists(save_dir):
      os.makedirs(save_dir, exist_ok=True)
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    if bool(riffusion_prefs['batch_folder_name']):
      audio_out = os.path.join(audio_out, riffusion_prefs['batch_folder_name'])
    os.makedirs(audio_out, exist_ok=True)
    #voice_dirs = os.listdir(os.path.join(root_dir, "audioldm-tts", 'audioldm', 'voices'))
    #print(str(voice_dirs))
    fname = format_filename(riffusion_prefs['prompt'])
    if fname[-1] == '.': fname = fname[:-1]
    file_prefix = riffusion_prefs['file_prefix']
    audio_name = f'{file_prefix}-{fname}'
    audio_name = audio_name[:int(prefs['file_max_length'])]

    init = riffusion_prefs['audio_file']
    if bool(init):
        if init.startswith('http'):
            init_audio = download_file(init)
        else:
            if os.path.isfile(init):
                init_audio = init
            else:
                init_audio = None
    else:
        init_audio = None

    if pipe_riffusion == None or (init_audio == None and riffusion_prefs['loaded_pipe'] == "image") or (init_audio != None and riffusion_prefs['loaded_pipe'] == "text"):
      clear_pipes()
      installer.status("...initializing pipeline")
      try:
        if init_audio == None:
            from diffusers import DiffusionPipeline #, custom_pipeline="AlanB/lpw_stable_diffusion_mod"
            pipe_riffusion = DiffusionPipeline.from_pretrained(model_id, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            pipe_riffusion = optimize_pipe(pipe_riffusion)
            riffusion_prefs['loaded_pipe'] = "text"
        else:
            from diffusers import StableDiffusionImg2ImgPipeline
            pipe_riffusion = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            pipe_riffusion = optimize_pipe(pipe_riffusion)
            riffusion_prefs['loaded_pipe'] = "image"
      except Exception as e:
        clear_last()
        alert_msg(page, "Error downloading Riffusion package", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    else:
        clear_pipes('riffusion')
    clear_last()
    prt(Text("  Generating Riffusion Sounds...", weight=FontWeight.BOLD))
    prt(progress)
    random_seed = get_seed(riffusion_prefs['seed'])
    generator = torch.Generator(device=torch_device).manual_seed(random_seed)
    try:
        if init_audio == None:
            params = SpectrogramParams()
            converter = SpectrogramImageConverter(params)
            specs = pipe_riffusion(
                riffusion_prefs['prompt'],
                negative_prompt=riffusion_prefs['negative_prompt'],
                guidance_scale=riffusion_prefs['guidance_scale'],
                steps=riffusion_prefs['steps'],
                width=riffusion_prefs['max_size'],
                height=riffusion_prefs['max_size'],
                batch_size=riffusion_prefs['batch_size'],
                generator=generator,
            ).images
        else:
            rate, data = wavfile.read(init_audio)
            data = np.mean(data, axis=1)
            data = data.astype(np.float32)
            data = data[rate*7:rate*14]
            spectrogram = spectrogram_from_waveform(waveform=data, sample_rate=rate, n_fft=8192, hop_length=512, win_length=8192)
            spec = image_from_spectrogram(spectrogram)
            specs = pipe_riffusion(
                prompt=riffusion_prefs['prompt'],
                negative_prompt=riffusion_prefs['negative_prompt'],
                image=spec,
                strength=riffusion_prefs['strength'],
                guidance_scale=riffusion_prefs['guidance_scale'],
                steps=riffusion_prefs['steps'],
                width=riffusion_prefs['max_size'],
                height=riffusion_prefs['max_size'],
                batch_size=riffusion_prefs['batch_size'],
                generator=generator,
            ).images
    except Exception as e:
      clear_last()
      alert_msg(page, "Error generating Spectrogram Image Converter from Diffusion...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
      return

    clear_last()
    clear_last()
    for spec in specs:
        audio_file = available_file(save_dir, audio_name, 0, ext="wav")
        image_file = available_file(save_dir, audio_name, 0)
        wav = converter.audio_from_spectrogram_image(image=spec)
        wav.export(audio_file, format='wav')
        spec.save(image_file)
        #a_out = Audio(src=audio_file, autoplay=False)
        #page.overlay.append(a_out)
        #page.update()
        display_name = audio_file
        if storage_type == "Colab Google Drive":
          audio_save = available_file(audio_out, fname, 0, ext='wav')
          image_save = available_file(audio_out, fname, 0)
          shutil.copy(audio_file, audio_save)
          shutil.copy(image_file, image_save)
          display_name = audio_save
        prt(AudioPlayer(src=audio_file, display=display_name, data=display_name, page=page))
        #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
    play_snd(Snd.ALERT, page)


def run_mubert(page):
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.mubert_output.controls.append(line)
      page.mubert_output.update()
    def clear_last(lines=1):
      clear_line(page.mubert_output, lines=lines)
    def play_audio(e):
      e.control.data.play()
    progress = ProgressBar(bar_height=8)
    installer = Installing("Downloading Mubert Packages...", )
    prt(installer)
    mubert_dir = os.path.join(root_dir, "mubert-songs")
    if bool(mubert_prefs['batch_folder_name']):
        mubert_dir = os.path.join(mubert_dir, mubert_prefs['batch_folder_name'])
    makedir(mubert_dir)
    import time
    pip_install("sentence_transformers httpx", installer=installer)
    from sentence_transformers import SentenceTransformer
    import httpx
    
    MUBERT_TAGS_STRING = 'tribal,action,kids,neo-classic,run 130,pumped,jazz / funk,ethnic,dubtechno,reggae,acid jazz,liquidfunk,funk,witch house,tech house,underground,artists,mystical,disco,sensorium,r&b,agender,psychedelic trance / psytrance,peaceful,run 140,piano,run 160,setting,meditation,christmas,ambient,horror,cinematic,electro house,idm,bass,minimal,underscore,drums,glitchy,beautiful,technology,tribal house,country pop,jazz & funk,documentary,space,classical,valentines,chillstep,experimental,trap,new jack swing,drama,post-rock,tense,corporate,neutral,happy,analog,funky,spiritual,sberzvuk special,chill hop,dramatic,catchy,holidays,fitness 90,optimistic,orchestra,acid techno,energizing,romantic,minimal house,breaks,hyper pop,warm up,dreamy,dark,urban,microfunk,dub,nu disco,vogue,keys,hardcore,aggressive,indie,electro funk,beauty,relaxing,trance,pop,hiphop,soft,acoustic,chillrave / ethno-house,deep techno,angry,dance,fun,dubstep,tropical,latin pop,heroic,world music,inspirational,uplifting,atmosphere,art,epic,advertising,chillout,scary,spooky,slow ballad,saxophone,summer,erotic,jazzy,energy 100,kara mar,xmas,atmospheric,indie pop,hip-hop,yoga,reggaeton,lounge,travel,running,folk,chillrave & ethno-house,detective,darkambient,chill,fantasy,minimal techno,special,night,tropical house,downtempo,lullaby,meditative,upbeat,glitch hop,fitness,neurofunk,sexual,indie rock,future pop,jazz,cyberpunk,melancholic,happy hardcore,family / kids,synths,electric guitar,comedy,psychedelic trance & psytrance,edm,psychedelic rock,calm,zen,bells,podcast,melodic house,ethnic percussion,nature,heavy,bassline,indie dance,techno,drumnbass,synth pop,vaporwave,sad,8-bit,chillgressive,deep,orchestral,futuristic,hardtechno,nostalgic,big room,sci-fi,tutorial,joyful,pads,minimal 170,drill,ethnic 108,amusing,sleepy ambient,psychill,italo disco,lofi,house,acoustic guitar,bassline house,rock,k-pop,synthwave,deep house,electronica,gabber,nightlife,sport & fitness,road trip,celebration,electro,disco house,electronic'
    MUBERT_TAGS = np.array(MUBERT_TAGS_STRING.split(','))
    MUBERT_LICENSE = "ttmmubertlicense#f0acYBenRcfeFpNT4wpYGaTQIyDI4mJGv5MfIhBFz97NXDwDNFHmMRsBSzmGsJwbTpP1A6i07AXcIeAHo5"
    MUBERT_MODE = "loop"
    MUBERT_TOKEN = "4951f6428e83172a4f39de05d5b3ab10d58560b8"

    def get_mubert_tags_embeddings(w2v_model):
        return w2v_model.encode(MUBERT_TAGS)

    def get_pat(email: str):
        r = httpx.post('https://api-b2b.mubert.com/v2/GetServiceAccess', json={"method": "GetServiceAccess", "params": {"email": email, "license": MUBERT_LICENSE, "token": MUBERT_TOKEN, "mode": MUBERT_MODE}})
        rdata = json.loads(r.text)
        if rdata['status'] != 1:
            alert_msg(page, "ERROR Requesting Mubert Service. Probably incorrect e-mail...")
        pat = rdata['data']['pat']
        return pat

    def find_similar(em, embeddings, method='cosine'):
        scores = []
        for ref in embeddings:
            if method == 'cosine':
                scores.append(1 - np.dot(ref, em) / (np.linalg.norm(ref) * np.linalg.norm(em)))
            if method == 'norm':
                scores.append(np.linalg.norm(ref - em))
        return np.array(scores), np.argsort(scores)

    def get_tags_for_prompts(w2v_model, mubert_tags_embeddings, prompts, top_n=3, debug=False):
        prompts_embeddings = w2v_model.encode(prompts)
        ret = []
        for i, pe in enumerate(prompts_embeddings):
            scores, idxs = find_similar(pe, mubert_tags_embeddings)
            top_tags = MUBERT_TAGS[idxs[:top_n]]
            top_prob = 1 - scores[idxs[:top_n]]
            if debug:
                prt(f"Prompt: {prompts[i]}\nTags: {', '.join(top_tags)}\nScores: {top_prob}\n\n\n")
            ret.append((prompts[i], list(top_tags)))
        return ret
    minilm = SentenceTransformer('all-MiniLM-L6-v2')
    mubert_tags_embeddings = get_mubert_tags_embeddings(minilm)

    def get_track_by_tags(tags, pat, duration, maxit=20, loop=False):
        if loop:
            mode = "loop"
        else:
            mode = "track"
        r = httpx.post('https://api-b2b.mubert.com/v2/RecordTrackTTM', json={"method": "RecordTrackTTM", "params": { "pat": pat,"duration": duration, "tags": tags, "mode": mode}})
        rdata = json.loads(r.text)
        assert rdata['status'] == 1, rdata['error']['text']
        trackurl = rdata['data']['tasks'][0]['download_link']
        prt('Generating your Mubert track... ')
        for i in range(maxit):
            r = httpx.get(trackurl)
            if r.status_code == 200:
                return trackurl
            time.sleep(1)

    def generate_track_by_prompt(email, prompt, duration, loop=False):
        try:
            pat = get_pat(email)
            _, tags = get_tags_for_prompts(minilm, mubert_tags_embeddings, [prompt, ])[0]
            return get_track_by_tags(tags, pat, int(duration), loop=loop), "Success", ", ".join(tags)
        except Exception as e:
            return None, str(e), ""
    #btn.click(fn=generate_track_by_prompt, inputs=[email, prompt, duration, is_loop], outputs=[out, result_msg, tags])
    clear_last()
    out, result_msg, tags = generate_track_by_prompt(mubert_prefs['email'], mubert_prefs['prompt'], mubert_prefs['duration'], loop=mubert_prefs['is_loop'])
    if out == None:
      alert_msg(page, "Error generating track by prompt. The API Key problably reached montly limit...",  content=Text(result_msg))
      return
    clear_last()
    clear_last()
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    mubert_songs = os.path.join(audio_out, 'mubert_songs')
    audio_name = format_filename(mubert_prefs['prompt'])
    audio_name = f"{mubert_prefs['file_prefix']}{audio_name}"
    if bool(mubert_prefs['batch_folder_name']):
      mubert_songs = os.path.join(audio_out, mubert_prefs['batch_folder_name'])
    os.makedirs(mubert_songs, exist_ok=True)
    fname = available_file(mubert_songs, audio_name, 0, ext="mp3")
    audio_file = download_file(out)
    shutil.copy(audio_file, fname)
    #a_out = Audio(src=fname, autoplay=False)
    #page.overlay.append(a_out)
    #page.update()
    display_name = fname
    #a.tofile(f"/content/dance-{i}.wav")
    if storage_type == "Colab Google Drive":
      audio_save = available_file(audio_out, audio_name, 0, ext='mp3')
      shutil.copy(fname, audio_save)
      display_name = audio_save
    prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))
    #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Column([Text(display_name), Text(tags, style=TextThemeStyle.DISPLAY_SMALL)])]))
    play_snd(Snd.ALERT, page)

def run_whisper(page):
    global whisper_prefs, whisper_requests, pipe_whisper, status
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.Whisper.controls.append(line)
      page.Whisper.update()
    def clear_last(lines=1):
      clear_line(page.Whisper, lines=lines)
    def clear_list():
      page.Whisper.controls = page.Whisper.controls[:1]
    def autoscroll(scroll=True):
      page.Whisper.auto_scroll = scroll
      page.Whisper.update()
    clear_list()
    autoscroll(True)
    progress = ProgressBar(bar_height=8)
    installer = Installing("Installing OpenAI Whisper Speech-to-Text Packages...")
    prt(installer)
    try:
        import whisper
    except Exception as e:
        installer.status("...openai/whisper.git")
        run_sp("pip install git+https://github.com/openai/whisper.git -q", realtime=False)
        import whisper
        pass
    if 'loaded_whisper' not in status:
        status['loaded_whisper'] = whisper_prefs['model_size']
    if status['loaded_whisper'] == whisper_prefs['model_size']:
        clear_pipes("whisper")
    else:
        clear_pipes()
    newer = '-v' in whisper_prefs['model_size']
    if newer:
        pip_install("git+https://github.com/huggingface/transformers.git|transformers accelerate", installer=installer)
        from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
        from transformers.pipelines.audio_utils import ffmpeg_read
    if pipe_whisper is None:
        if not newer:
            installer.status(f"...load_model {whisper_prefs['model_size']}")
            pipe_whisper = whisper.load_model(whisper_prefs['model_size'])
            status['loaded_whisper'] = whisper_prefs['model_size']
        else:
            torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
            model_id = f"openai/whisper-{whisper_prefs['model_size']}"
            pipe_whisper = AutoModelForSpeechSeq2Seq.from_pretrained(
                model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
            ).to(torch_device)
            processor = AutoProcessor.from_pretrained(model_id)
            pipe_whisper = pipeline(
                "automatic-speech-recognition",
                model=model,
                tokenizer=processor.tokenizer,
                feature_extractor=processor.feature_extractor,
                max_new_tokens=128,
                chunk_length_s=30,
                batch_size=8,
                return_timestamps=True,
                torch_dtype=torch_dtype,
                device=torch_device,
            )
    from_language = ""
    def transcribe(audio):
        nonlocal installer, from_language
        if not newer:
            installer.status("...load_audio")
            audio = whisper.load_audio(audio)
            if whisper_prefs['trim_audio']:
                installer.status("...pad_or_trim audio")
                audio = whisper.pad_or_trim(audio)
            if whisper_prefs['simple_transcribe']:
                installer.status("...Whisper transcribe")
                options = {
                    "task": "transcribe"
                }
                result = whisper.transcribe(pipe_whisper, audio, **options)
            else:
                installer.status("...log_mel_spectrogram")
                mel = whisper.log_mel_spectrogram(audio).to(pipe_whisper.device)
                if whisper_prefs['detect_language'] or whisper_prefs['translate']:
                    installer.status("...detect_language")
                    _, probs = pipe_whisper.detect_language(mel)
                    from_language = max(probs, key=probs.get)
                    prt(f"Detected language: {from_language}")
                installer.status("...DecodingOptions")
                options = whisper.DecodingOptions(fp16 = False)
                result = whisper.decode(pipe_whisper, mel, options)
            return result.text
        else:
            installer.status("...load audio")
            inputs = ffmpeg_read(audio, pipe_whisper.feature_extractor.sampling_rate)
            inputs = {"array": inputs, "sampling_rate": pipe.feature_extractor.sampling_rate}
            generate_kwargs={"language": f"<|en|>"}#{"task": "translate"})
            installer.status("...Whisper transcribe")
            result = pipe_whisper(inputs)
            return result['text']
    audio_path = whisper_prefs['audio_file'].strip()
    local_path = ""
    if audio_path.startswith("http"):
        if audio_path.startswith("https://youtu") or 'youtube' in audio_path:
            get_ffmpeg(installer)
            try:
                import yt_dlp
            except ImportError as e:
                installer.status("...installing yt_dlp")
                run_sp("pip install yt_dlp", realtime=False)
                import yt_dlp
                pass
            f_name = ""
            def cb(d):
                nonlocal f_name
                if d["status"] == "downloading" and "total_bytes_estimate" in d:# and d["total_bytes"] > 0:
                    if d["total_bytes_estimate"] > 0:
                        dl_progress = float(d["downloaded_bytes"]) / float(d["total_bytes_estimate"])
                        progress.value = dl_progress
                        progress.update()
                else:
                    f_name = d['filename']
            installer.status("...getting YouTube file")
            prt(progress)
            ydl_opts = {
                'format': 'm4a/bestaudio/best',
                "progress_hooks": [cb],
                'verbose': False,
                'quiet': True,
                'postprocessors': [{
                    'key': 'FFmpegExtractAudio',
                    'preferredcodec': 'mp3',
                }]
            }
            try:
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    error_code = ydl.download(audio_path)
            except Exception as e:
                alert_msg(page, f"ERROR: Couldn't download YouTube video for some reason...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            clear_last()
            #print(error_code)
            if error_code != 0:
                alert_msg(page, f"ERROR CODE {error_code}: Couldn't download YouTube video for some reason...")
                return
            local_path = f"{f_name.rpartition('.')[0]}.mp3"
            #file_object = yt.formats.filter(only_audio=True)[0].download()
            #f_name = yt.safe_filename()
            #.download(convert='mp3')
            #file_object = yt.download() #b variable stores the filename and meta(if available) as object of Output class.
            #installer.status("...converting to mp3")
            #local_path = AudioSegment.from_file(file_object).export(f"{f_name}.mp3", format="mp3")
            #extras.Convert(file_object,'mp3',add_meta=True)
            #local_path = file_object.file_path
        else:
            installer.status("...downloading file")
            local_path = download_file(audio_path)
    else:
        local_path = audio_path
    
    if (local_path.endswith("mp4") or local_path.endswith("avi")):
        get_ffmpeg(installer)
        import ffmpeg
        installer.status("...converting to mp3")
        video = ffmpeg.input(local_path)
        audio = video.audio
        filename, extension = os.path.splitext(os.path.basename(local_path))
        local_path = os.path.join(os.path.dirname(local_path), f"{filename}.mp3")
        audio.output(local_path)
    elif not (local_path.endswith("mp3") or local_path.endswith("wav")):
        alert_msg(page, f"ERROR: File path must be an mp3, wav, mp4 or avi file...")
        return
    if not os.path.exists(local_path):
        alert_msg(page, f"ERROR: File not found...")
        return
    installer.status("")
    installer.show_progress(False)
    installer.set_message("Running Whisper-AI on your Dialog...")
    prt(progress)
    try:
        #TODO: Split long files into smaller chunks to transcribe queue
        #ffmpeg -hide_banner -i <YOUR AUDIO FILE> -c copy -map 0 -segment_time 1:0:0 -f segment <OUTPUT DIR>/segment_%03d
        transcription = transcribe(local_path)
    except Exception as e:
        alert_msg(page, f"ERROR: Couldn't Transcribe audio for some reason. Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    clear_last()
    installer.status("")
    installer.set_message("Generated Whisper-AI Transcription:")
    prt(Text(transcription, size=20, selectable=True))
    use_ai = whisper_prefs['reformat'] or whisper_prefs['rewrite'] or whisper_prefs['summarize'] or whisper_prefs['describe'] or whisper_prefs['article'] or whisper_prefs['keypoints'] or whisper_prefs['keywords']
    if use_ai:
        good_key = True
        if 'GPT' in whisper_prefs['AI_engine']:
            try:
                if not bool(prefs['OpenAI_api_key']): good_key = False
            except NameError: good_key = False
            if not good_key:
                alert_msg(page, f"Missing OpenAI_api_key... Define your key in Settings.")
                return
            else:
                try:
                    import openai
                except ModuleNotFoundError:
                    installer.status("...installing openai")
                    run_sp("pip install --upgrade openai -qq", realtime=False)
                    pass
                finally:
                    import openai
                try:
                    from openai import OpenAI
                    openai_client = OpenAI(api_key=prefs['OpenAI_api_key'])
                except:
                    alert_msg(page, "Invalid OpenAI API Key. Change in Settings...")
                    return
                installer.status("")
        elif whisper_prefs['AI_engine'] == "Google Gemini":
            if not bool(prefs['PaLM_api_key']):
                alert_msg(page, "You must provide your Google Gemini MakerSuite API key in Settings first")
                return
            try:
                import google.generativeai as genai
                if force_update("generativeai"): raise ModuleNotFoundError("Forcing update")
            except:
                installer.status("Installing Google MakerSuite Library...")
                run_sp("pip install --upgrade google-generativeai", realtime=False)
                import google.generativeai as genai
                pass
            try:
                genai.configure(api_key=prefs['PaLM_api_key'])
            except:
                alert_msg(page, "Invalid Google Gemini API Key. Change in Settings...")
                return
            gemini_model = genai.GenerativeModel(model_name='gemini-pro')
            installer.status("")
    def question(request):
        if whisper_prefs['AI_engine'] == "OpenAI GPT-3":
            response = openai_client.completions.create(engine="text-davinci-003", prompt=request, max_tokens=2400, temperature=whisper_prefs['AI_temperature'], presence_penalty=1)
            result = response.choices[0].text.strip()
        elif whisper_prefs['AI_engine'] == "ChatGPT-3.5 Turbo":
            response = openai_client.chat.completions.create(model="gpt-3.5-turbo-0125", temperature=whisper_prefs['AI_temperature'], messages=[{"role": "user", "content": request}])
            result = response.choices[0].message.content.strip()
        elif "GPT-4" in whisper_prefs['AI_engine']:
            gpt_model = "gpt-4-turbo" if "Turbo" in whisper_prefs['AI_engine'] else "gpt-4o" if "4o" in whisper_prefs['AI_engine'] else "gpt-4"
            response = openai_client.chat.completions.create(model=gpt_model, temperature=whisper_prefs['AI_temperature'], messages=[{"role": "user", "content": request}])
            result = response.choices[0].message.content.strip()
        elif whisper_prefs['AI_engine'] == "Google Gemini":
            response = gemini_model.generate_content(request, generation_config={
                'temperature': whisper_prefs['AI_temperature'],
                'max_output_tokens': 1024
            })
            #response = palm.generate_text(model='models/text-bison-001', prompt=request, temperature=whisper_prefs['AI_temperature'], max_output_tokens=1024)
            result = response.text.strip()
        if '*' in result:
            result = result.replace('*', '').strip()
        return result
    if whisper_prefs['reformat']:
        prt("Reformat of transcript:")
        response = question(f"{whisper_requests['reformat']}\n{transcription}")
        prt(Text(response, size=18, selectable=True))
    if whisper_prefs['rewrite']:
        prt("Rewrite edit of transcript:")
        response = question(f"{whisper_requests['rewrite']}\n{transcription}")
        prt(Text(response, size=18, selectable=True))
    if whisper_prefs['summarize']:
        prt("Summary of transcript:")
        response = question(f"{whisper_requests['summarize']}\n{transcription}")
        prt(Text(response, size=18, selectable=True))
    if whisper_prefs['describe']:
        prt("Description of transcript:")
        response = question(f"{whisper_requests['describe']}\n{transcription}")
        prt(Text(response, size=18, selectable=True))
    if whisper_prefs['article']:
        prt("Article of transcript:")
        response = question(f"{whisper_requests['article']}\n{transcription}")
        prt(Text(response, size=18, selectable=True))
    if whisper_prefs['keypoints']:
        prt("Key Points of transcript:")
        response = question(f"{whisper_requests['keypoints']}\n{transcription}")
        prt(Text(response, size=18, selectable=True))
    if whisper_prefs['keywords']:
        prt("SEO Keywords of transcript:")
        response = question(f"{whisper_requests['keywords']}\n{transcription}")
        prt(Text(response, size=18, selectable=True))
    if whisper_prefs['translate']:
        prt("Translation of transcript:")
        response = question(f"Translate the following text from {from_language} into {whisper_prefs['to_language']}.\n{transcription}")
        prt(Text(response, size=18, selectable=True))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_voice_fixer(page):
    global voice_fixer_prefs, pipe_voice_fixer
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.VoiceFixer.controls.append(line)
      page.VoiceFixer.update()
    def clear_last(lines=1):
      clear_line(page.VoiceFixer, lines=lines)
    def clear_list():
      page.VoiceFixer.controls = page.VoiceFixer.controls[:1]
    def autoscroll(scroll=True):
      page.VoiceFixer.auto_scroll = scroll
      page.VoiceFixer.update()
    def play_audio(e):
      e.control.data.play()
    if not (voice_fixer_prefs['mode_0'] or voice_fixer_prefs['mode_1'] or voice_fixer_prefs['mode_2']):
        alert_msg(page, f"You must select at least one Fixer Mode to restore...")
        return
    clear_list()
    autoscroll(True)
    progress = ProgressBar(bar_height=8)
    installer = Installing("Installing VoiceFixer Packages...")
    prt(installer)
    clear_pipes("voice_fixer")
    try:
        from voicefixer import VoiceFixer
    except ModuleNotFoundError as e:
        installer.status("...installing voicefixer (takes a while)")
        run_sp("pip install voicefixer --upgrade", realtime=False)
        from voicefixer import VoiceFixer
        pass
    if pipe_voice_fixer == None:
        installer.status("...initializing voicefixer model")
        pipe_voice_fixer = VoiceFixer()
    audio_path = voice_fixer_prefs['audio_file'].strip()
    local_path = ""
    if audio_path.startswith("http"):
        if audio_path.startswith("https://youtu") or 'youtube' in audio_path:
            pip_install("ffmpeg yt_dlp", installer=installer)
            import yt_dlp
            f_name = ""
            def cb(d):
                nonlocal f_name
                if d["status"] == "downloading" and "total_bytes_estimate" in d:# and d["total_bytes"] > 0:
                    if d["total_bytes_estimate"] > 0:
                        dl_progress = float(d["downloaded_bytes"]) / float(d["total_bytes_estimate"])
                        progress.value = dl_progress
                        progress.update()
                else:
                    f_name = d['filename']
            installer.status("...getting YouTube file")
            prt(progress)
            ydl_opts = {
                'format': 'm4a/bestaudio/best',
                "progress_hooks": [cb],
                'verbose': False,
                'quiet': True,
                'postprocessors': [{
                    'key': 'FFmpegExtractAudio',
                    'preferredcodec': 'mp3',
                }]
            }
            try:
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    error_code = ydl.download(audio_path)
            except Exception as e:
                alert_msg(page, f"ERROR: Couldn't download YouTube video for some reason...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            clear_last()
            #print(error_code)
            if error_code != 0:
                alert_msg(page, f"ERROR CODE {error_code}: Couldn't download YouTube video for some reason...")
                return
            local_path = f"{f_name.rpartition('.')[0]}.mp3"
        else:
            installer.status("...downloading file")
            local_path = download_file(audio_path)
    else:
        local_path = audio_path
    if not (local_path.endswith("mp3") or local_path.endswith("wav") or local_path.endswith("flac")):
        alert_msg(page, f"ERROR: File path must be a wav, mp3 or flac file...")
        return
    if not os.path.exists(local_path):
        alert_msg(page, f"ERROR: Audio File not found...")
        return
    if local_path.endswith("mp3"):
        get_ffmpeg(installer)
        import ffmpeg
        try:
            import pydub
        except ImportError:
            installer.status("...installing pydub")
            run_sp("pip install -q pydub", realtime=False)
            import pydub
            pass
        from pydub import AudioSegment
        installer.status("...converting from mp3")
        sound = AudioSegment.from_mp3(local_path)
        wav_file = local_path.rpartition(slash)[2].rpartition(".")[0]
        local_path = available_file(root_dir, wav_file, 0, ext='wav')
        sound = sound.set_frame_rate(44100)
        installer.status("...converting to wav")
        sound.export(local_path, format="wav")
    #save_dir = os.path.join(root_dir, 'audio_out', voice_fixer_prefs['batch_folder_name'])
    #if not os.path.exists(save_dir):
    #    os.makedirs(save_dir, exist_ok=True)
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    if bool(voice_fixer_prefs['batch_folder_name']):
        audio_out = os.path.join(audio_out, voice_fixer_prefs['batch_folder_name'])
    os.makedirs(audio_out, exist_ok=True)
    if bool(voice_fixer_prefs['audio_name']):
        fname = format_filename(voice_fixer_prefs['audio_name'], force_underscore=True)
    else: fname = "output"
    fname = f"{voice_fixer_prefs['file_prefix']}{fname}"
    output_file = available_file(audio_out, fname, 0, ext='wav')
    modes = []
    if voice_fixer_prefs['mode_0']: modes.append(0)
    if voice_fixer_prefs['mode_1']: modes.append(1)
    if voice_fixer_prefs['mode_2']: modes.append(2)
    clear_last()
    try:
        for mode in modes:
            prt(f"Running VoiceFixer mode {mode} on your Recording...")
            prt(progress)
            output_file = available_file(audio_out, f"{fname}-mode{mode}", 0, ext='wav', no_num=True)
            pipe_voice_fixer.restore(input=local_path, output=output_file, cuda=torch_device == "cuda", mode=mode)
            clear_last()
            clear_last()
            prt(AudioPlayer(src=output_file, display=output_file, page=page))
            #a_out = Audio(src=output_file, autoplay=False)
            #page.overlay.append(a_out)
            #page.update()
            #display_name = output_file
            #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
    except Exception as e:
        alert_msg(page, f"ERROR: Couldn't Restore audio for some reason. Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        clear_last()
        return
    autoscroll(False)
    play_snd(Snd.ALERT, page)


loaded_StableUnCLIP = None
def run_unCLIP(page, from_list=False):
    global unCLIP_prefs, pipe_unCLIP, loaded_StableUnCLIP
    if not check_diffusers(page): return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.unCLIP.controls.append(line)
        if update:
          page.unCLIP.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.unCLIP, lines=lines)
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.unCLIP.controls = page.unCLIP.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.unCLIP.auto_scroll = scroll
        page.unCLIP.update()
      else:
        page.unCLIP.auto_scroll = scroll
        page.unCLIP.update()
    progress = ProgressBar(bar_height=8)
    if unCLIP_prefs['use_StableUnCLIP_pipeline']:
      total_steps = unCLIP_prefs['prior_num_inference_steps']
    else:
      total_steps = unCLIP_prefs['prior_num_inference_steps'] + unCLIP_prefs['decoder_num_inference_steps'] + unCLIP_prefs['super_res_num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    unCLIP_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        unCLIP_prompts.append(p.prompt)
    else:
      if not bool(unCLIP_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      unCLIP_prompts.append(unCLIP_prefs['prompt'])
    autoscroll(True)
    clear_list()
    from PIL.PngImagePlugin import PngInfo
    clear_pipes('unCLIP')
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    model_id = "kakaobrain/karlo-v1-alpha"
    stable = "Stable " if unCLIP_prefs['use_StableUnCLIP_pipeline'] else ""
    if pipe_unCLIP != None and ((loaded_StableUnCLIP == True and not unCLIP_prefs['use_StableUnCLIP_pipeline']) or (loaded_StableUnCLIP == False and unCLIP_prefs['use_StableUnCLIP_pipeline'])):
        del pipe_unCLIP
        flush()
        pipe_unCLIP = None
    if pipe_unCLIP == None:
        prt(Installing(f"  Downloading {stable}unCLIP Kakaobrain Karlo Pipeline... It's a big one, see console for progress."))
        try:
            if unCLIP_prefs['use_StableUnCLIP_pipeline']:
              from diffusers import UnCLIPScheduler, DDPMScheduler, StableUnCLIPPipeline
              from diffusers.models import PriorTransformer
              from transformers import CLIPTokenizer, CLIPTextModelWithProjection
              prior = PriorTransformer.from_pretrained(model_id, subfolder="prior", torch_dtype=torch.float16)
              prior_text_model_id = "openai/clip-vit-large-patch14"
              prior_tokenizer = CLIPTokenizer.from_pretrained(prior_text_model_id)
              prior_text_model = CLIPTextModelWithProjection.from_pretrained(prior_text_model_id, torch_dtype=torch.float16)
              prior_scheduler = UnCLIPScheduler.from_pretrained(model_id, subfolder="prior_scheduler")
              #prior_scheduler = DDPMScheduler.from_config(prior_scheduler.config)
              prior_scheduler = pipeline_scheduler(prior_scheduler, from_scheduler=False)
              stable_unclip_model_id = "stabilityai/stable-diffusion-2-1-unclip-small"
              pipe_unCLIP = StableUnCLIPPipeline.from_pretrained(
                  stable_unclip_model_id,
                  torch_dtype=torch.float16,
                  variant="fp16",
                  prior_tokenizer=prior_tokenizer,
                  prior_text_encoder=prior_text_model,
                  prior=prior,
                  prior_scheduler=prior_scheduler,
              )
              pipe_unCLIP.to(torch_device)
              pipe_unCLIP.enable_attention_slicing()
              pipe_unCLIP.enable_sequential_cpu_offload()
              #from diffusers import DiffusionPipeline
              #pipe_unCLIP = DiffusionPipeline.from_pretrained(model_id, custom_pipeline="stable_unclip", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, decoder_pipe_kwargs=dict(image_encoder=None))
              #pipe_unCLIP.to(torch_device)
              #pipe_unCLIP = optimize_pipe(pipe_unCLIP)
              loaded_StableUnCLIP = True
            else:
              from diffusers import UnCLIPPipeline
              pipe_unCLIP = UnCLIPPipeline.from_pretrained(model_id, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
              #pipe_unCLIP.to(torch_device)
              pipe_unCLIP = optimize_pipe(pipe_unCLIP, freeu=False)
              loaded_StableUnCLIP = False
        except Exception as e:
            clear_last()
            alert_msg(page, f"Error Downloading {stable}unCLIP Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        pipe_unCLIP.set_progress_bar_config(disable=True)
        clear_last()
    s = "s" if unCLIP_prefs['num_images'] > 1 else ""
    prt(f" Generating {stable}unCLIP{s} of your Image...")
    batch_output = os.path.join(stable_dir, unCLIP_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], unCLIP_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for pr in unCLIP_prompts:
        for num in range(unCLIP_prefs['num_images']):
            prt(progress)
            autoscroll(False)
            random_seed = get_seed(unCLIP_prefs['seed'])
            generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            try:
                if unCLIP_prefs['use_StableUnCLIP_pipeline']:#decoder_num_inference_steps=unCLIP_prefs['decoder_num_inference_steps'], super_res_num_inference_steps=unCLIP_prefs['super_res_num_inference_steps'], decoder_guidance_scale=unCLIP_prefs['decoder_guidance_scale'],
                  images = pipe_unCLIP([pr], prior_num_inference_steps=unCLIP_prefs['prior_num_inference_steps'], prior_guidance_scale=unCLIP_prefs['prior_guidance_scale'], num_images_per_prompt=1, width=512, height=512, generator=generator, callback=callback_fnc, callback_steps=1).images
                else:
                  images = pipe_unCLIP([pr], prior_num_inference_steps=unCLIP_prefs['prior_num_inference_steps'], decoder_num_inference_steps=unCLIP_prefs['decoder_num_inference_steps'], super_res_num_inference_steps=unCLIP_prefs['super_res_num_inference_steps'], prior_guidance_scale=unCLIP_prefs['prior_guidance_scale'], decoder_guidance_scale=unCLIP_prefs['decoder_guidance_scale'], num_images_per_prompt=1, generator=generator, callback=callback_fnc, callback_steps=1).images
            except Exception as e:
                clear_last()
                alert_msg(page, f"Error running {stable}unCLIP Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            autoscroll(True)
            clear_last()
            fname = format_filename(pr)

            if prefs['file_suffix_seed']: fname += f"-{random_seed}"
            for image in images:
                image_path = available_file(os.path.join(stable_dir, unCLIP_prefs['batch_folder_name']), fname, num)
                unscaled_path = image_path
                output_file = image_path.rpartition(slash)[2]
                image.save(image_path)
                out_path = os.path.dirname(image_path)
                upscaled_path = os.path.join(out_path, output_file)
                if not unCLIP_prefs['display_upscaled_image'] or not unCLIP_prefs['apply_ESRGAN_upscale']:
                    prt(Row([ImageButton(src=unscaled_path, width=512, height=512, data=upscaled_path, page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if unCLIP_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    w = int(unCLIP_prefs['width'] * unCLIP_prefs["enlarge_scale"])
                    h = int(unCLIP_prefs['height'] * unCLIP_prefs["enlarge_scale"])
                    prt(Row([Text(f'Enlarging {unCLIP_prefs["enlarge_scale"]}X to {w}x{h}')], alignment=MainAxisAlignment.CENTER))
                    upscale_image(image_path, upscaled_path, scale=unCLIP_prefs["enlarge_scale"])
                    image_path = upscaled_path
                    clear_last()
                if prefs['save_image_metadata']:
                    img = PILImage.open(image_path)
                    metadata = PngInfo()
                    metadata.add_text("artist", prefs['meta_ArtistName'])
                    metadata.add_text("copyright", prefs['meta_Copyright'])
                    metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {unCLIP_prefs['enlarge_scale']}x with ESRGAN" if unCLIP_prefs['apply_ESRGAN_upscale'] else "")
                    metadata.add_text("pipeline", f"{stable}unCLIP")
                    if prefs['save_config_in_metadata']:
                      metadata.add_text("title", pr)
                      config_json = unCLIP_prefs.copy()
                      config_json['model_path'] = model_id
                      config_json['seed'] = random_seed
                      del config_json['num_images']
                      del config_json['display_upscaled_image']
                      del config_json['batch_folder_name']
                      if not config_json['apply_ESRGAN_upscale']:
                        del config_json['enlarge_scale']
                        del config_json['apply_ESRGAN_upscale']
                      metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                    img.save(image_path, pnginfo=metadata)
                #TODO: PyDrive
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                time.sleep(0.2)
                if unCLIP_prefs['display_upscaled_image']:
                    time.sleep(0.6)
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(unCLIP_prefs["enlarge_scale"]), height=512 * float(unCLIP_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_unCLIP_image_variation(page, from_list=False):
    global unCLIP_image_variation_prefs, pipe_unCLIP_image_variation
    if not check_diffusers(page): return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.UnCLIP_ImageVariation.controls.append(line)
        if update:
          page.UnCLIP_ImageVariation.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.UnCLIP_ImageVariation, lines=lines)
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.UnCLIP_ImageVariation.controls = page.UnCLIP_ImageVariation.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.UnCLIP_ImageVariation.auto_scroll = scroll
        page.UnCLIP_ImageVariation.update()
      else:
        page.UnCLIP_ImageVariation.auto_scroll = scroll
        page.UnCLIP_ImageVariation.update()
    progress = ProgressBar(bar_height=8)
    total_steps = unCLIP_image_variation_prefs['decoder_num_inference_steps'] + unCLIP_image_variation_prefs['super_res_num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    unCLIP_image_variation_inits = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if bool(p['init_image']):
          unCLIP_image_variation_inits.append(p.prompt)
    else:
      if not bool(unCLIP_image_variation_prefs['init_image']):
        alert_msg(page, "You need to add a Initial Image first... ")
        return
      unCLIP_image_variation_inits.append(unCLIP_image_variation_prefs['init_image'])
    autoscroll(True)
    clear_list()
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_pipes('unCLIP_image_variation')
    flush()
    if pipe_unCLIP_image_variation == None:
        from diffusers import UnCLIP_ImageVariationPipeline
        prt(Installing(" Downloading unCLIP Image Variation Kakaobrain Karlo Pipeline... It's a big one, see console for progress."))
        try:
            pipe_unCLIP_image_variation = UnCLIP_ImageVariationPipeline.from_pretrained("kakaobrain/karlo-v1-alpha-image-variations", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_unCLIP_image_variation.to(torch_device)
            pipe_unCLIP_image_variation = optimize_pipe(pipe_unCLIP_image_variation, freeu=False)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Downloading unCLIP Image Variation Pipeline", content=Text(str(e)))
            return
        pipe_unCLIP_image_variation.set_progress_bar_config(disable=True)
        clear_last()
    s = "s" if unCLIP_image_variation_prefs['num_images'] > 1 else ""
    prt(f"Generating unCLIP Image Variation{s} of your Image...")
    batch_output = os.path.join(stable_dir, unCLIP_image_variation_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], unCLIP_image_variation_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for init in unCLIP_image_variation_inits:
        if init.startswith('http'):
          init_img = PILImage.open(requests.get(init, stream=True).raw)
        else:
          if os.path.isfile(init):
            init_img = PILImage.open(init)
          else:
            alert_msg(page, f"ERROR: Couldn't find your init_image {init}")
            return
        width, height = init_img.size
        width, height = scale_dimensions(width, height, unCLIP_image_variation_prefs['max_size'])
        init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)
        init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        for num in range(unCLIP_image_variation_prefs['num_images']):
            prt(progress)
            autoscroll(False)
            random_seed = get_seed(int(unCLIP_image_variation_prefs['seed']) + num)
            generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            try:
                images = pipe_unCLIP_image_variation(image=init_img, decoder_num_inference_steps=unCLIP_image_variation_prefs['decoder_num_inference_steps'], super_res_num_inference_steps=unCLIP_image_variation_prefs['super_res_num_inference_steps'], decoder_guidance_scale=unCLIP_image_variation_prefs['decoder_guidance_scale'], num_images_per_prompt=1, generator=generator).images #, callback=callback_fnc, callback_steps=1
            except Exception as e:
                clear_last()
                alert_msg(page, "Error running unCLIP Image Variation Pipeline", content=Text(str(e)))
                return
            clear_last()
            autoscroll(True)
            #fname = format_filename(unCLIP_image_variation_prefs['file_name'])
            fname = init.rpartition(slash)[2].rpartition('.')[0]
            if prefs['file_suffix_seed']: fname += f"-{random_seed}"
            for image in images:
                image_path = available_file(os.path.join(stable_dir, unCLIP_image_variation_prefs['batch_folder_name']), fname, num)
                unscaled_path = image_path
                output_file = image_path.rpartition(slash)[2]
                image.save(image_path)
                out_path = os.path.dirname(image_path)
                upscaled_path = os.path.join(out_path, output_file)
                if not unCLIP_image_variation_prefs['display_upscaled_image'] or not unCLIP_image_variation_prefs['apply_ESRGAN_upscale']:
                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if unCLIP_image_variation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    upscale_image(image_path, upscaled_path, scale=unCLIP_image_variation_prefs["enlarge_scale"])
                    image_path = upscaled_path
                    if unCLIP_image_variation_prefs['display_upscaled_image']:
                        time.sleep(0.6)
                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(unCLIP_image_variation_prefs["enlarge_scale"]), height=512 * float(unCLIP_image_variation_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if prefs['save_image_metadata']:
                    img = PILImage.open(image_path)
                    metadata = PngInfo()
                    metadata.add_text("artist", prefs['meta_ArtistName'])
                    metadata.add_text("copyright", prefs['meta_Copyright'])
                    metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {unCLIP_image_variation_prefs['enlarge_scale']}x with ESRGAN" if unCLIP_image_variation_prefs['apply_ESRGAN_upscale'] else "")
                    metadata.add_text("pipeline", "unCLIP_image_variation")
                    if prefs['save_config_in_metadata']:
                      #metadata.add_text("title", unCLIP_image_variation_prefs['file_name'])
                      config_json = unCLIP_image_variation_prefs.copy()
                      config_json['model_path'] = "fusing/karlo-image-variations-diffusers"
                      config_json['seed'] = random_seed
                      del config_json['num_images']
                      del config_json['display_upscaled_image']
                      del config_json['batch_folder_name']
                      del config_json['file_name']
                      if not config_json['apply_ESRGAN_upscale']:
                        del config_json['enlarge_scale']
                        del config_json['apply_ESRGAN_upscale']
                      metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                    img.save(image_path, pnginfo=metadata)
                #TODO: PyDrive
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_image_variation_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_image_variation_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                time.sleep(0.2)
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_unCLIP_interpolation(page, from_list=False):
    global unCLIP_interpolation_prefs, pipe_unCLIP_interpolation, loaded_StableUnCLIP
    if not check_diffusers(page): return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.unCLIP_interpolation_output.controls.append(line)
        if update:
          page.unCLIP_interpolation_output.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.unCLIP_interpolation_output, lines=lines)
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.unCLIP_Interpolation.controls = page.unCLIP_Interpolation.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.unCLIP_Interpolation.auto_scroll = scroll
        page.unCLIP_Interpolation.update()
      else:
        page.unCLIP_Interpolation.auto_scroll = scroll
        page.unCLIP_Interpolation.update()
    progress = ProgressBar(bar_height=8)
    total_steps = unCLIP_interpolation_prefs['prior_num_inference_steps'] + unCLIP_interpolation_prefs['decoder_num_inference_steps'] + unCLIP_interpolation_prefs['super_res_num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    unCLIP_interpolation_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        unCLIP_interpolation_prompts.append({'start_prompt': p.prompt, 'end_prompt':p['prompt2']})
    else:
      if not bool(unCLIP_interpolation_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      unCLIP_interpolation_prompts.append({'start_prompt': unCLIP_interpolation_prefs['prompt'], 'end_prompt':unCLIP_interpolation_prefs['end_prompt']})
    autoscroll(True)
    clear_list()
    from PIL.PngImagePlugin import PngInfo
    clear_pipes()#'unCLIP_interpolation')
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    model_id = "kakaobrain/karlo-v1-alpha"
    stable = "Stable "
    if pipe_unCLIP_interpolation != None:
        del pipe_unCLIP_interpolation
        flush()
        pipe_unCLIP_interpolation = None
    if pipe_unCLIP_interpolation == None:
        prt(Installing(f"Downloading {stable}unCLIP Kakaobrain Karlo Pipeline... It's a big one, see console for progress."))
        try:
            from diffusers import DiffusionPipeline
            pipe_unCLIP_interpolation = DiffusionPipeline.from_pretrained(model_id, custom_pipeline="AlanB/unclip_text_interpolation_mod", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None) #, decoder_pipe_kwargs=dict(image_encoder=None)
            pipe_unCLIP_interpolation.to(torch_device)
            pipe_unCLIP_interpolation.enable_attention_slicing()
            pipe_unCLIP_interpolation.enable_sequential_cpu_offload()
            loaded_StableUnCLIP = True
        except Exception as e:
            clear_last()
            alert_msg(page, f"Error Downloading {stable}unCLIP Pipeline", content=Text(str(e)))
            return
        #pipe_unCLIP_interpolation.set_progress_bar_config(disable=True)
        clear_last()
    s = "s" if unCLIP_interpolation_prefs['num_images'] > 1 else ""
    prt(f" Generating {stable}unCLIP{s} of your Image...")
    batch_output = os.path.join(stable_dir, unCLIP_interpolation_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], unCLIP_interpolation_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for pr in unCLIP_interpolation_prompts:
        for num in range(unCLIP_interpolation_prefs['num_images']):
            prt(progress)
            autoscroll(False)
            random_seed = get_seed(int(unCLIP_interpolation_prefs['seed']) + num)
            generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            try:
                images = pipe_unCLIP_interpolation(start_prompt=pr['start_prompt'], end_prompt=pr['end_prompt'], steps=unCLIP_interpolation_prefs['steps'], prior_num_inference_steps=unCLIP_interpolation_prefs['prior_num_inference_steps'], decoder_num_inference_steps=unCLIP_interpolation_prefs['decoder_num_inference_steps'], super_res_inference_steps=unCLIP_interpolation_prefs['super_res_inference_steps'], prior_guidance_scale=unCLIP_interpolation_prefs['prior_guidance_scale'], decoder_guidance_scale=unCLIP_interpolation_prefs['decoder_guidance_scale'], callback=callback_fnc, generator=generator).images
            except Exception as e:
                clear_last()
                alert_msg(page, f"Error running {stable}unCLIP Interpolation Pipeline", content=Text(str(e)))
                return
            clear_last()
            autoscroll(True)
            file_max_length = int(prefs['file_max_length']) / 2
            fname = f"{format_filename(pr['start_prompt'], max_length=file_max_length)}-to-{format_filename(pr['end_prompt'], max_length=file_max_length)}"

            if prefs['file_suffix_seed']: fname += f"-{random_seed}"
            for image in images:
                image_path = available_file(os.path.join(stable_dir, unCLIP_interpolation_prefs['batch_folder_name']), fname, num)
                unscaled_path = image_path
                output_file = image_path.rpartition(slash)[2]
                image.save(image_path)
                out_path = os.path.dirname(image_path)
                upscaled_path = os.path.join(out_path, output_file)
                if not unCLIP_interpolation_prefs['display_upscaled_image'] or not unCLIP_interpolation_prefs['apply_ESRGAN_upscale']:
                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if unCLIP_interpolation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    upscale_image(image_path, upscaled_path, scale=unCLIP_interpolation_prefs["enlarge_scale"])
                    image_path = upscaled_path

                if prefs['save_image_metadata']:
                    img = PILImage.open(image_path)
                    metadata = PngInfo()
                    metadata.add_text("artist", prefs['meta_ArtistName'])
                    metadata.add_text("copyright", prefs['meta_Copyright'])
                    metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {unCLIP_interpolation_prefs['enlarge_scale']}x with ESRGAN" if unCLIP_interpolation_prefs['apply_ESRGAN_upscale'] else "")
                    metadata.add_text("pipeline", f"{stable}unCLIP Text Interpolation")
                    if prefs['save_config_in_metadata']:
                      metadata.add_text("title", f"{pr['start_prompt']}-to-{pr['end_prompt']}")
                      config_json = unCLIP_interpolation_prefs.copy()
                      config_json['model_path'] = model_id
                      config_json['seed'] = random_seed
                      del config_json['num_images']
                      del config_json['display_upscaled_image']
                      del config_json['batch_folder_name']
                      if not config_json['apply_ESRGAN_upscale']:
                        del config_json['enlarge_scale']
                        del config_json['apply_ESRGAN_upscale']
                      metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                    img.save(image_path, pnginfo=metadata)
                #TODO: PyDrive
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_interpolation_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_interpolation_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                time.sleep(0.2)
                if unCLIP_interpolation_prefs['display_upscaled_image']:
                    time.sleep(0.6)
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(unCLIP_interpolation_prefs["enlarge_scale"]), height=512 * float(unCLIP_interpolation_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_unCLIP_image_interpolation(page, from_list=False):
    global unCLIP_image_interpolation_prefs, pipe_unCLIP_image_interpolation
    if not check_diffusers(page): return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.UnCLIP_ImageInterpolation.controls.append(line)
        if update:
          page.UnCLIP_ImageInterpolation.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.UnCLIP_ImageInterpolation, lines=lines)
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.UnCLIP_ImageInterpolation.controls = page.UnCLIP_ImageInterpolation.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.UnCLIP_ImageInterpolation.auto_scroll = scroll
        page.UnCLIP_ImageInterpolation.update()
      else:
        page.UnCLIP_ImageInterpolation.auto_scroll = scroll
        page.UnCLIP_ImageInterpolation.update()
    progress = ProgressBar(bar_height=8)
    total_steps = unCLIP_image_interpolation_prefs['decoder_num_inference_steps'] + unCLIP_image_interpolation_prefs['super_res_num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    unCLIP_image_interpolation_inits = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if bool(p['init_image']) and bool(p['mask_image']):
          unCLIP_image_interpolation_inits.append({'init_image':p['init_image'], 'end_image':p['mask_image']})
    else:
      if not bool(unCLIP_image_interpolation_prefs['init_image']) or not bool(unCLIP_image_interpolation_prefs['end_image']):
        alert_msg(page, "You need to add a Initial and Ending Image first... ")
        return
      unCLIP_image_interpolation_inits.append({'init_image':unCLIP_image_interpolation_prefs['init_image'], 'end_image':unCLIP_image_interpolation_prefs['end_image']})
    autoscroll(True)
    clear_list()
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_pipes('unCLIP_image_interpolation')
    dtype = torch.float16 if not prefs['higher_vram_mode'] else torch.float32 if torch.cuda.is_available() else torch.bfloat16
    if pipe_unCLIP_image_interpolation == None:
        from diffusers import DiffusionPipeline
        prt(Installing("Downloading unCLIP Image Interpolation Kakaobrain Karlo Pipeline... It's a big one, see console for progress."))
        try:
            pipe_unCLIP_image_interpolation = DiffusionPipeline.from_pretrained("kakaobrain/karlo-v1-alpha-image-variations", custom_pipeline="unclip_image_interpolation", torch_dtype=dtype, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_unCLIP_image_interpolation.to(torch_device)
            pipe_unCLIP_image_interpolation = optimize_pipe(pipe_unCLIP_image_interpolation, freeu=False)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Downloading unCLIP Image Interpolation Pipeline", content=Text(str(e)))
            return
        pipe_unCLIP_image_interpolation.set_progress_bar_config(disable=True)
        clear_last()
    s = "s" if unCLIP_image_interpolation_prefs['num_images'] > 1 else ""
    prt(f"Generating unCLIP Image Interpolation{s} of your Image...")
    batch_output = os.path.join(stable_dir, unCLIP_image_interpolation_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], unCLIP_image_interpolation_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for interpolation_images in unCLIP_image_interpolation_inits:
        if interpolation_images['init_image'].startswith('http'):
          init_img = PILImage.open(requests.get(interpolation_images['init_image'], stream=True).raw)
        else:
          if os.path.isfile(interpolation_images['init_image']):
            init_img = PILImage.open(interpolation_images['init_image'])
          else:
            alert_msg(page, f"ERROR: Couldn't find your init_image {interpolation_images['init_image']}")
            return
        width, height = init_img.size
        width, height = scale_dimensions(width, height, unCLIP_image_interpolation_prefs['max_size'])
        init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)
        init_img = ImageOps.exif_transpose(init_img).convert("RGB")

        if interpolation_images['end_image'].startswith('http'):
          end_img = PILImage.open(requests.get(interpolation_images['end_image'], stream=True).raw)
        else:
          if os.path.isfile(interpolation_images['end_image']):
            end_img = PILImage.open(interpolation_images['end_image'])
          else:
            alert_msg(page, f"ERROR: Couldn't find your init_end_imageimage {interpolation_images['end_image']}")
            return
        width, height = end_img.size
        width, height = scale_dimensions(width, height, unCLIP_image_interpolation_prefs['max_size'])
        end_img = end_img.resize((width, height), resample=PILImage.BICUBIC)
        end_img = ImageOps.exif_transpose(end_img).convert("RGB")
        prt(progress)
        autoscroll(False)
        num = 0
        random_seed = get_seed(int(unCLIP_image_interpolation_prefs['seed']) + num)
        generator = torch.Generator(device=torch_device).manual_seed(random_seed)
        try:
            images = pipe_unCLIP_image_interpolation(image=[init_img, end_img], steps=unCLIP_image_interpolation_prefs['interpolation_steps'], decoder_num_inference_steps=unCLIP_image_interpolation_prefs['decoder_num_inference_steps'], super_res_num_inference_steps=unCLIP_image_interpolation_prefs['super_res_num_inference_steps'], decoder_guidance_scale=unCLIP_image_interpolation_prefs['decoder_guidance_scale'], num_images_per_prompt=unCLIP_image_interpolation_prefs['num_images'], generator=generator).images #, callback=callback_fnc, callback_steps=1
        except Exception as e:
            clear_last()
            alert_msg(page, "Error running unCLIP Image Interpolation Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
            return
        autoscroll(True)
        clear_last()
        #fname = format_filename(unCLIP_image_interpolation_prefs['file_name'])
        fname_init = interpolation_images['init_image'].rpartition(slash)[2].rpartition('.')[0]
        fname_end = interpolation_images['end_image'].rpartition(slash)[2].rpartition('.')[0]
        fname = f"{fname_init[:int(prefs['file_max_length']/2)]}-to-{fname_end[:int(prefs['file_max_length']/2)]}"
        if prefs['file_suffix_seed']: fname += f"-{random_seed}"
        for image in images:
            random_seed += num
            image_path = available_file(os.path.join(stable_dir, unCLIP_image_interpolation_prefs['batch_folder_name']), fname, num)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            image.save(image_path)
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)
            if not unCLIP_image_interpolation_prefs['display_upscaled_image'] or not unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=unCLIP_image_interpolation_prefs["enlarge_scale"])
                image_path = upscaled_path
                if unCLIP_image_interpolation_prefs['display_upscaled_image']:
                    time.sleep(0.6)
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(unCLIP_image_interpolation_prefs["enlarge_scale"]), height=512 * float(unCLIP_image_interpolation_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if prefs['save_image_metadata']:
                img = PILImage.open(image_path)
                metadata = PngInfo()
                metadata.add_text("artist", prefs['meta_ArtistName'])
                metadata.add_text("copyright", prefs['meta_Copyright'])
                metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {unCLIP_image_interpolation_prefs['enlarge_scale']}x with ESRGAN" if unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale'] else "")
                metadata.add_text("pipeline", "unCLIP Image Interpolation")
                if prefs['save_config_in_metadata']:
                    #metadata.add_text("title", unCLIP_image_interpolation_prefs['file_name'])
                    config_json = unCLIP_image_interpolation_prefs.copy()
                    config_json['model_path'] = "kakaobrain/karlo-v1-alpha-image-variations"
                    config_json['seed'] = random_seed
                    del config_json['num_images']
                    del config_json['display_upscaled_image']
                    del config_json['batch_folder_name']
                    del config_json['file_name']
                    if not config_json['apply_ESRGAN_upscale']:
                        del config_json['enlarge_scale']
                        del config_json['apply_ESRGAN_upscale']
                    metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                img.save(image_path, pnginfo=metadata)
            #TODO: PyDrive
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], unCLIP_image_interpolation_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], unCLIP_image_interpolation_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
            num +=1
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_magic_mix(page, from_list=False):
    global magic_mix_prefs, pipe_magic_mix
    if not check_diffusers(page): return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.MagicMix.controls.append(line)
        if update:
          page.MagicMix.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.MagicMix, lines=lines)
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.MagicMix.controls = page.MagicMix.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.MagicMix.auto_scroll = scroll
        page.MagicMix.update()
      else:
        page.MagicMix.auto_scroll = scroll
        page.MagicMix.update()
    progress = ProgressBar(bar_height=8)
    total_steps = magic_mix_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    magic_mix_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        magic_mix_prompts.append(p.prompt)
    else:
      if not bool(magic_mix_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      magic_mix_prompts.append(magic_mix_prefs['prompt'])
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    else:
      clear_list()
    autoscroll(True)
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    if magic_mix_prefs['init_image'].startswith('http'):
      init_img = PILImage.open(requests.get(magic_mix_prefs['init_image'], stream=True).raw)
    else:
      if os.path.isfile(magic_mix_prefs['init_image']):
        init_img = PILImage.open(magic_mix_prefs['init_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your init_image {magic_mix_prefs['init_image']}")
        return
    width, height = init_img.size
    width, height = scale_dimensions(width, height, magic_mix_prefs['max_size'])
    init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)
    init_img = ImageOps.exif_transpose(init_img).convert("RGB")
    '''tform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize(
            (width, height),
            interpolation=transforms.InterpolationMode.BICUBIC,
            antialias=False,
            ),
        transforms.Normalize(
          [0.48145466, 0.4578275, 0.40821073],
          [0.26862954, 0.26130258, 0.27577711]),
    ])
    init_img = tform(init_img).to(torch_device)'''
    clear_pipes('magic_mix')
    model = get_model(prefs['model_ckpt'])['path']
    scheduler_mode = magic_mix_prefs['scheduler_mode']
    if scheduler_mode == "LMS Discrete":
      from diffusers import LMSDiscreteScheduler
      schedule = LMSDiscreteScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "PNDM":
      from diffusers import PNDMScheduler
      schedule = PNDMScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "DDIM":
      from diffusers import DDIMScheduler
      schedule = DDIMScheduler.from_pretrained(model, subfolder="scheduler")
    if pipe_magic_mix == None or magic_mix_prefs['scheduler_mode'] != magic_mix_prefs['scheduler_last']:
        from diffusers import DiffusionPipeline
        prt(Installing("Downloading MagicMix Pipeline... "))
        try:
            pipe_magic_mix = DiffusionPipeline.from_pretrained(model, custom_pipeline="AlanB/magic_mix_mod", scheduler=schedule, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_magic_mix.to(torch_device)
            pipe_magic_mix = optimize_pipe(pipe_magic_mix, freeu=False)
            magic_mix_prefs['scheduler_last'] = magic_mix_prefs['scheduler_mode']
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Downloading MagicMix Pipeline", content=Text(str(e)))
            return
        #pipe_magic_mix.set_progress_bar_config(disable=True)
        clear_last()
    s = "es" if magic_mix_prefs['num_images'] > 1 else ""
    prt(f"Generating MagicMix{s} of your Image...")
    batch_output = os.path.join(stable_dir, magic_mix_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], magic_mix_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for pr in magic_mix_prompts:
        for num in range(magic_mix_prefs['num_images']):
            prt(progress)
            autoscroll(False)
            random_seed = get_seed(int(magic_mix_prefs['seed']) + num)
            #generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            try:
                image = pipe_magic_mix(img=init_img, prompt=pr, steps=magic_mix_prefs['num_inference_steps'], kmin=magic_mix_prefs['kmin'], kmax=magic_mix_prefs['kmax'], mix_factor=magic_mix_prefs['mix_factor'], guidance_scale=magic_mix_prefs['guidance_scale'], seed=random_seed, callback=callback_fnc, callback_steps=1)#.images
            except Exception as e:
                clear_last()
                alert_msg(page, "Error running MagicMix Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
                return
            autoscroll(True)
            clear_last()
            fname = format_filename(pr)

            if prefs['file_suffix_seed']: fname += f"-{random_seed}"
            #for image in images:
            image_path = available_file(os.path.join(stable_dir, magic_mix_prefs['batch_folder_name']), fname, num)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            image.save(image_path)
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)
            if not magic_mix_prefs['display_upscaled_image'] or not magic_mix_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if magic_mix_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=magic_mix_prefs["enlarge_scale"])
                image_path = upscaled_path
                if magic_mix_prefs['display_upscaled_image']:
                    time.sleep(0.6)
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(magic_mix_prefs["enlarge_scale"]), height=height * float(magic_mix_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if prefs['save_image_metadata']:
                img = PILImage.open(image_path)
                metadata = PngInfo()
                metadata.add_text("artist", prefs['meta_ArtistName'])
                metadata.add_text("copyright", prefs['meta_Copyright'])
                metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {magic_mix_prefs['enlarge_scale']}x with ESRGAN" if magic_mix_prefs['apply_ESRGAN_upscale'] else "")
                metadata.add_text("pipeline", "magic_mix")
                if prefs['save_config_in_metadata']:
                  metadata.add_text("title", pr)
                  config_json = magic_mix_prefs.copy()
                  config_json['model_path'] = model
                  config_json['seed'] = random_seed
                  del config_json['num_images']
                  del config_json['display_upscaled_image']
                  del config_json['batch_folder_name']
                  del config_json['file_name']
                  del config_json["scheduler_last"]
                  del config_json['max_size']
                  if not config_json['apply_ESRGAN_upscale']:
                    del config_json['enlarge_scale']
                    del config_json['apply_ESRGAN_upscale']
                  metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                img.save(image_path, pnginfo=metadata)
            #TODO: PyDrive
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], magic_mix_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], magic_mix_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            time.sleep(0.2)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_paint_by_example(page):
    global paint_by_example_prefs, prefs, status, pipe_paint_by_example
    if not check_diffusers(page): return
    if not bool(paint_by_example_prefs['original_image']) or (not bool(paint_by_example_prefs['alpha_mask']) and not bool(paint_by_example_prefs['mask_image'])):
      alert_msg(page, "You must provide the Original Image and the Mask Image to process...")
      return
    if not bool(paint_by_example_prefs['example_image']):
      alert_msg(page, "You must provide an Example Image to Transfer Subject from...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.PaintByExample.controls.append(line)
      page.PaintByExample.update()
    def clear_last(lines=1):
      clear_line(page.PaintByExample, lines=lines)
    def autoscroll(scroll=True):
      page.PaintByExample.auto_scroll = scroll
      page.PaintByExample.update()
    def clear_list():
      page.PaintByExample.controls = page.PaintByExample.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = paint_by_example_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    autoscroll(True)
    clear_list()
    prt(Installing("Installing Paint-by-Example Pipeline..."))
    import requests, random
    from io import BytesIO
    from PIL import ImageOps
    from PIL.PngImagePlugin import PngInfo
    if paint_by_example_prefs['original_image'].startswith('http'):
      #response = requests.get(paint_by_example_prefs['original_image'])
      #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
      original_img = PILImage.open(requests.get(paint_by_example_prefs['original_image'], stream=True).raw)
    else:
      if os.path.isfile(paint_by_example_prefs['original_image']):
        original_img = PILImage.open(paint_by_example_prefs['original_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your original_image {paint_by_example_prefs['original_image']}")
        return
    width, height = original_img.size
    width, height = scale_dimensions(width, height, paint_by_example_prefs['max_size'])
    if bool(paint_by_example_prefs['alpha_mask']):
      original_img = ImageOps.exif_transpose(original_img).convert("RGBA")
    else:
      original_img = ImageOps.exif_transpose(original_img).convert("RGB")
    original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
    mask_img = None
    if not bool(paint_by_example_prefs['mask_image']) and bool(paint_by_example_prefs['alpha_mask']):
      red, green, blue, alpha = PILImage.Image.split(original_img)
      #mask_img = ImageOps.invert(alpha.convert('RGB'))
      mask_img = alpha.convert('L')
    else:
      if paint_by_example_prefs['mask_image'].startswith('http'):
        mask_img = PILImage.open(requests.get(paint_by_example_prefs['mask_image'], stream=True).raw)
      else:
        if os.path.isfile(paint_by_example_prefs['mask_image']):
          mask_img = PILImage.open(paint_by_example_prefs['mask_image'])
        else:
          alert_msg(page, f"ERROR: Couldn't find your mask_image {paint_by_example_prefs['mask_image']}")
          return
      if paint_by_example_prefs['invert_mask']:
        mask_img = ImageOps.invert(mask_img.convert('RGB'))
    #mask_img = mask_img.convert("L")
    #mask_img = mask_img.convert("1")
    mask_img = mask_img.resize((width, height), resample=PILImage.NEAREST)
    mask_img = ImageOps.exif_transpose(mask_img).convert("RGB")
    #print(f'Resize to {width}x{height}')
    if paint_by_example_prefs['example_image'].startswith('http'):
      example_img = PILImage.open(requests.get(paint_by_example_prefs['example_image'], stream=True).raw)
    else:
      if os.path.isfile(paint_by_example_prefs['example_image']):
        example_img = PILImage.open(paint_by_example_prefs['example_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your Example Image {paint_by_example_prefs['example_image']}")
        return

    clear_pipes('paint_by_example')
    model_id = "Fantasy-Studio/Paint-by-Example"
    if pipe_paint_by_example is None:
      from diffusers import PaintByExamplePipeline, PNDMScheduler
      PNDMscheduler = PNDMScheduler(skip_prk_steps=True)
      pipe_paint_by_example = PaintByExamplePipeline.from_pretrained(model_id, scheduler=PNDMscheduler, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
      pipe_paint_by_example = pipe_paint_by_example.to("cpu")
    clear_last()
    prt("Generating Paint-by-Example of your Image... (slow because it uses CPU instead of GPU)")
    prt(progress)
    autoscroll(False)
    batch_output = os.path.join(stable_dir, paint_by_example_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], paint_by_example_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    random_seed = int(paint_by_example_prefs['seed']) if int(paint_by_example_prefs['seed']) > 0 else random.randint(0,4294967295)
    generator = torch.Generator(device="cpu").manual_seed(random_seed)
    #generator = torch.manual_seed(random_seed)
    try:
      images = pipe_paint_by_example(image=original_img, mask_image=mask_img, example_image=example_img, num_inference_steps=paint_by_example_prefs['num_inference_steps'], eta=paint_by_example_prefs['eta'], guidance_scale=paint_by_example_prefs['guidance_scale'], num_images_per_prompt=paint_by_example_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images
    except Exception as e:
      clear_last()
      alert_msg(page, f"ERROR: Couldn't Paint-by-Example your image for some reason.  Possibly out of memory or something wrong with my code...", content=Text(str(e)))
      return
    clear_last()
    clear_last()
    autoscroll(True)
    filename = paint_by_example_prefs['original_image'].rpartition(slash)[2].rpartition('.')[0]
    #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
    num = 0
    for image in images:
        random_seed += num
        fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
        image_path = available_file(os.path.join(stable_dir, paint_by_example_prefs['batch_folder_name']), fname, num)
        unscaled_path = image_path
        output_file = image_path.rpartition(slash)[2]
        image.save(image_path)
        out_path = os.path.dirname(image_path)
        upscaled_path = os.path.join(out_path, output_file)
        if not paint_by_example_prefs['display_upscaled_image'] or not paint_by_example_prefs['apply_ESRGAN_upscale']:
            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
            #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        if paint_by_example_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            upscale_image(image_path, upscaled_path, scale=paint_by_example_prefs["enlarge_scale"])
            image_path = upscaled_path
            if paint_by_example_prefs['display_upscaled_image']:
                time.sleep(0.6)
                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(paint_by_example_prefs["enlarge_scale"]), height=height * float(paint_by_example_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        if prefs['save_image_metadata']:
            img = PILImage.open(image_path)
            metadata = PngInfo()
            metadata.add_text("artist", prefs['meta_ArtistName'])
            metadata.add_text("copyright", prefs['meta_Copyright'])
            metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {paint_by_example_prefs['enlarge_scale']}x with ESRGAN" if paint_by_example_prefs['apply_ESRGAN_upscale'] else "")
            metadata.add_text("pipeline", "Paint-by-Example")
            if prefs['save_config_in_metadata']:
              config_json = paint_by_example_prefs.copy()
              config_json['model_path'] = model_id
              config_json['seed'] = random_seed
              del config_json['num_images']
              del config_json['max_size']
              del config_json['display_upscaled_image']
              del config_json['batch_folder_name']
              del config_json['invert_mask']
              del config_json['alpha_mask']
              if not config_json['apply_ESRGAN_upscale']:
                del config_json['enlarge_scale']
                del config_json['apply_ESRGAN_upscale']
              metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
            img.save(image_path, pnginfo=metadata)
        #TODO: PyDrive
        if storage_type == "Colab Google Drive":
            new_file = available_file(os.path.join(prefs['image_output'], paint_by_example_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        elif bool(prefs['image_output']):
            new_file = available_file(os.path.join(prefs['image_output'], paint_by_example_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        time.sleep(0.2)
        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
        num += 1
    autoscroll(True)
    play_snd(Snd.ALERT, page)


def run_instruct_pix2pix(page, from_list=False):
    global instruct_pix2pix_prefs, prefs, status, pipe_instruct_pix2pix
    if not check_diffusers(page): return
    if not bool(instruct_pix2pix_prefs['original_image']) and not instruct_pix2pix_prefs['use_init_video']:
      alert_msg(page, "You must provide the Original Image and the Mask Image to process...")
      return
    if not bool(instruct_pix2pix_prefs['init_video']) and instruct_pix2pix_prefs['use_init_video']:
      alert_msg(page, "You must provide the Input Initial Video Clip to process...")
      return
    if not bool(instruct_pix2pix_prefs['prompt']):
      alert_msg(page, "You must provide a Instructional Image Editing Prompt...")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.InstructPix2Pix.controls.append(line)
        if update:
          page.InstructPix2Pix.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.InstructPix2Pix, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
      else:
        page.InstructPix2Pix.auto_scroll = scroll
        page.InstructPix2Pix.update()
    progress = ProgressBar(bar_height=8)
    total_steps = instruct_pix2pix_prefs['num_inference_steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):#(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.InstructPix2Pix.controls = page.InstructPix2Pix.controls[:1]
    instruct_pix2pix_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        instruct = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'original_image': p['init_image'] if bool(p['init_image']) else instruct_pix2pix_prefs['original_image'], 'seed': p['seed']}
        instruct_pix2pix_prompts.append(instruct)
      page.tabs.selected_index = 4
      page.tabs.update()
    else:
      if not bool(instruct_pix2pix_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      instruct = {'prompt':instruct_pix2pix_prefs['prompt'], 'negative_prompt': instruct_pix2pix_prefs['negative_prompt'], 'original_image': instruct_pix2pix_prefs['original_image'], 'seed': instruct_pix2pix_prefs['seed']}
      instruct_pix2pix_prompts.append(instruct)
    autoscroll(True)
    clear_list()
    prt(Divider(thickness=2, height=4))
    installer = Installing(f"Installing Instruct-Pix2Pix{' SDXL' if instruct_pix2pix_prefs['use_SDXL'] else ''} Pipeline...")
    prt(installer)
    import requests, random
    from io import BytesIO
    from PIL import ImageOps
    from PIL.PngImagePlugin import PngInfo
    model_id = "timbrooks/instruct-pix2pix"
    model_id_SDXL = "diffusers/sdxl-instructpix2pix-768"
    if 'loaded_instructpix2pix' not in status:
      status['loaded_instructpix2pix'] = ''
    if status['loaded_instructpix2pix'] != (model_id_SDXL if instruct_pix2pix_prefs['use_SDXL'] else model_id):
      clear_pipes()
    else:
      clear_pipes('instruct_pix2pix')
    if pipe_instruct_pix2pix is None:
      if instruct_pix2pix_prefs['use_SDXL']:
        from diffusers import StableDiffusionXLInstructPix2PixPipeline
        pipe_instruct_pix2pix = StableDiffusionXLInstructPix2PixPipeline.from_pretrained(model_id_SDXL, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
        pipe_instruct_pix2pix = optimize_SDXL(pipe_instruct_pix2pix)
        status['loaded_instructpix2pix'] = model_id_SDXL
      else:
        from diffusers import StableDiffusionInstructPix2PixPipeline
        pipe_instruct_pix2pix = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
        pipe_instruct_pix2pix = optimize_pipe(pipe_instruct_pix2pix)
        #pipe_instruct_pix2pix = pipe_instruct_pix2pix.to(torch_device)
        status['loaded_instructpix2pix'] = model_id
    pipeline_scheduler(pipe_instruct_pix2pix)
    ip_adapter_arg = {}
    if instruct_pix2pix_prefs['use_ip_adapter']:
      installer.status(f"...initialize IP-Adapter")
      ip_adapter_img = None
      if instruct_pix2pix_prefs['ip_adapter_image'].startswith('http'):
        i_response = requests.get(instruct_pix2pix_prefs['ip_adapter_image'])
        ip_adapter_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
      else:
        if os.path.isfile(instruct_pix2pix_prefs['ip_adapter_image']):
          ip_adapter_img = PILImage.open(instruct_pix2pix_prefs['ip_adapter_image'])
        else:
          clear_last()
          prt(f"ERROR: Couldn't find your ip_adapter_image {instruct_pix2pix_prefs['ip_adapter_image']}")
      if bool(ip_adapter_img):
        ip_adapter_arg['ip_adapter_image'] = ip_adapter_img
      if bool(ip_adapter_arg):
          if instruct_pix2pix_prefs['use_SDXL']:
            ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == instruct_pix2pix_prefs['ip_adapter_SDXL_model'])
          else:
            ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == instruct_pix2pix_prefs['ip_adapter_model'])
          pipe_instruct_pix2pix.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
          pipe_instruct_pix2pix.set_ip_adapter_scale(instruct_pix2pix_prefs['ip_adapter_strength'])
    clear_last()
    prt("Generating Instruct-Pix2Pix of your Image...")
    prt(progress)
    max_size = instruct_pix2pix_prefs['max_size']
    batch_output = os.path.join(stable_dir, instruct_pix2pix_prefs['batch_folder_name'])
    output_dir = batch_output
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], instruct_pix2pix_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    if instruct_pix2pix_prefs['use_init_video']:
        init_vid = instruct_pix2pix_prefs['init_video']
        try:
            start_time = float(instruct_pix2pix_prefs['start_time'])
            end_time = float(instruct_pix2pix_prefs['end_time'])
            fps = int(instruct_pix2pix_prefs['fps'])
        except Exception:
            alert_msg(page, "Make sure your Numbers are actual numbers...")
            return
        if init_vid.startswith('http'):
            init_vid = download_file(init_vid, output_dir)
        else:
            if not os.path.isfile(init_vid):
              alert_msg(page, f"ERROR: Couldn't find your init_video {init_vid}")
              return
        prt("Extracting Frames from Video Clip")
        try:
            cap = cv2.VideoCapture(init_vid)
        except Exception as e:
            alert_msg(page, "ERROR Reading Video File. May be Incompatible Format...")
            clear_last()
            return
        count = 0
        video = []
        frames = []
        width = height = 0
        cap.set(cv2.CAP_PROP_FPS, fps)
        video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)
        start_frame = int(start_time * fps)
        if end_time == 0 or end_time == 0.0:
            end_frame = int(video_length)
        else:
            end_frame = int(end_time * fps)
        total = end_frame - start_frame
        for i in range(start_frame, end_frame):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            success, image = cap.read()
            if success:
                #filename = os.path.join(output_dir, f'{file_prefix}{count}.png')
                if width == 0:
                    shape = image.shape
                    width, height = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)
                image = cv2.resize(image, (width, height), interpolation = cv2.INTER_AREA)
                #cv2.imwrite(os.path.join(output_dir, filename), image)
                video.append(PILImage.fromarray(image))
                count += 1
        cap.release()
        clear_last()
        #reader = imageio.get_reader(instruct_pix2pix_prefs['init_video'], "ffmpeg")
        #frame_count = instruct_pix2pix_prefs['fps'] #TODO: This isn't frame count, do it right
        #video = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]
    else: video = None
    for pr in instruct_pix2pix_prompts:
      if not instruct_pix2pix_prefs['use_init_video']:
        if pr['original_image'].startswith('http'):
          #response = requests.get(instruct_pix2pix_prefs['original_image'])
          #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
          original_img = PILImage.open(requests.get(pr['original_image'], stream=True).raw)
        else:
          if os.path.isfile(pr['original_image']):
            original_img = PILImage.open(pr['original_image'])
          else:
            alert_msg(page, f"ERROR: Couldn't find your original_image {pr['original_image']}")
            return
        width, height = original_img.size
        width, height = scale_dimensions(width, height, instruct_pix2pix_prefs['max_size'])
        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
      for num in range(instruct_pix2pix_prefs['num_images']):
        prt(progress)
        random_seed = get_seed(int(pr['seed']) + num)
        generator = torch.Generator(device=torch_device).manual_seed(random_seed)
        #generator = torch.manual_seed(random_seed)
        try:
          if instruct_pix2pix_prefs['use_init_video']:
            from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor
            pipe_instruct_pix2pix.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=3))
            images = pipe_instruct_pix2pix([pr['prompt']] * len(video), image=video, negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None, num_inference_steps=instruct_pix2pix_prefs['num_inference_steps'], eta=instruct_pix2pix_prefs['eta'], image_guidance_scale=instruct_pix2pix_prefs['guidance_scale'], num_images_per_prompt=instruct_pix2pix_prefs['num_images'], generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_arg).images
          else:
            images = pipe_instruct_pix2pix(pr['prompt'], image=original_img, negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None, num_inference_steps=instruct_pix2pix_prefs['num_inference_steps'], eta=instruct_pix2pix_prefs['eta'], image_guidance_scale=instruct_pix2pix_prefs['guidance_scale'], num_images_per_prompt=instruct_pix2pix_prefs['num_images'], generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_arg).images
        except Exception as e:
          clear_last()
          alert_msg(page, f"ERROR: Couldn't run Instruct-Pix2Pix on your image for some reason.  Possibly out of memory or something wrong with my code...", content=Text(str(e)))
          gc.collect()
          torch.cuda.empty_cache()
          return
        clear_last()
        clear_last()
        filename = pr['original_image'].rpartition(slash)[2].rpartition('.')[0]
        filename = f"-{format_filename(pr['prompt'])}"
        filename = filename[:int(prefs['file_max_length'])]
        #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
        #num = 0
        for image in images:
            random_seed += num
            fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
            image_path = available_file(os.path.join(stable_dir, instruct_pix2pix_prefs['batch_folder_name']), fname, num)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            image.save(image_path)
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)
            if not instruct_pix2pix_prefs['display_upscaled_image'] or not instruct_pix2pix_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if instruct_pix2pix_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=instruct_pix2pix_prefs["enlarge_scale"])
                image_path = upscaled_path
                if instruct_pix2pix_prefs['display_upscaled_image']:
                    time.sleep(0.6)
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(instruct_pix2pix_prefs["enlarge_scale"]), height=height * float(instruct_pix2pix_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if prefs['save_image_metadata']:
                img = PILImage.open(image_path)
                metadata = PngInfo()
                metadata.add_text("artist", prefs['meta_ArtistName'])
                metadata.add_text("copyright", prefs['meta_Copyright'])
                metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {instruct_pix2pix_prefs['enlarge_scale']}x with ESRGAN" if instruct_pix2pix_prefs['apply_ESRGAN_upscale'] else "")
                metadata.add_text("pipeline", f"Instruct-Pix2Pix{' SDXL' if instruct_pix2pix_prefs['use_SDXL'] else ''}")
                if prefs['save_config_in_metadata']:
                  config_json = instruct_pix2pix_prefs.copy()
                  config_json['model_path'] = model_id_SDXL if instruct_pix2pix_prefs['use_SDXL'] else model_id
                  config_json['seed'] = random_seed
                  config_json['prompt'] = pr['prompt']
                  config_json['negative_prompt'] = pr['negative_prompt']
                  del config_json['num_images']
                  del config_json['max_size']
                  del config_json['display_upscaled_image']
                  del config_json['batch_folder_name']
                  if not config_json['apply_ESRGAN_upscale']:
                    del config_json['enlarge_scale']
                    del config_json['apply_ESRGAN_upscale']
                  metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                img.save(image_path, pnginfo=metadata)
            #TODO: PyDrive
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], instruct_pix2pix_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], instruct_pix2pix_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            time.sleep(0.2)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
            #num += 1
    play_snd(Snd.ALERT, page)


def run_ledits(page, from_list=False):
    global ledits_prefs, prefs, status, pipe_ledits
    if not check_diffusers(page): return
    if not bool(ledits_prefs['original_image']):
      alert_msg(page, "You must provide the Original Image and the Mask Image to process...")
      return
    if not bool(ledits_prefs['prompt']):
      alert_msg(page, "You must provide a Instructional Image Editing Prompt...")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.LEdits.controls.append(line)
        if update:
          page.LEdits.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.LEdits, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
      else:
        page.LEdits.auto_scroll = scroll
        page.LEdits.update()
    progress = ProgressBar(bar_height=8)
    def callback_fnc(pipe, step, timestep, callback_kwargs):#(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress
      total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.LEdits.controls = page.LEdits.controls[:1]
    ledits_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        instruct = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'original_image': p['init_image'] if bool(p['init_image']) else ledits_prefs['original_image'], 'seed': p['seed']}
        ledits_prompts.append(instruct)
      page.tabs.selected_index = 4
      page.tabs.update()
    else:
      if not bool(ledits_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      instruct = {'prompt':ledits_prefs['prompt'], 'negative_prompt': ledits_prefs['negative_prompt'], 'original_image': ledits_prefs['original_image'], 'seed': ledits_prefs['seed']}
      ledits_prompts.append(instruct)
    autoscroll(True)
    clear_list()
    prt(Divider(thickness=2, height=4))
    installer = Installing(f"Installing LEdits++{' SDXL' if ledits_prefs['use_SDXL'] else ''} Pipeline... See console for progress.")
    prt(installer)
    import requests, random
    from io import BytesIO
    from PIL import ImageOps
    model_id = get_model(prefs['model_ckpt'])['path']
    SDXL_model = get_SDXL_model(prefs['SDXL_model'])
    model_id_SDXL = SDXL_model['path']
    if 'loaded_ledits' not in status:
      status['loaded_ledits'] = ''
    if status['loaded_ledits'] != (model_id_SDXL if ledits_prefs['use_SDXL'] else model_id):
      clear_pipes()
    else:
      clear_pipes('ledits')
    if pipe_ledits is None:
      if ledits_prefs['use_SDXL']:
        from diffusers import LEditsPPPipelineStableDiffusionXL
        pipe_ledits = LEditsPPPipelineStableDiffusionXL.from_pretrained(model_id_SDXL, torch_dtype=torch.float16, add_watermarker=prefs['SDXL_watermark'], cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
        pipe_ledits = optimize_SDXL(pipe_ledits, freeu=False, vae_tiling=False)
        status['loaded_ledits'] = model_id_SDXL
      else:
        from diffusers import LEditsPPPipelineStableDiffusion
        pipe_ledits = LEditsPPPipelineStableDiffusion.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
        pipe_ledits = optimize_pipe(pipe_ledits, freeu=False)
        #pipe_ledits = pipe_ledits.to(torch_device)
        status['loaded_ledits'] = model_id
      pipe_ledits.set_progress_bar_config(disable=True)
    #pipeline_scheduler(pipe_ledits)
    ip_adapter_arg = {}
    if ledits_prefs['use_ip_adapter']:
      installer.status(f"...initialize IP-Adapter")
      ip_adapter_img = None
      if ledits_prefs['ip_adapter_image'].startswith('http'):
        i_response = requests.get(ledits_prefs['ip_adapter_image'])
        ip_adapter_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
      else:
        if os.path.isfile(ledits_prefs['ip_adapter_image']):
          ip_adapter_img = PILImage.open(ledits_prefs['ip_adapter_image'])
        else:
          clear_last()
          prt(f"ERROR: Couldn't find your ip_adapter_image {ledits_prefs['ip_adapter_image']}")
      if bool(ip_adapter_img):
        ip_adapter_arg['ip_adapter_image'] = ip_adapter_img
      if bool(ip_adapter_arg):
          if ledits_prefs['use_SDXL']:
            ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == ledits_prefs['ip_adapter_SDXL_model'])
          else:
            ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == ledits_prefs['ip_adapter_model'])
          pipe_ledits.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
          pipe_ledits.set_ip_adapter_scale(ledits_prefs['ip_adapter_strength'])
    clear_last()
    prt("Generating LEdits++ of your Image...")
    prt(progress)
    max_size = ledits_prefs['max_size']
    batch_output = os.path.join(stable_dir, ledits_prefs['batch_folder_name'])
    output_dir = batch_output
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], ledits_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    
    for pr in ledits_prompts:
      if pr['original_image'].startswith('http'):
          #response = requests.get(ledits_prefs['original_image'])
          #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
          original_img = PILImage.open(requests.get(pr['original_image'], stream=True).raw)
      else:
          if os.path.isfile(pr['original_image']):
              original_img = PILImage.open(pr['original_image'])
          else:
              alert_msg(page, f"ERROR: Couldn't find your original_image {pr['original_image']}")
              return
      width, height = original_img.size
      width, height = scale_dimensions(width, height, ledits_prefs['max_size'])
      original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
      editing_prompt = [ledits_prefs['source_prompt'], pr['prompt']]
      reverse_editing_direction=[True, False]
      edit_guidance_scale=[ledits_prefs['source_guidance_scale'], ledits_prefs['guidance_scale']]
      edit_threshold=[0.9, ledits_prefs['edit_threshold']] #'width':width, 'height':height
      invert_args = {'negative_prompt':pr['negative_prompt'] if bool(pr['negative_prompt']) else None} if ledits_prefs['use_SDXL'] else {}
      pipe_args = {'target_size':(width, height)} if ledits_prefs['use_SDXL'] else {}
      for num in range(ledits_prefs['num_images']):
          prt(progress)
          random_seed = get_seed(int(pr['seed']) + num)
          generator = torch.Generator(device=torch_device).manual_seed(random_seed)
          #generator = torch.manual_seed(random_seed)
          try:
              _ = pipe_ledits.invert(image=original_img, num_inversion_steps=int(ledits_prefs['num_inference_steps']), skip=float(ledits_prefs['skip']), **invert_args)
              images = pipe_ledits(editing_prompt=editing_prompt, reverse_editing_direction=reverse_editing_direction, edit_guidance_scale=edit_guidance_scale, edit_threshold=edit_threshold, negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None, num_images_per_prompt=ledits_prefs['num_images'], generator=generator, callback_on_step_end=callback_fnc, **pipe_args, **ip_adapter_arg).images
          except Exception as e:
              clear_last()
              alert_msg(page, f"ERROR: Couldn't run LEdits++ on your image for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]), debug_pref=ledits_prefs)
              flush()
              return
          clear_last()
          clear_last()
          filename = pr['original_image'].rpartition(slash)[2].rpartition('.')[0]
          filename = f"-{format_filename(pr['prompt'])}"
          filename = filename[:int(prefs['file_max_length'])]
          #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
          #num = 0
          for image in images:
              random_seed += num
              fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
              image_path = available_file(os.path.join(stable_dir, ledits_prefs['batch_folder_name']), fname, num)
              unscaled_path = image_path
              output_file = image_path.rpartition(slash)[2]
              image.save(image_path)
              out_path = os.path.dirname(image_path)
              upscaled_path = os.path.join(out_path, output_file)
              if not ledits_prefs['display_upscaled_image'] or not ledits_prefs['apply_ESRGAN_upscale']:
                  save_metadata(unscaled_path, pipe_ledits, f"LEdits++{' SDXL' if ledits_prefs['use_SDXL'] else ''}", model_id_SDXL if ledits_prefs['use_SDXL'] else model_id, random_seed, extra=pr)
                  prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                  #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
              if ledits_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                  upscale_image(image_path, upscaled_path, scale=ledits_prefs["enlarge_scale"])
                  image_path = upscaled_path
                  save_metadata(unscaled_path, pipe_ledits, f"LEdits++{' SDXL' if ledits_prefs['use_SDXL'] else ''}", model_id_SDXL if ledits_prefs['use_SDXL'] else model_id, random_seed, extra=pr)
                  if ledits_prefs['display_upscaled_image']:
                      prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(ledits_prefs["enlarge_scale"]), height=height * float(ledits_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                      #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
              if storage_type == "Colab Google Drive":
                  new_file = available_file(os.path.join(prefs['image_output'], ledits_prefs['batch_folder_name']), fname, num)
                  out_path = new_file
                  shutil.copy(image_path, new_file)
              elif bool(prefs['image_output']):
                  new_file = available_file(os.path.join(prefs['image_output'], ledits_prefs['batch_folder_name']), fname, num)
                  out_path = new_file
                  shutil.copy(image_path, new_file)
              prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
              #num += 1
    play_snd(Snd.ALERT, page)


def run_controlnet(page, from_list=False):
    global controlnet_prefs, prefs, status, pipe_controlnet, controlnet, controlnet_models
    if not check_diffusers(page): return
    if not bool(controlnet_prefs['original_image']) and len(controlnet_prefs['multi_controlnets']) == 0:
      alert_msg(page, "You must provide the Original Image to process...")
      return
    if not bool(controlnet_prefs['prompt']) and not from_list:
      alert_msg(page, "You must provide a Prompt to paint in your image...")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.ControlNet.controls.append(line)
        #page.controlnet_output.controls.append(line)
        if update:
          page.ControlNet.update()
          #page.controlnet_output.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.ControlNet, lines=lines)
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.ControlNet.controls = page.ControlNet.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
      else:
        page.ControlNet.auto_scroll = scroll
        page.ControlNet.update()
    progress = ProgressBar(bar_height=8)
    total_steps = controlnet_prefs['steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    controlnet_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        control = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'] if bool(p['negative_prompt']) else controlnet_prefs['negative_prompt'], 'original_image': p['init_image'] if bool(p['init_image']) else controlnet_prefs['original_image'], 'conditioning_scale': controlnet_prefs['conditioning_scale'], 'control_guidance_start': controlnet_prefs['control_guidance_start'], 'control_guidance_end': controlnet_prefs['control_guidance_end'], 'seed': p['seed']}
        controlnet_prompts.append(control)
      page.tabs.selected_index = 4
      page.tabs.update()
      #page.controlnet_output.controls.clear()
    else:
      if not bool(controlnet_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      original = controlnet_prefs['original_image']
      conditioning_scale = controlnet_prefs['conditioning_scale']
      control_guidance_start = controlnet_prefs['control_guidance_start']
      control_guidance_end = controlnet_prefs['control_guidance_end']
      if len(controlnet_prefs['multi_controlnets']) > 0:
        original = []
        conditioning_scale = []
        control_guidance_start = []
        control_guidance_end = []
        for c in controlnet_prefs['multi_controlnets']:
          original.append(c['original_image'])
          conditioning_scale.append(c['conditioning_scale'])
          control_guidance_start.append(c['control_guidance_start'])
          control_guidance_end.append(c['control_guidance_end'])
      control = {'prompt':controlnet_prefs['prompt'], 'negative_prompt': controlnet_prefs['negative_prompt'], 'original_image': original, 'conditioning_scale': conditioning_scale, 'control_guidance_start':control_guidance_start, 'control_guidance_end': control_guidance_end, 'seed': controlnet_prefs['seed']}
      if controlnet_prefs['use_init_video']:
        control['init_video'] = controlnet_prefs['init_video']
        control['start_time'] = controlnet_prefs['start_time']
        control['end_time'] = controlnet_prefs['end_time']
        control['fps'] = controlnet_prefs['fps']
      controlnet_prompts.append(control)
      #page.controlnet_output.controls.clear()
    autoscroll(True)
    clear_list()
    prt(Divider(thickness=2, height=4))
    installer = Installing("Installing ControlNet Packages...")
    prt(installer)
    if status['loaded_controlnet'] == controlnet_prefs["control_task"]:
        clear_pipes('controlnet')
    else:
        clear_pipes()
    import requests
    from io import BytesIO
    from PIL import ImageOps
    from PIL.PngImagePlugin import PngInfo
    try:
        try:
          from controlnet_aux import MLSDdetector
        except ModuleNotFoundError:
          installer.status("...installing controlnet-aux")
          run_sp("pip install --upgrade controlnet-aux", realtime=False)
          #run_sp("pip install git+https://github.com/patrickvonplaten/controlnet_aux.git")
          pass
        from controlnet_aux import MLSDdetector
        from controlnet_aux import OpenposeDetector
        from diffusers import StableDiffusionControlNetPipeline, StableDiffusionXLControlNetPipeline, ControlNetModel
        #run_sp("pip install scikit-image", realtime=False)
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR Installing Required Packages...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        flush()
        return
    canny_checkpoint = "lllyasviel/control_v11p_sd15_canny"
    scribble_checkpoint = "lllyasviel/control_v11p_sd15_scribble"
    openpose_checkpoint = "lllyasviel/control_v11p_sd15_openpose"
    depth_checkpoint = "lllyasviel/control_v11p_sd15_depth"
    HED_checkpoint = "lllyasviel/control_v11p_sd15_softedge"
    mlsd_checkpoint = "lllyasviel/control_v11p_sd15_mlsd"
    normal_checkpoint = "lllyasviel/control_v11p_sd15_normalbae"
    seg_checkpoint = "lllyasviel/control_v11p_sd15_seg"
    lineart_checkpoint = "lllyasviel/control_v11p_sd15_lineart"
    ip2p_checkpoint = "lllyasviel/control_v11e_sd15_ip2p"
    shuffle_checkpoint = "lllyasviel/control_v11e_sd15_shuffle"
    tile_checkpoint = "lllyasviel/control_v11f1e_sd15_tile"
    brightness_checkpoint = "ioclab/control_v1p_sd15_brightness"
    mediapipe_face_checkpoint = "CrucibleAI/ControlNetMediaPipeFace"
    hed = None
    openpose = None
    depth_estimator = None
    mlsd = None
    image_processor = None
    image_segmentor = None
    normal = None
    lineart = None
    shuffle = None
    face_detector = None
    def get_controlnet(task):
        nonlocal hed, openpose, depth_estimator, mlsd, image_processor, image_segmentor, normal, lineart, shuffle, face_detector
        if controlnet_models[task] != None:
            return controlnet_models[task]
        if task == "Canny Map Edge" or task == "Video Canny Edge":
            task = "Canny Map Edge"
            controlnet_models[task] = ControlNetModel.from_pretrained(canny_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Scribble":
            from controlnet_aux import HEDdetector
            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')
            controlnet_models[task] = ControlNetModel.from_pretrained(scribble_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "OpenPose" or task == "Video OpenPose":
            task = "OpenPose"
            from controlnet_aux import OpenposeDetector
            openpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')
            controlnet_models[task] = ControlNetModel.from_pretrained(openpose_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Marigold Depth":
            import diffusers
            depth_estimator = diffusers.MarigoldDepthPipeline.from_pretrained("prs-eth/marigold-depth-lcm-v1-0", torch_dtype=torch.float16, variant="fp16").to("cuda")
            controlnet_models[task] = ControlNetModel.from_pretrained(depth_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Depth":
            from transformers import pipeline
            depth_estimator = pipeline('depth-estimation')
            controlnet_models[task] = ControlNetModel.from_pretrained(depth_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Kandinsky Depth":
            from transformers import pipeline
            from diffusers import KandinskyV22PriorPipeline, KandinskyV22ControlnetPipeline
            depth_estimator = pipeline('depth-estimation')
            #controlnet_models[task] = ControlNetModel.from_pretrained(depth_checkpoint, torch_dtype=torch.float16).to(torch_device)
            controlnet_models[task] = KandinskyV22ControlnetPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-controlnet-depth", torch_dtype=torch.float16).to(torch_device)
        elif task == "HED":
            from controlnet_aux import HEDdetector
            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')
            #pidi_net = PidiNetDetector.from_pretrained('lllyasviel/Annotators')
            controlnet_models[task] = ControlNetModel.from_pretrained(HED_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "M-LSD":
            from controlnet_aux import MLSDdetector
            mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')
            controlnet_models[task] = ControlNetModel.from_pretrained(mlsd_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Normal Map":
            #from transformers import pipeline
            #depth_estimator = pipeline("depth-estimation", model ="Intel/dpt-hybrid-midas")
            from controlnet_aux import NormalBaeDetector
            normal = NormalBaeDetector.from_pretrained("lllyasviel/Annotators")
            controlnet_models[task] = ControlNetModel.from_pretrained(normal_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Segmented":
            from transformers import AutoImageProcessor, UperNetForSemanticSegmentation
            from controlnet_utils import ade_palette
            image_processor = AutoImageProcessor.from_pretrained("openmmlab/upernet-convnext-small")
            image_segmentor = UperNetForSemanticSegmentation.from_pretrained("openmmlab/upernet-convnext-small")
            controlnet_models[task] = ControlNetModel.from_pretrained(seg_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "LineArt":
            from controlnet_aux import LineartDetector
            lineart = LineartDetector.from_pretrained("lllyasviel/Annotators")
            controlnet_models[task] = ControlNetModel.from_pretrained(lineart_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Shuffle":
            from controlnet_aux import ContentShuffleDetector
            shuffle = ContentShuffleDetector()
            controlnet_models[task] = ControlNetModel.from_pretrained(shuffle_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Tile":
            controlnet_models[task] = ControlNetModel.from_pretrained(tile_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Brightness":
            controlnet_models[task] = ControlNetModel.from_pretrained(brightness_checkpoint, torch_dtype=torch.float16, use_safetensors=True)
        elif task == "Mediapipe Face":
            from controlnet_aux import MediapipeFaceDetector
            face_detector = MediapipeFaceDetector()
            controlnet_models[task] = ControlNetModel.from_pretrained(mediapipe_face_checkpoint, subfolder="diffusion_sd15", torch_dtype=torch.float16, use_safetensors=True)
        elif task == "QR Code Monster":
            controlnet_models[task] = ControlNetModel.from_pretrained('monster-labs/control_v1p_sd15_qrcode_monster', subfolder='v2', torch_dtype=torch.float16, use_safetensors=True)
        elif task == "Instruct Pix2Pix":
            controlnet_models[task] = ControlNetModel.from_pretrained(ip2p_checkpoint, torch_dtype=torch.float16).to(torch_device)

        return controlnet_models[task]
    width, height = 0, 0
    def resize_for_condition_image(input_image: PILImage, resolution: int):
        input_image = input_image.convert("RGB")
        W, H = input_image.size
        k = float(resolution) / min(H, W)
        H *= k
        W *= k
        H = int(round(H / 64.0)) * 64
        W = int(round(W / 64.0)) * 64
        img = input_image.resize((W, H), resample=PILImage.Resampling.LANCZOS)
        return img
    def prep_image(task, img):
        nonlocal hed, openpose, depth_estimator, mlsd, image_processor, image_segmentor, normal, lineart, shuffle, face_detector
        nonlocal width, height
        if isinstance(img, str):
          if img.startswith('http'):
              #response = requests.get(controlnet_prefs['original_image'])
              #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
              original_img = PILImage.open(requests.get(img, stream=True).raw)
          else:
              if os.path.isfile(img):
                  original_img = PILImage.open(img)
              else:
                  alert_msg(page, f"ERROR: Couldn't find your original_image {img}")
                  return
        width, height = original_img.size
        width, height = scale_dimensions(width, height, controlnet_prefs['max_size'])
        #print(f"Size: {width}x{height}")
        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        #return original_img
        try:
            if task == "Canny Map Edge" or task == "Video Canny Edge":
                input_image = np.array(original_img)
                input_image = cv2.Canny(input_image, controlnet_prefs['low_threshold'], controlnet_prefs['high_threshold'])
                input_image = input_image[:, :, None]
                input_image = np.concatenate([input_image, input_image, input_image], axis=2)
                original_img = PILImage.fromarray(input_image)
            elif task == "Scribble":
                original_img = hed(original_img, scribble=True)
            elif task == "OpenPose" or task == "Video OpenPose":
                original_img = openpose(original_img, hand_and_face=True)
            elif task == "Depth":
                original_img = depth_estimator(original_img)['depth']
                input_image = np.array(original_img)
                input_image = input_image[:, :, None]
                input_image = np.concatenate([input_image, input_image, input_image], axis=2)
                original_img = PILImage.fromarray(input_image)
            elif task == "Marigold Depth":
                random_seed = get_seed(controlnet_prefs['seed'])
                generator = torch.Generator(device=torch_device).manual_seed(random_seed)
                input_image = depth_estimator(original_img, generator=generator).prediction
                input_image = depth_estimator.image_processor.visualize_depth(input_image, color_map="binary")
                original_img = input_image[0]
            elif task == "Kandinsky Depth":
                original_img = depth_estimator(original_img)['depth']
                input_image = np.array(original_img)
                input_image = input_image[:, :, None]
                input_image = np.concatenate([input_image, input_image, input_image], axis=2)
                detected_map = torch.from_numpy(input_image).float() / 255.0
                original_img = detected_map.permute(2, 0, 1).unsqueeze(0).half().to("cuda")
                #original_img = PILImage.fromarray(input_image)
            elif task == "HED":
                original_img = hed(original_img, safe=True)
            elif task == "M-LSD":
                original_img = mlsd(original_img)
            elif task == "Normal Map":
                #depth_estimator = pipeline("depth-estimation", model="Intel/dpt-hybrid-midas" )
                '''original_img = depth_estimator(original_img)['predicted_depth'][0]
                input_image = original_img.numpy()
                image_depth = input_image.copy()
                image_depth -= np.min(image_depth)
                image_depth /= np.max(image_depth)
                bg_threhold = 0.4
                x = cv2.Sobel(input_image, cv2.CV_32F, 1, 0, ksize=3)
                x[image_depth < bg_threhold] = 0
                y = cv2.Sobel(input_image, cv2.CV_32F, 0, 1, ksize=3)
                y[image_depth < bg_threhold] = 0
                z = np.ones_like(x) * np.pi * 2.0
                input_image = np.stack([x, y, z], axis=2)
                input_image /= np.sum(input_image ** 2.0, axis=2, keepdims=True) ** 0.5
                input_image = (input_image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)
                original_img = PILImage.fromarray(input_image)'''
                original_img = normal(original_img)
            elif task == "Segmented":
                from controlnet_utils import ade_palette
                pixel_values = image_processor(original_img, return_tensors="pt").pixel_values
                with torch.no_grad():
                  outputs = image_segmentor(pixel_values)
                seg = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[original_img.size[::-1]])[0]
                color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3
                palette = np.array(ade_palette())
                for label, color in enumerate(palette):
                    color_seg[seg == label, :] = color
                color_seg = color_seg.astype(np.uint8)
                original_img = PILImage.fromarray(color_seg)
            elif task == "LineArt":
                original_img = lineart(original_img)
            elif task == "Shuffle":
                original_img = shuffle(original_img)
            elif task == "Tile":
                original_img = resize_for_condition_image(original_img, 1024)
            elif task == "Brightness":
                original_img = PILImage.fromarray(original_img).convert('L')
            elif task == "Mediapipe Face":
                original_img = face_detector(original_img, scribble=True)
                '''pip_install("mediapipe")
                import mediapipe as mp
                mp_drawing = mp.solutions.drawing_utils
                mp_drawing_styles = mp.solutions.drawing_styles
                mp_face_mesh = mp.solutions.face_mesh
                input_image = np.array(original_img)
                with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=5, refine_landmarks=True, min_detection_confidence=0.5) as face_mesh:
                    results = face_mesh.process(input_image)
                res = input_image[:, :, ::-1].copy()
                show_tesselation = show_contours = show_irises = True
                if results.multi_face_landmarks is not None:
                    for face_landmarks in results.multi_face_landmarks:
                        if show_tesselation:
                            mp_drawing.draw_landmarks(image=res, landmark_list=face_landmarks, connections=mp_face_mesh.FACEMESH_TESSELATION, landmark_drawing_spec=None, connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())
                        if show_contours:
                            mp_drawing.draw_landmarks(image=res, landmark_list=face_landmarks, connections=mp_face_mesh.FACEMESH_CONTOURS, landmark_drawing_spec=None, connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())
                        if show_irises:
                            mp_drawing.draw_landmarks(image=res, landmark_list=face_landmarks, connections=mp_face_mesh.FACEMESH_IRISES, landmark_drawing_spec=None, connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style())
                original_img = PILImage.fromarray(res[:, :, ::-1])'''
            return original_img
        except Exception as e:
            #clear_last()
            clear_last()
            alert_msg(page, f"ERROR Preparing ControlNet {controlnet_prefs['control_task']} Input Image...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            flush()
            return
    def prep_video(vid):
        nonlocal width, height
        if vid.startswith('http'):
            init_vid = download_file(vid, stable_dir)
        else:
            if os.path.isfile(vid):
                init_vid = vid
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_video {vid}")
                return
        try:
            start_time = float(controlnet_prefs['start_time'])
            end_time = float(controlnet_prefs['end_time'])
            fps = int(controlnet_prefs['fps'])
            max_size = controlnet_prefs['max_size']
        except Exception:
            alert_msg(page, "Make sure your Numbers are actual numbers...")
            return
        prt("Extracting Frames from Video Clip")
        try:
            cap = cv2.VideoCapture(init_vid)
        except Exception as e:
            alert_msg(page, "ERROR Reading Video File. May be Incompatible Format...")
            clear_last()
            return
        count = 0
        video = []
        frames = []
        width = height = 0
        cap.set(cv2.CAP_PROP_FPS, fps)
        video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)
        start_frame = int(start_time * fps)
        if end_time == 0 or end_time == 0.0:
            end_frame = int(video_length)
        else:
            end_frame = int(end_time * fps)
        total = end_frame - start_frame
        for i in range(start_frame, end_frame):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            success, image = cap.read()
            if success:
                #filename = os.path.join(output_dir, f'{file_prefix}{count}.png')
                if width == 0:
                    shape = image.shape
                    width, height = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)
                image = cv2.resize(image, (width, height), interpolation = cv2.INTER_AREA)
                #cv2.imwrite(os.path.join(output_dir, filename), image)
                image = prep_image(controlnet_prefs['control_task'], PILImage.fromarray(image))
                video.append(image)
                count += 1
        cap.release()
        clear_last()
        return video
    loaded_controlnet = None
    if len(controlnet_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_prefs['use_init_video']:
        controlnet = []
        loaded_controlnet = []
        for c in controlnet_prefs['multi_controlnets']:
            controlnet.append(get_controlnet(c['control_task']))
            loaded_controlnet.append(c['control_task'])
    else:
        controlnet = get_controlnet(controlnet_prefs['control_task'])
        loaded_controlnet = controlnet_prefs['control_task']
    for k, v in controlnet_models.items():
      if v != None and k in loaded_controlnet:
        del v
        controlnet_models[k] = None
    controlnet_type = "text2image"
    if controlnet_prefs['use_image2image']:
        if bool(controlnet_prefs['init_image']):
            if bool(controlnet_prefs['mask_image']) or controlnet_prefs['alpha_mask']:
                controlnet_type = "inpaint"
            else:
                controlnet_type = "image2image"
    use_ip_adapter = controlnet_prefs['use_ip_adapter']
    if use_ip_adapter:
        ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == controlnet_prefs['ip_adapter_model'])
    else:
        ip_adapter_model = None
    if controlnet_type != status['loaded_controlnet_type']:
        clear_pipes()
    model = get_model(prefs['model_ckpt'])
    model_path = model['path']
    if pipe_controlnet == None or status['loaded_controlnet'] != controlnet_prefs["control_task"]:
        #if controlnet_prefs["use_SDXL"]:
        #TODO: pipe_controlnet = StableDiffusionXLControlNetPipeline
        if controlnet_type == "text2image":
            pipe_controlnet = StableDiffusionControlNetPipeline.from_pretrained(model_path, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        elif controlnet_type == "image2image":
            from diffusers import StableDiffusionControlNetImg2ImgPipeline
            pipe_controlnet = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(model_path, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        elif controlnet_type == "inpaint":
            from diffusers import StableDiffusionControlNetInpaintPipeline
            pipe_controlnet = StableDiffusionControlNetInpaintPipeline.from_pretrained(model_path, controlnet=controlnet, safety_checker=None, variant="fp16", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        #pipe_controlnet.enable_model_cpu_offload()
        pipe_controlnet = optimize_pipe(pipe_controlnet, vae_slicing=True, vae_tiling=True)
        status['loaded_controlnet'] = loaded_controlnet #controlnet_prefs["control_task"]
        status['loaded_controlnet_type'] = controlnet_type
    #else:
        #pipe_controlnet.controlnet=controlnet
    pipe_controlnet = pipeline_scheduler(pipe_controlnet)
    if controlnet_prefs['use_init_video']:
        from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor
        pipe_controlnet.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))
        pipe_controlnet.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))
    init_img = None
    mask_img = None
    if bool(controlnet_prefs['init_image'] and controlnet_prefs['use_image2image']):
        if controlnet_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(controlnet_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(controlnet_prefs['init_image']):
                init_img = PILImage.open(controlnet_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {controlnet_prefs['init_image']}")
                return
        width, height = init_img.size
        width, height = scale_dimensions(width, height, controlnet_prefs['max_size'])
        if bool(controlnet_prefs['alpha_mask']):
            init_img = init_img.convert("RGBA")
        else:
            init_img = init_img.convert("RGB")
        init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        if not bool(controlnet_prefs['mask_image']) and bool(controlnet_prefs['alpha_mask']):
            mask_img = init_img.convert('RGBA')
            red, green, blue, alpha = PILImage.Image.split(init_img)
            mask_img = alpha.convert('L')
        elif bool(controlnet_prefs['mask_image']):
            if controlnet_prefs['mask_image'].startswith('http'):
                mask_img = PILImage.open(requests.get(controlnet_prefs['mask_image'], stream=True).raw)
            else:
                if os.path.isfile(controlnet_prefs['mask_image']):
                    mask_img = PILImage.open(controlnet_prefs['mask_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your mask_image {controlnet_prefs['mask_image']}")
                    return
            width, height = mask_img.size
            width, height = scale_dimensions(width, height, controlnet_prefs['max_size'])
            mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        if controlnet_prefs['invert_mask'] and not controlnet_prefs['alpha_mask']:
            from PIL import ImageOps
            mask_img = ImageOps.invert(mask_img.convert('RGB'))
    if use_ip_adapter:
        pipe_controlnet.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
        pipe_controlnet.set_ip_adapter_scale(controlnet_prefs['ip_adapter_strength'])
        if controlnet_prefs['ip_adapter_image'].startswith('http'):
            ip_adapter_image = PILImage.open(requests.get(controlnet_prefs['ip_adapter_image'], stream=True).raw)
        else:
            if os.path.isfile(controlnet_prefs['ip_adapter_image']):
                ip_adapter_image = PILImage.open(controlnet_prefs['ip_adapter_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your ip_adapter_image {controlnet_prefs['ip_adapter_image']}")
                return
        ip_adapter_image = ImageOps.exif_transpose(ip_adapter_image).convert("RGB")
        status['loaded_ip_adapter'] = ip_adapter_model
        ip_adapter_args = {'ip_adapter_image': ip_adapter_image}
    else:
        ip_adapter_args = {}
    clear_last()
    prt(f"Generating ControlNet {controlnet_prefs['control_task']} of your Image...")
    batch_output = os.path.join(stable_dir, controlnet_prefs['batch_folder_name'])
    makedir(batch_output)
    batch_output = os.path.join(prefs['image_output'], controlnet_prefs['batch_folder_name'])
    makedir(batch_output)
    for pr in controlnet_prompts:
        prt(progress)
        autoscroll(False)
        filename = f"{controlnet_prefs['file_prefix']}{format_filename(pr['prompt'])}"
        filename = filename[:int(prefs['file_max_length'])]
        if len(controlnet_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_prefs['use_init_video']:
            original_img = []
            for c in controlnet_prefs['multi_controlnets']:
                original_img.append(prep_image(c['control_task'], c['original_image']))
                if controlnet_prefs['show_processed_image']:
                    processed_img = available_file(batch_output, f"{filename}-{c['control_task'].partition(' ')[0]}", 0, no_num=True)
                    w, h = original_img[-1].size
                    original_img[-1].save(processed_img)
                    prt(Row([ImageButton(src=processed_img, data=processed_img, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))
        elif not controlnet_prefs['use_init_video']:
            original_img = prep_image(controlnet_prefs['control_task'], pr['original_image'])
            if controlnet_prefs['show_processed_image']:
                processed_img = available_file(batch_output, f"{filename}-{controlnet_prefs['control_task'].partition(' ')[0]}", 0, no_num=True)
                w, h = original_img.size
                original_img.save(processed_img)
                prt(Row([ImageButton(src=processed_img, data=processed_img, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))
        else:
            video_img = prep_video(pr['original_image'])
            latents = torch.randn((1, 4, 64, 64), device="cuda", dtype=torch.float16).repeat(len(video_img), 1, 1, 1)
        try:
            random_seed = get_seed(pr['seed'])
            generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            if controlnet_type == "text2image":
                if not controlnet_prefs['use_init_video']:
                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], num_images_per_prompt=controlnet_prefs['batch_size'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images
                else:
                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images
            elif controlnet_type == "image2image":
                if not controlnet_prefs['use_init_video']:
                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=init_img, control_image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], num_images_per_prompt=controlnet_prefs['batch_size'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images
                else:
                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=init_img, control_image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images
            elif controlnet_type == "inpaint":
                if not controlnet_prefs['use_init_video']:
                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=init_img, mask=mask_img, control_image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], num_images_per_prompt=controlnet_prefs['batch_size'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images
                else:
                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=init_img, mask=mask_img, control_image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images
            '''
            if not controlnet_prefs['use_init_video']:
                images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], num_images_per_prompt=controlnet_prefs['batch_size'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images
            else:
                images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images'''
        except Exception as e:
            #clear_last()
            clear_last()
            alert_msg(page, f"ERROR Generating ControlNet {controlnet_prefs['control_task']}...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            flush()
            return
        clear_pipes('controlnet')
        clear_last()
        #clear_last()
        autoscroll(True)
        #filename = pr['original_image'].rpartition(slash)[2].rpartition('.')[0]
        #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
        num = 0
        for image in images:
            random_seed += num
            fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
            image_path = available_file(os.path.join(stable_dir, controlnet_prefs['batch_folder_name']), fname, num)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            #PILImage.fromarray(image).save(image_path)
            image.save(image_path)
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)
            new_file = available_file(batch_output, fname, num)
            if not controlnet_prefs['display_upscaled_image'] or not controlnet_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=unscaled_path, data=new_file, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if controlnet_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=controlnet_prefs["enlarge_scale"])
                image_path = upscaled_path
            if prefs['save_image_metadata']:
                task = and_list(controlnet_prefs['control_task']) if isinstance(controlnet_prefs['control_task'], list) else controlnet_prefs['control_task']
                img = PILImage.open(image_path)
                metadata = PngInfo()
                metadata.add_text("artist", prefs['meta_ArtistName'])
                metadata.add_text("copyright", prefs['meta_Copyright'])
                metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {controlnet_prefs['enlarge_scale']}x with ESRGAN" if controlnet_prefs['apply_ESRGAN_upscale'] else "")
                metadata.add_text("pipeline", "ControlNet " + task)
                if prefs['save_config_in_metadata']:
                  config_json = controlnet_prefs.copy()
                  config_json['model_path'] = model_path
                  config_json['seed'] = random_seed
                  config_json['prompt'] = pr['prompt']
                  config_json['negative_prompt'] = pr['negative_prompt']
                  del config_json['batch_size']
                  del config_json['max_size']
                  del config_json['display_upscaled_image']
                  del config_json['batch_folder_name']
                  if not config_json['apply_ESRGAN_upscale']:
                    del config_json['enlarge_scale']
                    del config_json['apply_ESRGAN_upscale']
                  metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                img.save(image_path, pnginfo=metadata)
            #TODO: PyDrive
            if storage_type == "Colab Google Drive":
                #new_file = available_file(output_path, fname, num)
                #out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                #new_file = available_file(output_path, fname, num)
                #out_path = new_file
                shutil.copy(image_path, new_file)
            if controlnet_prefs['display_upscaled_image']:
                prt(Row([ImageButton(src=new_file, data=new_file, width=width * float(controlnet_prefs["enlarge_scale"]), height=height * float(controlnet_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
            num += 1
    autoscroll(False)
    del hed, openpose, depth_estimator, mlsd, image_processor, image_segmentor, normal, lineart, shuffle, face_detector
    play_snd(Snd.ALERT, page)

def run_controlnet_xl(page, from_list=False):
    global controlnet_xl_prefs, prefs, status, pipe_controlnet, controlnet, controlnet_xl_models
    if not check_diffusers(page): return
    if not bool(controlnet_xl_prefs['original_image']) and len(controlnet_xl_prefs['multi_controlnets']) == 0:
      alert_msg(page, "You must provide the Original Image to process...")
      return
    if not bool(controlnet_xl_prefs['prompt']) and not from_list:
      alert_msg(page, "You must provide a Prompt to paint in your image...")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.ControlNetXL.controls.append(line)
        #page.controlnet_xl_output.controls.append(line)
        if update:
          page.ControlNetXL.update()
          #page.controlnet_xl_output.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.ControlNetXL, lines=lines)
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.ControlNetXL.controls = page.ControlNetXL.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
      else:
        page.ControlNetXL.auto_scroll = scroll
        page.ControlNetXL.update()
    progress = ProgressBar(bar_height=8)
    total_steps = controlnet_xl_prefs['steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    controlnet_xl_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        control = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'] if bool(p['negative_prompt']) else controlnet_xl_prefs['negative_prompt'], 'original_image': p['init_image'] if bool(p['init_image']) else controlnet_xl_prefs['original_image'], 'conditioning_scale': controlnet_xl_prefs['conditioning_scale'], 'control_guidance_start': controlnet_xl_prefs['control_guidance_start'], 'control_guidance_end': controlnet_xl_prefs['control_guidance_end'], 'seed': p['seed']}
        controlnet_xl_prompts.append(control)
      page.tabs.selected_index = 4
      page.tabs.update()
      #page.controlnet_xl_output.controls.clear()
    else:
      if not bool(controlnet_xl_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      original = controlnet_xl_prefs['original_image']
      conditioning_scale = controlnet_xl_prefs['conditioning_scale']
      control_guidance_start = controlnet_xl_prefs['control_guidance_start']
      control_guidance_end = controlnet_xl_prefs['control_guidance_end']
      if len(controlnet_xl_prefs['multi_controlnets']) > 0:
        original = []
        conditioning_scale = []
        control_guidance_start = []
        control_guidance_end = []
        for c in controlnet_xl_prefs['multi_controlnets']:
          original.append(c['original_image'])
          conditioning_scale.append(c['conditioning_scale'])
          control_guidance_start.append(c['control_guidance_start'])
          control_guidance_end.append(c['control_guidance_end'])
      control = {'prompt':controlnet_xl_prefs['prompt'], 'negative_prompt': controlnet_xl_prefs['negative_prompt'], 'original_image': original, 'conditioning_scale': conditioning_scale, 'control_guidance_start':control_guidance_start, 'control_guidance_end': control_guidance_end, 'seed': controlnet_xl_prefs['seed']}
      if controlnet_xl_prefs['use_init_video']:
        control['init_video'] = controlnet_xl_prefs['init_video']
        control['start_time'] = controlnet_xl_prefs['start_time']
        control['end_time'] = controlnet_xl_prefs['end_time']
        control['fps'] = controlnet_xl_prefs['fps']
      controlnet_xl_prompts.append(control)
      #page.controlnet_xl_output.controls.clear()
    autoscroll(True)
    clear_list()
    prt(Divider(thickness=2, height=4))
    installer = Installing("Installing ControlNetXL Packages...")
    prt(installer)
    if status['loaded_controlnet'] == controlnet_xl_prefs["control_task"]:
        clear_pipes('controlnet')
    else:
        clear_pipes()
    import requests
    from io import BytesIO
    from PIL import ImageOps
    from PIL.PngImagePlugin import PngInfo
    try:
        try:
          from controlnet_aux import MLSDdetector
        except ModuleNotFoundError:
          installer.status("...installing controlnet-aux")
          run_sp("pip install --upgrade controlnet-aux", realtime=False)
          #run_sp("pip install git+https://github.com/patrickvonplaten/controlnet_aux.git")
          pass
        from controlnet_aux import MLSDdetector
        from controlnet_aux import OpenposeDetector
        from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL
        #run_sp("pip install scikit-image", realtime=False)
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR Installing Required Packages...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        flush()
        return
    canny_checkpoint = "xinsir/controlnet-canny-sdxl-1.0"
    #canny_checkpoint = "diffusers/controlnet-canny-sdxl-1.0"
    canny_small_checkpoint = "diffusers/controlnet-canny-sdxl-1.0-small"
    canny_mid_checkpoint = "diffusers/controlnet-canny-sdxl-1.0-mid"
    depth_checkpoint = "diffusers/controlnet-depth-sdxl-1.0"
    depth_small_checkpoint = "diffusers/controlnet-depth-sdxl-1.0-small"
    depth_mid_checkpoint = "diffusers/controlnet-depth-sdxl-1.0-mid"
    seg_checkpoint = "SargeZT/sdxl-controlnet-seg"
    softedge_checkpoint = "SargeZT/controlnet-sd-xl-1.0-softedge-dexined"#"SargeZT/sdxl-controlnet-softedge"
    lineart_checkpoint = "zbulrush/controlnet-sd-xl-1.0-lineart"
    openpose_checkpoint = "xinsir/controlnet-openpose-sdxl-1.0"
    #openpose_checkpoint = "OzzyGT/controlnet-openpose-sdxl-1.0"
    scribble_checkpoint = "xinsir/controlnet-scribble-sdxl-1.0"
    #scribble_checkpoint = "lllyasviel/control_v11p_sd15_scribble"
    scribble_anime_checkpoint = "xinsir/anime-painter"
    HED_checkpoint = "lllyasviel/control_v11p_sd15_softedge"
    mlsd_checkpoint = "lllyasviel/control_v11p_sd15_mlsd"
    normal_checkpoint = "lllyasviel/control_v11p_sd15_normalbae"
    ip2p_checkpoint = "lllyasviel/control_v11e_sd15_ip2p"
    shuffle_checkpoint = "lllyasviel/control_v11e_sd15_shuffle"
    tile_checkpoint = "lllyasviel/control_v11f1e_sd15_tile"
    brightness_checkpoint = "ioclab/control_v1p_sd15_brightness"
    union_checkpoint = "xinsir/controlnet-union-sdxl-1.0"
    hed = None
    openpose = None
    depth_estimator = None
    feature_extractor = None
    mlsd = None
    image_processor = None
    image_segmentor = None
    normal = None
    lineart = None
    shuffle = None
    original_img = None
    def get_controlnet(task):
        nonlocal hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle
        if controlnet_xl_models[task] != None:
            return controlnet_xl_models[task]
        if "Canny Map" in task or task == "Video Canny Edge":
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(canny_mid_checkpoint if 'mid' in task else canny_small_checkpoint if 'small' in task else canny_checkpoint, torch_dtype=torch.float16).to(torch_device)
            task = "Canny Map Edge"
        elif "Scribble" in task:
            from controlnet_aux import HEDdetector
            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(scribble_anime_checkpoint if "Anime" in task else scribble_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "OpenPose" or task == "Video OpenPose":
            task = "OpenPose"
            from controlnet_aux import OpenposeDetector
            openpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(openpose_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Kandinsky Depth":
            from transformers import pipeline
            from diffusers import KandinskyV22PriorPipeline, KandinskyV22ControlnetPipeline
            depth_estimator = pipeline('depth-estimation')
            #controlnet_xl_models[task] = ControlNetModel.from_pretrained(depth_checkpoint, torch_dtype=torch.float16).to(torch_device)
            controlnet_xl_models[task] = KandinskyV22ControlnetPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-controlnet-depth", torch_dtype=torch.float16).to(torch_device)
        elif task == "Marigold Depth":
            import diffusers
            depth_estimator = diffusers.MarigoldDepthPipeline.from_pretrained("prs-eth/marigold-depth-lcm-v1-0", torch_dtype=torch.float16, variant="fp16").to(torch_device)
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(depth_checkpoint, variant="fp16", use_safetensors=True, torch_dtype=torch.float16).to(torch_device)
        elif "Depth" in task:
            from transformers import DPTFeatureExtractor, DPTForDepthEstimation
            depth_estimator = DPTForDepthEstimation.from_pretrained("Intel/dpt-hybrid-midas").to("cuda")
            feature_extractor = DPTFeatureExtractor.from_pretrained("Intel/dpt-hybrid-midas")
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(depth_mid_checkpoint if 'mid' in task else depth_small_checkpoint if 'small' in task else depth_checkpoint, variant="fp16", use_safetensors=True, torch_dtype=torch.float16).to(torch_device)
        elif task == "Softedge":
            from controlnet_aux import HEDdetector, PidiNetDetector
            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')
            hed = PidiNetDetector.from_pretrained('lllyasviel/Annotators') #pidi_net
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(softedge_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "HED":
            from controlnet_aux import HEDdetector
            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')
            #pidi_net = PidiNetDetector.from_pretrained('lllyasviel/Annotators')
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(HED_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "M-LSD":
            from controlnet_aux import MLSDdetector
            mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNetXL')
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(mlsd_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Normal Map":
            #from transformers import pipeline
            #depth_estimator = pipeline("depth-estimation", model ="Intel/dpt-hybrid-midas")
            from controlnet_aux import NormalBaeDetector
            normal = NormalBaeDetector.from_pretrained("lllyasviel/Annotators")
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(normal_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Segmented":
            from transformers import AutoImageProcessor, UperNetForSemanticSegmentation
            from controlnet_utils import ade_palette
            image_processor = AutoImageProcessor.from_pretrained("openmmlab/upernet-convnext-small")
            image_segmentor = UperNetForSemanticSegmentation.from_pretrained("openmmlab/upernet-convnext-small")
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(seg_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "LineArt":
            from controlnet_aux import LineartDetector
            lineart = LineartDetector.from_pretrained("lllyasviel/Annotators")
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(lineart_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Shuffle":
            from controlnet_aux import ContentShuffleDetector
            shuffle = ContentShuffleDetector()
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(shuffle_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Tile":
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(tile_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Brightness":
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(brightness_checkpoint, torch_dtype=torch.float16, use_safetensors=True)
        elif task == "Instruct Pix2Pix":
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(ip2p_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "ControlNet++ Union":
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(union_checkpoint, torch_dtype=torch.float16, use_safetensors=True).to(torch_device)
        return controlnet_xl_models[task]
    width, height = 0, 0
    def resize_for_condition_image(input_image: PILImage, resolution: int):
        input_image = input_image.convert("RGB")
        W, H = input_image.size
        k = float(resolution) / min(H, W)
        H *= k
        W *= k
        H = int(round(H / 64.0)) * 64
        W = int(round(W / 64.0)) * 64
        img = input_image.resize((W, H), resample=PILImage.Resampling.LANCZOS)
        return img
    def prep_image(task, img):
        nonlocal hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle
        nonlocal width, height
        if isinstance(img, list):
          img = img[0]
        if isinstance(img, str):
          if img.startswith('http'):
              #response = requests.get(controlnet_xl_prefs['original_image'])
              #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
              original_img = PILImage.open(requests.get(img, stream=True).raw)
          else:
              if os.path.isfile(img):
                  original_img = PILImage.open(img)
              else:
                  alert_msg(page, f"ERROR: Couldn't find your original_image {img}")
                  return
          width, height = original_img.size
          width, height = scale_dimensions(width, height, controlnet_xl_prefs['max_size'])
          #print(f"Size: {width}x{height}")
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        #return original_img
        try:
            if 'Canny' in task: # == "Canny Map Edge" or task == "Video Canny Edge":
                input_image = np.array(original_img)
                input_image = cv2.Canny(input_image, controlnet_xl_prefs['low_threshold'], controlnet_xl_prefs['high_threshold'])
                input_image = input_image[:, :, None]
                input_image = np.concatenate([input_image, input_image, input_image], axis=2)
                original_img = PILImage.fromarray(input_image)
            elif "Scribble" in task:
                original_img = hed(original_img, scribble=True)
            elif task == "OpenPose" or task == "Video OpenPose":
                original_img = openpose(original_img, hand_and_face=True)
            elif task == "Kandinsky Depth":
                original_img = depth_estimator(original_img)['depth']
                input_image = np.array(original_img)
                input_image = input_image[:, :, None]
                input_image = np.concatenate([input_image, input_image, input_image], axis=2)
                detected_map = torch.from_numpy(input_image).float() / 255.0
                original_img = detected_map.permute(2, 0, 1).unsqueeze(0).half().to("cuda")
                #original_img = PILImage.fromarray(input_image)
            elif task == "Marigold Depth":
                random_seed = get_seed(controlnet_xl_prefs['seed'])
                generator = torch.Generator(device=torch_device).manual_seed(random_seed)
                input_image = depth_estimator(original_img, generator=generator).prediction
                input_image = depth_estimator.image_processor.visualize_depth(input_image, color_map="binary")
                original_img = input_image[0]
            elif "Depth" in task:
                original_img = feature_extractor(images=original_img, return_tensors="pt").pixel_values.to("cuda")
                with torch.no_grad(), torch.autocast("cuda"):
                    depth_map = depth_estimator(original_img).predicted_depth
                depth_map = torch.nn.functional.interpolate(
                    depth_map.unsqueeze(1),
                    size=(1024, 1024),
                    mode="bicubic",
                    align_corners=False,
                )
                depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)
                depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)
                depth_map = (depth_map - depth_min) / (depth_max - depth_min)
                original_img = torch.cat([depth_map] * 3, dim=1)
                original_img = original_img.permute(0, 2, 3, 1).cpu().numpy()[0]
                original_img = PILImage.fromarray((original_img * 255.0).clip(0, 255).astype(np.uint8))
            elif task == "Softedge":
                original_img = hed(original_img, safe=True)
            elif task == "HED":
                original_img = hed(original_img, safe=True)
            elif task == "M-LSD":
                original_img = mlsd(original_img)
            elif task == "Normal Map":
                #depth_estimator = pipeline("depth-estimation", model="Intel/dpt-hybrid-midas" )
                '''original_img = depth_estimator(original_img)['predicted_depth'][0]
                input_image = original_img.numpy()
                image_depth = input_image.copy()
                image_depth -= np.min(image_depth)
                image_depth /= np.max(image_depth)
                bg_threhold = 0.4
                x = cv2.Sobel(input_image, cv2.CV_32F, 1, 0, ksize=3)
                x[image_depth < bg_threhold] = 0
                y = cv2.Sobel(input_image, cv2.CV_32F, 0, 1, ksize=3)
                y[image_depth < bg_threhold] = 0
                z = np.ones_like(x) * np.pi * 2.0
                input_image = np.stack([x, y, z], axis=2)
                input_image /= np.sum(input_image ** 2.0, axis=2, keepdims=True) ** 0.5
                input_image = (input_image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)
                original_img = PILImage.fromarray(input_image)'''
                original_img = normal(original_img)
            elif task == "Segmented":
                from controlnet_utils import ade_palette
                pixel_values = image_processor(original_img, return_tensors="pt").pixel_values
                with torch.no_grad():
                  outputs = image_segmentor(pixel_values)
                seg = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[original_img.size[::-1]])[0]
                color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3
                palette = np.array(ade_palette())
                for label, color in enumerate(palette):
                    color_seg[seg == label, :] = color
                color_seg = color_seg.astype(np.uint8)
                original_img = PILImage.fromarray(color_seg)
            elif task == "LineArt":
                original_img = lineart(original_img)
            elif task == "Shuffle":
                original_img = shuffle(original_img)
            elif task == "Tile":
                original_img = resize_for_condition_image(original_img, 1024)
            elif task == "Brightness":
                original_img = PILImage.fromarray(original_img).convert('L')
            return original_img
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Preparing ControlNet-XL {controlnet_xl_prefs['control_task']} Input Image...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            flush()
            return
    def prep_video(vid):
        nonlocal width, height
        if vid.startswith('http'):
            init_vid = download_file(vid, stable_dir)
        else:
            if os.path.isfile(vid):
                init_vid = vid
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_video {vid}")
                return
        try:
            start_time = float(controlnet_xl_prefs['start_time'])
            end_time = float(controlnet_xl_prefs['end_time'])
            fps = int(controlnet_xl_prefs['fps'])
            max_size = controlnet_xl_prefs['max_size']
        except Exception:
            alert_msg(page, "Make sure your Numbers are actual numbers...")
            return
        prt("Extracting Frames from Video Clip")
        try:
            cap = cv2.VideoCapture(init_vid)
        except Exception as e:
            alert_msg(page, "ERROR Reading Video File. May be Incompatible Format...")
            clear_last()
            return
        count = 0
        video = []
        frames = []
        width = height = 0
        cap.set(cv2.CAP_PROP_FPS, fps)
        video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)
        start_frame = int(start_time * fps)
        if end_time == 0 or end_time == 0.0:
            end_frame = int(video_length)
        else:
            end_frame = int(end_time * fps)
        total = end_frame - start_frame
        for i in range(start_frame, end_frame):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            success, image = cap.read()
            if success:
                #filename = os.path.join(output_dir, f'{file_prefix}{count}.png')
                if width == 0:
                    shape = image.shape
                    width, height = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)
                image = cv2.resize(image, (width, height), interpolation = cv2.INTER_AREA)
                #cv2.imwrite(os.path.join(output_dir, filename), image)
                image = prep_image(controlnet_xl_prefs['control_task'], PILImage.fromarray(image))
                video.append(image)
                count += 1
        cap.release()
        clear_last()
        return video
    loaded_controlnet = None
    if len(controlnet_xl_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_xl_prefs['use_init_video']:
        controlnet = []
        loaded_controlnet = []
        for c in controlnet_xl_prefs['multi_controlnets']:
            controlnet.append(get_controlnet(c['control_task']))
            loaded_controlnet.append(c['control_task'])
        if len(controlnet) == 1:
            controlnet = controlnet[0]
            loaded_controlnet = loaded_controlnet[0]
    else:
        controlnet = get_controlnet(controlnet_xl_prefs['control_task'])
        loaded_controlnet = controlnet_xl_prefs['control_task']
    for k, v in controlnet_xl_models.items():
      if v != None and k in loaded_controlnet:
        del v
        controlnet_xl_models[k] = None
    controlnet_type = "text2image"
    if controlnet_xl_prefs['use_image2image']:
        if bool(controlnet_xl_prefs['init_image']):
            if bool(controlnet_xl_prefs['mask_image']) or controlnet_xl_prefs['alpha_mask']:
                controlnet_type = "inpaint"
            else:
                controlnet_type = "image2image"
    use_ip_adapter = controlnet_xl_prefs['use_ip_adapter']
    if use_ip_adapter:
        ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == controlnet_xl_prefs['ip_adapter_SDXL_model'])
    else:
        ip_adapter_model = None
    if 'loaded_controlnet_pag' not in status: status['loaded_controlnet_pag'] = False
    if controlnet_type != status['loaded_controlnet_type'] or (controlnet_xl_prefs['use_pag'] != status['loaded_controlnet_pag'] and not controlnet_xl_prefs['use_image2image']):
        clear_pipes()
    else:
        clear_pipes('controlnet')
    #model = get_model(prefs['model_ckpt'])
    model_path = "stabilityai/stable-diffusion-xl-base-1.0"
    if controlnet_xl_prefs['use_pag'] and not controlnet_xl_prefs['use_image2image']:
        pag_applied_layers = []
        if pag_prefs['applied_layer_down']: pag_applied_layers.append("down")
        if pag_prefs['applied_layer_mid']: pag_applied_layers.append("mid")
        if pag_prefs['applied_layer_up']: pag_applied_layers.append("up")
    if pipe_controlnet == None or status['loaded_controlnet'] != controlnet_xl_prefs["control_task"]:
        vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)
        if controlnet_type == "text2image":
            if controlnet_xl_prefs['use_pag']:
                from diffusers import StableDiffusionXLControlNetPAGPipeline
                pipe_controlnet = StableDiffusionXLControlNetPAGPipeline.from_pretrained(model_path, controlnet=controlnet, vae=vae, safety_checker=None, variant="fp16", torch_dtype=torch.float16, pag_applied_layers=pag_applied_layers, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                status['loaded_pag_layers'] = pag_applied_layers
            else:
                pipe_controlnet = StableDiffusionXLControlNetPipeline.from_pretrained(model_path, controlnet=controlnet, vae=vae, safety_checker=None, variant="fp16", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        elif controlnet_type == "image2image":
            from diffusers import StableDiffusionXLControlNetImg2ImgPipeline
            pipe_controlnet = StableDiffusionXLControlNetImg2ImgPipeline.from_pretrained(model_path, controlnet=controlnet, vae=vae, safety_checker=None, variant="fp16", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        elif controlnet_type == "inpaint":
            from diffusers import StableDiffusionXLControlNetInpaintPipeline
            pipe_controlnet = StableDiffusionXLControlNetInpaintPipeline.from_pretrained(model_path, controlnet=controlnet, vae=vae, safety_checker=None, variant="fp16", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        #pipe_controlnet.enable_model_cpu_offload()
        pipe_controlnet = optimize_SDXL(pipe_controlnet, vae_slicing=True, vae_tiling=True)
        status['loaded_controlnet'] = loaded_controlnet #controlnet_xl_prefs["control_task"]
        status['loaded_controlnet_type'] = controlnet_type
        status['loaded_controlnet_pag'] = controlnet_xl_prefs['use_pag']
    elif controlnet_xl_prefs['use_pag'] and not controlnet_xl_prefs['use_image2image']:
      if pag_applied_layers != status['loaded_pag_layers']:
          pipe_controlnet.set_pag_applied_layers(pag_applied_layers)
          status['loaded_pag_layers'] = pag_applied_layers
    pipe_controlnet = pipeline_scheduler(pipe_controlnet)
    if controlnet_xl_prefs['use_init_video']:
        from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor
        pipe_controlnet.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))
        pipe_controlnet.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))
    init_img = None
    mask_img = None
    if bool(controlnet_xl_prefs['init_image'] and controlnet_xl_prefs['use_image2image']):
        if controlnet_xl_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(controlnet_xl_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(controlnet_xl_prefs['init_image']):
                init_img = PILImage.open(controlnet_xl_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {controlnet_xl_prefs['init_image']}")
                return
        width, height = init_img.size
        width, height = scale_dimensions(width, height, controlnet_xl_prefs['max_size'])
        if bool(controlnet_xl_prefs['alpha_mask']):
            init_img = init_img.convert("RGBA")
        else:
            init_img = init_img.convert("RGB")
        init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        if not bool(controlnet_xl_prefs['mask_image']) and bool(controlnet_xl_prefs['alpha_mask']):
            mask_img = init_img.convert('RGBA')
            red, green, blue, alpha = PILImage.Image.split(init_img)
            mask_img = alpha.convert('L')
        elif bool(controlnet_xl_prefs['mask_image']):
            if controlnet_xl_prefs['mask_image'].startswith('http'):
                mask_img = PILImage.open(requests.get(controlnet_xl_prefs['mask_image'], stream=True).raw)
            else:
                if os.path.isfile(controlnet_xl_prefs['mask_image']):
                    mask_img = PILImage.open(controlnet_xl_prefs['mask_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your mask_image {controlnet_xl_prefs['mask_image']}")
                    return
            width, height = mask_img.size
            width, height = scale_dimensions(width, height, controlnet_xl_prefs['max_size'])
            mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        if controlnet_xl_prefs['invert_mask'] and not controlnet_xl_prefs['alpha_mask']:
            from PIL import ImageOps
            mask_img = ImageOps.invert(mask_img.convert('RGB'))
    if use_ip_adapter:
        pipe_controlnet.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
        pipe_controlnet.set_ip_adapter_scale(controlnet_xl_prefs['ip_adapter_strength'])
        if controlnet_xl_prefs['ip_adapter_image'].startswith('http'):
            ip_adapter_image = PILImage.open(requests.get(controlnet_xl_prefs['ip_adapter_image'], stream=True).raw)
        else:
            if os.path.isfile(controlnet_xl_prefs['ip_adapter_image']):
                ip_adapter_image = PILImage.open(controlnet_xl_prefs['ip_adapter_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your ip_adapter_image {controlnet_xl_prefs['ip_adapter_image']}")
                return
        ip_adapter_image = ImageOps.exif_transpose(ip_adapter_image).convert("RGB")
        status['loaded_ip_adapter'] = ip_adapter_model
        ip_adapter_args = {'ip_adapter_image': ip_adapter_image}
    else:
        ip_adapter_args = {}
    pag_img_arg = {'pag_scale': controlnet_xl_prefs['pag_scale']} if controlnet_xl_prefs['use_pag'] else {}
    clear_last()
    prt(f"Generating ControlNet-XL {controlnet_xl_prefs['control_task']} of your Image...")
    batch_output = os.path.join(stable_dir, controlnet_xl_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], controlnet_xl_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for pr in controlnet_xl_prompts:
        prt(progress)
        autoscroll(False)
        filename = f"{controlnet_xl_prefs['file_prefix']}{format_filename(pr['prompt'])}"
        filename = filename[:int(prefs['file_max_length'])]
        if len(controlnet_xl_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_xl_prefs['use_init_video']:
            original_img = []
            for c in controlnet_xl_prefs['multi_controlnets']:
                original_img.append(prep_image(c['control_task'], c['original_image']))
                if controlnet_xl_prefs['show_processed_image']:
                    processed_img = available_file(batch_output, f"{filename}-{c['control_task'].partition(' ')[0]}", 0, no_num=True)
                    w, h = original_img[-1].size
                    original_img[-1].save(processed_img)
                    prt(Row([ImageButton(src=processed_img, data=processed_img, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))
        elif not controlnet_xl_prefs['use_init_video']:
            original_img = prep_image(controlnet_xl_prefs['control_task'], pr['original_image'])
            if controlnet_xl_prefs['show_processed_image']:
                processed_img = available_file(batch_output, f"{filename}-{controlnet_xl_prefs['control_task'].partition(' ')[0]}", 0, no_num=True)
                w, h = original_img.size
                original_img.save(processed_img)
                prt(Row([ImageButton(src=processed_img, data=processed_img, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))
        else:
            video_img = prep_video(pr['original_image'])
            latents = torch.randn((1, 4, 64, 64), device="cuda", dtype=torch.float16).repeat(len(video_img), 1, 1, 1)
        try:
            random_seed = get_seed(pr['seed'])
            generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            if controlnet_type == "text2image":
                if not controlnet_xl_prefs['use_init_video']:
                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xl_prefs['steps'], guidance_scale=controlnet_xl_prefs['guidance_scale'], eta=controlnet_xl_prefs['eta'], num_images_per_prompt=controlnet_xl_prefs['batch_size'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args, **pag_img_arg).images
                else:
                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xl_prefs['steps'], guidance_scale=controlnet_xl_prefs['guidance_scale'], eta=controlnet_xl_prefs['eta'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args, **pag_img_arg).images
            elif controlnet_type == "image2image":
                if not controlnet_xl_prefs['use_init_video']:
                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=init_img, control_image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xl_prefs['steps'], guidance_scale=controlnet_xl_prefs['guidance_scale'], eta=controlnet_xl_prefs['eta'], num_images_per_prompt=controlnet_xl_prefs['batch_size'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images
                else:
                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=init_img, control_image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xl_prefs['steps'], guidance_scale=controlnet_xl_prefs['guidance_scale'], eta=controlnet_xl_prefs['eta'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images
            elif controlnet_type == "inpaint":
                if not controlnet_xl_prefs['use_init_video']:
                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=init_img, mask=mask_img, control_image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xl_prefs['steps'], guidance_scale=controlnet_xl_prefs['guidance_scale'], eta=controlnet_xl_prefs['eta'], num_images_per_prompt=controlnet_xl_prefs['batch_size'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images
                else:
                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=init_img, mask=mask_img, control_image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xl_prefs['steps'], guidance_scale=controlnet_xl_prefs['guidance_scale'], eta=controlnet_xl_prefs['eta'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images
        except Exception as e:
            #clear_last()
            clear_last()
            alert_msg(page, f"ERROR Generating ControlNet-XL {controlnet_xl_prefs['control_task']}...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            flush()
            return
        #clear_pipes('controlnet')
        clear_last()
        #clear_last()
        autoscroll(True)
        #filename = pr['original_image'].rpartition(slash)[2].rpartition('.')[0]
        
        #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
        num = 0
        for image in images:
            random_seed += num
            fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
            image_path = available_file(os.path.join(stable_dir, controlnet_xl_prefs['batch_folder_name']), fname, num)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            #PILImage.fromarray(image).save(image_path)
            image.save(image_path)
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)
            new_file = available_file(batch_output, fname, num)
            if not controlnet_xl_prefs['display_upscaled_image'] or not controlnet_xl_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=unscaled_path, data=new_file, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if controlnet_xl_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=controlnet_xl_prefs["enlarge_scale"])
                image_path = upscaled_path
            if prefs['save_image_metadata']:
                task = and_list(controlnet_xl_prefs['control_task']) if isinstance(controlnet_xl_prefs['control_task'], list) else controlnet_xl_prefs['control_task']
                img = PILImage.open(image_path)
                metadata = PngInfo()
                metadata.add_text("artist", prefs['meta_ArtistName'])
                metadata.add_text("copyright", prefs['meta_Copyright'])
                metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {controlnet_xl_prefs['enlarge_scale']}x with ESRGAN" if controlnet_xl_prefs['apply_ESRGAN_upscale'] else "")
                metadata.add_text("pipeline", "ControlNetXL " + task)
                if prefs['save_config_in_metadata']:
                  config_json = controlnet_xl_prefs.copy()
                  config_json['model_path'] = model_path
                  config_json['seed'] = random_seed
                  config_json['prompt'] = pr['prompt']
                  config_json['negative_prompt'] = pr['negative_prompt']
                  del config_json['batch_size']
                  del config_json['max_size']
                  del config_json['display_upscaled_image']
                  del config_json['batch_folder_name']
                  if not config_json['apply_ESRGAN_upscale']:
                    del config_json['enlarge_scale']
                    del config_json['apply_ESRGAN_upscale']
                  metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                img.save(image_path, pnginfo=metadata)
            #TODO: PyDrive
            if storage_type == "Colab Google Drive":
                #new_file = available_file(output_path, fname, num)
                #out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                #new_file = available_file(output_path, fname, num)
                #out_path = new_file
                shutil.copy(image_path, new_file)
            if controlnet_xl_prefs['display_upscaled_image']:
                prt(Row([ImageButton(src=new_file, data=new_file, width=width * float(controlnet_xl_prefs["enlarge_scale"]), height=height * float(controlnet_xl_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
            num += 1
    autoscroll(False)
    del hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle
    play_snd(Snd.ALERT, page)

def run_controlnet_sd3(page, from_list=False, with_params=False):
    global controlnet_sd3_prefs, prefs, status, pipe_controlnet, controlnet, controlnet_sd3_models
    if not check_diffusers(page): return
    if not bool(controlnet_sd3_prefs['original_image']) and len(controlnet_sd3_prefs['multi_controlnets']) == 0:
      alert_msg(page, "You must provide the Original Image to process...")
      return
    if not bool(controlnet_sd3_prefs['prompt']) and not from_list:
      alert_msg(page, "You must provide a Prompt to paint in your image...")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.ControlNetSD3.controls.append(line)
        #page.controlnet_sd3_output.controls.append(line)
        if update:
          page.ControlNetSD3.update()
          #page.controlnet_sd3_output.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.ControlNetSD3, lines=lines)
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.ControlNetSD3.controls = page.ControlNetSD3.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
      else:
        page.ControlNetSD3.auto_scroll = scroll
        page.ControlNetSD3.update()
    progress = ProgressBar(bar_height=8)
    total_steps = controlnet_sd3_prefs['steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    controlnet_sd3_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
    else:
      if not bool(controlnet_sd3_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
    original = controlnet_sd3_prefs['original_image']
    conditioning_scale = controlnet_sd3_prefs['conditioning_scale']
    control_guidance_start = controlnet_sd3_prefs['control_guidance_start']
    control_guidance_end = controlnet_sd3_prefs['control_guidance_end']
    if len(controlnet_sd3_prefs['multi_controlnets']) > 0:
      original = []
      conditioning_scale = []
      control_guidance_start = []
      control_guidance_end = []
      for c in controlnet_sd3_prefs['multi_controlnets']:
        original.append(c['original_image'])
        conditioning_scale.append(c['conditioning_scale'])
        control_guidance_start.append(c['control_guidance_start'])
        control_guidance_end.append(c['control_guidance_end'])
    control = {'prompt':controlnet_sd3_prefs['prompt'], 'negative_prompt': controlnet_sd3_prefs['negative_prompt'], 'original_image': original, 'conditioning_scale': conditioning_scale, 'control_guidance_start':control_guidance_start, 'control_guidance_end': control_guidance_end, 'seed': controlnet_sd3_prefs['seed']}
    if controlnet_sd3_prefs['use_init_video']:
      control['init_video'] = controlnet_sd3_prefs['init_video']
      control['start_time'] = controlnet_sd3_prefs['start_time']
      control['end_time'] = controlnet_sd3_prefs['end_time']
      control['fps'] = controlnet_sd3_prefs['fps']
    if from_list:
      for p in prompts:
        c = control.copy()
        c['prompt'] = p.prompt
        c['negative_prompt'] = p['negative_prompt'] if bool(p['negative_prompt']) else controlnet_sd3_prefs['negative_prompt']
        if not with_params: #TODO: Add more params
          c['seed'] = p['seed']
        #control = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'] if bool(p['negative_prompt']) else controlnet_sd3_prefs['negative_prompt'], 'original_image': p['init_image'] if bool(p['init_image']) else controlnet_sd3_prefs['original_image'], 'conditioning_scale': controlnet_sd3_prefs['conditioning_scale'], 'control_guidance_start': controlnet_sd3_prefs['control_guidance_start'], 'control_guidance_end': controlnet_sd3_prefs['control_guidance_end'], 'seed': p['seed']}
        controlnet_sd3_prompts.append(c)
      page.tabs.selected_index = 4
      page.tabs.update()
      #page.controlnet_sd3_output.controls.clear()
    else:
      controlnet_sd3_prompts.append(control)
      #page.controlnet_sd3_output.controls.clear()
    autoscroll(True)
    clear_list()
    prt(Divider(thickness=2, height=4))
    installer = Installing("Installing ControlNet-SD3 Packages... See console for progress.")
    prt(installer)
    if status['loaded_controlnet'] == controlnet_sd3_prefs["control_task"]:
        clear_pipes('controlnet')
    else:
        clear_pipes()
    import requests
    from io import BytesIO
    from PIL import ImageOps
    from PIL.PngImagePlugin import PngInfo
    try:
        pip_install("controlnet-aux", installer=installer)
        from controlnet_aux import MLSDdetector
        from controlnet_aux import OpenposeDetector
        from diffusers import AutoencoderKL
        from diffusers.models import SD3ControlNetModel, SD3MultiControlNetModel
        #run_sp("pip install scikit-image", realtime=False)
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR Installing Required Packages...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        flush()
        return
    canny_checkpoint = "InstantX/SD3-Controlnet-Canny"
    #canny_checkpoint = "diffusers/controlnet-canny-sd3-1.0"
    canny_small_checkpoint = "diffusers/controlnet-canny-sd3-1.0-small"
    canny_mid_checkpoint = "diffusers/controlnet-canny-sd3-1.0-mid"
    depth_checkpoint = "diffusers/controlnet-depth-sd3-1.0"
    depth_small_checkpoint = "diffusers/controlnet-depth-sd3-1.0-small"
    depth_mid_checkpoint = "diffusers/controlnet-depth-sd3-1.0-mid"
    seg_checkpoint = "SargeZT/sd3-controlnet-seg"
    softedge_checkpoint = "SargeZT/controlnet-sd-xl-1.0-softedge-dexined"#"SargeZT/sd3-controlnet-softedge"
    lineart_checkpoint = "zbulrush/controlnet-sd-xl-1.0-lineart"
    openpose_checkpoint = "InstantX/SD3-Controlnet-Pose"
    #openpose_checkpoint = "OzzyGT/controlnet-openpose-sd3-1.0"
    scribble_checkpoint = "xinsir/controlnet-scribble-sd3-1.0"
    #scribble_checkpoint = "lllyasviel/control_v11p_sd15_scribble"
    scribble_anime_checkpoint = "xinsir/anime-painter"
    HED_checkpoint = "lllyasviel/control_v11p_sd15_softedge"
    mlsd_checkpoint = "lllyasviel/control_v11p_sd15_mlsd"
    normal_checkpoint = "lllyasviel/control_v11p_sd15_normalbae"
    ip2p_checkpoint = "lllyasviel/control_v11e_sd15_ip2p"
    shuffle_checkpoint = "lllyasviel/control_v11e_sd15_shuffle"
    tile_checkpoint = "InstantX/SD3-Controlnet-Tile"
    brightness_checkpoint = "ioclab/control_v1p_sd15_brightness"
    hed = None
    openpose = None
    depth_estimator = None
    feature_extractor = None
    mlsd = None
    image_processor = None
    image_segmentor = None
    normal = None
    lineart = None
    shuffle = None
    original_img = None
    def get_controlnet(task):
        nonlocal hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle
        if controlnet_sd3_models[task] != None:
            return controlnet_sd3_models[task]
        if "Canny Map" in task or task == "Video Canny Edge":
            controlnet_sd3_models[task] = SD3ControlNetModel.from_pretrained(canny_mid_checkpoint if 'mid' in task else canny_small_checkpoint if 'small' in task else canny_checkpoint, torch_dtype=torch.float16).to(torch_device)
            task = "Canny Map Edge"
        elif "Scribble" in task:
            from controlnet_aux import HEDdetector
            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')
            controlnet_sd3_models[task] = SD3ControlNetModel.from_pretrained(scribble_anime_checkpoint if "Anime" in task else scribble_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "OpenPose" or task == "Video OpenPose":
            task = "OpenPose"
            from controlnet_aux import OpenposeDetector
            openpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')
            controlnet_sd3_models[task] = SD3ControlNetModel.from_pretrained(openpose_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Marigold Depth":
            import diffusers
            depth_estimator = diffusers.MarigoldDepthPipeline.from_pretrained("prs-eth/marigold-depth-lcm-v1-0", torch_dtype=torch.float16, variant="fp16").to(torch_device)
            controlnet_sd3_models[task] = SD3ControlNetModel.from_pretrained(depth_checkpoint, variant="fp16", use_safetensors=True, torch_dtype=torch.float16).to(torch_device)
        elif "Depth" in task:
            from transformers import DPTFeatureExtractor, DPTForDepthEstimation
            depth_estimator = DPTForDepthEstimation.from_pretrained("Intel/dpt-hybrid-midas").to("cuda")
            feature_extractor = DPTFeatureExtractor.from_pretrained("Intel/dpt-hybrid-midas")
            controlnet_sd3_models[task] = SD3ControlNetModel.from_pretrained(depth_mid_checkpoint if 'mid' in task else depth_small_checkpoint if 'small' in task else depth_checkpoint, variant="fp16", use_safetensors=True, torch_dtype=torch.float16).to(torch_device)
        elif task == "Softedge":
            from controlnet_aux import HEDdetector, PidiNetDetector
            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')
            hed = PidiNetDetector.from_pretrained('lllyasviel/Annotators') #pidi_net
            controlnet_sd3_models[task] = SD3ControlNetModel.from_pretrained(softedge_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "HED":
            from controlnet_aux import HEDdetector
            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')
            #pidi_net = PidiNetDetector.from_pretrained('lllyasviel/Annotators')
            controlnet_sd3_models[task] = SD3ControlNetModel.from_pretrained(HED_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "M-LSD":
            from controlnet_aux import MLSDdetector
            mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNetSD3')
            controlnet_sd3_models[task] = SD3ControlNetModel.from_pretrained(mlsd_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Normal Map":
            #from transformers import pipeline
            #depth_estimator = pipeline("depth-estimation", model ="Intel/dpt-hybrid-midas")
            from controlnet_aux import NormalBaeDetector
            normal = NormalBaeDetector.from_pretrained("lllyasviel/Annotators")
            controlnet_sd3_models[task] = SD3ControlNetModel.from_pretrained(normal_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Segmented":
            from transformers import AutoImageProcessor, UperNetForSemanticSegmentation
            from controlnet_utils import ade_palette
            image_processor = AutoImageProcessor.from_pretrained("openmmlab/upernet-convnext-small")
            image_segmentor = UperNetForSemanticSegmentation.from_pretrained("openmmlab/upernet-convnext-small")
            controlnet_sd3_models[task] = SD3ControlNetModel.from_pretrained(seg_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "LineArt":
            from controlnet_aux import LineartDetector
            lineart = LineartDetector.from_pretrained("lllyasviel/Annotators")
            controlnet_sd3_models[task] = SD3ControlNetModel.from_pretrained(lineart_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Shuffle":
            from controlnet_aux import ContentShuffleDetector
            shuffle = ContentShuffleDetector()
            controlnet_sd3_models[task] = SD3ControlNetModel.from_pretrained(shuffle_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Tile":
            controlnet_sd3_models[task] = SD3ControlNetModel.from_pretrained(tile_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Brightness":
            controlnet_sd3_models[task] = SD3ControlNetModel.from_pretrained(brightness_checkpoint, torch_dtype=torch.float16, use_safetensors=True)
        elif task == "Instruct Pix2Pix":
            controlnet_sd3_models[task] = SD3ControlNetModel.from_pretrained(ip2p_checkpoint, torch_dtype=torch.float16).to(torch_device)

        return controlnet_sd3_models[task]
    width, height = 0, 0
    def resize_for_condition_image(input_image: PILImage, resolution: int):
        input_image = input_image.convert("RGB")
        W, H = input_image.size
        k = float(resolution) / min(H, W)
        H *= k
        W *= k
        H = int(round(H / 64.0)) * 64
        W = int(round(W / 64.0)) * 64
        img = input_image.resize((W, H), resample=PILImage.Resampling.LANCZOS)
        return img
    def prep_image(task, img):
        nonlocal hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle
        nonlocal width, height
        if isinstance(img, list):
          img = img[0]
        if isinstance(img, str):
          if img.startswith('http'):
              #response = requests.get(controlnet_sd3_prefs['original_image'])
              #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
              original_img = PILImage.open(requests.get(img, stream=True).raw)
          else:
              if os.path.isfile(img):
                  original_img = PILImage.open(img)
              else:
                  alert_msg(page, f"ERROR: Couldn't find your original_image {img}")
                  return
          width, height = original_img.size
          width, height = scale_dimensions(width, height, controlnet_sd3_prefs['max_size'])
          #print(f"Size: {width}x{height}")
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        #return original_img
        try:
            if 'Canny' in task: # == "Canny Map Edge" or task == "Video Canny Edge":
                input_image = np.array(original_img)
                input_image = cv2.Canny(input_image, controlnet_sd3_prefs['low_threshold'], controlnet_sd3_prefs['high_threshold'])
                input_image = input_image[:, :, None]
                input_image = np.concatenate([input_image, input_image, input_image], axis=2)
                original_img = PILImage.fromarray(input_image)
            elif "Scribble" in task:
                original_img = hed(original_img, scribble=True)
            elif task == "OpenPose" or task == "Video OpenPose":
                original_img = openpose(original_img, hand_and_face=True)
            elif task == "Kandinsky Depth":
                original_img = depth_estimator(original_img)['depth']
                input_image = np.array(original_img)
                input_image = input_image[:, :, None]
                input_image = np.concatenate([input_image, input_image, input_image], axis=2)
                detected_map = torch.from_numpy(input_image).float() / 255.0
                original_img = detected_map.permute(2, 0, 1).unsqueeze(0).half().to("cuda")
                #original_img = PILImage.fromarray(input_image)
            elif task == "Marigold Depth":
                random_seed = get_seed(controlnet_sd3_prefs['seed'])
                generator = torch.Generator(device=torch_device).manual_seed(random_seed)
                input_image = depth_estimator(original_img, generator=generator).prediction
                input_image = depth_estimator.image_processor.visualize_depth(input_image, color_map="binary")
                original_img = input_image[0]
            elif "Depth" in task:
                original_img = feature_extractor(images=original_img, return_tensors="pt").pixel_values.to("cuda")
                with torch.no_grad(), torch.autocast("cuda"):
                    depth_map = depth_estimator(original_img).predicted_depth
                depth_map = torch.nn.functional.interpolate(
                    depth_map.unsqueeze(1),
                    size=(1024, 1024),
                    mode="bicubic",
                    align_corners=False,
                )
                depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)
                depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)
                depth_map = (depth_map - depth_min) / (depth_max - depth_min)
                original_img = torch.cat([depth_map] * 3, dim=1)
                original_img = original_img.permute(0, 2, 3, 1).cpu().numpy()[0]
                original_img = PILImage.fromarray((original_img * 255.0).clip(0, 255).astype(np.uint8))
            elif task == "Softedge":
                original_img = hed(original_img, safe=True)
            elif task == "HED":
                original_img = hed(original_img, safe=True)
            elif task == "M-LSD":
                original_img = mlsd(original_img)
            elif task == "Normal Map":
                original_img = normal(original_img)
            elif task == "Segmented":
                from controlnet_utils import ade_palette
                pixel_values = image_processor(original_img, return_tensors="pt").pixel_values
                with torch.no_grad():
                  outputs = image_segmentor(pixel_values)
                seg = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[original_img.size[::-1]])[0]
                color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3
                palette = np.array(ade_palette())
                for label, color in enumerate(palette):
                    color_seg[seg == label, :] = color
                color_seg = color_seg.astype(np.uint8)
                original_img = PILImage.fromarray(color_seg)
            elif task == "LineArt":
                original_img = lineart(original_img)
            elif task == "Shuffle":
                original_img = shuffle(original_img)
            elif task == "Tile":
                original_img = resize_for_condition_image(original_img, 1024)
            elif task == "Brightness":
                original_img = PILImage.fromarray(original_img).convert('L')
            return original_img
        except Exception as e:
            alert_msg(page, f"ERROR Preparing ControlNet-SD3 {controlnet_sd3_prefs['control_task']} Input Image {img}...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            flush()
            return
    def prep_video(vid):
        nonlocal width, height
        if vid.startswith('http'):
            init_vid = download_file(vid, stable_dir)
        else:
            if os.path.isfile(vid):
                init_vid = vid
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_video {vid}")
                return
        try:
            start_time = float(controlnet_sd3_prefs['start_time'])
            end_time = float(controlnet_sd3_prefs['end_time'])
            fps = int(controlnet_sd3_prefs['fps'])
            max_size = controlnet_sd3_prefs['max_size']
        except Exception:
            alert_msg(page, "Make sure your Numbers are actual numbers...")
            return
        prt("Extracting Frames from Video Clip")
        try:
            cap = cv2.VideoCapture(init_vid)
        except Exception as e:
            alert_msg(page, "ERROR Reading Video File. May be Incompatible Format...")
            clear_last()
            return
        count = 0
        video = []
        frames = []
        width = height = 0
        cap.set(cv2.CAP_PROP_FPS, fps)
        video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)
        start_frame = int(start_time * fps)
        if end_time == 0 or end_time == 0.0:
            end_frame = int(video_length)
        else:
            end_frame = int(end_time * fps)
        total = end_frame - start_frame
        for i in range(start_frame, end_frame):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            success, image = cap.read()
            if success:
                #filename = os.path.join(output_dir, f'{file_prefix}{count}.png')
                if width == 0:
                    shape = image.shape
                    width, height = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)
                image = cv2.resize(image, (width, height), interpolation = cv2.INTER_AREA)
                #cv2.imwrite(os.path.join(output_dir, filename), image)
                image = prep_image(controlnet_sd3_prefs['control_task'], PILImage.fromarray(image))
                video.append(image)
                count += 1
        cap.release()
        clear_last()
        return video
    loaded_controlnet = None
    if len(controlnet_sd3_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_sd3_prefs['use_init_video']:
        controlnet = []
        loaded_controlnet = []
        for c in controlnet_sd3_prefs['multi_controlnets']:
            controlnet.append(get_controlnet(c['control_task']))
            loaded_controlnet.append(c['control_task'])
        if len(controlnet) == 1:
            controlnet = controlnet[0]
            loaded_controlnet = loaded_controlnet[0]
        else:
            controlnet = SD3MultiControlNetModel(controlnet)
    else:
        controlnet = get_controlnet(controlnet_sd3_prefs['control_task'])
        loaded_controlnet = controlnet_sd3_prefs['control_task']
    for k, v in controlnet_sd3_models.items():
      if v != None and k in loaded_controlnet:
        del v
        controlnet_sd3_models[k] = None
    controlnet_type = "text2image"
    if controlnet_sd3_prefs['use_image2image']:
        if bool(controlnet_sd3_prefs['init_image']):
            if bool(controlnet_sd3_prefs['mask_image']) or controlnet_sd3_prefs['alpha_mask']:
                controlnet_type = "inpaint"
            else:
                controlnet_type = "image2image"
    use_ip_adapter = controlnet_sd3_prefs['use_ip_adapter']
    if use_ip_adapter:
        ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == controlnet_sd3_prefs['ip_adapter_SD3_model'])
    else:
        ip_adapter_model = None
    if controlnet_type != status['loaded_controlnet_type']:
        clear_pipes()
    #model = get_model(prefs['model_ckpt'])
    model_path = get_SD3_model(prefs['SD3_model'])['path']
    if pipe_controlnet == None or status['loaded_controlnet'] != controlnet_sd3_prefs["control_task"]:
        #vae = AutoencoderKL.from_pretrained("madebyollin/sd3-vae-fp16-fix", torch_dtype=torch.float16)
        if controlnet_type == "text2image":
            from diffusers import StableDiffusion3ControlNetPipeline #, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None
            pipe_controlnet = StableDiffusion3ControlNetPipeline.from_pretrained(model_path, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16)
        elif controlnet_type == "image2image":
            from diffusers import StableDiffusion3ControImg2ImglNetPipeline
            pipe_controlnet = StableDiffusion3ControlNetImg2ImgPipeline.from_pretrained(model_path, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        elif controlnet_type == "inpaint":
            from diffusers import StableDiffusion3ControlNetInpaintPipeline
            pipe_controlnet = StableDiffusion3ControlNetInpaintPipeline.from_pretrained(model_path, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        if prefs['SD3_cpu_offload']:
            pipe_controlnet.enable_model_cpu_offload()
        else:
            pipe_controlnet.to("cuda")
        #pipe_controlnet.enable_model_cpu_offload()
        #pipe_controlnet = optimize_SD3(pipe_controlnet, vae_slicing=True, vae_tiling=True)
        status['loaded_controlnet'] = loaded_controlnet #controlnet_sd3_prefs["control_task"]
        status['loaded_controlnet_type'] = controlnet_type
    #pipe_controlnet = pipeline_scheduler(pipe_controlnet)
    if controlnet_sd3_prefs['use_init_video']:
        from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor
        pipe_controlnet.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))
        pipe_controlnet.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))
    init_img = None
    mask_img = None
    if bool(controlnet_sd3_prefs['init_image'] and controlnet_sd3_prefs['use_image2image']):
        if controlnet_sd3_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(controlnet_sd3_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(controlnet_sd3_prefs['init_image']):
                init_img = PILImage.open(controlnet_sd3_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {controlnet_sd3_prefs['init_image']}")
                return
        width, height = init_img.size
        width, height = scale_dimensions(width, height, controlnet_sd3_prefs['max_size'])
        if bool(controlnet_sd3_prefs['alpha_mask']):
            init_img = init_img.convert("RGBA")
        else:
            init_img = init_img.convert("RGB")
        init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        if not bool(controlnet_sd3_prefs['mask_image']) and bool(controlnet_sd3_prefs['alpha_mask']):
            mask_img = init_img.convert('RGBA')
            red, green, blue, alpha = PILImage.Image.split(init_img)
            mask_img = alpha.convert('L')
        elif bool(controlnet_sd3_prefs['mask_image']):
            if controlnet_sd3_prefs['mask_image'].startswith('http'):
                mask_img = PILImage.open(requests.get(controlnet_sd3_prefs['mask_image'], stream=True).raw)
            else:
                if os.path.isfile(controlnet_sd3_prefs['mask_image']):
                    mask_img = PILImage.open(controlnet_sd3_prefs['mask_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your mask_image {controlnet_sd3_prefs['mask_image']}")
                    return
            width, height = mask_img.size
            width, height = scale_dimensions(width, height, controlnet_sd3_prefs['max_size'])
            mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        if controlnet_sd3_prefs['invert_mask'] and not controlnet_sd3_prefs['alpha_mask']:
            from PIL import ImageOps
            mask_img = ImageOps.invert(mask_img.convert('RGB'))
    if use_ip_adapter:
        pipe_controlnet.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
        pipe_controlnet.set_ip_adapter_scale(controlnet_sd3_prefs['ip_adapter_strength'])
        if controlnet_sd3_prefs['ip_adapter_image'].startswith('http'):
            ip_adapter_image = PILImage.open(requests.get(controlnet_sd3_prefs['ip_adapter_image'], stream=True).raw)
        else:
            if os.path.isfile(controlnet_sd3_prefs['ip_adapter_image']):
                ip_adapter_image = PILImage.open(controlnet_sd3_prefs['ip_adapter_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your ip_adapter_image {controlnet_sd3_prefs['ip_adapter_image']}")
                return
        ip_adapter_image = ImageOps.exif_transpose(ip_adapter_image).convert("RGB")
        status['loaded_ip_adapter'] = ip_adapter_model
        ip_adapter_args = {'ip_adapter_image': ip_adapter_image}
    else:
        ip_adapter_args = {}
    clear_last()
    prt(f"Generating ControlNet-SD3 {controlnet_sd3_prefs['control_task']} of your Image...")
    batch_output = os.path.join(stable_dir, controlnet_sd3_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], controlnet_sd3_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for pr in controlnet_sd3_prompts:
        prt(progress)
        autoscroll(False)
        filename = f"{controlnet_sd3_prefs['file_prefix']}{format_filename(pr['prompt'])}"
        filename = filename[:int(prefs['file_max_length'])]
        if len(controlnet_sd3_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_sd3_prefs['use_init_video']:
            original_img = []
            for c in controlnet_sd3_prefs['multi_controlnets']:
                original_img.append(prep_image(c['control_task'], c['original_image']))
                if controlnet_sd3_prefs['show_processed_image']:
                    w, h = original_img[-1].size
                    src_base64 = pil_to_base64(original_img[-1])
                    prt(Row([ImageButton(src_base64=src_base64, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))
        elif not controlnet_sd3_prefs['use_init_video']:
            original_img = prep_image(controlnet_sd3_prefs['control_task'], pr['original_image'])
            if controlnet_sd3_prefs['show_processed_image']:
                w, h = original_img.size
                src_base64 = pil_to_base64(original_img)
                prt(Row([ImageButton(src_base64=src_base64, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))
        else:
            video_img = prep_video(pr['original_image'])
            latents = torch.randn((1, 4, 64, 64), device="cuda", dtype=torch.float16).repeat(len(video_img), 1, 1, 1)
        try:
            random_seed = get_seed(pr['seed'])
            generator = torch.Generator(device="cpu").manual_seed(random_seed)
            if controlnet_type == "text2image":
                if not controlnet_sd3_prefs['use_init_video']:
                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], control_image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], num_inference_steps=controlnet_sd3_prefs['steps'], guidance_scale=controlnet_sd3_prefs['guidance_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_images_per_prompt=controlnet_sd3_prefs['batch_size'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images
                else:
                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_sd3_prefs['steps'], guidance_scale=controlnet_sd3_prefs['guidance_scale'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images
            elif controlnet_type == "image2image":
                if not controlnet_sd3_prefs['use_init_video']:
                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=init_img, control_image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_sd3_prefs['steps'], guidance_scale=controlnet_sd3_prefs['guidance_scale'], num_images_per_prompt=controlnet_sd3_prefs['batch_size'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images
                else:
                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=init_img, control_image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_sd3_prefs['steps'], guidance_scale=controlnet_sd3_prefs['guidance_scale'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images
            elif controlnet_type == "inpaint":
                if not controlnet_sd3_prefs['use_init_video']:
                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=init_img, mask=mask_img, control_image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_sd3_prefs['steps'], guidance_scale=controlnet_sd3_prefs['guidance_scale'], num_images_per_prompt=controlnet_sd3_prefs['batch_size'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images
                else:
                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=init_img, mask=mask_img, control_image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_sd3_prefs['steps'], guidance_scale=controlnet_sd3_prefs['guidance_scale'], height=height, width=width, generator=generator, callback_on_step_end=callback_fnc, **ip_adapter_args).images
        except Exception as e:
            #clear_last()
            clear_last()
            alert_msg(page, f"ERROR Generating ControlNet-SD3 {controlnet_sd3_prefs['control_task']}...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            flush()
            return
        #clear_pipes('controlnet')
        clear_last()
        #clear_last()
        autoscroll(True)
        #filename = pr['original_image'].rpartition(slash)[2].rpartition('.')[0]
        #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
        num = 0
        for image in images:
            random_seed += num
            fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
            image_path = available_file(os.path.join(stable_dir, controlnet_sd3_prefs['batch_folder_name']), fname, num)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            #PILImage.fromarray(image).save(image_path)
            image.save(image_path)
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)
            new_file = available_file(batch_output, fname, num)
            if not controlnet_sd3_prefs['display_upscaled_image'] or not controlnet_sd3_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=unscaled_path, data=new_file, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if controlnet_sd3_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=controlnet_sd3_prefs["enlarge_scale"])
                image_path = upscaled_path
            if prefs['save_image_metadata']:
                task = and_list(controlnet_sd3_prefs['control_task']) if isinstance(controlnet_sd3_prefs['control_task'], list) else controlnet_sd3_prefs['control_task']
                img = PILImage.open(image_path)
                metadata = PngInfo()
                metadata.add_text("artist", prefs['meta_ArtistName'])
                metadata.add_text("copyright", prefs['meta_Copyright'])
                metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {controlnet_sd3_prefs['enlarge_scale']}x with ESRGAN" if controlnet_sd3_prefs['apply_ESRGAN_upscale'] else "")
                metadata.add_text("pipeline", "ControlNet-SD3 " + task)
                if prefs['save_config_in_metadata']:
                  config_json = controlnet_sd3_prefs.copy()
                  config_json['model_path'] = model_path
                  config_json['seed'] = random_seed
                  config_json['prompt'] = pr['prompt']
                  config_json['negative_prompt'] = pr['negative_prompt']
                  del config_json['batch_size']
                  del config_json['max_size']
                  del config_json['display_upscaled_image']
                  del config_json['batch_folder_name']
                  if not config_json['apply_ESRGAN_upscale']:
                    del config_json['enlarge_scale']
                    del config_json['apply_ESRGAN_upscale']
                  metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                img.save(image_path, pnginfo=metadata)
            #TODO: PyDrive
            if storage_type == "Colab Google Drive":
                #new_file = available_file(output_path, fname, num)
                #out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                #new_file = available_file(output_path, fname, num)
                #out_path = new_file
                shutil.copy(image_path, new_file)
            if controlnet_sd3_prefs['display_upscaled_image']:
                prt(Row([ImageButton(src=new_file, data=new_file, width=width * float(controlnet_sd3_prefs["enlarge_scale"]), height=height * float(controlnet_sd3_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
            num += 1
    autoscroll(False)
    del hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle
    play_snd(Snd.ALERT, page)

def run_controlnet_xs(page, from_list=False):
    global controlnet_xs_prefs, prefs, status, pipe_controlnet, controlnet, controlnet_xs_models
    if not check_diffusers(page): return
    if not bool(controlnet_xs_prefs['original_image']) and len(controlnet_xs_prefs['multi_controlnets']) == 0:
      alert_msg(page, "You must provide the Original Image to process...")
      return
    if not bool(controlnet_xs_prefs['prompt']) and not from_list:
      alert_msg(page, "You must provide a Prompt to paint in your image...")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.ControlNetXS.controls.append(line)
        #page.controlnet_xs_output.controls.append(line)
        if update:
          page.ControlNetXS.update()
          #page.controlnet_xs_output.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.ControlNetXS, lines=lines)
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.ControlNetXS.controls = page.ControlNetXS.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
      else:
        page.ControlNetXS.auto_scroll = scroll
        page.ControlNetXS.update()
    progress = ProgressBar(bar_height=8)
    total_steps = controlnet_xs_prefs['steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None: #(pipe, step, timestep, callback_kwargs):
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    controlnet_xs_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        control = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'] if bool(p['negative_prompt']) else controlnet_xs_prefs['negative_prompt'], 'original_image': p['init_image'] if bool(p['init_image']) else controlnet_xs_prefs['original_image'], 'conditioning_scale': controlnet_xs_prefs['conditioning_scale'], 'control_guidance_start': controlnet_xs_prefs['control_guidance_start'], 'control_guidance_end': controlnet_xs_prefs['control_guidance_end'], 'seed': p['seed']}
        controlnet_xs_prompts.append(control)
      page.tabs.selected_index = 4
      page.tabs.update()
      #page.controlnet_xs_output.controls.clear()
    else:
      if not bool(controlnet_xs_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      original = controlnet_xs_prefs['original_image']
      conditioning_scale = controlnet_xs_prefs['conditioning_scale']
      control_guidance_start = controlnet_xs_prefs['control_guidance_start']
      control_guidance_end = controlnet_xs_prefs['control_guidance_end']
      if len(controlnet_xs_prefs['multi_controlnets']) > 0:
        original = []
        conditioning_scale = []
        control_guidance_start = []
        control_guidance_end = []
        for c in controlnet_xs_prefs['multi_controlnets']:
          original.append(c['original_image'])
          conditioning_scale.append(c['conditioning_scale'])
          control_guidance_start.append(c['control_guidance_start'])
          control_guidance_end.append(c['control_guidance_end'])
      control = {'prompt':controlnet_xs_prefs['prompt'], 'negative_prompt': controlnet_xs_prefs['negative_prompt'], 'original_image': original, 'conditioning_scale': conditioning_scale, 'control_guidance_start':control_guidance_start, 'control_guidance_end': control_guidance_end, 'seed': controlnet_xs_prefs['seed']}
      if controlnet_xs_prefs['use_init_video']:
        control['init_video'] = controlnet_xs_prefs['init_video']
        control['start_time'] = controlnet_xs_prefs['start_time']
        control['end_time'] = controlnet_xs_prefs['end_time']
        control['fps'] = controlnet_xs_prefs['fps']
      controlnet_xs_prompts.append(control)
      #page.controlnet_xs_output.controls.clear()
    autoscroll(True)
    clear_list()
    prt(Divider(thickness=2, height=4))
    installer = Installing("Installing ControlNet-XS Packages...")
    prt(installer)
    if status['loaded_controlnet'] == controlnet_xs_prefs["control_task"]:
        clear_pipes('controlnet')
    else:
        clear_pipes()
    import requests
    from io import BytesIO
    from PIL import ImageOps
    from PIL.PngImagePlugin import PngInfo
    try:
        try:
          from controlnet_aux import MLSDdetector
        except ModuleNotFoundError:
          installer.status("...installing controlnet-aux")
          run_sp("pip install --upgrade controlnet-aux", realtime=False)
          #run_sp("pip install git+https://github.com/patrickvonplaten/controlnet_aux.git")
          pass
        from controlnet_aux import MLSDdetector
        from controlnet_aux import OpenposeDetector
        from diffusers import StableDiffusionXLControlNetXSPipeline, StableDiffusionControlNetXSPipeline, ControlNetXSModel, ControlNetModel, AutoencoderKL
        #run_sp("pip install scikit-image", realtime=False)
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR Installing Required Packages...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        flush()
        return
    canny_checkpoint = "UmerHA/ConrolNetXS-SD2.1-canny"
    canny_SDXL_checkpoint = "UmerHA/ConrolNetXS-SDXL-canny"
    depth_checkpoint = "UmerHA/ConrolNetXS-SD2.1-depth"
    depth_SDXL_checkpoint = "UmerHA/ConrolNetXS-SDXL-depth"
    hed = None
    openpose = None
    depth_estimator = None
    feature_extractor = None
    mlsd = None
    image_processor = None
    image_segmentor = None
    normal = None
    lineart = None
    shuffle = None
    original_img = None
    def get_controlnet(task):
        nonlocal hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle
        if controlnet_xs_models[task] != None:
            return controlnet_xs_models[task]
        if "Canny Map" in task or task == "Video Canny Edge":
            controlnet_xs_models[task] = ControlNetXSModel.from_pretrained(canny_SDXL_checkpoint if controlnet_xs_prefs['use_SDXL'] else canny_checkpoint, torch_dtype=torch.float16).to(torch_device)
            task = "Canny Map Edge"
        elif "Marigold" in task:
            import diffusers
            depth_estimator = diffusers.MarigoldDepthPipeline.from_pretrained("prs-eth/marigold-lcm-v1-0", torch_dtype=torch.float16, variant="fp16").to(torch_device)
            controlnet_xs_models[task] = ControlNetXSModel.from_pretrained(depth_SDXL_checkpoint if controlnet_xs_prefs['use_SDXL'] else depth_checkpoint, variant="fp16", use_safetensors=True, torch_dtype=torch.float16).to(torch_device)
        elif "Depth" in task:
            from transformers import DPTFeatureExtractor, DPTForDepthEstimation
            depth_estimator = DPTForDepthEstimation.from_pretrained("Intel/dpt-hybrid-midas").to("cuda")
            feature_extractor = DPTFeatureExtractor.from_pretrained("Intel/dpt-hybrid-midas")
            controlnet_xs_models[task] = ControlNetXSModel.from_pretrained(depth_SDXL_checkpoint if controlnet_xs_prefs['use_SDXL'] else depth_checkpoint, variant="fp16", use_safetensors=True, torch_dtype=torch.float16).to(torch_device)
        return controlnet_xs_models[task]
    width, height = 0, 0
    def resize_for_condition_image(input_image: PILImage, resolution: int):
        input_image = input_image.convert("RGB")
        W, H = input_image.size
        k = float(resolution) / min(H, W)
        H *= k
        W *= k
        H = int(round(H / 64.0)) * 64
        W = int(round(W / 64.0)) * 64
        img = input_image.resize((W, H), resample=PILImage.Resampling.LANCZOS)
        return img
    def prep_image(task, img):
        nonlocal hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle
        nonlocal width, height
        if isinstance(img, str):
          if img.startswith('http'):
              #response = requests.get(controlnet_xs_prefs['original_image'])
              #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
              original_img = PILImage.open(requests.get(img, stream=True).raw)
          else:
              if os.path.isfile(img):
                  original_img = PILImage.open(img)
              else:
                  alert_msg(page, f"ERROR: Couldn't find your original_image {img}")
                  return
          width, height = original_img.size
          width, height = scale_dimensions(width, height, controlnet_xs_prefs['max_size'])
          #print(f"Size: {width}x{height}")
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        #return original_img
        try:
            if 'Canny' in task: # == "Canny Map Edge" or task == "Video Canny Edge":
                input_image = np.array(original_img)
                input_image = cv2.Canny(input_image, controlnet_xs_prefs['low_threshold'], controlnet_xs_prefs['high_threshold'])
                input_image = input_image[:, :, None]
                input_image = np.concatenate([input_image, input_image, input_image], axis=2)
                original_img = PILImage.fromarray(input_image)
            elif "Marigold Depth" in task:
                random_seed = get_seed(controlnet_xs_prefs['seed'])
                generator = torch.Generator(device=torch_device).manual_seed(random_seed)
                input_image = depth_estimator(original_img, generator=generator).prediction
                input_image = depth_estimator.image_processor.visualize_depth(input_image, color_map="binary")
                original_img = input_image[0]
            elif "Depth" in task:
                original_img = feature_extractor(images=original_img, return_tensors="pt").pixel_values.to("cuda")
                with torch.no_grad(), torch.autocast("cuda"):
                    depth_map = depth_estimator(original_img).predicted_depth
                depth_map = torch.nn.functional.interpolate(
                    depth_map.unsqueeze(1),
                    size=(1024, 1024),
                    mode="bicubic",
                    align_corners=False,
                )
                depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)
                depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)
                depth_map = (depth_map - depth_min) / (depth_max - depth_min)
                original_img = torch.cat([depth_map] * 3, dim=1)
                original_img = original_img.permute(0, 2, 3, 1).cpu().numpy()[0]
                original_img = PILImage.fromarray((original_img * 255.0).clip(0, 255).astype(np.uint8))
            return original_img
        except Exception as e:
            #clear_last()
            clear_last()
            alert_msg(page, f"ERROR Preparing ControlNet-XL {controlnet_xs_prefs['control_task']} Input Image...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            flush()
            return
    def prep_video(vid):
        nonlocal width, height
        if vid.startswith('http'):
            init_vid = download_file(vid, stable_dir)
        else:
            if os.path.isfile(vid):
                init_vid = vid
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_video {vid}")
                return
        try:
            start_time = float(controlnet_xs_prefs['start_time'])
            end_time = float(controlnet_xs_prefs['end_time'])
            fps = int(controlnet_xs_prefs['fps'])
            max_size = controlnet_xs_prefs['max_size']
        except Exception:
            alert_msg(page, "Make sure your Numbers are actual numbers...")
            return
        prt("Extracting Frames from Video Clip")
        try:
            cap = cv2.VideoCapture(init_vid)
        except Exception as e:
            alert_msg(page, "ERROR Reading Video File. May be Incompatible Format...")
            clear_last()
            return
        count = 0
        video = []
        frames = []
        width = height = 0
        cap.set(cv2.CAP_PROP_FPS, fps)
        video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)
        start_frame = int(start_time * fps)
        if end_time == 0 or end_time == 0.0:
            end_frame = int(video_length)
        else:
            end_frame = int(end_time * fps)
        total = end_frame - start_frame
        for i in range(start_frame, end_frame):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            success, image = cap.read()
            if success:
                #filename = os.path.join(output_dir, f'{file_prefix}{count}.png')
                if width == 0:
                    shape = image.shape
                    width, height = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)
                image = cv2.resize(image, (width, height), interpolation = cv2.INTER_AREA)
                #cv2.imwrite(os.path.join(output_dir, filename), image)
                image = prep_image(controlnet_xs_prefs['control_task'], PILImage.fromarray(image))
                video.append(image)
                count += 1
        cap.release()
        clear_last()
        return video
    loaded_controlnet = None
    if len(controlnet_xs_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_xs_prefs['use_init_video']:
        controlnet = []
        loaded_controlnet = []
        for c in controlnet_xs_prefs['multi_controlnets']:
            controlnet.append(get_controlnet(c['control_task']))
            loaded_controlnet.append(c['control_task'])
        if len(controlnet) == 1:
            controlnet = controlnet[0]
            loaded_controlnet = loaded_controlnet[0]
    else:
        controlnet = get_controlnet(controlnet_xs_prefs['control_task'])
        loaded_controlnet = controlnet_xs_prefs['control_task']
    for k, v in controlnet_xs_models.items():
      if v != None and k in loaded_controlnet:
        del v
        controlnet_xs_models[k] = None
    controlnet_type = "text2image"
    if controlnet_xs_prefs['use_image2image']:
        if bool(controlnet_xs_prefs['init_image']):
            if bool(controlnet_xs_prefs['mask_image']) or controlnet_xs_prefs['alpha_mask']:
                controlnet_type = "inpaint"
            else:
                controlnet_type = "image2image"
    use_ip_adapter = controlnet_xs_prefs['use_ip_adapter']
    if use_ip_adapter:
        ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == controlnet_xs_prefs['ip_adapter_SDXL_model'])
    else:
        ip_adapter_model = None
    
    #model = get_model(prefs['model_ckpt'])
    model_path = "stabilityai/stable-diffusion-xl-base-1.0" if controlnet_xs_prefs['use_SDXL'] else "stabilityai/stable-diffusion-2-1" 
    if controlnet_type != status['loaded_controlnet_type'] or model_path != status['loaded_model']:
        clear_pipes()
    
    if pipe_controlnet == None or status['loaded_controlnet'] != controlnet_xs_prefs["control_task"]:
        if controlnet_xs_prefs['use_SDXL']:
            vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)
            if controlnet_type == "text2image":
                pipe_controlnet = StableDiffusionXLControlNetXSPipeline.from_pretrained(model_path, controlnet=controlnet, vae=vae, safety_checker=None, variant="fp16", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            elif controlnet_type == "image2image":
                from diffusers import StableDiffusionXLControlNetImg2ImgPipeline
                pipe_controlnet = StableDiffusionXLControlNetImg2ImgPipeline.from_pretrained(model_path, controlnet=controlnet, vae=vae, safety_checker=None, variant="fp16", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            elif controlnet_type == "inpaint":
                from diffusers import StableDiffusionXLControlNetInpaintPipeline
                pipe_controlnet = StableDiffusionXLControlNetInpaintPipeline.from_pretrained(model_path, controlnet=controlnet, vae=vae, safety_checker=None, variant="fp16", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_controlnet.enable_model_cpu_offload()
            #pipe_controlnet = optimize_SDXL(pipe_controlnet, vae_slicing=True, vae_tiling=True)
        else:
            if controlnet_type == "text2image":
                pipe_controlnet = StableDiffusionControlNetXSPipeline.from_pretrained(model_path, controlnet=controlnet, safety_checker=None, variant="fp16", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            elif controlnet_type == "image2image":
                from diffusers import StableDiffusionControlNetImg2ImgPipeline
                pipe_controlnet = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(model_path, controlnet=controlnet, safety_checker=None, variant="fp16", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            elif controlnet_type == "inpaint":
                from diffusers import StableDiffusionControlNetInpaintPipeline
                pipe_controlnet = StableDiffusionControlNetInpaintPipeline.from_pretrained(model_path, controlnet=controlnet, safety_checker=None, variant="fp16", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_controlnet.enable_model_cpu_offload()
            #pipe_controlnet = optimize_SDXL(pipe_controlnet, vae_slicing=True, vae_tiling=True)
        if prefs['enable_torch_compile']:
            installer.status(f"...Torch compiling unet")
            pipe_controlnet.to(memory_format=torch.channels_last)
            pipe_controlnet.unet.to(memory_format=torch.channels_last)
            pipe_controlnet.unet = torch.compile(pipe_controlnet.unet, mode="reduce-overhead", fullgraph=True)
            pipe_controlnet.controlnet.to(memory_format=torch.channels_last)
            pipe_controlnet.controlnet = torch.compile(pipe_controlnet.unet, mode="reduce-overhead", fullgraph=True)
        elif controlnet_xs_prefs['cpu_offload']:
            pipe_controlnet.enable_model_cpu_offload()
        else:
            pipe_controlnet.to("cuda")
        status['loaded_controlnet'] = loaded_controlnet #controlnet_xs_prefs["control_task"]
        status['loaded_controlnet_type'] = controlnet_type
        status['loaded_model'] = model_path
    pipe_controlnet = pipeline_scheduler(pipe_controlnet)
    if controlnet_xs_prefs['use_init_video']:
        from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor
        pipe_controlnet.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))
        pipe_controlnet.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))
    init_img = None
    mask_img = None
    if bool(controlnet_xs_prefs['init_image'] and controlnet_xs_prefs['use_image2image']):
        if controlnet_xs_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(controlnet_xs_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(controlnet_xs_prefs['init_image']):
                init_img = PILImage.open(controlnet_xs_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {controlnet_xs_prefs['init_image']}")
                return
        width, height = init_img.size
        width, height = scale_dimensions(width, height, controlnet_xs_prefs['max_size'])
        if bool(controlnet_xs_prefs['alpha_mask']):
            init_img = init_img.convert("RGBA")
        else:
            init_img = init_img.convert("RGB")
        init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        if not bool(controlnet_xs_prefs['mask_image']) and bool(controlnet_xs_prefs['alpha_mask']):
            mask_img = init_img.convert('RGBA')
            red, green, blue, alpha = PILImage.Image.split(init_img)
            mask_img = alpha.convert('L')
        elif bool(controlnet_xs_prefs['mask_image']):
            if controlnet_xs_prefs['mask_image'].startswith('http'):
                mask_img = PILImage.open(requests.get(controlnet_xs_prefs['mask_image'], stream=True).raw)
            else:
                if os.path.isfile(controlnet_xs_prefs['mask_image']):
                    mask_img = PILImage.open(controlnet_xs_prefs['mask_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your mask_image {controlnet_xs_prefs['mask_image']}")
                    return
            width, height = mask_img.size
            width, height = scale_dimensions(width, height, controlnet_xs_prefs['max_size'])
            mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        if controlnet_xs_prefs['invert_mask'] and not controlnet_xs_prefs['alpha_mask']:
            from PIL import ImageOps
            mask_img = ImageOps.invert(mask_img.convert('RGB'))
    if use_ip_adapter:
        pipe_controlnet.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
        pipe_controlnet.set_ip_adapter_scale(controlnet_xs_prefs['ip_adapter_strength'])
        if controlnet_xs_prefs['ip_adapter_image'].startswith('http'):
            ip_adapter_image = PILImage.open(requests.get(controlnet_xs_prefs['ip_adapter_image'], stream=True).raw)
        else:
            if os.path.isfile(controlnet_xs_prefs['ip_adapter_image']):
                ip_adapter_image = PILImage.open(controlnet_xs_prefs['ip_adapter_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your ip_adapter_image {controlnet_xs_prefs['ip_adapter_image']}")
                return
        ip_adapter_image = ImageOps.exif_transpose(ip_adapter_image).convert("RGB")
        status['loaded_ip_adapter'] = ip_adapter_model
        ip_adapter_args = {'ip_adapter_image': ip_adapter_image}
    else:
        ip_adapter_args = {}
    clear_last()
    prt(f"Generating ControlNet-XS {controlnet_xs_prefs['control_task']} of your Image...")
    batch_output = os.path.join(stable_dir, controlnet_xs_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], controlnet_xs_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for pr in controlnet_xs_prompts:
        prt(progress)
        autoscroll(False)
        filename = f"{controlnet_xs_prefs['file_prefix']}{format_filename(pr['prompt'])}"
        filename = filename[:int(prefs['file_max_length'])]
        if len(controlnet_xs_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_xs_prefs['use_init_video']:
            original_img = []
            for c in controlnet_xs_prefs['multi_controlnets']:
                original_img.append(prep_image(c['control_task'], c['original_image']))
                if controlnet_xs_prefs['show_processed_image']:
                    processed_img = available_file(batch_output, f"{filename}-{c['control_task'].partition(' ')[0]}", 0, no_num=True)
                    w, h = original_img[-1].size
                    original_img[-1].save(processed_img)
                    prt(Row([ImageButton(src=processed_img, data=processed_img, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))
        elif not controlnet_xs_prefs['use_init_video']:
            original_img = prep_image(controlnet_xs_prefs['control_task'], pr['original_image'])
            if controlnet_xs_prefs['show_processed_image']:
                processed_img = available_file(batch_output, f"{filename}-{controlnet_xs_prefs['control_task'].partition(' ')[0]}", 0, no_num=True)
                w, h = original_img.size
                original_img.save(processed_img)
                prt(Row([ImageButton(src=processed_img, data=processed_img, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))
        else:
            video_img = prep_video(pr['original_image'])
            latents = torch.randn((1, 4, 64, 64), device="cuda", dtype=torch.float16).repeat(len(video_img), 1, 1, 1)
   
        try:
            random_seed = get_seed(pr['seed'])
            generator = torch.Generator(device="cpu").manual_seed(random_seed)
            if controlnet_type == "text2image":
                if not controlnet_xs_prefs['use_init_video']:
                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xs_prefs['steps'], guidance_scale=controlnet_xs_prefs['guidance_scale'], eta=controlnet_xs_prefs['eta'], num_images_per_prompt=controlnet_xs_prefs['batch_size'], height=height, width=width, generator=generator, callback=callback_fnc, **ip_adapter_args).images
                else:
                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xs_prefs['steps'], guidance_scale=controlnet_xs_prefs['guidance_scale'], eta=controlnet_xs_prefs['eta'], height=height, width=width, generator=generator, callback=callback_fnc, **ip_adapter_args).images
            elif controlnet_type == "image2image":
                if not controlnet_xs_prefs['use_init_video']:
                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=init_img, control_image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xs_prefs['steps'], guidance_scale=controlnet_xs_prefs['guidance_scale'], eta=controlnet_xs_prefs['eta'], num_images_per_prompt=controlnet_xs_prefs['batch_size'], height=height, width=width, generator=generator, callback=callback_fnc, **ip_adapter_args).images
                else:
                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=init_img, control_image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xs_prefs['steps'], guidance_scale=controlnet_xs_prefs['guidance_scale'], eta=controlnet_xs_prefs['eta'], height=height, width=width, generator=generator, callback=callback_fnc, **ip_adapter_args).images
            elif controlnet_type == "inpaint":
                if not controlnet_xs_prefs['use_init_video']:
                    images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=init_img, mask=mask_img, control_image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xs_prefs['steps'], guidance_scale=controlnet_xs_prefs['guidance_scale'], eta=controlnet_xs_prefs['eta'], num_images_per_prompt=controlnet_xs_prefs['batch_size'], height=height, width=width, generator=generator, callback=callback_fnc, **ip_adapter_args).images
                else:
                    images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=init_img, mask=mask_img, control_image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xs_prefs['steps'], guidance_scale=controlnet_xs_prefs['guidance_scale'], eta=controlnet_xs_prefs['eta'], height=height, width=width, generator=generator, callback=callback_fnc, **ip_adapter_args).images
        except Exception as e:
            #clear_last()
            clear_last()
            alert_msg(page, f"ERROR Generating ControlNet-XL {controlnet_xs_prefs['control_task']}...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            flush()
            return
        clear_pipes('controlnet')
        clear_last()
        #clear_last()
        autoscroll(True)
        #filename = pr['original_image'].rpartition(slash)[2].rpartition('.')[0]
        
        #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
        num = 0
        for image in images:
            random_seed += num
            fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
            image_path = available_file(os.path.join(stable_dir, controlnet_xs_prefs['batch_folder_name']), fname, num)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            #PILImage.fromarray(image).save(image_path)
            image.save(image_path)
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)
            new_file = available_file(batch_output, fname, num)
            if not controlnet_xs_prefs['display_upscaled_image'] or not controlnet_xs_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=unscaled_path, data=new_file, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if controlnet_xs_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=controlnet_xs_prefs["enlarge_scale"])
                image_path = upscaled_path
            if prefs['save_image_metadata']:
                task = and_list(controlnet_xs_prefs['control_task']) if isinstance(controlnet_xs_prefs['control_task'], list) else controlnet_xs_prefs['control_task']
                img = PILImage.open(image_path)
                metadata = PngInfo()
                metadata.add_text("artist", prefs['meta_ArtistName'])
                metadata.add_text("copyright", prefs['meta_Copyright'])
                metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {controlnet_xs_prefs['enlarge_scale']}x with ESRGAN" if controlnet_xs_prefs['apply_ESRGAN_upscale'] else "")
                metadata.add_text("pipeline", "ControlNet-XS " + task)
                if prefs['save_config_in_metadata']:
                  config_json = controlnet_xs_prefs.copy()
                  config_json['model_path'] = model_path
                  config_json['seed'] = random_seed
                  config_json['prompt'] = pr['prompt']
                  config_json['negative_prompt'] = pr['negative_prompt']
                  del config_json['batch_size']
                  del config_json['max_size']
                  del config_json['display_upscaled_image']
                  del config_json['batch_folder_name']
                  if not config_json['apply_ESRGAN_upscale']:
                    del config_json['enlarge_scale']
                    del config_json['apply_ESRGAN_upscale']
                  metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                img.save(image_path, pnginfo=metadata)
            #TODO: PyDrive
            if storage_type == "Colab Google Drive":
                #new_file = available_file(output_path, fname, num)
                #out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                #new_file = available_file(output_path, fname, num)
                #out_path = new_file
                shutil.copy(image_path, new_file)
            if controlnet_xs_prefs['display_upscaled_image']:
                prt(Row([ImageButton(src=new_file, data=new_file, width=width * float(controlnet_xs_prefs["enlarge_scale"]), height=height * float(controlnet_xs_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
            num += 1
    autoscroll(False)
    del hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle
    play_snd(Snd.ALERT, page)


def run_controlnet_tile_upscale(page, source_image, prompt="best quality", scale_factor=2.5):
    global controlnet_prefs, prefs, status, pipe_controlnet, controlnet, controlnet_models
    if not check_diffusers(page): return
    from diffusers import ControlNetModel, DiffusionPipeline

    def resize_for_condition_image(input_image: PILImage, resolution: int):
        input_image = input_image.convert("RGB")
        W, H = input_image.size
        k = float(resolution) / min(H, W)
        H *= k
        W *= k
        H = int(round(H / 64.0)) * 64
        W = int(round(W / 64.0)) * 64
        img = input_image.resize((W, H), resample=Image.LANCZOS)
        return img

    controlnet = ControlNetModel.from_pretrained('lllyasviel/control_v11f1e_sd15_tile',
                                                torch_dtype=torch.float16)
    pipe_controlnet = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5",
                                            custom_pipeline="stable_diffusion_controlnet_img2img",
                                            controlnet=controlnet,
                                            torch_dtype=torch.float16).to('cuda')
    pipe_controlnet.enable_xformers_memory_efficient_attention()
    if isinstance(source_image, str):
        from diffusers.utils import load_image
        source_image = load_image(source_image)

    condition_image = resize_for_condition_image(source_image, 1024)
    image = pipe_controlnet(prompt=prompt,
                negative_prompt="blur, lowres, bad anatomy, bad hands, cropped, worst quality",
                image=condition_image,
                controlnet_conditioning_image=condition_image,
                width=condition_image.size[0],
                height=condition_image.size[1],
                strength=1.0,
                generator=torch.manual_seed(0),
                num_inference_steps=32,
                ).images[0]
    return image

def run_controlnet_video2video(page):
    global controlnet_video2video_prefs, status
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.ControlNet_Video2Video.controls.append(line)
      page.ControlNet_Video2Video.update()
    def clear_last(lines=1):
      clear_line(page.ControlNet_Video2Video, lines=lines)
    def autoscroll(scroll=True):
        page.ControlNet_Video2Video.auto_scroll = scroll
        page.ControlNet_Video2Video.update()
    if not bool(controlnet_video2video_prefs['init_video']):
        alert_msg(page, "You must provide a target init video...")
        return
    if not bool(controlnet_video2video_prefs['prompt']):
        alert_msg(page, "You must provide an interesting prompt to guide the video...")
        return
    page.ControlNet_Video2Video.controls = page.ControlNet_Video2Video.controls[:1]
    autoscroll()
    installer = Installing("Installing ControlNet Video2Video Libraries...")
    prt(installer)
    controlnet_video2video_dir = os.path.join(root_dir, "controlnetvideo")
    if not os.path.exists(controlnet_video2video_dir):
        try:
            installer.status("...cloning un1tz3r0/controlnetvideo.git")
            run_sp("git clone https://github.com/un1tz3r0/controlnetvideo.git", cwd=root_dir, realtime=False)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing ControlNet Video Requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    pip_install("ffmpeg opencv-python|cv2 click moviepy mediapipe controlnet-aux|controlnet_aux watchdog", q=True, installer=installer)
    try:
        import xformers
    except ModuleNotFoundError:
        installer.status("...installing FaceBook's Xformers")
        #run_sp("pip install --pre -U triton", realtime=False)
        run_sp(f"pip install -U xformers=={'0.0.25' if upgrade_torch else '0.0.22.post7'} --index-url https://download.pytorch.org/whl/cu121", realtime=False)
        status['installed_xformers'] = True
        pass
    clear_pipes()

    from PIL import ImageOps
    if bool(controlnet_video2video_prefs['output_name']):
        fname = format_filename(controlnet_video2video_prefs['output_name'], force_underscore=True)
    elif bool(controlnet_video2video_prefs['prompt']):
        fname = format_filename(controlnet_video2video_prefs['prompt'], force_underscore=True)
    elif bool(controlnet_video2video_prefs['batch_folder_name']):
        fname = format_filename(controlnet_video2video_prefs['batch_folder_name'], force_underscore=True)
    else: fname = "output"
    if bool(controlnet_video2video_prefs['file_prefix']):
        fname = f"{controlnet_video2video_prefs['file_prefix']}{fname}"
    if bool(controlnet_video2video_prefs['batch_folder_name']):
        batch_output = os.path.join(stable_dir, controlnet_video2video_prefs['batch_folder_name'])
    else: batch_output = stable_dir
    makedir(batch_output)
    output_path = os.path.join(prefs['image_output'], controlnet_video2video_prefs['batch_folder_name'])
    makedir(output_path)
    init_vid = controlnet_video2video_prefs['init_video']
    if init_vid.startswith('http'):
        init_vid = download_file(init_vid, batch_output)
    else:
        if not os.path.isfile(init_vid):
            alert_msg(page, f"ERROR: Couldn't find your init_video {init_vid}")
            return
    clear_last()
    progress = ProgressBar(bar_height=8)
    prt(f"Generating your ControlNet Video...")
    #prt(progress)
    autoscroll(False)
    total_steps = controlnet_video2video_prefs['steps']
    def callback_fnc(step: int) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}"
      progress.update()
    output_file = available_file(output_path, fname, 0, ext='mp4', no_num=True)
    #output_file = os.path.join(output_path, f"{fname}{'.mp4' if is_video else '.png'}")
    cmd = f'python controlnetvideo.py "{init_vid}" --controlnet {controlnet_video2video_prefs["control_task"].lower()}'
    cmd += f' --prompt "{controlnet_video2video_prefs["prompt"]}"'
    if bool(controlnet_video2video_prefs["negative_prompt"]):
      cmd += f' --negative-prompt "{controlnet_video2video_prefs["negative_prompt"]}"'
    cmd += f" --prompt-strength {controlnet_video2video_prefs['prompt_strength']}"
    cmd += f" --num-inference-steps {controlnet_video2video_prefs['steps']}"
    cmd += f" --init-image-strength {controlnet_video2video_prefs['init_image_strength']}"
    cmd += f" --controlnet-strength {controlnet_video2video_prefs['controlnet_strength']}"
    cmd += f" --feedthrough-strength {controlnet_video2video_prefs['feedthrough_strength']}"
    cmd += f" --motion-alpha {controlnet_video2video_prefs['motion_alpha']} --motion-sigma {controlnet_video2video_prefs['motion_sigma']}"
    cmd += f" --color-amount {controlnet_video2video_prefs['color_amount']}"
    cmd += f" --color-fix {controlnet_video2video_prefs['color_fix'].lower()}"
    cmd += f" --start-time {controlnet_video2video_prefs['start_time']}"
    if controlnet_video2video_prefs["end_time"] != 0.0:
      cmd += f" --end-time {controlnet_video2video_prefs['end_time']}"
    if controlnet_video2video_prefs["duration"] != 0.0:
      cmd += f" --duration {controlnet_video2video_prefs['duration']}"
    if not controlnet_video2video_prefs["fix_orientation"]:
      cmd += f" --no-fix-orientation"
    cmd += f" --canny-low-thr {controlnet_video2video_prefs['low_threshold']} --canny-high-thr {controlnet_video2video_prefs['high_threshold']}"
    cmd += f" --mlsd-score-thr {controlnet_video2video_prefs['mlsd_score_thr']} --mlsd-dist-thr {controlnet_video2video_prefs['mlsd_dist_thr']}"
    cmd += f" --max-dimension {controlnet_video2video_prefs['max_dimension']} --min-dimension {controlnet_video2video_prefs['min_dimension']}"
    cmd += f" --round-dims-to {controlnet_video2video_prefs['round_dims_to']}"
    if controlnet_video2video_prefs['no_audio']: cmd += " --no-audio"
    if controlnet_video2video_prefs['skip_dumped_frames']: cmd += " --skip-dumped-frames"
    frames = os.path.join(batch_output, '{n:08d}.png') #TODO Add fname
    cmd += f" --dump-frames '{frames}'"
    cmd += f' "{output_file}"'
    w = 0
    h = 0
    img_idx = 0
    from watchdog.observers import Observer
    from watchdog.events import LoggingEventHandler, FileSystemEventHandler
    class Handler(FileSystemEventHandler):
      def __init__(self):
        super().__init__()
      def on_created(self, event):
        nonlocal img_idx, w, h
        if event.is_directory:
          return None
        elif event.event_type == 'created' and event.src_path.endswith("png"):
          autoscroll(True)
          if w == 0:
            time.sleep(0.8)
            try:
              frame = PILImage.open(event.src_path)
              w, h = frame.size
              clear_last()
            except Exception:
              pass
          clear_last()
          if controlnet_video2video_prefs['save_frames']:
            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])
          else:
            fpath = event.src_path
          #prt(Divider(height=6, thickness=2))
          prt(Row([ImageButton(src=event.src_path, data=fpath, width=w, height=h, subtitle=f"Frame {img_idx} - {event.src_path}", center=True, page=page)], alignment=MainAxisAlignment.CENTER))
          prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))
          page.update()
          prt(progress)
          if controlnet_video2video_prefs['save_frames']:
            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])
            shutil.copy(event.src_path, fpath)
          time.sleep(0.2)
          autoscroll(False)
          img_idx += 1
    image_handler = Handler()
    observer = Observer()
    observer.schedule(image_handler, batch_output, recursive=True)
    observer.start()
    prt(f"Running {cmd}")
    prt(progress)
    try:
        #os.system(f'cd {controlnet_video2video_dir};{cmd}')
        #if is_Colab:
        #  os.chdir(controlnet_video2video_dir)
        #  $cmd
        #  os.chdir(root_dir)
        #else:
        #TODO: Parse output to get percent current for progress callback_fnc
        run_sp(cmd, cwd=controlnet_video2video_dir, realtime=controlnet_video2video_prefs['show_console'])
    except Exception as e:
        clear_last()
        observer.stop()
        alert_msg(page, "Error running controlnetvideo.py!", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    clear_last()
    observer.stop()
    #clear_last()
    autoscroll(True)
    #TODO: Upscale Image
    if os.path.isfile(output_file):
        prt(Row([VideoContainer(output_file)], alignment=MainAxisAlignment.CENTER))
        #prt(Row([VideoPlayer(video_file=output_file, width=width, height=height)], alignment=MainAxisAlignment.CENTER))
    else:
        prt("Error Generating Output File! Maybe NSFW Image detected?")
    prt(Row([Text(output_file)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_deepfloyd(page, from_list=False):
    global deepfloyd_prefs, prefs, status, pipe_deepfloyd, pipe_deepfloyd2, pipe_deepfloyd3
    import torch
    #if not bool(deepfloyd_prefs['init_image']):
    #  alert_msg(page, "You must provide the Original Image and the Mask Image to process...")
    #  return
    if not bool(deepfloyd_prefs['prompt']):
      alert_msg(page, "You must provide a Text-to-Image Prompt...")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.DeepFloyd.controls.append(line)
        if update:
          page.DeepFloyd.update()
    def clear_last(lines=1, update=True):
      if from_list:
        clear_line(page.imageColumn, lines, update)
      else:
        clear_line(page.DeepFloyd, lines, update)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
      else:
        page.DeepFloyd.auto_scroll = scroll
        page.DeepFloyd.update()
    progress = ProgressBar(bar_height=8)
    total_steps = deepfloyd_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.DeepFloyd.controls = page.DeepFloyd.controls[:1]
    deepfloyd_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        df_prompt = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'init_image': p['init_image'] if bool(p['init_image']) else deepfloyd_prefs['init_image'], 'mask_image': p['mask_image'] if bool(p['mask_image']) else deepfloyd_prefs['mask_image'], 'image_strength': p['init_image_strength'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps': p['steps'], 'seed': p['seed']}
        deepfloyd_prompts.append(df_prompt)
      page.tabs.selected_index = 4
      page.tabs.update()
    else:
      if not bool(deepfloyd_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      df_prompt = {'prompt':deepfloyd_prefs['prompt'], 'negative_prompt': deepfloyd_prefs['negative_prompt'], 'init_image': deepfloyd_prefs['init_image'], 'mask_image': deepfloyd_prefs['mask_image'], 'image_strength': deepfloyd_prefs['image_strength'], 'guidance_scale':deepfloyd_prefs['guidance_scale'], 'num_inference_steps': deepfloyd_prefs['num_inference_steps'], 'seed': deepfloyd_prefs['seed']}
      deepfloyd_prompts.append(df_prompt)
    autoscroll(True)
    clear_list()
    prt(Divider(thickness=2, height=4))
    #if not status['installed_diffusers']:
    installer = Installing("Installing Diffusers & Required Packages...")
    prt(installer)
    try:
        import accelerate
        #TODO: Uninstall other version first
    except ImportError:
        installer.status("...Accelerate")
        run_process("pip install git+https://github.com/huggingface/accelerate.git", page=page)
        pass
    #if deepfloyd_prefs['low_memory']:
    #    pip_install("bitsandbytes", upgrade=True, installer=installer)
    try:
        import diffusers
        if force_update("diffusers"): raise ModuleNotFoundError("Forcing update")
    except ModuleNotFoundError:
        installer.status("...HuggingFace Diffusers")
        #run_process("pip install --upgrade diffusers~=0.16", page=page)
        #run_process("pip install --upgrade git+https://github.com/Skquark/diffusers.git", page=page)
        run_process("pip install --upgrade git+https://github.com/Skquark/diffusers.git@main#egg=diffusers[torch]", page=page)
        pass
    try:
        import transformers
        if force_update("transformers"): raise ModuleNotFoundError("Forcing update")
    except ModuleNotFoundError:
        installer.status("...Transformers")
        run_process("pip install -qq --upgrade git+https://github.com/huggingface/transformers", page=page)
        #run_process("pip install --upgrade transformers~=4.28", page=page)
        pass
    try:
        import safetensors
        from safetensors import safe_open
    except ModuleNotFoundError:
        installer.status("...SafeTensors v0.3")
        run_process("pip install --upgrade safetensors~=0.3", page=page)
        import safetensors
        from safetensors import safe_open
        pass
    try:
        import sentencepiece
    except ImportError:
        installer.status("...SentencePiece")
        run_sp("pip install --upgrade sentencepiece", realtime=True) #~=0.1
        import sentencepiece
        pass
    
    installer.set_message("Installing DeepFloyd IF Required Packages...")
    if deepfloyd_prefs['low_memory']:
        #clear_last(update=False)
        #prt(Installing("Installing DeepFloyd IF Required Packages..."))
        try:
          import bitsandbytes
        except ImportError:
          installer.status("...BitsandBytes")
          #os.environ['LD_LIBRARY_PATH'] += "/usr/lib/wsl/lib:$LD_LIBRARY_PATH"
          #run_sp("export LD_LIBRARY_PATH=/usr/lib/wsl/lib:$LD_LIBRARY_PATH", realtime=False)
          run_sp("pip install --upgrade bitsandbytes", realtime=True) #~=0.38
          import bitsandbytes
          pass
    try:
        import torch
    except ModuleNotFoundError:
        installer.status("...Torch v2.0")
        run_process("pip install --upgrade torch~=2.0", page=page)
        import torch
        pass
    try:
        from huggingface_hub import notebook_login, HfFolder, login
    except ModuleNotFoundError:
        installer.status("...HuggingFace Hub")
        run_process("pip install huggingface_hub --upgrade", page=page)
        from huggingface_hub import notebook_login, HfFolder, login
        pass
    if not os.path.exists(HfFolder.path_token):
        try:
          login(token=prefs['HuggingFace_api_key'], add_to_git_credential=True)
        except Exception:
          alert_msg(page, "ERROR Logging into HuggingFace... Check your API Key or Internet conenction.")
          return

    import requests, random
    from io import BytesIO
    from PIL import ImageOps
    from PIL.PngImagePlugin import PngInfo
    from diffusers import DiffusionPipeline
    from diffusers.utils import pt_to_pil
    from diffusers import IFPipeline, IFImg2ImgPipeline, IFInpaintingPipeline, IFSuperResolutionPipeline
    #from diffusers.pipelines.deepfloyd_if.safety_checker IFSafetyChecker
    from transformers import T5EncoderModel, T5Tokenizer
    from diffusers.models.attention_processor import AttnAddedKVProcessor
    #run_sp("accelerate config default", realtime=False)
    clear_pipes('deepfloyd')
    torch.cuda.empty_cache()
    #torch.cuda.reset_max_memory_allocated()
    #torch.cuda.reset_peak_memory_stats()
    if deepfloyd_prefs['model_size'].startswith("X"):
        model_id = "DeepFloyd/IF-I-XL-v1.0"
        model_id_II = "DeepFloyd/IF-II-XL-v1.0"
    elif deepfloyd_prefs['model_size'].startswith("L"):
        model_id = "DeepFloyd/IF-I-L-v1.0"
        model_id_II = "DeepFloyd/IF-II-L-v1.0"
    elif deepfloyd_prefs['model_size'].startswith("M"):
        model_id = "DeepFloyd/IF-I-M-v1.0"
        model_id_II = "DeepFloyd/IF-II-M-v1.0"
    clear_last(update=False)
    if 'last_deepfloyd_mode' not in status:
      status['last_deepfloyd_mode'] = ""
    max_size = deepfloyd_prefs['max_size']
    batch_output = os.path.join(stable_dir, deepfloyd_prefs['batch_folder_name'])
    output_dir = batch_output
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], deepfloyd_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)

    for pr in deepfloyd_prompts:
        init_img = None
        mask_img = None
        if bool(pr['init_image']):
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            width, height = init_img.size
            width, height = scale_dimensions(width, height, deepfloyd_prefs['max_size'])
            if bool(deepfloyd_prefs['alpha_mask']):
                init_img = init_img.convert("RGBA")
            else:
                init_img = init_img.convert("RGB")
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            if not bool(pr['mask_image']) and bool(deepfloyd_prefs['alpha_mask']):
                mask_img = init_img.convert('RGBA')
                red, green, blue, alpha = PILImage.Image.split(init_img)
                mask_img = alpha.convert('L')
            elif bool(pr['mask_image']):
                if pr['mask_image'].startswith('http'):
                    #response = requests.get(deepfloyd_prefs['init_image'])
                    #init_img = PILImage.open(BytesIO(response.content)).convert("RGB")
                    mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)
                else:
                    if os.path.isfile(pr['mask_image']):
                        mask_img = PILImage.open(pr['mask_image'])
                    else:
                        alert_msg(page, f"ERROR: Couldn't find your mask_image {pr['mask_image']}")
                        return
                width, height = mask_img.size
                width, height = scale_dimensions(width, height, deepfloyd_prefs['max_size'])
                mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        if deepfloyd_prefs['invert_mask'] and not deepfloyd_prefs['alpha_mask']:
            from PIL import ImageOps
            mask_img = ImageOps.invert(mask_img.convert('RGB'))
        for num in range(deepfloyd_prefs['num_images']):
            random_seed = get_seed(int(pr['seed']) + num)
            generator = torch.Generator(device="cpu").manual_seed(random_seed)
            try:
                installer = Installing("Running DeepFloyd-IF Text Encoder...")
                prt(installer)
                if deepfloyd_prefs['low_memory']:
                    #, load_in_8bit=True
                    installer.status("...text_encoder T5EncoderModel") #, variant="8bit"
                    text_encoder = T5EncoderModel.from_pretrained(model_id, subfolder="text_encoder", device_map="auto", load_in_8bit=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                    installer.status("...DiffusionPipeline")
                    pipe_deepfloyd = DiffusionPipeline.from_pretrained(model_id, text_encoder=text_encoder, unet=None, use_safetensors=True, device_map=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                    pipe_deepfloyd.unet.set_attn_processor(AttnAddedKVProcessor())
                    # Still getting errors here! WTF?
                    #images = pipe_deepfloyd(pr['prompt'], image=init_img, negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None, num_inference_steps=deepfloyd_prefs['num_inference_steps'], eta=deepfloyd_prefs['eta'], image_guidance_scale=deepfloyd_prefs['guidance_scale'], num_images_per_prompt=deepfloyd_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images
                    installer.status("...encode_prompts")
                    prompt_embeds, negative_embeds = pipe_deepfloyd.encode_prompt(pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None)
                    del text_encoder
                    del pipe_deepfloyd
                    pipe_deepfloyd = None
                installer.status("...clearing pipes")
                flush()
                clear_last(update=False)
                safety_modules = {}
                if init_img == None:
                    prt(Installing("Stage 1: Installing DeepFloyd-IF Pipeline..."))
                    # if prefs['disable_nsfw_filter'] else IFSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker").to(torch_device)
                    total_steps = pr['num_inference_steps']
                    #clear_last()
                    if deepfloyd_prefs['low_memory']:
                        pipe_deepfloyd = IFPipeline.from_pretrained(model_id, text_encoder=None, device_map="sequential", use_safetensors=True, variant="fp16", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
                        pipe_deepfloyd.unet.set_attn_processor(AttnAddedKVProcessor())
                        #pipe_deepfloyd.enable_model_cpu_offload()
                        #pipe_deepfloyd.unet.to(memory_format=torch.channels_last)
                        #pipe_deepfloyd.unet = torch.compile(pipe_deepfloyd.unet, mode="reduce-overhead", fullgraph=True)
                    else:
                        if not (deepfloyd_prefs['keep_pipelines'] and pipe_deepfloyd != None and status['last_deepfloyd_mode'] != "text2image"):
                            #install.status("...DiffusionPipeline")
                            pipe_deepfloyd = DiffusionPipeline.from_pretrained(model_id, variant="fp16", torch_dtype=torch.float16, use_safetensors=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
                            pipe_deepfloyd.unet.set_attn_processor(AttnAddedKVProcessor())
                            pipe_deepfloyd.to(torch_device)
                            #pipe_deepfloyd.enable_model_cpu_offload()
                            if prefs['enable_torch_compile']:
                                pipe_deepfloyd.unet.to(memory_format=torch.channels_last)
                                pipe_deepfloyd.unet = torch.compile(pipe_deepfloyd.unet, mode="reduce-overhead", fullgraph=True)
                        #install.status("...encode_prompts")
                        prompt_embeds, negative_embeds = pipe_deepfloyd.encode_prompt(pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None)
                    clear_last()
                    prt(progress)
                    images = pipe_deepfloyd(
                        prompt_embeds=prompt_embeds,
                        negative_prompt_embeds=negative_embeds,
                        num_inference_steps = pr['num_inference_steps'],
                        guidance_scale = pr['guidance_scale'],
                        output_type="pt",
                        generator=generator,
                        callback=callback_fnc, callback_steps=1,
                    ).images
                    safety_modules = {
                        "feature_extractor": pipe_deepfloyd.feature_extractor,
                        "safety_checker": pipe_deepfloyd.safety_checker,
                        "watermarker": pipe_deepfloyd.watermarker,
                    }
                    if not deepfloyd_prefs['keep_pipelines']:
                        del pipe_deepfloyd
                        flush()
                        pipe_deepfloyd = None
                    total_steps = deepfloyd_prefs['superres_num_inference_steps']
                    clear_last()
                    prt(Installing("Stage 2: Installing DeepFloyd Super Resolution Pipeline..."))
                    #IFSuperResolutionPipeline
                    if pipe_deepfloyd2 == None or status['last_deepfloyd_mode'] != "text2image":
                        pipe_deepfloyd2 = DiffusionPipeline.from_pretrained(model_id_II, text_encoder=None, variant="fp16", use_safetensors=True, torch_dtype=torch.float16)
                        pipe_deepfloyd2.unet.set_attn_processor(AttnAddedKVProcessor())
                        if not deepfloyd_prefs['low_memory']:
                            pipe_deepfloyd2.enable_model_cpu_offload()
                    clear_last()
                    prt(progress)
                    images = pipe_deepfloyd2(
                        image=images,
                        prompt_embeds=prompt_embeds,
                        negative_prompt_embeds=negative_embeds,
                        num_inference_steps = deepfloyd_prefs['superres_num_inference_steps'],
                        guidance_scale = deepfloyd_prefs['superres_guidance_scale'],
                        output_type="pt",
                        generator=generator,
                        callback=callback_fnc, callback_steps=1,
                    ).images
                    status['last_deepfloyd_mode'] = "text2image"
                elif init_img != None and mask_img == None:
                    prt(Installing("Stage 1: Installing DeepFloyd-IF Image2Image Pipeline..."))
                    if deepfloyd_prefs['low_memory']:
                        pipe_deepfloyd = IFImg2ImgPipeline.from_pretrained(model_id, variant="fp16", torch_dtype=torch.float16, use_safetensors=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
                        pipe_deepfloyd.unet.set_attn_processor(AttnAddedKVProcessor())
                        #pipe_deepfloyd.enable_model_cpu_offload()
                    else:
                        if not (deepfloyd_prefs['keep_pipelines'] and pipe_deepfloyd != None and status['last_deepfloyd_mode'] != "image2image"):
                            #install.status("...DiffusionPipeline")
                            pipe_deepfloyd = IFImg2ImgPipeline.from_pretrained(model_id, variant="fp16", torch_dtype=torch.float16, use_safetensors=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
                            pipe_deepfloyd.unet.set_attn_processor(AttnAddedKVProcessor())
                            pipe_deepfloyd.to(torch_device)
                            #pipe_deepfloyd.enable_model_cpu_offload()
                            if prefs['enable_torch_compile']:
                                pipe_deepfloyd.unet.to(memory_format=torch.channels_last)
                                pipe_deepfloyd.unet = torch.compile(pipe_deepfloyd.unet, mode="reduce-overhead", fullgraph=True)
                        #install.status("...encode_prompts")
                        prompt_embeds, negative_embeds = pipe_deepfloyd.encode_prompt(pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None)
                    total_steps = pr['num_inference_steps']
                    clear_last()
                    prt(progress)
                    images = pipe_deepfloyd(
                        image=init_img,
                        strength=pr['image_strength'],
                        prompt_embeds=prompt_embeds,
                        negative_prompt_embeds=negative_embeds,
                        num_inference_steps = pr['num_inference_steps'],
                        guidance_scale = pr['guidance_scale'],
                        output_type="pt",
                        generator=generator,
                        callback=callback_fnc, callback_steps=1,
                    ).images
                    safety_modules = {
                        "feature_extractor": pipe_deepfloyd.feature_extractor,
                        "safety_checker": pipe_deepfloyd.safety_checker,
                        "watermarker": pipe_deepfloyd.watermarker,
                    }
                    if not deepfloyd_prefs['keep_pipelines']:
                        del pipe_deepfloyd
                        flush()
                        pipe_deepfloyd = None
                    total_steps = deepfloyd_prefs['superres_num_inference_steps']
                    clear_last()
                    prt(Installing("Stage 2: Installing DeepFloyd Img2Img Super Resolution Pipeline..."))
                    from diffusers import IFImg2ImgSuperResolutionPipeline
                    if pipe_deepfloyd2 == None or status['last_deepfloyd_mode'] != "image2image":
                        pipe_deepfloyd2 = IFImg2ImgSuperResolutionPipeline.from_pretrained("DeepFloyd/IF-II-L-v1.0", text_encoder=None, variant="fp16", use_safetensors=True, torch_dtype=torch.float16, device_map="auto")
                        pipe_deepfloyd2.unet.set_attn_processor(AttnAddedKVProcessor())
                        if not deepfloyd_prefs['low_memory']:
                            pipe_deepfloyd2.enable_model_cpu_offload()
                    clear_last()
                    prt(progress)
                    images = pipe_deepfloyd2(
                        image=images,
                        origional_image=init_img,
                        strength=pr['image_strength'],
                        prompt_embeds=prompt_embeds,
                        negative_prompt_embeds=negative_embeds,
                        num_inference_steps = deepfloyd_prefs['superres_num_inference_steps'],
                        guidance_scale = deepfloyd_prefs['superres_guidance_scale'],
                        output_type="pt",
                        generator=generator,
                        callback=callback_fnc, callback_steps=1,
                    ).images
                    status['last_deepfloyd_mode'] = "image2image"
                elif init_img != None and mask_img != None:
                    prt(Installing("Stage 1: Installing DeepFloyd-IF Inpainting Pipeline..."))
                    # if prefs['disable_nsfw_filter'] else IFSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker").to(torch_device)
                    if deepfloyd_prefs['low_memory']:
                        pipe_deepfloyd = IFInpaintingPipeline.from_pretrained(model_id, variant="fp16", torch_dtype=torch.float16, use_safetensors=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
                        pipe_deepfloyd.unet.set_attn_processor(AttnAddedKVProcessor())
                        pipe_deepfloyd.enable_model_cpu_offload()
                    else:
                        if not (deepfloyd_prefs['keep_pipelines'] and pipe_deepfloyd != None and status['last_deepfloyd_mode'] != "inpainting"):
                            #install.status("...DiffusionPipeline")
                            pipe_deepfloyd = IFInpaintingPipeline.from_pretrained(model_id, variant="fp16", torch_dtype=torch.float16, use_safetensors=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
                            pipe_deepfloyd.unet.set_attn_processor(AttnAddedKVProcessor())
                            pipe_deepfloyd.to(torch_device)
                            #pipe_deepfloyd.enable_model_cpu_offload()
                            if prefs['enable_torch_compile']:
                                pipe_deepfloyd.unet.to(memory_format=torch.channels_last)
                                pipe_deepfloyd.unet = torch.compile(pipe_deepfloyd.unet, mode="reduce-overhead", fullgraph=True)
                        #install.status("...encode_prompts")
                        prompt_embeds, negative_embeds = pipe_deepfloyd.encode_prompt(pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None)
                    total_steps = pr['num_inference_steps']
                    clear_last()
                    prt(progress)
                    images = pipe_deepfloyd(
                        image=init_img,
                        mask_image=mask_img,
                        strength=pr['image_strength'],
                        prompt_embeds=prompt_embeds,
                        negative_prompt_embeds=negative_embeds,
                        num_inference_steps = pr['num_inference_steps'],
                        guidance_scale = pr['guidance_scale'],
                        output_type="pt",
                        generator=generator,
                        callback=callback_fnc, callback_steps=1,
                    ).images
                    safety_modules = {
                        "feature_extractor": pipe_deepfloyd.feature_extractor,
                        "safety_checker": pipe_deepfloyd.safety_checker,
                        "watermarker": pipe_deepfloyd.watermarker,
                    }
                    if not deepfloyd_prefs['keep_pipelines']:
                        del pipe_deepfloyd
                        flush()
                        pipe_deepfloyd = None
                    clear_last()
                    prt(Installing("Stage 2: Installing DeepFloyd Inpainting Super Resolution Pipeline..."))
                    from diffusers import IFInpaintingSuperResolutionPipeline
                    if pipe_deepfloyd2 == None or status['last_deepfloyd_mode'] != "inpainting":
                        pipe_deepfloyd2 = IFInpaintingSuperResolutionPipeline.from_pretrained("DeepFloyd/IF-II-L-v1.0", text_encoder=None, use_safetensors=True, variant="fp16", torch_dtype=torch.float16, device_map=None)
                        pipe_deepfloyd2.unet.set_attn_processor(AttnAddedKVProcessor())
                        if not deepfloyd_prefs['low_memory']:
                            pipe_deepfloyd2.enable_model_cpu_offload()
                    total_steps = deepfloyd_prefs['superres_num_inference_steps']
                    clear_last()
                    prt(progress)
                    images = pipe_deepfloyd2(
                        image=images,
                        origional_image=init_img,
                        mask_image=mask_img,
                        strength=pr['image_strength'],
                        prompt_embeds=prompt_embeds,
                        negative_prompt_embeds=negative_embeds,
                        num_inference_steps = deepfloyd_prefs['superres_num_inference_steps'],
                        guidance_scale = deepfloyd_prefs['superres_guidance_scale'],
                        output_type="pt",
                        generator=generator,
                        callback=callback_fnc, callback_steps=1,
                    ).images
                    status['last_deepfloyd_mode'] = "inpainting"
                if not deepfloyd_prefs['keep_pipelines']:
                    del pipe_deepfloyd2
                    flush()
                    pipe_deepfloyd2 = None
                prt(Installing("Stage 3: Installing Stable Diffusion X4 Upscaler Pipeline..."))
                if pipe_deepfloyd3 == None:
                    pipe_deepfloyd3 = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-x4-upscaler", **safety_modules, use_safetensors=True, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                    pipe_deepfloyd3.enable_model_cpu_offload()
                total_steps = deepfloyd_prefs['upscale_num_inference_steps']
                clear_last()
                prt(progress)
                images = pipe_deepfloyd3(prompt=pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None, image=images, noise_level=100, num_inference_steps=deepfloyd_prefs['upscale_num_inference_steps'], guidance_scale=deepfloyd_prefs['upscale_guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).images
                if deepfloyd_prefs['apply_watermark']:
                    from diffusers.pipelines.deepfloyd_if import IFWatermarker
                    watermarker = IFWatermarker.from_pretrained(model_id, subfolder="watermarker")
                    watermarker.apply_watermark(images, pipe_deepfloyd3.unet.config.sample_size)
                    del watermarker
                if not deepfloyd_prefs['keep_pipelines']:
                    del pipe_deepfloyd3
                    flush()
                    pipe_deepfloyd3 = None

            except EnvironmentError as e:
                clear_last()
                alert_msg(page, f"ERROR: You must accept the license on the DeepFloyd model card first.", content=Text(str(e)))
                #del pipe_deepfloyd
                flush()
                return
            except Exception as e:
                clear_last()
                alert_msg(page, f"ERROR: Couldn't run IF-DeepFloyd on your image for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                clear_pipes()
                return

            #clear_last()
            clear_last()
            filename = f"{deepfloyd_prefs['file_prefix']}{format_filename(pr['prompt'])}"
            #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
            #num = 0
            for image in images:
                random_seed += num
                fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
                image_path = available_file(os.path.join(stable_dir, deepfloyd_prefs['batch_folder_name']), fname, num)
                unscaled_path = image_path
                output_file = image_path.rpartition(slash)[2]
                image = pt_to_pil(image)
                image.save(image_path)
                out_path = os.path.dirname(image_path)
                upscaled_path = os.path.join(out_path, output_file)
                if not deepfloyd_prefs['display_upscaled_image'] or not deepfloyd_prefs['apply_ESRGAN_upscale']:
                    save_metadata(image_path, deepfloyd_prefs, "DeepFloyd-IF", model_id, random_seed, extra=pr)
                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if deepfloyd_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    #w = int(arg['width'] * prefs["enlarge_scale"])
                    #h = int(arg['height'] * prefs["enlarge_scale"])
                    #prt(Row([Text(f'Enlarging {prefs["enlarge_scale"]}X to {w}x{h}')], alignment=MainAxisAlignment.CENTER))
                    prt(Row([Text(f'Enlarging Real-ESRGAN {prefs["enlarge_scale"]}X')], alignment=MainAxisAlignment.CENTER))
                    upscale_image(image_path, upscaled_path, scale=deepfloyd_prefs["enlarge_scale"])
                    image_path = upscaled_path
                    clear_last()
                    save_metadata(image_path, deepfloyd_prefs, "DeepFloyd-IF", model_id, random_seed, extra=pr)
                    if deepfloyd_prefs['display_upscaled_image']:
                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(deepfloyd_prefs["enlarge_scale"]), height=height * float(deepfloyd_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], deepfloyd_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], deepfloyd_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
                #num += 1
    if not deepfloyd_prefs['keep_pipelines']:
        clear_pipes()
    play_snd(Snd.ALERT, page)

def run_amused(page, from_list=False, with_params=False):
    global amused_prefs, pipe_amused, prefs, status
    if not check_diffusers(page): return
    amused_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            amused_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':amused_prefs['guidance_scale'], 'num_inference_steps':amused_prefs['num_inference_steps'], 'width':amused_prefs['width'], 'height':amused_prefs['height'], 'init_image':amused_prefs['init_image'], 'mask_image':amused_prefs['mask_image'], 'init_image_strength':amused_prefs['init_image_strength'], 'num_images':amused_prefs['num_images'], 'seed':amused_prefs['seed']})
        else:
            amused_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'init_image':p['init_image'], 'mask_image':p['mask_image'], 'init_image_strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(amused_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      amused_prompts.append({'prompt': amused_prefs['prompt'], 'negative_prompt':amused_prefs['negative_prompt'], 'guidance_scale':amused_prefs['guidance_scale'], 'num_inference_steps':amused_prefs['num_inference_steps'], 'width':amused_prefs['width'], 'height':amused_prefs['height'], 'init_image':amused_prefs['init_image'], 'mask_image':amused_prefs['mask_image'], 'init_image_strength':amused_prefs['init_image_strength'], 'num_images':amused_prefs['num_images'], 'seed':amused_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.Amused.controls.append(line)
        if update:
          page.Amused.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.Amused, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.Amused.auto_scroll = scroll
        page.Amused.update()
      else:
        page.Amused.auto_scroll = scroll
        page.Amused.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.Amused.controls = page.Amused.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = amused_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Amused Engine & Models... See console for progress.")
    prt(installer)
    clear_pipes("amused")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    cpu_offload = amused_prefs['cpu_offload']
    amused_model = "amused/amused-512" if amused_prefs['amused_model'] == "amused-512" else "amused/amused-256" if amused_prefs['amused_model'] == "amused-256" else amused_prefs['amused_custom_model']
    if 'loaded_amused' not in status: status['loaded_amused'] = ""
    if 'loaded_amused_mode' not in status: status['loaded_amused_mode'] = ""
    if amused_model != status['loaded_amused']:
        clear_pipes()
    #from optimum.intel import OVLatentConsistencyModelPipeline
    #pipe = OVLatentConsistencyModelPipeline.from_pretrained("rupeshs/Amused-dreamshaper-v7-openvino-int8", ov_config={"CACHE_DIR": ""})
    mem_kwargs = {} if prefs['higher_vram_mode'] else {'variant': "fp16", 'torch_dtype': torch.float16}
    from diffusers import AmusedPipeline, AmusedImg2ImgPipeline, AmusedInpaintPipeline
    def get_amused_pipe(mode="Text2Image"):
        global status
        try:
            if mode == "Inpaint":
                pipe_amused = AmusedInpaintPipeline.from_pretrained(amused_model, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **mem_kwargs)
            elif mode == "Image2Image":
                pipe_amused = AmusedImg2ImgPipeline.from_pretrained(amused_model, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **mem_kwargs)
            else:
                pipe_amused = AmusedPipeline.from_pretrained(amused_model, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **mem_kwargs)
            pipe_amused.to(torch_device)
            pipe_amused.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Amused...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        status['loaded_amused'] = amused_model
        status['loaded_amused_mode'] = mode
        return pipe_amused
    
    if pipe_amused == None:
        installer.status(f"...initialize Amused Pipeline")
        try:
            if bool(amused_prefs['init_image']) and bool(amused_prefs['mask_image']):
                pipe_amused = AmusedInpaintPipeline.from_pretrained(amused_model, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **mem_kwargs)
                status['loaded_amused_mode'] = "Inpaint"
            elif bool(amused_prefs['init_image']):
                pipe_amused = AmusedImg2ImgPipeline.from_pretrained(amused_model, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **mem_kwargs)
                status['loaded_amused_mode'] = "Image2Image"
            else:
                pipe_amused = AmusedPipeline.from_pretrained(amused_model, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **mem_kwargs)
                status['loaded_amused_mode'] = "Text2Image"
            #if prefs['enable_torch_compile']:
            #    installer.status(f"...Torch compiling transformer")
            #    pipe_amused.transformer = torch.compile(pipe_amused.transformer, mode="reduce-overhead", fullgraph=True)
            #    pipe_amused = pipe_amused.to(torch_device)
            #elif cpu_offload:
            #    pipe_amused.enable_model_cpu_offload()
            #else:
            pipe_amused.to(torch_device)
            pipe_amused.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Amused...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        status['loaded_amused'] = amused_model
    else:
        clear_pipes('amused')
    
    clear_last()
    s = "" if len(amused_prompts) == 1 else "s"
    prt(f"Generating your Amused Image{s}...")
    for pr in amused_prompts:
        prt(progress)
        autoscroll(False)
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(int(pr['seed']))
        generator = torch.Generator().manual_seed(random_seed)
        init_img = None
        mask_img = None
        if bool(pr['init_image']):
            fname = pr['init_image'].rpartition(slash)[2]
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            max_size = max(pr['width'], pr['height'])
            width, height = init_img.size
            width, height = scale_dimensions(width, height, max_size, multiple=32)
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        if bool(pr['mask_image']):
            fname = pr['mask_image'].rpartition(slash)[2]
            if pr['mask_image'].startswith('http'):
                mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['mask_image']):
                    mask_img = PILImage.open(pr['mask_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your mask_image {pr['mask_image']}")
                    return
            max_size = max(pr['width'], pr['height'])
            width, height = mask_img.size
            width, height = scale_dimensions(width, height, max_size, multiple=32)
            mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            mask_img = ImageOps.exif_transpose(init_img).convert("RGB")
        if mask_img != None and init_img != None:
            mode = "Inpainting"
            mods = {'image': init_img, 'mask': mask_img, 'strength': pr['init_image_strength']}
        elif init_img != None:
            mode = "Image2Image"
            mods = {'image': init_img, 'strength': pr['init_image_strength']}
        else:
            mode = "Text2Image"
            mods = {'height': pr['height'], 'width': pr['width']}
        if mode != status['loaded_amused_mode']:
            prt(Installing(f"Initializing Amused {mode} Pipeline..."))
            clear_pipes()
            pipe_amused = get_amused_pipe(mode)
            clear_last()
        try:
            images = pipe_amused(
                prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                num_images_per_prompt=pr['num_images'],
                num_inference_steps=pr['num_inference_steps'],
                guidance_scale=pr['guidance_scale'],
                generator=generator,
                callback=callback_fnc,
                callback_steps = 1,
                **mods,
            ).images
        except Exception as e:
            clear_last()
            clear_last()
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        #clear_last()
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(amused_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, amused_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{amused_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            batch_output = os.path.join(prefs['image_output'], amused_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': amused_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)

            save_metadata(image_path, amused_prefs, f"Amused {mode}", amused_model, random_seed, extra=pr)
            if not amused_prefs['display_upscaled_image'] or not amused_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if amused_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=amused_prefs["enlarge_scale"], face_enhance=amused_prefs["face_enhance"])
                image_path = upscaled_path
                os.chdir(stable_dir)
                if amused_prefs['display_upscaled_image']:
                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(amused_prefs["enlarge_scale"]), height=pr['height'] * float(amused_prefs["enlarge_scale"]), data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], amused_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], amused_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_wuerstchen(page, from_list=False, with_params=False):
    global wuerstchen_prefs, pipe_wuerstchen, prefs
    if not check_diffusers(page): return
    wuerstchen_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            wuerstchen_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':wuerstchen_prefs['guidance_scale'], 'steps':wuerstchen_prefs['steps'], 'width':wuerstchen_prefs['width'], 'height':wuerstchen_prefs['height'], 'num_images':wuerstchen_prefs['num_images'], 'seed':wuerstchen_prefs['seed']})
        else:
            wuerstchen_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(wuerstchen_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      wuerstchen_prompts.append({'prompt': wuerstchen_prefs['prompt'], 'negative_prompt':wuerstchen_prefs['negative_prompt'], 'guidance_scale':wuerstchen_prefs['guidance_scale'], 'steps':wuerstchen_prefs['steps'], 'width':wuerstchen_prefs['width'], 'height':wuerstchen_prefs['height'], 'num_images':wuerstchen_prefs['num_images'], 'seed':wuerstchen_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.Wuerstchen.controls.append(line)
        if update:
          page.Wuerstchen.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.Wuerstchen, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.Wuerstchen.auto_scroll = scroll
        page.Wuerstchen.update()
      else:
        page.Wuerstchen.auto_scroll = scroll
        page.Wuerstchen.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.Wuerstchen.controls = page.Wuerstchen.controls[:1]
    progress = ProgressBar(bar_height=8)
    prior_steps = wuerstchen_prefs['prior_steps']
    total_steps = wuerstchen_prefs['steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):#(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    def prior_callback_fnc(pipe, step, timestep, callback_kwargs):#(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      prior_callback_fnc.has_been_called = True
      nonlocal progress, prior_steps
      percent = (step +1)/ prior_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {prior_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Würstchen Engine & Models... See console for progress.")
    prt(installer)
    clear_pipes("wuerstchen")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    from diffusers.pipelines.wuerstchen import DEFAULT_STAGE_C_TIMESTEPS
    cpu_offload = False
    if pipe_wuerstchen == None:
        #clear_pipes('wuerstchen')
        try:
            from diffusers import WuerstchenCombinedPipeline
            pipe_wuerstchen = WuerstchenCombinedPipeline.from_pretrained("warp-ai/wuerstchen", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            if prefs['enable_torch_compile']:
                installer.status(f"...Torch compiling unet")
                #pipe_wuerstchen.unet.to(memory_format=torch.channels_last)
                pipe_wuerstchen.unet = torch.compile(pipe_wuerstchen.unet, mode="reduce-overhead", fullgraph=True)
                pipe_wuerstchen = pipe_wuerstchen.to("cuda")
            elif cpu_offload:
                pipe_wuerstchen.enable_model_cpu_offload()
            else:
                pipe_wuerstchen.to("cuda")
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Würstchen, try running without installing Diffusers first...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    else:
        clear_pipes('wuerstchen')
    clear_last()
    s = "" if len(wuerstchen_prompts) == 1 else "s"
    for pr in wuerstchen_prompts:
        total_steps = pr['steps']
        for n in range(pr['num_images']):
            prt(f"Generating your Würstchen Image{s}...")
            prt(progress)
            nudge(page.imageColumn if from_list else page.Wuerstchen, page)
            autoscroll(False)
            random_seed = get_seed(int(pr['seed']) + n)
            generator = torch.Generator(device="cuda").manual_seed(random_seed)
            try:
                images = pipe_wuerstchen(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    prior_guidance_scale=wuerstchen_prefs['prior_guidance_scale'],
                    prior_num_inference_steps=wuerstchen_prefs['prior_steps'],
                    prior_timesteps=DEFAULT_STAGE_C_TIMESTEPS,
                    #num_images_per_prompt=pr['num_images'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=pr['steps'],
                    decoder_guidance_scale=pr['guidance_scale'],
                    generator=generator,
                    prior_callback_on_step_end=prior_callback_fnc,
                    callback_on_step_end=callback_fnc,
                ).images
            except Exception as e:
                clear_last()
                clear_last()
                alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            clear_last(2)
            autoscroll(True)
            txt2img_output = stable_dir
            batch_output = prefs['image_output']
            txt2img_output = stable_dir
            if bool(wuerstchen_prefs['batch_folder_name']):
                txt2img_output = os.path.join(stable_dir, wuerstchen_prefs['batch_folder_name'])
            if not os.path.exists(txt2img_output):
                os.makedirs(txt2img_output)
            if images is None:
                prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
                return
            idx = 0
            for image in images:
                fname = format_filename(pr['prompt'])
                #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
                fname = f'{wuerstchen_prefs["file_prefix"]}{fname}'
                image_path = available_file(txt2img_output, fname, 1)
                image.save(image_path)
                output_file = image_path.rpartition(slash)[2]
                if not wuerstchen_prefs['display_upscaled_image'] or not wuerstchen_prefs['apply_ESRGAN_upscale']:
                    save_metadata(image_path, wuerstchen_prefs, f"Würstchen", "wuerstchen-community/wuerstchen-2-2-decoder", random_seed, extra=pr)
                    prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
                batch_output = os.path.join(prefs['image_output'], wuerstchen_prefs['batch_folder_name'])
                if not os.path.exists(batch_output):
                    os.makedirs(batch_output)
                if storage_type == "PyDrive Google Drive":
                    newFolder = gdrive.CreateFile({'title': wuerstchen_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                    newFolder.Upload()
                    batch_output = newFolder
                out_path = os.path.dirname(image_path)
                upscaled_path = os.path.join(out_path, output_file)

                if wuerstchen_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    upscale_image(image_path, upscaled_path, scale=wuerstchen_prefs["enlarge_scale"], faceenhance=wuerstchen_prefs["face_enhance"])
                    image_path = upscaled_path
                    save_metadata(image_path, wuerstchen_prefs, f"Würstchen", "wuerstchen-community/wuerstchen-2-2-decoder", random_seed, extra=pr)
                    if wuerstchen_prefs['display_upscaled_image']:
                        prt(Row([Img(src=asset_dir(upscaled_path), width=pr['width'] * float(wuerstchen_prefs["enlarge_scale"]), height=pr['height'] * float(wuerstchen_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], wuerstchen_prefs['batch_folder_name']), fname, 0)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], wuerstchen_prefs['batch_folder_name']), fname, 0)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_stable_cascade(page, from_list=False, with_params=False):
    global stable_cascade_prefs, pipe_stable_cascade_prior, pipe_stable_cascade_decoder, prefs
    if not check_diffusers(page): return
    stable_cascade_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            stable_cascade_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':stable_cascade_prefs['guidance_scale'], 'steps':stable_cascade_prefs['steps'], 'width':stable_cascade_prefs['width'], 'height':stable_cascade_prefs['height'], 'num_images':stable_cascade_prefs['num_images'], 'seed':stable_cascade_prefs['seed']})
        else:
            stable_cascade_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(stable_cascade_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      stable_cascade_prompts.append({'prompt': stable_cascade_prefs['prompt'], 'negative_prompt':stable_cascade_prefs['negative_prompt'], 'guidance_scale':stable_cascade_prefs['guidance_scale'], 'steps':stable_cascade_prefs['steps'], 'width':stable_cascade_prefs['width'], 'height':stable_cascade_prefs['height'], 'num_images':stable_cascade_prefs['num_images'], 'seed':stable_cascade_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.StableCascade.controls.append(line)
        if update:
          page.StableCascade.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.StableCascade, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.StableCascade.auto_scroll = scroll
        page.StableCascade.update()
      else:
        page.StableCascade.auto_scroll = scroll
        page.StableCascade.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.StableCascade.controls = page.StableCascade.controls[:1]
    progress = ProgressBar(bar_height=8)
    prior_steps = stable_cascade_prefs['prior_steps']
    total_steps = stable_cascade_prefs['steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):#(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"Decoder {step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    def prior_callback_fnc(pipe, step, timestep, callback_kwargs):#(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      prior_callback_fnc.has_been_called = True
      nonlocal progress, prior_steps
      percent = (step +1)/ prior_steps
      progress.value = percent
      progress.tooltip = f"Prior {step +1} / {prior_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Stable Cascade Engine & Models... See console for progress.")
    prt(installer)
    clear_pipes("stable_cascade")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    #from diffusers.pipelines.stable_cascade import DEFAULT_STAGE_C_TIMESTEPS
    cpu_offload = False
    model_id = "stabilityai/stable-cascade" #"warp-ai/Wuerstchen-v3"
    if pipe_stable_cascade_prior == None or pipe_stable_cascade_decoder == None:
        #clear_pipes('stable_cascade')
        try:
            #TODO: Change to StableCascadeCombinedPipeline
            from diffusers import StableCascadeDecoderPipeline, StableCascadePriorPipeline
            opt = {'variant':'bf16', 'torch_dtype': torch.bfloat16} if prefs['higher_vram_mode'] else {'variant':'bf16', 'torch_dtype': torch.float16} #, revision="refs/pr/2", revision="refs/pr/44"
            pipe_stable_cascade_prior = StableCascadePriorPipeline.from_pretrained(model_id+"-prior", torch_dtype=torch.bfloat16, variant="bf16", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None).to("cuda")
            pipe_stable_cascade_decoder = StableCascadeDecoderPipeline.from_pretrained(model_id, **opt, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None).to("cuda")
            '''if prefs['enable_torch_compile']:
                installer.status(f"...Torch compiling unet")
                #pipe_stable_cascade.unet.to(memory_format=torch.channels_last)
                pipe_stable_cascade.unet = torch.compile(pipe_stable_cascade.unet, mode="reduce-overhead", fullgraph=True)
                pipe_stable_cascade = pipe_stable_cascade.to("cuda")'''
            '''if cpu_offload:
                pipe_stable_cascade_prior.enable_model_cpu_offload()
                pipe_stable_cascade_decoder.enable_model_cpu_offload()
            else:
                pipe_stable_cascade_prior.to("cuda")
                pipe_stable_cascade_decoder.to("cuda")'''
            pipe_stable_cascade_prior.set_progress_bar_config(disable=True)
            pipe_stable_cascade_decoder.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Stable Cascade Pipeline... If error cutlassF, upgrade Torch.", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    else:
        clear_pipes('stable_cascade')
    clear_last()
    s = "" if len(stable_cascade_prompts) == 1 else "s"
    for pr in stable_cascade_prompts:
        total_steps = pr['steps']
        for n in range(pr['num_images']):
            prt(f"Generating your Stable Cascade Image{s}...")
            prt(progress)
            nudge(page.imageColumn if from_list else page.StableCascade, page)
            autoscroll(False)
            random_seed = get_seed(int(pr['seed']) + n)
            generator = torch.Generator(device="cuda").manual_seed(random_seed)
            try:
                prior_output = pipe_stable_cascade_prior(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    guidance_scale=stable_cascade_prefs['prior_guidance_scale'],
                    #prior_num_inference_steps=stable_cascade_prefs['prior_steps'],
                    #prior_timesteps=DEFAULT_STAGE_C_TIMESTEPS,
                    num_images_per_prompt=pr['num_images'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=stable_cascade_prefs['prior_steps'],
                    #decoder_guidance_scale=pr['guidance_scale'],
                    generator=generator,
                    callback_on_step_end=prior_callback_fnc,
                    #callback_on_step_end=callback_fnc,
                )#TODO: Fix batch size embeddings
                images = pipe_stable_cascade_decoder(
                    image_embeddings=prior_output.image_embeddings if prefs['higher_vram_mode'] else prior_output.image_embeddings.to(torch.float16),
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    #prior_guidance_scale=stable_cascade_prefs['prior_guidance_scale'],
                    #prior_num_inference_steps=stable_cascade_prefs['prior_steps'],
                    #prior_timesteps=DEFAULT_STAGE_C_TIMESTEPS,
                    num_images_per_prompt=pr['num_images'],
                    #height=pr['height'],
                    #width=pr['width'],
                    num_inference_steps=pr['steps'],
                    guidance_scale=pr['guidance_scale'],
                    output_type="pil",
                    generator=generator,
                    #prior_callback_on_step_end=prior_callback_fnc,
                    callback_on_step_end=callback_fnc,
                ).images
            except Exception as e:
                clear_last()
                alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            #clear_last()
            clear_last(2)
            autoscroll(True)
            txt2img_output = stable_dir
            batch_output = prefs['image_output']
            txt2img_output = stable_dir
            if bool(stable_cascade_prefs['batch_folder_name']):
                txt2img_output = os.path.join(stable_dir, stable_cascade_prefs['batch_folder_name'])
            if not os.path.exists(txt2img_output):
                os.makedirs(txt2img_output)
            if images is None:
                prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
                return
            idx = 0
            for image in images:
                fname = format_filename(pr['prompt'])
                #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
                fname = f'{stable_cascade_prefs["file_prefix"]}{fname}'
                image_path = available_file(txt2img_output, fname, 1)
                image.save(image_path)
                output_file = image_path.rpartition(slash)[2]
                if not stable_cascade_prefs['display_upscaled_image'] or not stable_cascade_prefs['apply_ESRGAN_upscale']:
                    save_metadata(image_path, stable_cascade_prefs, f"Stable Cascade", "warp-ai/Wuerstchen-v3", random_seed, extra=pr)
                    prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
                batch_output = os.path.join(prefs['image_output'], stable_cascade_prefs['batch_folder_name'])
                if not os.path.exists(batch_output):
                    os.makedirs(batch_output)
                if storage_type == "PyDrive Google Drive":
                    newFolder = gdrive.CreateFile({'title': stable_cascade_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                    newFolder.Upload()
                    batch_output = newFolder
                out_path = os.path.dirname(image_path)
                upscaled_path = os.path.join(out_path, output_file)

                if stable_cascade_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    upscale_image(image_path, upscaled_path, scale=stable_cascade_prefs["enlarge_scale"], faceenhance=stable_cascade_prefs["face_enhance"])
                    image_path = upscaled_path
                    save_metadata(image_path, stable_cascade_prefs, f"Stable Cascade", "warp-ai/Wuerstchen-v3", random_seed, extra=pr)
                    if stable_cascade_prefs['display_upscaled_image']:
                        prt(Row([Img(src=asset_dir(upscaled_path), width=pr['width'] * float(stable_cascade_prefs["enlarge_scale"]), height=pr['height'] * float(stable_cascade_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], stable_cascade_prefs['batch_folder_name']), fname, 0)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], stable_cascade_prefs['batch_folder_name']), fname, 0)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_pixart_alpha(page, from_list=False, with_params=False):
    global pixart_alpha_prefs, pipe_pixart_alpha, pipe_pixart_alpha_encoder, prefs
    if not check_diffusers(page): return
    pixart_alpha_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            pixart_alpha_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':pixart_alpha_prefs['guidance_scale'], 'num_inference_steps':pixart_alpha_prefs['num_inference_steps'], 'width':pixart_alpha_prefs['width'], 'height':pixart_alpha_prefs['height'], 'num_images':pixart_alpha_prefs['num_images'], 'seed':pixart_alpha_prefs['seed']})
        else:
            pixart_alpha_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(pixart_alpha_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      pixart_alpha_prompts.append({'prompt': pixart_alpha_prefs['prompt'], 'negative_prompt':pixart_alpha_prefs['negative_prompt'], 'guidance_scale':pixart_alpha_prefs['guidance_scale'], 'num_inference_steps':pixart_alpha_prefs['num_inference_steps'], 'width':pixart_alpha_prefs['width'], 'height':pixart_alpha_prefs['height'], 'num_images':pixart_alpha_prefs['num_images'], 'seed':pixart_alpha_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.PixArtAlpha.controls.append(line)
        if update:
          page.PixArtAlpha.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.PixArtAlpha, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.PixArtAlpha.auto_scroll = scroll
        page.PixArtAlpha.update()
      else:
        page.PixArtAlpha.auto_scroll = scroll
        page.PixArtAlpha.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.PixArtAlpha.controls = page.PixArtAlpha.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = pixart_alpha_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    def callback_fn(pipe, step, timestep, callback_kwargs):
      callback_fn.has_been_called = True
      nonlocal progress
      total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing PixArt-α Engine & Models... See console for progress, may take a while.")
    prt(installer)
    clear_pipes("pixart_alpha")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    pip_install("sentencepiece", installer=installer, upgrade=True)
    use_8bit = pixart_alpha_prefs['use_8bit']
    if use_8bit:
        try:
          #os.environ['LD_LIBRARY_PATH'] += "/usr/lib/wsl/lib:$LD_LIBRARY_PATH"
          import bitsandbytes
        except ModuleNotFoundError:
          run_sp("pip install bitsandbytes", realtime=False, upgrade=True)
          import bitsandbytes
          pass
        #pip_install("bitsandbytes", q=True, installer=installer)
    if pixart_alpha_prefs['clean_caption']:
        pip_install("beautifulsoup4|bs4 ftfy", installer=installer)
    text_encoder = None
    cpu_offload = pixart_alpha_prefs['cpu_offload']
    pixart_model = "PixArt-alpha/PixArt-XL-2-1024-MS" if pixart_alpha_prefs['pixart_model'] == "PixArt-XL-2-1024-MS" else "PixArt-alpha/PixArt-XL-2-512x512" if pixart_alpha_prefs['pixart_model'] == "PixArt-XL-2-512x512" else "PixArt-alpha/PixArt-LCM-XL-2-1024-MS" if pixart_alpha_prefs['pixart_model'] == "PixArt-LCM-XL-2-1024-MS" else "frutiemax/TwistedReality-pixart-1024ms" if pixart_alpha_prefs['pixart_model'] == "TwistedReality-PixArt-1024ms" else "frutiemax/TwistedReality-pixart-512ms" if pixart_alpha_prefs['pixart_model'] == "TwistedReality-PixArt-512ms" else pixart_alpha_prefs['ustom_model']
    if 'loaded_pixart_8bit' not in status: status['loaded_pixart_8bit'] = use_8bit
    if 'loaded_pixart' not in status: status['loaded_pixart'] = ""
    if pixart_model != status['loaded_pixart'] or use_8bit != status['loaded_pixart_8bit']:
        clear_pipes()
    scheduler = {'scheduler': 'LCM'} if 'LCM' in pixart_model else {}
    if pipe_pixart_alpha == None:
        installer.status(f"...initialize PixArtAlpha Pipeline")
        try:
            from diffusers import PixArtAlphaPipeline
            if not use_8bit:
                pipe_pixart_alpha = PixArtAlphaPipeline.from_pretrained(pixart_model, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                pipe_pixart_alpha = pipeline_scheduler(pipe_pixart_alpha, **scheduler)
                if prefs['enable_torch_compile']:
                    installer.status(f"...Torch compiling transformer")
                    pipe_pixart_alpha.transformer = torch.compile(pipe_pixart_alpha.transformer, mode="reduce-overhead", fullgraph=True)
                    pipe_pixart_alpha = pipe_pixart_alpha.to(torch_device)
                elif cpu_offload:
                    pipe_pixart_alpha.enable_model_cpu_offload()
                else:
                    pipe_pixart_alpha.to(torch_device)
            else:
                from transformers import T5EncoderModel
                installer.status(f"...loading text encoder")
                text_encoder = T5EncoderModel.from_pretrained(pixart_model, subfolder="text_encoder", load_in_8bit=True, device_map="auto")
                pipe_pixart_alpha_encoder = PixArtAlphaPipeline.from_pretrained(pixart_model, text_encoder=text_encoder, transformer=None, device_map="auto", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                installer.status(f"...loading pipeline")
                pipe_pixart_alpha = PixArtAlphaPipeline.from_pretrained(pixart_model, text_encoder=None, torch_dtype=torch.float16).to("cuda")
                pipe_pixart_alpha = pipeline_scheduler(pipe_pixart_alpha, **scheduler)
            pipe_pixart_alpha.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing PixArt-α...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        status['loaded_pixart'] = pixart_model
        status['loaded_pixart_8bit'] = use_8bit
    else:
        clear_pipes('pixart_alpha')
        if prefs['scheduler_mode'] != status['loaded_scheduler']:
            pipe_pixart_alpha = pipeline_scheduler(pipe_pixart_alpha, **scheduler)
    if pipe_SDXL_refiner == None and pixart_alpha_prefs['use_refiner']:
        try:
            from diffusers import StableDiffusionXLImg2ImgPipeline
            pipe_SDXL_refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained("stabilityai/stable-diffusion-xl-refiner-1.0", torch_dtype=torch.float16, use_safetensors=True,
                text_encoder_2=pipe_pixart_alpha.text_encoder_2,
                vae=pipe_pixart_alpha.vae,
                add_watermarker=False,
                variant="fp16",
                cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
                **safety,
            )
            pipe_SDXL_refiner = optimize_SDXL(pipe_SDXL_refiner, lora=False, vae_slicing=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing SDXL Refiner...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    refiner = {'output_type':"latent"} if pixart_alpha_prefs['use_refiner'] else {}
    #'denoising_end': pixart_alpha_prefs['SDXL_high_noise_frac'], 
    clear_last()
    s = "" if len(pixart_alpha_prompts) == 1 else "s"
    prt(f"Generating your PixArt-α Image{s}...")
    for pr in pixart_alpha_prompts:
        prt(progress)
        autoscroll(False)
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(pr['seed'])
        generator = torch.Generator(device="cuda").manual_seed(random_seed)
        guidance_scale = pr['guidance_scale']
        num_inference_steps = pr['num_inference_steps']
        if 'LCM' in pixart_model:
            guidance_scale = 0.
            if num_inference_steps > 10:
                num_inference_steps = 8
        try:
            if not use_8bit:
                images = pipe_pixart_alpha(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    num_images_per_prompt=pr['num_images'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=num_inference_steps,
                    guidance_scale=guidance_scale,
                    clean_caption=pixart_alpha_prefs['clean_caption'],
                    use_resolution_binning=pixart_alpha_prefs['resolution_binning'],
                    #mask_feature=pixart_alpha_prefs['mask_feature'],resolution_binning
                    generator=generator,
                    callback=callback_fnc,
                    **refiner,
                ).images
            else:
                with torch.no_grad():
                    prompt_embeds, prompt_attention_mask, negative_embeds, negative_prompt_attention_mask = pipe_pixart_alpha_encoder.encode_prompt(pr['prompt'], negative_prompt=pr['negative_prompt'], num_images_per_prompt=pr['num_images'], clean_caption=pixart_alpha_prefs['clean_caption'])
                #del text_encoder
                #del pipe_pixart_alpha_encoder
                #flush()
                pipe_pixart_alpha = PixArtAlphaPipeline.from_pretrained(pixart_model, text_encoder=None, torch_dtype=torch.float16).to("cuda")
                latents = pipe_pixart_alpha(
                    negative_prompt=None, 
                    prompt_embeds=prompt_embeds,
                    negative_prompt_embeds=negative_embeds,
                    prompt_attention_mask=prompt_attention_mask,
                    negative_prompt_attention_mask=negative_prompt_attention_mask,
                    height=pr['height'],
                    width=pr['width'],
                    num_images_per_prompt=pr['num_images'],
                    num_inference_steps=num_inference_steps,
                    guidance_scale=guidance_scale,
                    output_type="latent",
                    clean_caption=pixart_alpha_prefs['clean_caption'],
                    use_resolution_binning=pixart_alpha_prefs['resolution_binning'],
                    generator=generator,
                    callback=callback_fnc,
                    **refiner,
                ).images
                del pipe_pixart_alpha.transformer
                flush()
                with torch.no_grad():
                    images = pipe_pixart_alpha.vae.decode(latents / pipe_pixart_alpha.vae.config.scaling_factor, return_dict=False)
        except Exception as e:
            clear_last()
            clear_last()
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        if pixart_alpha_prefs['use_refiner']:
            progress.value = None
            try:
                images = pipe_SDXL_refiner(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    num_images_per_prompt=pr['num_images'],
                    image=images,
                    num_inference_steps=pr['steps'],
                    denoising_start=pixart_alpha_prefs['SDXL_high_noise_frac'],
                    generator=generator,
                    callback_on_step_end=callback_fn,
                ).images
            except Exception as e:
                clear_last(2)
                alert_msg(page, f"ERROR: Something went wrong refining images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
        #clear_last()
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(pixart_alpha_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, pixart_alpha_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{pixart_alpha_prefs["file_prefix"]}{fname}'
            if use_8bit and not pixart_alpha_prefs['use_refiner']:
                image = pipe_pixart_alpha.image_processor.postprocess(image, output_type="pil")[0]
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            
            batch_output = os.path.join(prefs['image_output'], pixart_alpha_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': pixart_alpha_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)
            save_metadata(image_path, pixart_alpha_prefs, f"PixArt-α", pixart_model, random_seed, extra=pr)
            if not pixart_alpha_prefs['display_upscaled_image'] or not pixart_alpha_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if pixart_alpha_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=pixart_alpha_prefs["enlarge_scale"], face_enhance=pixart_alpha_prefs["face_enhance"])
                image_path = upscaled_path
                if pixart_alpha_prefs['display_upscaled_image']:
                    #prt(Row([Img(src=upscaled_path, width=pr['width'] * float(pixart_alpha_prefs["enlarge_scale"]), height=pr['height'] * float(pixart_alpha_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(pixart_alpha_prefs["enlarge_scale"]), height=pr['height'] * float(pixart_alpha_prefs["enlarge_scale"]), data=upscaled_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], pixart_alpha_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], pixart_alpha_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    del text_encoder
    #del pipe_pixart_alpha_encoder
    flush()
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_pixart_sigma(page, from_list=False, with_params=False):
    global pixart_sigma_prefs, pipe_pixart_sigma, pipe_pixart_sigma_encoder, prefs
    if not check_diffusers(page): return
    if int(status['cpu_memory']) < 16:
        alert_msg(page, f"Sorry, you need at least 16GB CPU RAM to run this. {'Change Runtime to High-RAM and try again.' if is_Colab else 'Upgrade your memory if you want to use it.'}")
        return
    pixart_sigma_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            pixart_sigma_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':pixart_sigma_prefs['guidance_scale'], 'num_inference_steps':pixart_sigma_prefs['num_inference_steps'], 'width':pixart_sigma_prefs['width'], 'height':pixart_sigma_prefs['height'], 'num_images':pixart_sigma_prefs['num_images'], 'seed':pixart_sigma_prefs['seed']})
        else:
            pixart_sigma_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(pixart_sigma_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      pixart_sigma_prompts.append({'prompt': pixart_sigma_prefs['prompt'], 'negative_prompt':pixart_sigma_prefs['negative_prompt'], 'guidance_scale':pixart_sigma_prefs['guidance_scale'], 'num_inference_steps':pixart_sigma_prefs['num_inference_steps'], 'width':pixart_sigma_prefs['width'], 'height':pixart_sigma_prefs['height'], 'num_images':pixart_sigma_prefs['num_images'], 'seed':pixart_sigma_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.PixArtSigma.controls.append(line)
        if update:
          page.PixArtSigma.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.PixArtSigma, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.PixArtSigma.auto_scroll = scroll
        page.PixArtSigma.update()
      else:
        page.PixArtSigma.auto_scroll = scroll
        page.PixArtSigma.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.PixArtSigma.controls = page.PixArtSigma.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = pixart_sigma_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing PixArt-Σ Engine & Models... See console for progress, may take a while.")
    prt(installer)
    clear_pipes("pixart_sigma")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    pip_install("sentencepiece", installer=installer, upgrade=True)
    use_8bit = pixart_sigma_prefs['use_8bit']
    if use_8bit:
        try:
          #os.environ['LD_LIBRARY_PATH'] += "/usr/lib/wsl/lib:$LD_LIBRARY_PATH"
          import bitsandbytes
        except ModuleNotFoundError:
          run_sp("pip install bitsandbytes", realtime=False, upgrade=True)
          import bitsandbytes
          pass
        #pip_install("bitsandbytes", q=True, installer=installer)
    if pixart_sigma_prefs['clean_caption']:
        pip_install("beautifulsoup4|bs4 ftfy", installer=installer)
    text_encoder = None
    cpu_offload = pixart_sigma_prefs['cpu_offload']
    pixart_model = "PixArt-alpha/PixArt-Sigma-XL-2-1024-MS" if pixart_sigma_prefs['pixart_model'] == "PixArt-Sigma-XL-2-1024-MS" else "PixArt-alpha/PixArt-Sigma-XL-2-512-MS" if pixart_sigma_prefs['pixart_model'] == "PixArt-Sigma-XL-2-512-MS" else "PixArt-alpha/PixArt-Sigma-XL-2-2K-MS" if pixart_sigma_prefs['pixart_model'] == "PixArt-Sigma-XL-2-2K-MS" else "AlanB/SigmaJourney-1024ms" if pixart_sigma_prefs['pixart_model'] == "SigmaJourney-1024ms" else pixart_sigma_prefs['custom_model']
    pixart_vae = "PixArt-alpha/pixart_sigma_sdxlvae_T5_diffusers"
    if 'loaded_pixart_8bit' not in status: status['loaded_pixart_8bit'] = use_8bit
    if 'loaded_pixart' not in status: status['loaded_pixart'] = ""
    if pixart_model != status['loaded_pixart'] or use_8bit != status['loaded_pixart_8bit']:
        clear_pipes()
    scheduler = {'scheduler': 'LCM'} if 'LCM' in pixart_model else {}
    if pipe_pixart_sigma == None:
        installer.status(f"...initialize PixArt-Sigma Pipeline")
        try:
            from diffusers import PixArtSigmaPipeline, Transformer2DModel
            if not use_8bit:
                installer.status(f"...initialize PixArtSigma Transformer 2D")
                transformer = Transformer2DModel.from_pretrained(
                    pixart_model,
                    subfolder="transformer",
                    torch_dtype=torch.float16,
                    use_additional_conditions=False,
                    use_safetensors=True,
                )
                installer.status(f"...initialize PixArt-Sigma Pipeline")
                pipe_pixart_sigma = PixArtSigmaPipeline.from_pretrained(pixart_vae, transformer=transformer, torch_dtype=torch.float16, use_safetensors=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                pipe_pixart_sigma = pipeline_scheduler(pipe_pixart_sigma, **scheduler)
                if prefs['enable_torch_compile']:
                    installer.status(f"...Torch compiling transformer")
                    pipe_pixart_sigma.transformer = torch.compile(pipe_pixart_sigma.transformer, mode="reduce-overhead", fullgraph=True)
                    pipe_pixart_sigma = pipe_pixart_sigma.to(torch_device)
                elif cpu_offload:
                    pipe_pixart_sigma.enable_model_cpu_offload()
                else:
                    pipe_pixart_sigma.to(torch_device)
            else:
                from transformers import T5EncoderModel
                installer.status(f"...loading text encoder")
                text_encoder = T5EncoderModel.from_pretrained(pixart_model, subfolder="text_encoder", load_in_8bit=True, device_map="auto")
                pipe_pixart_sigma_encoder = PixArtSigmaPipeline.from_pretrained(pixart_model, text_encoder=text_encoder, transformer=None, device_map="balanced", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                installer.status(f"...loading pipeline")
                pipe_pixart_sigma = PixArtSigmaPipeline.from_pretrained(pixart_model, text_encoder=None, torch_dtype=torch.float16).to("cuda")
                pipe_pixart_sigma = pipeline_scheduler(pipe_pixart_sigma, **scheduler)
            pipe_pixart_sigma.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing PixArt-Σ...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        status['loaded_pixart'] = pixart_model
        status['loaded_pixart_8bit'] = use_8bit
    else:
        clear_pipes('pixart_sigma')
        if prefs['scheduler_mode'] != status['loaded_scheduler']:
            pipe_pixart_sigma = pipeline_scheduler(pipe_pixart_sigma, **scheduler)
    clear_last()
    s = "" if len(pixart_sigma_prompts) == 1 else "s"
    prt(f"Generating your PixArt-Σ Image{s}...")
    for pr in pixart_sigma_prompts:
        prt(progress)
        autoscroll(False)
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(pr['seed'])
        generator = torch.Generator(device="cpu" if cpu_offload else "cuda").manual_seed(random_seed)
        guidance_scale = pr['guidance_scale']
        num_inference_steps = pr['num_inference_steps']
        if 'LCM' in pixart_model:
            guidance_scale = 0.
            if num_inference_steps > 10:
                num_inference_steps = 8
        try:
            if not use_8bit:
                images = pipe_pixart_sigma(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    num_images_per_prompt=pr['num_images'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=num_inference_steps,
                    guidance_scale=guidance_scale,
                    clean_caption=pixart_sigma_prefs['clean_caption'],
                    use_resolution_binning=pixart_sigma_prefs['resolution_binning'],
                    #mask_feature=pixart_sigma_prefs['mask_feature'],resolution_binning
                    generator=generator,
                    callback=callback_fnc,
                ).images
            else:
                with torch.no_grad():
                    prompt_embeds, prompt_attention_mask, negative_embeds, negative_prompt_attention_mask = pipe_pixart_sigma_encoder.encode_prompt(pr['prompt'], negative_prompt=pr['negative_prompt'], num_images_per_prompt=pr['num_images'], clean_caption=pixart_sigma_prefs['clean_caption'])
                pipe_pixart_sigma = PixArtSigmaPipeline.from_pretrained(pixart_model, text_encoder=None, torch_dtype=torch.float16).to("cuda")
                latents = pipe_pixart_sigma(
                    negative_prompt=None, 
                    prompt_embeds=prompt_embeds,
                    negative_prompt_embeds=negative_embeds,
                    prompt_attention_mask=prompt_attention_mask,
                    negative_prompt_attention_mask=negative_prompt_attention_mask,
                    height=pr['height'],
                    width=pr['width'],
                    num_images_per_prompt=pr['num_images'],
                    num_inference_steps=num_inference_steps,
                    guidance_scale=guidance_scale,
                    output_type="latent",
                    clean_caption=pixart_sigma_prefs['clean_caption'],
                    use_resolution_binning=pixart_sigma_prefs['resolution_binning'],
                    generator=generator,
                    callback=callback_fnc,
                ).images
                del pipe_pixart_sigma.transformer
                flush()
                with torch.no_grad():
                    images = pipe_pixart_sigma.vae.decode(latents / pipe_pixart_sigma.vae.config.scaling_factor, return_dict=False)
        except Exception as e:
            clear_last()
            clear_last()
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        #clear_last()
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(pixart_sigma_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, pixart_sigma_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{pixart_sigma_prefs["file_prefix"]}{fname}'
            if use_8bit:
                image = pipe_pixart_sigma.image_processor.postprocess(image, output_type="pil")[0]
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            batch_output = os.path.join(prefs['image_output'], pixart_sigma_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': pixart_sigma_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)

            save_metadata(image_path, pixart_sigma_prefs, f"PixArt-Σ", pixart_model, random_seed, extra=pr)
            if not pixart_sigma_prefs['display_upscaled_image'] or not pixart_sigma_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if pixart_sigma_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=pixart_sigma_prefs["enlarge_scale"], face_enhance=pixart_sigma_prefs["face_enhance"])
                image_path = upscaled_path
                if pixart_sigma_prefs['display_upscaled_image']:
                    #prt(Row([Img(src=upscaled_path, width=pr['width'] * float(pixart_sigma_prefs["enlarge_scale"]), height=pr['height'] * float(pixart_sigma_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(pixart_sigma_prefs["enlarge_scale"]), height=pr['height'] * float(pixart_sigma_prefs["enlarge_scale"]), data=upscaled_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], pixart_sigma_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], pixart_sigma_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    del text_encoder
    #del pipe_pixart_sigma_encoder
    flush()
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_hunyuan(page, from_list=False, with_params=False):
    global hunyuan_dit_prefs, pipe_hunyuan, prefs, controlnet_hunyuan_models, pipe_SDXL_refiner
    if not check_diffusers(page): return
    if int(status['cpu_memory']) <= 8:
      alert_msg(page, f"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run Hunyuan DiT right now. Either Change runtime type to High-RAM mode and restart.")
      return
    hunyuan_dit_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            hunyuan_dit_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':hunyuan_dit_prefs['guidance_scale'], 'steps':hunyuan_dit_prefs['steps'], 'width':hunyuan_dit_prefs['width'], 'height':hunyuan_dit_prefs['height'], 'num_images':hunyuan_dit_prefs['num_images'], 'seed':hunyuan_dit_prefs['seed']})
        else:
            hunyuan_dit_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(hunyuan_dit_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      hunyuan_dit_prompts.append({'prompt': hunyuan_dit_prefs['prompt'], 'negative_prompt':hunyuan_dit_prefs['negative_prompt'], 'guidance_scale':hunyuan_dit_prefs['guidance_scale'], 'steps':hunyuan_dit_prefs['steps'], 'width':hunyuan_dit_prefs['width'], 'height':hunyuan_dit_prefs['height'], 'num_images':hunyuan_dit_prefs['num_images'], 'seed':hunyuan_dit_prefs['seed']})
    if hunyuan_dit_prefs['use_controlnet']:
      original = hunyuan_dit_prefs['original_image']
      conditioning_scale = hunyuan_dit_prefs['conditioning_scale']
      if len(hunyuan_dit_prefs['multi_controlnets']) > 0:
        original = []
        conditioning_scale = []
        for c in hunyuan_dit_prefs['multi_controlnets']:
          original.append(c['original_image'])
          conditioning_scale.append(c['conditioning_scale'])
      
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.Hunyuan.controls.append(line)
        if update:
          page.Hunyuan.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.Hunyuan, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.Hunyuan.auto_scroll = scroll
        page.Hunyuan.update()
      else:
        page.Hunyuan.auto_scroll = scroll
        page.Hunyuan.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.Hunyuan.controls = page.Hunyuan.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = hunyuan_dit_prefs['steps']
    def callback_fnc(step: int, timestep: int, callback_kwargs) -> None: #(pipe, step, timestep, callback_kwargs):#
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    def callback_fn(pipe, step, timestep, callback_kwargs):
      callback_fn.has_been_called = True
      nonlocal progress
      total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    model_id = f"Tencent-Hunyuan/{hunyuan_dit_prefs['hunyuan_model']}" if hunyuan_dit_prefs['hunyuan_model'] != "Custom" else hunyuan_dit_prefs['custom_model']
    if 'loaded_hunyuan_model' not in status: status['loaded_hunyuan_model'] = ''
    installer = Installing(f"Installing Tencent-HunyuanDiT Engine & Models... See console log for progress.")
    cpu_offload = hunyuan_dit_prefs['cpu_offload']
    prt(installer)
    if status['loaded_hunyuan_model'] != model_id or (hunyuan_dit_prefs['use_controlnet'] and status['loaded_controlnet'] != hunyuan_dit_prefs["control_task"]):
        clear_pipes()
    else:
        clear_pipes('hunyuan')
    if hunyuan_dit_prefs['use_controlnet']:
        try:
            try:
              from controlnet_aux import MLSDdetector
            except ModuleNotFoundError:
              installer.status("...installing controlnet-aux")
              run_sp("pip install --upgrade controlnet-aux", realtime=False)
              #run_sp("pip install git+https://github.com/patrickvonplaten/controlnet_aux.git")
              pass
            from controlnet_aux import MLSDdetector
            from controlnet_aux import OpenposeDetector
            from diffusers import HunyuanDiT2DModel, HunyuanDiTControlNetPipeline, AutoencoderKL
            from diffusers.models import HunyuanDiT2DControlNetModel, HunyuanDiT2DMultiControlNetModel
            #run_sp("pip install scikit-image", realtime=False)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Installing Required Packages...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            flush()
            return
    canny_checkpoint = "Tencent-Hunyuan/HunyuanDiT-v1.1-ControlNet-Diffusers-Canny"
    depth_checkpoint = "Tencent-Hunyuan/HunyuanDiT-v1.1-ControlNet-Diffusers-Depth"
    pose_checkpoint = "Tencent-Hunyuan/HunyuanDiT-v1.1-ControlNet-Diffusers-Pose"
    hed = None
    openpose = None
    depth_estimator = None
    feature_extractor = None
    original_img = None
    def get_controlnet(task):
        nonlocal hed, openpose, depth_estimator, feature_extractor
        if controlnet_hunyuan_models[task] != None:
            return controlnet_hunyuan_models[task]
        installer.status(f"...loading {task} ControlNet")
        if "Canny" in task:
            controlnet_hunyuan_models[task] = HunyuanDiT2DControlNetModel.from_pretrained(canny_checkpoint, torch_dtype=torch.float16)
            task = "Canny"
        elif "Marigold" in task:
            import diffusers
            depth_estimator = diffusers.MarigoldDepthPipeline.from_pretrained("prs-eth/marigold-lcm-v1-0", torch_dtype=torch.float16, variant="fp16").to(torch_device)
            controlnet_hunyuan_models[task] = HunyuanDiT2DControlNetModel.from_pretrained(depth_checkpoint, torch_dtype=torch.float16)
        elif "Depth" in task:
            from transformers import DPTFeatureExtractor, DPTForDepthEstimation
            depth_estimator = DPTForDepthEstimation.from_pretrained("Intel/dpt-hybrid-midas").to("cuda")
            feature_extractor = DPTFeatureExtractor.from_pretrained("Intel/dpt-hybrid-midas")
            controlnet_hunyuan_models[task] = HunyuanDiT2DControlNetModel.from_pretrained(depth_checkpoint, torch_dtype=torch.float16)
        elif "Pose" in task:
            task = "OpenPose"
            from controlnet_aux import OpenposeDetector
            openpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')
            controlnet_hunyuan_models[task] = HunyuanDiT2DControlNetModel.from_pretrained(pose_checkpoint, torch_dtype=torch.float16)
        return controlnet_hunyuan_models[task]
    def prep_image(task, img):
        nonlocal hed, openpose, depth_estimator, feature_extractor
        #nonlocal width, height
        #installer.status(f"...preparing {task} image")
        if isinstance(img, list):
          img = img[0]
        if isinstance(img, str):
          if img.startswith('http'):
              #response = requests.get(controlnet_xl_prefs['original_image'])
              #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
              original_img = PILImage.open(requests.get(img, stream=True).raw)
          else:
              if os.path.isfile(img):
                  original_img = PILImage.open(img)
              else:
                  alert_msg(page, f"ERROR: Couldn't find your original_image {img}")
                  return
          max_size = max(hunyuan_dit_prefs['width'], hunyuan_dit_prefs['height'])
          width, height = original_img.size
          width, height = scale_dimensions(width, height, max_size)
          #print(f"Size: {width}x{height}")
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        #return original_img
        try:
            if 'Canny' in task: # == "Canny Map Edge" or task == "Video Canny Edge":
                input_image = np.array(original_img)
                input_image = cv2.Canny(input_image, hunyuan_dit_prefs['low_threshold'], hunyuan_dit_prefs['high_threshold'])
                input_image = input_image[:, :, None]
                input_image = np.concatenate([input_image, input_image, input_image], axis=2)
                original_img = PILImage.fromarray(input_image)
            elif "Pose" in task:
                original_img = openpose(original_img, hand_and_face=True)
            elif task == "Depth":
                original_img = depth_estimator(original_img)['depth']
                input_image = np.array(original_img)
                input_image = input_image[:, :, None]
                input_image = np.concatenate([input_image, input_image, input_image], axis=2)
                detected_map = torch.from_numpy(input_image).float() / 255.0
                original_img = detected_map.permute(2, 0, 1).unsqueeze(0).half().to("cuda")
                #original_img = PILImage.fromarray(input_image)
            elif task == "Marigold Depth":
                random_seed = get_seed(hunyuan_dit_prefs['seed'])
                generator = torch.Generator(device=torch_device).manual_seed(random_seed)
                input_image = depth_estimator(original_img, generator=generator).prediction
                input_image = depth_estimator.image_processor.visualize_depth(input_image, color_map="binary")
                original_img = input_image[0]
            elif "Depth" in task:
                original_img = feature_extractor(images=original_img, return_tensors="pt").pixel_values.to("cuda")
                with torch.no_grad(), torch.autocast("cuda"):
                    depth_map = depth_estimator(original_img).predicted_depth
                depth_map = torch.nn.functional.interpolate(
                    depth_map.unsqueeze(1),
                    size=(1024, 1024),
                    mode="bicubic",
                    align_corners=False,
                )
                depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)
                depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)
                depth_map = (depth_map - depth_min) / (depth_max - depth_min)
                original_img = torch.cat([depth_map] * 3, dim=1)
                original_img = original_img.permute(0, 2, 3, 1).cpu().numpy()[0]
                original_img = PILImage.fromarray((original_img * 255.0).clip(0, 255).astype(np.uint8))
            return original_img
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Preparing ControlNet-XL {hunyuan_dit_prefs['control_task']} Input Image...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            flush()
            return
    if hunyuan_dit_prefs['use_controlnet']:
        installer.status("...getting ControlNets")
        loaded_controlnet = None
        if len(hunyuan_dit_prefs['multi_controlnets']) > 0:# and not from_list:
            controlnet = []
            loaded_controlnet = []
            for c in hunyuan_dit_prefs['multi_controlnets']:
                controlnet.append(get_controlnet(c['control_task']))
                loaded_controlnet.append(c['control_task'])
            if len(controlnet) == 1:
                controlnet = controlnet[0]
                loaded_controlnet = loaded_controlnet[0]
            else:
                controlnet = HunyuanDiT2DMultiControlNetModel(controlnet)
        else:
            controlnet = get_controlnet(hunyuan_dit_prefs['control_task'])
            loaded_controlnet = hunyuan_dit_prefs['control_task']
        for k, v in controlnet_hunyuan_models.items():
          if v != None and k in loaded_controlnet:
            del v
            controlnet_hunyuan_models[k] = None
    if pipe_hunyuan == None:
        installer.status("...loading HunyuanDiT Pipeline")
        try:
            if hunyuan_dit_prefs['use_controlnet']:
                pipe_hunyuan = HunyuanDiTControlNetPipeline.from_pretrained(model_id, controlnet=controlnet, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            else:
                from diffusers import HunyuanDiTPipeline
                pipe_hunyuan = HunyuanDiTPipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            if prefs['enable_torch_compile']:
                installer.status(f"...Torch compiling unet")
                pipe_hunyuan.unet.to(memory_format=torch.channels_last)
                pipe_hunyuan.unet = torch.compile(pipe_hunyuan.unet, mode="reduce-overhead", fullgraph=True)
                pipe_hunyuan = pipe_hunyuan.to("cuda")
            elif cpu_offload:
                pipe_hunyuan.enable_model_cpu_offload()
            else:
                pipe_hunyuan.to("cuda")
            pipe_hunyuan.set_progress_bar_config(disable=True)
            status['loaded_hunyuan_model'] = model_id
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Hunyuan...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    if pipe_SDXL_refiner == None and hunyuan_dit_prefs['use_refiner']:
        try:
            from diffusers import StableDiffusionXLImg2ImgPipeline
            pipe_SDXL_refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained("stabilityai/stable-diffusion-xl-refiner-1.0", torch_dtype=torch.float16, use_safetensors=True,
                text_encoder_2=pipe_hunyuan.text_encoder_2,
                vae=pipe_hunyuan.vae,
                add_watermarker=False,
                variant="fp16",
                cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
                **safety,
            )
            pipe_SDXL_refiner = optimize_SDXL(pipe_SDXL_refiner, lora=False, vae_slicing=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing SDXL Refiner...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    refiner = {'output_type':"latent"} if hunyuan_dit_prefs['use_refiner'] else {}
    clear_last() #'denoising_end': hunyuan_dit_prefs['SDXL_high_noise_frac'], 
    n = 0
    for pr in hunyuan_dit_prompts:
        prt(f"{f'[{n + 1}/{len(hunyuan_dit_prompts)}]  ' if from_list else ''}{pr['prompt']}")
        prt(progress)
        nudge(page.imageColumn if from_list else page.Hunyuan, page=page)
        autoscroll(False)
        control_args = {}
        if hunyuan_dit_prefs['use_controlnet']:
            if len(hunyuan_dit_prefs['multi_controlnets']) > 0:
                original_img = []
                for c in hunyuan_dit_prefs['multi_controlnets']:
                    original_img.append(prep_image(c['control_task'], c['original_image']))
                    if hunyuan_dit_prefs['show_processed_image']:
                        w, h = original_img[-1].size
                        src_base64 = pil_to_base64(original_img[-1])
                        prt(Row([ImageButton(src_base64=src_base64, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))
            else:
                original_img = prep_image(hunyuan_dit_prefs['control_task'], hunyuan_dit_prefs['original_image'])
                if hunyuan_dit_prefs['show_processed_image']:
                    w, h = original_img.size
                    src_base64 = pil_to_base64(original_img)
                    prt(Row([ImageButton(src_base64=src_base64, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))
            control_args = {'control_image': original_img, 'controlnet_conditioning_scale': conditioning_scale}
        total_steps = pr['steps']
        random_seed = get_seed(pr['seed'])
        generator = torch.Generator(device="cuda").manual_seed(random_seed)
        try:
            images = pipe_hunyuan(
                prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                num_images_per_prompt=pr['num_images'],
                width=pr['width'],
                height=pr['height'],
                num_inference_steps=pr['steps'],
                guidance_scale=pr['guidance_scale'],
                generator=generator,
                callback_on_step_end=callback_fn,
                **refiner,
                **control_args,
            ).images
        except Exception as e:
            clear_last(2)
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        if hunyuan_dit_prefs['use_refiner']:
            progress.value = None
            try:
                images = pipe_SDXL_refiner(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    num_images_per_prompt=pr['num_images'],
                    image=images,
                    num_inference_steps=pr['steps'],
                    denoising_start=hunyuan_dit_prefs['SDXL_high_noise_frac'],
                    generator=generator,
                    callback_on_step_end=callback_fn,
                ).images
            except Exception as e:
                clear_last(2)
                alert_msg(page, f"ERROR: Something went wrong refining images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
        clear_last(2)
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(hunyuan_dit_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, hunyuan_dit_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        #print(str(images))
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            fname = f'{hunyuan_dit_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            if not hunyuan_dit_prefs['display_upscaled_image'] or not hunyuan_dit_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, hunyuan_dit_prefs, f"Hunyuan-DiT", model_id, random_seed, extra=pr)
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            batch_output = os.path.join(prefs['image_output'], hunyuan_dit_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)

            if hunyuan_dit_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=hunyuan_dit_prefs["enlarge_scale"], face_enhance=hunyuan_dit_prefs["face_enhance"])
                image_path = upscaled_path
                save_metadata(upscaled_path, hunyuan_dit_prefs, f"Hunyuan-DiT", model_id, random_seed, extra=pr)
                if hunyuan_dit_prefs['display_upscaled_image']:
                    prt(Row([Img(src=asset_dir(upscaled_path), width=pr['width'] * float(hunyuan_dit_prefs["enlarge_scale"]), height=pr['height'] * float(hunyuan_dit_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            new_file = available_file(os.path.join(prefs['image_output'], hunyuan_dit_prefs['batch_folder_name']), fname, 0)
            out_path = new_file
            shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
        n += 1
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_lumina(page, from_list=False, with_params=False):
    global lumina_next_prefs, pipe_lumina, prefs
    if not check_diffusers(page): return
    if int(status['cpu_memory']) <= 8:
      alert_msg(page, f"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run Lumina DiT right now. Either Change runtime type to High-RAM mode and restart.")
      return
    lumina_next_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            lumina_next_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':lumina_next_prefs['guidance_scale'], 'steps':lumina_next_prefs['steps'], 'width':lumina_next_prefs['width'], 'height':lumina_next_prefs['height'], 'num_images':lumina_next_prefs['num_images'], 'seed':lumina_next_prefs['seed']})
        else:
            lumina_next_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(lumina_next_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      lumina_next_prompts.append({'prompt': lumina_next_prefs['prompt'], 'negative_prompt':lumina_next_prefs['negative_prompt'], 'guidance_scale':lumina_next_prefs['guidance_scale'], 'steps':lumina_next_prefs['steps'], 'width':lumina_next_prefs['width'], 'height':lumina_next_prefs['height'], 'num_images':lumina_next_prefs['num_images'], 'seed':lumina_next_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.Lumina.controls.append(line)
        if update:
          page.Lumina.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.Lumina, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.Lumina.auto_scroll = scroll
        page.Lumina.update()
      else:
        page.Lumina.auto_scroll = scroll
        page.Lumina.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.Lumina.controls = page.Lumina.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = lumina_next_prefs['steps']
    def callback_fnc(step: int, timestep: int, callback_kwargs) -> None: #(pipe, step, timestep, callback_kwargs):#
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    def callback_fn(pipe, step, timestep, callback_kwargs):
      callback_fn.has_been_called = True
      nonlocal progress
      total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    model_id = "Alpha-VLLM/Lumina-Next-SFT-diffusers" if lumina_next_prefs['lumina_model'] == "Lumina-Next-SFT-diffusers" else lumina_next_prefs['custom_model']
    if 'loaded_lumina_model' not in status: status['loaded_lumina_model'] = ''
    installer = Installing(f"Installing Lumina-Next Engine & Models... See console log for progress.")
    cpu_offload = lumina_next_prefs['cpu_offload']
    prt(installer)
    if status['loaded_lumina_model'] != model_id:
        clear_pipes()
    else:
        clear_pipes('lumina')
    if pipe_lumina == None:
        try:
            from diffusers import LuminaText2ImgPipeline
            pipe_lumina = LuminaText2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            if prefs['enable_torch_compile']:
                installer.status(f"...Torch compiling unet")
                pipe_lumina = pipe_lumina.to("cuda")
                pipe_lumina.transformer.to(memory_format=torch.channels_last)
                pipe_lumina.vae.to(memory_format=torch.channels_last)
                pipe_lumina.transformer = torch.compile(pipe_lumina.transformer, mode="max-autotune", fullgraph=True)
                pipe_lumina.vae.decode = torch.compile(pipe_lumina.vae.decode, mode="max-autotune", fullgraph=True)
            elif cpu_offload:
                pipe_lumina.enable_model_cpu_offload()
            else:
                pipe_lumina = pipe_lumina.to("cuda")
                #pipe_lumina.transformer.enable_forward_chunking(chunk_size=1, dim=1)
            #pipe_lumina.set_progress_bar_config(disable=True)
            status['loaded_lumina_model'] = model_id
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Lumina, try running without installing Diffusers first...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    clear_last()
    n = 0
    for pr in lumina_next_prompts:
        prt(f"{f'[{n + 1}/{len(lumina_next_prompts)}]  ' if from_list else ''}{pr['prompt']}")
        prt(progress)
        nudge(page.imageColumn if from_list else page.Lumina, page=page)
        autoscroll(False)
        total_steps = pr['steps']
        random_seed = get_seed(pr['seed'])
        generator = torch.Generator(device="cpu" if cpu_offload else torch_device).manual_seed(random_seed)
        try:
            images = pipe_lumina(
                prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                num_images_per_prompt=pr['num_images'],
                width=pr['width'],
                height=pr['height'],
                num_inference_steps=pr['steps'],
                guidance_scale=pr['guidance_scale'],
                generator=generator,
                callback_on_step_end=callback_fn,
            ).images
        except Exception as e:
            clear_last(2)
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        clear_last(2)
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(lumina_next_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, lumina_next_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        #print(str(images))
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            fname = f'{lumina_next_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            if not lumina_next_prefs['display_upscaled_image'] or not lumina_next_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, lumina_next_prefs, f"Lumina-Next-DiT", model_id, random_seed, extra=pr)
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            batch_output = os.path.join(prefs['image_output'], lumina_next_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)

            if lumina_next_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=lumina_next_prefs["enlarge_scale"], face_enhance=lumina_next_prefs["face_enhance"])
                image_path = upscaled_path
                save_metadata(upscaled_path, lumina_next_prefs, f"Lumina-Next-DiT", model_id, random_seed, extra=pr)
                if lumina_next_prefs['display_upscaled_image']:
                    prt(Row([Img(src=asset_dir(upscaled_path), width=pr['width'] * float(lumina_next_prefs["enlarge_scale"]), height=pr['height'] * float(lumina_next_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            new_file = available_file(os.path.join(prefs['image_output'], lumina_next_prefs['batch_folder_name']), fname, 0)
            out_path = new_file
            shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
        n += 1
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_kolors(page, from_list=False, with_params=False):
    global kolors_prefs, pipe_kolors, prefs, status
    if not check_diffusers(page): return
    kolors_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            kolors_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':kolors_prefs['guidance_scale'], 'num_inference_steps':kolors_prefs['num_inference_steps'], 'width':kolors_prefs['width'], 'height':kolors_prefs['height'], 'init_image':kolors_prefs['init_image'], 'init_image_strength':kolors_prefs['init_image_strength'], 'num_images':kolors_prefs['num_images'], 'seed':kolors_prefs['seed']})
        else:
            kolors_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'init_image':p['init_image'], 'init_image_strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(kolors_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      kolors_prompts.append({'prompt': kolors_prefs['prompt'], 'negative_prompt':kolors_prefs['negative_prompt'], 'guidance_scale':kolors_prefs['guidance_scale'], 'num_inference_steps':kolors_prefs['num_inference_steps'], 'width':kolors_prefs['width'], 'height':kolors_prefs['height'], 'init_image':kolors_prefs['init_image'], 'init_image_strength':kolors_prefs['init_image_strength'], 'num_images':kolors_prefs['num_images'], 'seed':kolors_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.Kolors.controls.append(line)
        if update:
          page.Kolors.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.Kolors, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.Kolors.auto_scroll = scroll
        page.Kolors.update()
      else:
        page.Kolors.auto_scroll = scroll
        page.Kolors.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.Kolors.controls = page.Kolors.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = kolors_prefs['num_inference_steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Kolors Engine & Models... See console for progress.")
    prt(installer)
    clear_pipes("kolors")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    pip_install("triton", installer=installer)
    cpu_offload = kolors_prefs['cpu_offload']
    kolors_model = "Kwai-Kolors/Kolors-diffusers" if kolors_prefs['kolors_model'] == "Kwai-Kolors/Kolors-diffusers" else kolors_prefs['custom_model']
    status.setdefault('loaded_kolors', '')
    status.setdefault('loaded_kolors_mode', '')
    if kolors_model != status['loaded_kolors']:
        clear_pipes()
    #from optimum.intel import OVLatentConsistencyModelPipeline
    #pipe = OVLatentConsistencyModelPipeline.from_pretrained("rupeshs/Kolors-dreamshaper-v7-openvino-int8", ov_config={"CACHE_DIR": ""})
    
    from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image
    if pipe_kolors == None:
        installer.status(f"...initialize Kolors Pipeline")
        try:
            if bool(kolors_prefs['init_image']):
                pipe_kolors = AutoPipelineForImage2Image.from_pretrained(kolors_model, torch_dtype=torch.float16, variant="fp16", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                status['loaded_kolors_mode'] = "Image2Image"
            else:
                pipe_kolors = AutoPipelineForText2Image.from_pretrained(kolors_model, torch_dtype=torch.float16, variant="fp16", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                status['loaded_kolors_mode'] = "Text2Image"
            #pipe_kolors.scheduler = KolorsScheduler.from_config(pipe_kolors.scheduler.config)
            if cpu_offload:
                pipe_kolors.enable_model_cpu_offload()
            else:
                pipe_kolors.to(torch_device)
            pipe_kolors = pipeline_scheduler(pipe_kolors, use_karras_sigmas=True)
            pipe_kolors.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Kolors...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        status['loaded_kolors'] = kolors_model
    else:
        clear_pipes('kolors')
    clear_last()
    s = "" if len(kolors_prompts) == 1 else "s"
    prt(f"Generating your Kolors Image{s}...")
    for pr in kolors_prompts:
        prt(progress)
        autoscroll(False)
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(int(pr['seed']))
        generator = torch.Generator(device="cpu").manual_seed(random_seed)
        init_img = None
        if bool(pr['init_image']):
            fname = pr['init_image'].rpartition(slash)[2]
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        try:
            if init_img is not None:
                if status['loaded_kolors_mode'] != "Image2Image":
                    pipe_kolors = AutoPipelineForImage2Image.from_pipe(pipe_kolors)
                    status['loaded_kolors_mode'] = "Image2Image"
                images = pipe_kolors(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    num_images_per_prompt=pr['num_images'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=pr['num_inference_steps'],
                    guidance_scale=pr['guidance_scale'],
                    init_image=init_img,
                    init_image_strength=pr['init_image_strength'],
                    generator=generator,
                    callback_on_step_end=callback_fnc,
                ).images
            else:
                if status['loaded_kolors_mode'] != "Text2Image":
                    pipe_kolors = AutoPipelineForText2Image.from_pipe(pipe_kolors)
                    status['loaded_kolors_mode'] = "Text2Image"
                images = pipe_kolors(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    num_images_per_prompt=pr['num_images'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=pr['num_inference_steps'],
                    guidance_scale=pr['guidance_scale'],
                    generator=generator,
                    callback_on_step_end=callback_fnc,
                ).images
        except Exception as e:
            clear_last()
            clear_last()
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        #clear_last()
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(kolors_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, kolors_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{kolors_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            if not kolors_prefs['display_upscaled_image'] or not kolors_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, kolors_prefs, f"Kolors", kolors_model, random_seed, extra=pr)
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            batch_output = os.path.join(prefs['image_output'], kolors_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': kolors_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)

            if kolors_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=kolors_prefs["enlarge_scale"], face_enhance=kolors_prefs["face_enhance"])
                image_path = upscaled_path
                os.chdir(stable_dir)
                save_metadata(image_path, kolors_prefs, f"Kolors", kolors_model, random_seed, extra=pr)
                if kolors_prefs['display_upscaled_image']:
                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(kolors_prefs["enlarge_scale"]), height=pr['height'] * float(kolors_prefs["enlarge_scale"]), data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], kolors_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], kolors_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_auraflow(page, from_list=False, with_params=False):
    global auraflow_prefs, pipe_auraflow, prefs
    if not check_diffusers(page): return
    if int(status['cpu_memory']) <= 8:
      alert_msg(page, f"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run AuraFlow right now. Either Change runtime type to High-RAM mode and restart.")
      return
    auraflow_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            auraflow_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':auraflow_prefs['guidance_scale'], 'steps':auraflow_prefs['steps'], 'width':auraflow_prefs['width'], 'height':auraflow_prefs['height'], 'num_images':auraflow_prefs['num_images'], 'seed':auraflow_prefs['seed']})
        else:
            auraflow_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(auraflow_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      auraflow_prompts.append({'prompt': auraflow_prefs['prompt'], 'negative_prompt':auraflow_prefs['negative_prompt'], 'guidance_scale':auraflow_prefs['guidance_scale'], 'steps':auraflow_prefs['steps'], 'width':auraflow_prefs['width'], 'height':auraflow_prefs['height'], 'num_images':auraflow_prefs['num_images'], 'seed':auraflow_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.AuraFlow.controls.append(line)
        if update:
          page.AuraFlow.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.AuraFlow, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.AuraFlow.auto_scroll = scroll
        page.AuraFlow.update()
      else:
        page.AuraFlow.auto_scroll = scroll
        page.AuraFlow.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.AuraFlow.controls = page.AuraFlow.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = auraflow_prefs['steps']
    def callback_fnc(step: int, timestep: int, callback_kwargs) -> None: #(pipe, step, timestep, callback_kwargs):#
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    def callback_fn(pipe, step, timestep, callback_kwargs):
      callback_fn.has_been_called = True
      nonlocal progress
      total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    model_id = "fal/AuraFlow" if auraflow_prefs['auraflow_model'] == "fal/AuraFlow" else auraflow_prefs['custom_model']
    if 'loaded_auraflow_model' not in status: status['loaded_auraflow_model'] = ''
    installer = Installing(f"Installing AuraFlow Engine & Models... See console log for progress.")
    cpu_offload = auraflow_prefs['cpu_offload']
    prt(installer)
    if status['loaded_auraflow_model'] != model_id:
        clear_pipes()
    else:
        clear_pipes('auraflow')
    if pipe_auraflow == None:
        try:
            from diffusers import AuraFlowPipeline
            pipe_auraflow = AuraFlowPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            if cpu_offload:
                pipe_auraflow.enable_model_cpu_offload()
            else:
                pipe_auraflow = pipe_auraflow.to("cuda")
                if prefs['enable_torch_compile']:
                    installer.status(f"...Torch compiling unet")
                    torch._inductor.config.conv_1x1_as_mm = True
                    torch._inductor.config.coordinate_descent_tuning = True
                    torch._inductor.config.epilogue_fusion = False
                    torch._inductor.config.coordinate_descent_check_all_directions = True
                    pipe_auraflow.transformer.to(memory_format=torch.channels_last)
                    pipe_auraflow.vae.to(memory_format=torch.channels_last)
                    pipe_auraflow.transformer = torch.compile(pipe_auraflow.transformer, mode="max-autotune", fullgraph=True)
                    pipe_auraflow.vae.decode = torch.compile(pipe_auraflow.vae.decode, mode="max-autotune", fullgraph=True)
                #pipe_auraflow.transformer.enable_forward_chunking(chunk_size=1, dim=1)
            #pipe_auraflow.set_progress_bar_config(disable=True)
            status['loaded_auraflow_model'] = model_id
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing AuraFlow, try running without installing Diffusers first...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    clear_last()
    n = 0
    for pr in auraflow_prompts:
        prt(f"{f'[{n + 1}/{len(auraflow_prompts)}]  ' if from_list else ''}{pr['prompt']}")
        prt(progress)
        nudge(page.imageColumn if from_list else page.AuraFlow, page=page)
        autoscroll(False)
        total_steps = pr['steps']
        random_seed = get_seed(pr['seed'])
        generator = torch.Generator(device="cpu" if cpu_offload else torch_device).manual_seed(random_seed)
        try:
            images = pipe_auraflow(
                prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                num_images_per_prompt=pr['num_images'],
                width=pr['width'],
                height=pr['height'],
                num_inference_steps=pr['steps'],
                guidance_scale=pr['guidance_scale'],
                generator=generator,
                callback_on_step_end=callback_fn,
            ).images
        except Exception as e:
            clear_last(2)
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        clear_last(2)
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(auraflow_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, auraflow_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        #print(str(images))
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            fname = f'{auraflow_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            if not auraflow_prefs['display_upscaled_image'] or not auraflow_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, auraflow_prefs, f"AuraFlow", model_id, random_seed, extra=pr)
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            batch_output = os.path.join(prefs['image_output'], auraflow_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)
            if auraflow_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=auraflow_prefs["enlarge_scale"], face_enhance=auraflow_prefs["face_enhance"])
                image_path = upscaled_path
                save_metadata(upscaled_path, auraflow_prefs, f"AuraFlow", model_id, random_seed, extra=pr)
                if auraflow_prefs['display_upscaled_image']:
                    prt(Row([Img(src=asset_dir(upscaled_path), width=pr['width'] * float(auraflow_prefs["enlarge_scale"]), height=pr['height'] * float(auraflow_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            new_file = available_file(os.path.join(prefs['image_output'], auraflow_prefs['batch_folder_name']), fname, 0)
            out_path = new_file
            shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
        n += 1
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_layer_diffusion(page, from_list=False, with_params=False):
    global layer_diffusion_prefs, pipe_layer_diffusion, prefs, ld_text_encoder, ld_text_encoder_2, ld_vae, ld_unet, ld_transparent_decoder, ld_transparent_encoder
    if not check_diffusers(page): return
    if int(status['cpu_memory']) <= 8:
      alert_msg(page, f"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run LayerDiffusion right now. Either Change runtime type to High-RAM mode and restart.")
      return
    layer_diffusion_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            layer_diffusion_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':layer_diffusion_prefs['guidance_scale'], 'steps':layer_diffusion_prefs['steps'], 'width':layer_diffusion_prefs['width'], 'height':layer_diffusion_prefs['height'], 'num_images':layer_diffusion_prefs['num_images'], 'init_image':layer_diffusion_prefs['init_image'], 'init_image_strength':layer_diffusion_prefs['init_image_strength'], 'seed':layer_diffusion_prefs['seed']})
        else:
            layer_diffusion_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'num_images':p['batch_size'], 'init_image':p['init_image'], 'init_image_strength':p['init_image_strength'], 'seed':p['seed']})
    else:
      if not bool(layer_diffusion_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      layer_diffusion_prompts.append({'prompt': layer_diffusion_prefs['prompt'], 'negative_prompt':layer_diffusion_prefs['negative_prompt'], 'guidance_scale':layer_diffusion_prefs['guidance_scale'], 'steps':layer_diffusion_prefs['steps'], 'width':layer_diffusion_prefs['width'], 'height':layer_diffusion_prefs['height'], 'num_images':layer_diffusion_prefs['num_images'], 'init_image':layer_diffusion_prefs['init_image'], 'init_image_strength':layer_diffusion_prefs['init_image_strength'], 'seed':layer_diffusion_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.LayerDiffusion.controls.append(line)
        if update:
          page.LayerDiffusion.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.LayerDiffusion, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.LayerDiffusion.auto_scroll = scroll
        page.LayerDiffusion.update()
      else:
        page.LayerDiffusion.auto_scroll = scroll
        page.LayerDiffusion.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.LayerDiffusion.controls = page.LayerDiffusion.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = layer_diffusion_prefs['steps']
    def callback_fnc(step: int, timestep: int, callback_kwargs) -> None: #(pipe, step, timestep, callback_kwargs):#
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    def callback_fn(pipe, step, timestep, callback_kwargs):
      callback_fn.has_been_called = True
      nonlocal progress
      total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    SDXL_model = get_SDXL_model(prefs['SDXL_model'])
    model_id = SDXL_model['path']
    if 'loaded_layer_diffusion_model' not in status: status['loaded_layer_diffusion_model'] = ''
    installer = Installing(f"Installing Layer Diffusion Engine & {SDXL_model['name']}... See console log for progress.")
    #cpu_offload = layer_diffusion_prefs['cpu_offload']
    prt(installer)
    layer_diffuse_dir = os.path.join(root_dir, 'LayerDiffuse_DiffusersCLI')
    if not os.path.exists(layer_diffuse_dir): #TODO: check force_update
        installer.status("...cloning lllyasviel/LayerDiffuse_DiffusersCLI")#
        run_sp("git clone https://github.com/lllyasviel/LayerDiffuse_DiffusersCLI.git", cwd=root_dir, realtime=False)
    if layer_diffuse_dir not in sys.path:
        sys.path.append(layer_diffuse_dir)
    pip_install("bitsandbytes==0.43.1 protobuf==3.20 opencv-python tensorboardX einops peft", installer=installer)
    if status['loaded_layer_diffusion_model'] != model_id:
        clear_pipes()
    else:
        clear_pipes('layer_diffusion')
    import numpy as np
    import memory_management
    import safetensors.torch as sf
    from PIL import Image as PILImage
    from PIL import ImageOps
    from diffusers_kdiffusion_sdxl import KDiffusionStableDiffusionXLPipeline
    from diffusers import AutoencoderKL, UNet2DConditionModel
    from diffusers.models.attention_processor import AttnProcessor2_0
    from transformers import CLIPTextModel, CLIPTokenizer
    from lib_layerdiffuse.vae import TransparentVAEDecoder, TransparentVAEEncoder
    from lib_layerdiffuse.utils import download_model
    
    if pipe_layer_diffusion == None:
        try:
            installer.status("...loading CLIP Tokenizer & Autoencoder")
            tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder="tokenizer")
            tokenizer_2 = CLIPTokenizer.from_pretrained(model_id, subfolder="tokenizer_2")
            ld_text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder="text_encoder", torch_dtype=torch.float16, variant="fp16")
            ld_text_encoder_2 = CLIPTextModel.from_pretrained(model_id, subfolder="text_encoder_2", torch_dtype=torch.float16, variant="fp16")
            ld_vae = AutoencoderKL.from_pretrained(model_id, subfolder="vae", torch_dtype=torch.bfloat16, variant="fp16")  # bfloat16 vae
            ld_unet = UNet2DConditionModel.from_pretrained(model_id, subfolder="unet", torch_dtype=torch.float16, variant="fp16")
            ld_unet.set_attn_processor(AttnProcessor2_0())
            ld_vae.set_attn_processor(AttnProcessor2_0())
            installer.status("...downloading Models")
            path_ld_diffusers_sdxl_attn = download_model(url='https://huggingface.co/lllyasviel/LayerDiffuse_Diffusers/resolve/main/ld_diffusers_sdxl_attn.safetensors', local_path=os.path.join(layer_diffuse_dir, 'models', 'ld_diffusers_sdxl_attn.safetensors'))
            path_ld_diffusers_sdxl_vae_transparent_encoder = download_model(url='https://huggingface.co/lllyasviel/LayerDiffuse_Diffusers/resolve/main/ld_diffusers_sdxl_vae_transparent_encoder.safetensors', local_path=os.path.join(layer_diffuse_dir, 'models', 'ld_diffusers_sdxl_vae_transparent_encoder.safetensors'))
            path_ld_diffusers_sdxl_vae_transparent_decoder = download_model(url='https://huggingface.co/lllyasviel/LayerDiffuse_Diffusers/resolve/main/ld_diffusers_sdxl_vae_transparent_decoder.safetensors', local_path=os.path.join(layer_diffuse_dir, 'models', 'ld_diffusers_sdxl_vae_transparent_decoder.safetensors'))
            installer.status("...loading unet states")
            sd_offset = sf.load_file(path_ld_diffusers_sdxl_attn)
            sd_origin = ld_unet.state_dict()
            keys = sd_origin.keys()
            sd_merged = {}
            for k in sd_origin.keys():
                if k in sd_offset:
                    sd_merged[k] = sd_origin[k] + sd_offset[k]
                else:
                    sd_merged[k] = sd_origin[k]
            ld_unet.load_state_dict(sd_merged, strict=True)
            del sd_offset, sd_origin, sd_merged, keys, k
            installer.status("...initialize Transparent VAE Pipe")
            ld_transparent_encoder = TransparentVAEEncoder(path_ld_diffusers_sdxl_vae_transparent_encoder)
            ld_transparent_decoder = TransparentVAEDecoder(path_ld_diffusers_sdxl_vae_transparent_decoder)
            pipe_layer_diffusion = KDiffusionStableDiffusionXLPipeline(
                vae=ld_vae,
                text_encoder=ld_text_encoder,
                tokenizer=tokenizer,
                text_encoder_2=ld_text_encoder_2,
                tokenizer_2=tokenizer_2,
                unet=ld_unet,
                scheduler=None,
            )
            status['loaded_layer_diffusion_model'] = model_id
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing LayerDiffusion, try running without installing Diffusers first...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    clear_last()
    n = 0
    for pr in layer_diffusion_prompts:
        pb = Progress(f"{f'[{n + 1}/{len(layer_diffusion_prompts)}]  ' if from_list else ''}{pr['prompt']}", steps=pr['steps'])
        prt(pb)
        nudge(page.imageColumn if from_list else page.LayerDiffusion, page=page)
        autoscroll(False)
        total_steps = pr['steps']
        random_seed = get_seed(pr['seed'])
        init_img = None
        if bool(pr['init_image']):
            pb.status("...initialize image")
            fname = pr['init_image'].rpartition(slash)[2]
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            max_size = max(pr['width'], pr['height'])
            width, height = init_img.size
            width, height = scale_dimensions(width, height, max_size, multiple=32)
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        try:
            with torch.inference_mode():
                pb.status("...loading text encoders")
                generator = torch.Generator(device=memory_management.gpu).manual_seed(random_seed)
                memory_management.load_models_to_gpu([ld_text_encoder, ld_text_encoder_2])
                positive_cond, positive_pooler = pipe_layer_diffusion.encode_cropped_prompt_77tokens(pr['prompt'])
                negative_cond, negative_pooler = pipe_layer_diffusion.encode_cropped_prompt_77tokens(pr['negative_prompt'])
                pb.status("...encoding transparent latents")
                if init_img is not None:
                    memory_management.load_models_to_gpu([ld_vae, ld_transparent_decoder, ld_transparent_encoder])
                    initial_latent = [np.array(init_img)]
                    initial_latent = ld_transparent_encoder(ld_vae, initial_latent) * ld_vae.config.scaling_factor
                    memory_management.load_models_to_gpu([ld_unet])
                    initial_latent = initial_latent.to(dtype=ld_unet.dtype, device=ld_unet.device)
                else:
                    memory_management.load_models_to_gpu([ld_unet])
                    initial_latent = torch.zeros(size=(1, 4, int(pr['height']/8), int(pr['width']/8)), dtype=ld_unet.dtype, device=ld_unet.device)#(1, 4, 144, 112)
                pb.status("...running layer diffusion")
                latents = pipe_layer_diffusion(
                    initial_latent=initial_latent,
                    strength=1.0 if init_img is None else pr['init_image_strength'],
                    num_inference_steps=pr['steps'],
                    batch_size=pr['num_images'],
                    #width=pr['width'],
                    #height=pr['height'],
                    prompt_embeds=positive_cond,
                    negative_prompt_embeds=negative_cond,
                    pooled_prompt_embeds=positive_pooler,
                    negative_pooled_prompt_embeds=negative_pooler,
                    generator=generator,
                    guidance_scale=pr['guidance_scale'],
                ).images
                pb.status("...decoding transparent latents")
                memory_management.load_models_to_gpu([ld_vae, ld_transparent_decoder, ld_transparent_encoder])
                latents = latents.to(dtype=ld_vae.dtype, device=ld_vae.device) / ld_vae.config.scaling_factor
                result_list, vis_list = ld_transparent_decoder(ld_vae, latents)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        clear_last()
        autoscroll(True)
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(layer_diffusion_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, layer_diffusion_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        #print(str(images))
        if result_list is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        for i, image in enumerate(result_list):
            fname = format_filename(pr['prompt'])
            fname = f'{layer_diffusion_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, i)
            new_file = available_file(os.path.join(prefs['image_output'], layer_diffusion_prefs['batch_folder_name']), fname, i)
            #vis_path = available_file(os.path.join(prefs['image_output'], layer_diffusion_prefs['batch_folder_name']), fname + "-vis", i)
            img = PILImage.fromarray(image)
            w, h = img.size
            img.save(image_path, format='PNG')
            #PILImage.fromarray(vis_list[i]).save(vis_path, format='PNG')
            output_file = image_path.rpartition(slash)[2]
            if not layer_diffusion_prefs['display_upscaled_image'] or not layer_diffusion_prefs['apply_ESRGAN_upscale']:
                #prt(Row([ImageButton(src=vis_path, width=w, height=h, data=vis_path, page=page)], alignment=MainAxisAlignment.CENTER))
                save_metadata(image_path, layer_diffusion_prefs, f"Layer Diffusion", model_id, random_seed, extra=pr)
                prt(Row([ImageButton(src=image_path, width=w, height=h, data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            batch_output = os.path.join(prefs['image_output'], layer_diffusion_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)
            if layer_diffusion_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=layer_diffusion_prefs["enlarge_scale"], face_enhance=layer_diffusion_prefs["face_enhance"])
                image_path = upscaled_path
                save_metadata(upscaled_path, layer_diffusion_prefs, f"Layer Diffusion", model_id, random_seed, extra=pr)
                if layer_diffusion_prefs['display_upscaled_image']:
                    prt(Row([Img(src=asset_dir(upscaled_path), width=w * float(layer_diffusion_prefs["enlarge_scale"]), height=h * float(layer_diffusion_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            out_path = new_file
            shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
        n += 1
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_differential_diffusion(page, from_list=False, with_params=False):
    global differential_diffusion_prefs, pipe_differential_diffusion, prefs, status
    if not check_diffusers(page): return
    if not bool(differential_diffusion_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
    differential_diffusion_prompts = []
    if from_list:
        if len(prompts) < 1:
            alert_msg(page, "You need to add Prompts to your List first... ")
            return
        for p in prompts:
            if with_params:
                differential_diffusion_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':differential_diffusion_prefs['guidance_scale'], 'num_inference_steps':differential_diffusion_prefs['num_inference_steps'], 'width':differential_diffusion_prefs['width'], 'height':differential_diffusion_prefs['height'], 'init_image':differential_diffusion_prefs['init_image'], 'mask_image':differential_diffusion_prefs['mask_image'], 'init_image_strength':differential_diffusion_prefs['init_image_strength'], 'num_images':differential_diffusion_prefs['num_images'], 'seed':differential_diffusion_prefs['seed']})
            else:
                differential_diffusion_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'init_image':p['init_image'], 'mask_image':p['mask_image'], 'init_image_strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
        if not bool(differential_diffusion_prefs['init_image']) or not bool(differential_diffusion_prefs['mask_image']):
            alert_msg(page, "You must provide an initial image and mask image to process your image generation...")
            return
        differential_diffusion_prompts.append({'prompt': differential_diffusion_prefs['prompt'], 'negative_prompt':differential_diffusion_prefs['negative_prompt'], 'guidance_scale':differential_diffusion_prefs['guidance_scale'], 'num_inference_steps':differential_diffusion_prefs['num_inference_steps'], 'width':differential_diffusion_prefs['width'], 'height':differential_diffusion_prefs['height'], 'init_image':differential_diffusion_prefs['init_image'], 'mask_image':differential_diffusion_prefs['mask_image'], 'init_image_strength':differential_diffusion_prefs['init_image_strength'], 'num_images':differential_diffusion_prefs['num_images'], 'seed':differential_diffusion_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.Differential_Diffusion.controls.append(line)
        if update:
          page.Differential_Diffusion.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.Differential_Diffusion, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.Differential_Diffusion.auto_scroll = scroll
        page.Differential_Diffusion.update()
      else:
        page.Differential_Diffusion.auto_scroll = scroll
        page.Differential_Diffusion.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.Differential_Diffusion.controls = page.Differential_Diffusion.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = differential_diffusion_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    def callback_fn(pipe, step, timestep, callback_kwargs):
      callback_fn.has_been_called = True
      nonlocal progress
      total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Differential Diffusion Engine & Models... See console for progress.")
    prt(installer)
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    cpu_offload = differential_diffusion_prefs['cpu_offload']
    use_SD3 = differential_diffusion_prefs['use_SD3']
    #model_id = differential_diffusion_prefs['differential_diffusion_model'] if differential_diffusion_prefs['differential_diffusion_model'] != "Custom" else differential_diffusion_prefs['differential_diffusion_custom_model'] #"differential_diffusion/differential_diffusion-512" if differential_diffusion_prefs['differential_diffusion_model'] == "differential_diffusion-512" else "differential_diffusion/differential_diffusion-256" if differential_diffusion_prefs['differential_diffusion_model'] == "differential_diffusion-256" else differential_diffusion_prefs['differential_diffusion_custom_model']
    if use_SD3:
        SD3_model = get_SD3_model(prefs['SD3_model'])
        model_id = SD3_model['path']
        variant = {'variant': SD3_model['variant']} if 'variant' in SD3_model else {}
    else:
        SDXL_model = get_SDXL_model(prefs['SDXL_model'])
        model_id = SDXL_model['path']
        variant = {'variant': SDXL_model['revision']} if 'revision' in SDXL_model else {}
        variant = {'variant': SDXL_model['variant']} if 'variant' in SDXL_model else variant
    if 'loaded_differential_diffusion' not in status: status['loaded_differential_diffusion'] = ""
    if model_id != status['loaded_differential_diffusion']:
        clear_pipes()
    else:
        clear_pipes("differential_diffusion")
    #from optimum.intel import OVLatentConsistencyModelPipeline
    #pipe = OVLatentConsistencyModelPipeline.from_pretrained("rupeshs/Differential_Diffusion-dreamshaper-v7-openvino-int8", ov_config={"CACHE_DIR": ""})
    #mem_kwargs = {} if prefs['higher_vram_mode'] else {'variant': "fp16", 'torch_dtype': torch.float16}
    from diffusers import DPMSolverMultistepScheduler
    import transformers
    from examples.community.pipeline_stable_diffusion_xl_differential_img2img import (
        StableDiffusionXLDifferentialImg2ImgPipeline,
    )
    from examples.community.pipeline_stable_diffusion_3_differential_img2img import (
        StableDiffusion3DifferentialImg2ImgPipeline,
    )
    if pipe_differential_diffusion == None:
        installer.status(f"...initialize Differential Diffusion Pipeline")
        try:
            if use_SD3:
                pipe_differential_diffusion = StableDiffusion3DifferentialImg2ImgPipeline.from_pretrained(model_id, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **variant)
            else:
                pipe_differential_diffusion = StableDiffusionXLDifferentialImg2ImgPipeline.from_pretrained(model_id, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **variant)
            #if prefs['enable_torch_compile']:
            #    installer.status(f"...Torch compiling transformer")
            #    pipe_differential_diffusion.transformer = torch.compile(pipe_differential_diffusion.transformer, mode="reduce-overhead", fullgraph=True)
            #    pipe_differential_diffusion = pipe_differential_diffusion.to(torch_device)
            #elif cpu_offload:
            #    pipe_differential_diffusion.enable_model_cpu_offload()
            #else:
            pipe_differential_diffusion.to(torch_device)
            pipe_differential_diffusion.scheduler = DPMSolverMultistepScheduler.from_config(pipe_differential_diffusion.scheduler.config, use_karras_sigmas=True)
            #pipe_differential_diffusion = pipe_scheduler(pipe_differential_diffusion)
            pipe_differential_diffusion.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Differential Diffusion...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        status['loaded_differential_diffusion'] = model_id
    else:
        clear_pipes('differential_diffusion')
    ip_adapter_arg = {}
    if differential_diffusion_prefs['use_ip_adapter'] and not use_SD3:
        installer.status(f"...initialize IP-Adapter")
        ip_adapter_img = None
        if differential_diffusion_prefs['ip_adapter_image'].startswith('http'):
            i_response = requests.get(differential_diffusion_prefs['ip_adapter_image'])
            ip_adapter_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
        else:
            if os.path.isfile(differential_diffusion_prefs['ip_adapter_image']):
                ip_adapter_img = PILImage.open(differential_diffusion_prefs['ip_adapter_image'])
            else:
                clear_last()
                prt(f"ERROR: Couldn't find your ip_adapter_image {differential_diffusion_prefs['ip_adapter_image']}")
        if bool(ip_adapter_img):
            ip_adapter_arg['ip_adapter_image'] = ip_adapter_img
        if bool(ip_adapter_arg):
            ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == differential_diffusion_prefs['ip_adapter_model'])
            pipe_differential_diffusion.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
            pipe_differential_diffusion.set_ip_adapter_scale(differential_diffusion_prefs['ip_adapter_strength'])
    def preprocess_image(image):
        image = image.convert("RGB")
        image = transforms.CenterCrop((image.size[1] // 64 * 64, image.size[0] // 64 * 64))(image)
        image = transforms.ToTensor()(image)
        image = image * 2 - 1
        image = image.unsqueeze(0).to("cuda")
        return image
    def preprocess_map(map):
        map = map.convert("L")
        map = transforms.CenterCrop((map.size[1] // 64 * 64, map.size[0] // 64 * 64))(map)
        map = transforms.ToTensor()(map)
        map = map.to("cuda")
        return map
    clear_last()
    s = "" if len(differential_diffusion_prompts) == 1 else "s"
    prt(f"Generating your Differential Diffusion Image{s}...")
    for pr in differential_diffusion_prompts:
        prt(progress)
        autoscroll(False)
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(int(pr['seed']))
        generator = torch.Generator().manual_seed(random_seed)
        init_img = None
        mask_img = None
        if bool(pr['init_image']):
            fname = pr['init_image'].rpartition(slash)[2]
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            width, height = init_img.size
            width, height = scale_dimensions(width, height, differential_diffusion_prefs['max_size'], multiple=32)
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
            init_img = preprocess_image(init_img)
        if bool(pr['mask_image']):
            fname = pr['mask_image'].rpartition(slash)[2]
            if pr['mask_image'].startswith('http'):
                mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['mask_image']):
                    mask_img = PILImage.open(pr['mask_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your mask_image {pr['mask_image']}")
                    return
            width, height = mask_img.size
            width, height = scale_dimensions(width, height, differential_diffusion_prefs['max_size'], multiple=32)
            mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            mask_img = ImageOps.exif_transpose(init_img).convert("RGB")
            mask_img = preprocess_map(mask_img)
        try:
            images = pipe_differential_diffusion(
                prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                original_image=init_img,
                map=mask_img,
                strength=pr['init_image_strength'],
                num_images_per_prompt=pr['num_images'],
                num_inference_steps=pr['num_inference_steps'],
                guidance_scale=pr['guidance_scale'],
                eta=differential_diffusion_prefs['eta'],
                generator=generator,
                #aesthetic_score: float = 6.0,
                #negative_aesthetic_score: float = 2.5,
                callback_on_step_end=callback_fn,
                **ip_adapter_arg,
            ).images
        except Exception as e:
            clear_last(2)
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        #clear_last()
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(differential_diffusion_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, differential_diffusion_prefs['batch_folder_name'])
        makedir(txt2img_output)
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        for idx, image in range(images):
            fname = format_filename(pr['prompt'])
            #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{differential_diffusion_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            batch_output = os.path.join(prefs['image_output'], differential_diffusion_prefs['batch_folder_name'])
            makedir(batch_output)
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': differential_diffusion_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)
            if differential_diffusion_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=differential_diffusion_prefs["enlarge_scale"], face_enhance=differential_diffusion_prefs["face_enhance"])
                image_path = upscaled_path
                os.chdir(stable_dir)
                save_metadata(image_path, differential_diffusion_prefs, f"Differential Diffusion Image2Image", model_id, random_seed, extra=pr)
                if differential_diffusion_prefs['display_upscaled_image']:
                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(differential_diffusion_prefs["enlarge_scale"]), height=pr['height'] * float(differential_diffusion_prefs["enlarge_scale"]), data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if not differential_diffusion_prefs['display_upscaled_image'] or not differential_diffusion_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, differential_diffusion_prefs, f"Differential Diffusion Image2Image", model_id, random_seed, extra=pr)
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], differential_diffusion_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], differential_diffusion_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_lmd_plus(page, from_list=False, with_params=False):
    global lmd_plus_prefs, pipe_lmd_plus, prefs
    from sdd_utils import llm_template
    if not check_diffusers(page): return
    lmd_plus_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            lmd_plus_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':lmd_plus_prefs['guidance_scale'], 'num_inference_steps':lmd_plus_prefs['num_inference_steps'], 'width':lmd_plus_prefs['width'], 'height':lmd_plus_prefs['height'], 'num_images':lmd_plus_prefs['num_images'], 'seed':lmd_plus_prefs['seed']})
        else:
            lmd_plus_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(lmd_plus_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      lmd_plus_prompts.append({'prompt': lmd_plus_prefs['prompt'], 'negative_prompt':lmd_plus_prefs['negative_prompt'], 'guidance_scale':lmd_plus_prefs['guidance_scale'], 'num_inference_steps':lmd_plus_prefs['num_inference_steps'], 'width':lmd_plus_prefs['width'], 'height':lmd_plus_prefs['height'], 'num_images':lmd_plus_prefs['num_images'], 'seed':lmd_plus_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.LMD_Plus.controls.append(line)
        if update:
          page.LMD_Plus.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.LMD_Plus, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.LMD_Plus.auto_scroll = scroll
        page.LMD_Plus.update()
      else:
        page.LMD_Plus.auto_scroll = scroll
        page.LMD_Plus.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.LMD_Plus.controls = page.LMD_Plus.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = lmd_plus_prefs['num_inference_steps']
    def callback_fn(pipe, step, timestep, callback_kwargs):
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing LMD+ Engine & Models... See console for progress.")
    prt(installer)
    clear_pipes("lmd_plus")
    if 'GPT' in lmd_plus_prefs['AI_engine']:
        try:
            import openai
        except:
            installer.status("...installing OpenAI Library")
            run_sp("pip install --upgrade openai", realtime=False)
            import openai
            pass
        try:
            from openai import OpenAI
            openai_client = OpenAI(api_key=prefs['OpenAI_api_key'])
        except:
            alert_msg(page, "Invalid OpenAI API Key. Change in Settings...")
            return
        status['installed_OpenAI'] = True
    elif lmd_plus_prefs['AI_engine'] == "Google Gemini":
        if not bool(prefs['PaLM_api_key']):
            alert_msg(page, "You must provide your Google Gemini MakerSuite API key in Settings first")
            return
        try:
            import google.generativeai as genai
            if force_update("generativeai"): raise ModuleNotFoundError("Forcing update")
        except:
            installer.status("...installing Google MakerSuite Library")
            run_sp("pip install --upgrade google-generativeai", realtime=False)
            import google.generativeai as genai
            pass
        try:
            genai.configure(api_key=prefs['PaLM_api_key'])
        except:
            alert_msg(page, "Invalid Google Gemini API Key. Change in Settings...")
            return
        gemini_model = genai.GenerativeModel(model_name='gemini-pro')
    def get_response(prompt_full, AI_engine="ChatGPT-3.5 Turbo", temperature=0.7):
        if AI_engine == "OpenAI GPT-3":
            response = openai_client.completions.create(engine="text-davinci-003", prompt=prompt_full, max_tokens=2400, temperature=prefs['prompt_generator']['AI_temperature'], presence_penalty=1)
            result = response.choices[0].text.strip()
        elif AI_engine == "ChatGPT-3.5 Turbo":
            response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo-0125",
                temperature=temperature,
                messages=[{"role": "user", "content": prompt_full}]
            )
            result = response.choices[0].message.content.strip()
        elif "GPT-4" in AI_engine:
            gpt_model = "gpt-4-turbo" if "Turbo" in AI_engine else "gpt-4o" if "4o" in AI_engine else "gpt-4"
            response = openai_client.chat.completions.create(
                model=gpt_model,
                temperature=temperature,
                messages=[{"role": "user", "content": prompt_full}]
            )
            result = response.choices[0].message.content.strip()
        elif AI_engine == "Google Gemini":
            completion = gemini_model.generate_content(prompt_full, generation_config={
                'temperature': temperature,
                'max_output_tokens': 1024
            })
            #completion = palm.generate_text(model='models/text-bison-001', prompt=prompt_full, temperature=temperature, max_output_tokens=1024)
            result = completion.text.strip()
        return result
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    cpu_offload = lmd_plus_prefs['cpu_offload']
    lmd_plus_model = "longlian/lmd_plus" if lmd_plus_prefs['lmd_plus_model'] == "longlian/lmd_plus" else lmd_plus_prefs['custom_model']
    if 'loaded_lmd_plus' not in status: status['loaded_lmd_plus'] = ""
    if lmd_plus_model != status['loaded_lmd_plus']:
        clear_pipes()
    if pipe_lmd_plus == None:
        installer.status(f"...initialize LMD+ Pipeline")
        try:
            torch.backends.cuda.enable_flash_sdp(False)
            torch.backends.cuda.enable_mem_efficient_sdp(True)
            from diffusers import DiffusionPipeline
            pipe_lmd_plus = DiffusionPipeline.from_pretrained(lmd_plus_model, custom_pipeline="AlanB/llm_grounded_diffusion_fix", variant="fp16", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None) #, requires_safety_checker=False
            #custom_pipeline="llm_grounded_diffusion"
            #pipe_lmd_plus = pipeline_scheduler(pipe_lmd_plus)
            if prefs['enable_torch_compile']:
                installer.status(f"...Torch compiling transformer")
                pipe_lmd_plus.transformer = torch.compile(pipe_lmd_plus.transformer, mode="reduce-overhead", fullgraph=True)
                pipe_lmd_plus = pipe_lmd_plus.to(torch_device)
            elif cpu_offload:
                pipe_lmd_plus.enable_model_cpu_offload()
            else:
                pipe_lmd_plus = pipe_lmd_plus.to(torch_device)
            pipe_lmd_plus.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing LMD+...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        status['loaded_lmd_plus'] = lmd_plus_model
    else:
        clear_pipes('lmd_plus')
        #if prefs['scheduler_mode'] != status['loaded_scheduler']:
        #    pipe_lmd_plus = pipeline_scheduler(pipe_lmd_plus)
    clear_last()
    s = "" if len(lmd_plus_prompts) == 1 else "s"
    prt(f"Generating your LMD+ Image{s}...")
    for pr in lmd_plus_prompts:
        prt(progress)
        autoscroll(False)
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(int(pr['seed']))
        generator = torch.Generator().manual_seed(random_seed)
        prompt_full = llm_template.format(prompt=pr['prompt'].strip().rstrip("."), width=pr['width'], height=pr['height'])
        response = get_response(prompt_full, AI_engine=lmd_plus_prefs['AI_engine'])
        clear_last()
        prt(Markdown(response))
        prt(progress)
        phrases, boxes, bg_prompt, neg_prompt = pipe_lmd_plus.parse_llm_response(response.strip())
        if bool(pr['negative_prompt']):
            neg_prompt += f", {pr['negative_prompt']}"
        try:
            images = pipe_lmd_plus(
                prompt=pr['prompt'],
                negative_prompt=neg_prompt,
                phrases=phrases,
                boxes=boxes,
                num_images_per_prompt=pr['num_images'],
                height=pr['height'],
                width=pr['width'],
                num_inference_steps=pr['num_inference_steps'],
                guidance_scale=pr['guidance_scale'],
                gligen_scheduled_sampling_beta=lmd_plus_prefs['gligen_scheduled_sampling_beta'],
                #generator=generator,
                callback=callback_fnc,
                lmd_guidance_kwargs={}
            ).images
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        #clear_last()
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(lmd_plus_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, lmd_plus_prefs['batch_folder_name'])
        makedir(txt2img_output)
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{lmd_plus_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            if not lmd_plus_prefs['display_upscaled_image'] or not lmd_plus_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, lmd_plus_prefs, f"LMD+", lmd_plus_model, random_seed, extra=pr)
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            batch_output = os.path.join(prefs['image_output'], lmd_plus_prefs['batch_folder_name'])
            makedir(batch_output)
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': lmd_plus_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)

            if lmd_plus_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=lmd_plus_prefs["enlarge_scale"], face_enhance=lmd_plus_prefs["face_enhance"])
                image_path = upscaled_path
                os.chdir(stable_dir)
                save_metadata(image_path, lmd_plus_prefs, f"LMD+", lmd_plus_model, random_seed, extra=pr)
                if lmd_plus_prefs['display_upscaled_image']:
                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(lmd_plus_prefs["enlarge_scale"]), height=pr['height'] * float(lmd_plus_prefs["enlarge_scale"]), data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path, width=pr['width'] * float(lmd_plus_prefs["enlarge_scale"]), height=pr['height'] * float(lmd_plus_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], lmd_plus_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], lmd_plus_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_lcm(page, from_list=False, with_params=False):
    global lcm_prefs, pipe_lcm, prefs, status
    if not check_diffusers(page): return
    lcm_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            lcm_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':lcm_prefs['guidance_scale'], 'num_inference_steps':lcm_prefs['num_inference_steps'], 'width':lcm_prefs['width'], 'height':lcm_prefs['height'], 'init_image':lcm_prefs['init_image'], 'init_image_strength':lcm_prefs['init_image_strength'], 'num_images':lcm_prefs['num_images'], 'seed':lcm_prefs['seed']})
        else:
            lcm_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'init_image':p['init_image'], 'init_image_strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(lcm_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      lcm_prompts.append({'prompt': lcm_prefs['prompt'], 'negative_prompt':lcm_prefs['negative_prompt'], 'guidance_scale':lcm_prefs['guidance_scale'], 'num_inference_steps':lcm_prefs['num_inference_steps'], 'width':lcm_prefs['width'], 'height':lcm_prefs['height'], 'init_image':lcm_prefs['init_image'], 'init_image_strength':lcm_prefs['init_image_strength'], 'num_images':lcm_prefs['num_images'], 'seed':lcm_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.LCM.controls.append(line)
        if update:
          page.LCM.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.LCM, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.LCM.auto_scroll = scroll
        page.LCM.update()
      else:
        page.LCM.auto_scroll = scroll
        page.LCM.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.LCM.controls = page.LCM.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = lcm_prefs['num_inference_steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing LCM Engine & Models... See console for progress.")
    prt(installer)
    clear_pipes("lcm")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    cpu_offload = lcm_prefs['cpu_offload']
    lcm_model = "SimianLuo/LCM_Dreamshaper_v7" if lcm_prefs['lcm_model'] == "LCM_Dreamshaper_v7" else "Lykon/dreamshaper-8-lcm" if lcm_prefs['lcm_model'] == "LCM_Dreamshaper_v8" else lcm_prefs['lcm_custom_model']
    if 'loaded_lcm' not in status: status['loaded_lcm'] = ""
    if 'loaded_lcm_mode' not in status: status['loaded_lcm_mode'] = ""
    if lcm_model != status['loaded_lcm']:
        clear_pipes()
    #from optimum.intel import OVLatentConsistencyModelPipeline
    #pipe = OVLatentConsistencyModelPipeline.from_pretrained("rupeshs/LCM-dreamshaper-v7-openvino-int8", ov_config={"CACHE_DIR": ""})
    
    from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image, LCMScheduler
    if pipe_lcm == None:
        installer.status(f"...initialize LCM Pipeline")
        try:
            if bool(lcm_prefs['init_image']):
                pipe_lcm = AutoPipelineForImage2Image.from_pretrained(lcm_model, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                status['loaded_lcm_mode'] = "Image2Image"
            else:
                pipe_lcm = AutoPipelineForText2Image.from_pretrained(lcm_model, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                status['loaded_lcm_mode'] = "Text2Image"
            #pipe_lcm = pipeline_scheduler(pipe_lcm)
            pipe_lcm.scheduler = LCMScheduler.from_config(pipe_lcm.scheduler.config)
            if prefs['enable_torch_compile']:
                installer.status(f"...Torch compiling transformer")
                pipe_lcm.transformer = torch.compile(pipe_lcm.transformer, mode="reduce-overhead", fullgraph=True)
                pipe_lcm = pipe_lcm.to(torch_device)
            elif cpu_offload:
                pipe_lcm.enable_model_cpu_offload()
            else:
                pipe_lcm.to(torch_device)
            pipe_lcm.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing LCM...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        status['loaded_lcm'] = lcm_model
    else:
        clear_pipes('lcm')
    ip_adapter_arg = {}
    if lcm_prefs['use_ip_adapter']:
        installer.status(f"...initialize IP-Adapter")
        ip_adapter_img = None
        if lcm_prefs['ip_adapter_image'].startswith('http'):
          i_response = requests.get(lcm_prefs['ip_adapter_image'])
          ip_adapter_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
        else:
          if os.path.isfile(lcm_prefs['ip_adapter_image']):
            ip_adapter_img = PILImage.open(lcm_prefs['ip_adapter_image'])
          else:
            clear_last()
            prt(f"ERROR: Couldn't find your ip_adapter_image {lcm_prefs['ip_adapter_image']}")
        if bool(ip_adapter_img):
          ip_adapter_arg['ip_adapter_image'] = ip_adapter_img
        if bool(ip_adapter_arg):
            ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == lcm_prefs['ip_adapter_model'])
            pipe_lcm.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
            pipe_lcm.set_ip_adapter_scale(lcm_prefs['ip_adapter_strength'])

    clear_last()
    s = "" if len(lcm_prompts) == 1 else "s"
    prt(f"Generating your LCM Image{s}...")
    for pr in lcm_prompts:
        prt(progress)
        autoscroll(False)
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(int(pr['seed']))
        generator = torch.Generator(device="cpu").manual_seed(random_seed)
        init_img = None
        if bool(pr['init_image']):
            fname = pr['init_image'].rpartition(slash)[2]
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        try:
            if init_img is not None:
                if status['loaded_lcm_mode'] != "Image2Image":
                    pipe_lcm = AutoPipelineForImage2Image.from_pipe(pipe_lcm)
                    status['loaded_lcm_mode'] = "Image2Image"
                images = pipe_lcm(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    num_images_per_prompt=pr['num_images'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=pr['num_inference_steps'],
                    guidance_scale=pr['guidance_scale'],
                    init_image=init_img,
                    init_image_strength=pr['init_image_strength'],
                    generator=generator,
                    #callback_on_step_end=callback_fnc,
                    **ip_adapter_arg,
                ).images
            else:
                if status['loaded_lcm_mode'] != "Text2Image":
                    pipe_lcm = AutoPipelineForText2Image.from_pipe(pipe_lcm)
                    status['loaded_lcm_mode'] = "Text2Image"
                images = pipe_lcm(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    num_images_per_prompt=pr['num_images'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=pr['num_inference_steps'],
                    guidance_scale=pr['guidance_scale'],
                    generator=generator,
                    #callback_on_step_end=callback_fnc,
                    **ip_adapter_arg,
                ).images
        except Exception as e:
            clear_last()
            clear_last()
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        #clear_last()
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(lcm_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, lcm_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{lcm_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            if not lcm_prefs['display_upscaled_image'] or not lcm_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, lcm_prefs, f"LCM", lcm_model, random_seed, extra=pr)
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            batch_output = os.path.join(prefs['image_output'], lcm_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': lcm_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)

            if lcm_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=lcm_prefs["enlarge_scale"], face_enhance=lcm_prefs["face_enhance"])
                image_path = upscaled_path
                os.chdir(stable_dir)
                save_metadata(image_path, lcm_prefs, f"LCM", lcm_model, random_seed, extra=pr)
                if lcm_prefs['display_upscaled_image']:
                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(lcm_prefs["enlarge_scale"]), height=pr['height'] * float(lcm_prefs["enlarge_scale"]), data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], lcm_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], lcm_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_lcm_interpolation(page):
    global lcm_interpolation_prefs, pipe_lcm_interpolation, prefs, status
    if not check_diffusers(page): return
    if len(lcm_interpolation_prefs['mixes']) < 2:
      alert_msg(page, "You must provide layers to interpolate to process your image generation...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.LCMInterpolation.controls.append(line)
      page.LCMInterpolation.update()
    def clear_last(lines=1):
      clear_line(page.LCMInterpolation, lines=lines)
    def autoscroll(scroll=True):
      page.LCMInterpolation.auto_scroll = scroll
      page.LCMInterpolation.update()
    def clear_list():
      page.LCMInterpolation.controls = page.LCMInterpolation.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = lcm_interpolation_prefs['steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):
      #callback_fnc.has_been_called = True
      nonlocal progress
      total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    clear_list()
    autoscroll(True)
    prt(Installing("Installing LCM Interpolation Engine & Models... See console log for progress."))
    import requests
    from io import BytesIO
    from PIL import ImageOps
    from diffusers import DiffusionPipeline
    images_texts = []
    mix_names = []
    for mix in lcm_interpolation_prefs['mixes']:
        if 'prompt' in mix:
            images_texts.append(mix['prompt'])
            mix_names.append(mix['prompt'])
    mix_name = " - ".join(mix_names)
    #print(f'Resize to {width}x{height}')
    model = "SimianLuo/LCM_Dreamshaper_v7" #get_model(prefs['model_ckpt'])['path']
    if pipe_lcm_interpolation == None or status['loaded_model'] != model:
        clear_pipes()
        try:
            pipe_lcm_interpolation = DiffusionPipeline.from_pretrained(model, custom_pipeline="latent_consistency_interpolate", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_lcm_interpolation = optimize_pipe(pipe_lcm_interpolation)
            #pipe_lcm_interpolation.enable_vae_slicing()
            if prefs['enable_torch_compile']:
                pipe_lcm_interpolation.unet.to(memory_format=torch.channels_last)
                pipe_lcm_interpolation.unet = torch.compile(pipe_lcm_interpolation.unet, mode="reduce-overhead", fullgraph=True)
            else:
                pipe_lcm_interpolation.to("cuda")
            status['loaded_model'] = model
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing LCM, try running without installing Diffusers first...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
            return
    else:
        clear_pipes("lcm_interpolation")
    clear_last()
    prt("Generating your LCM Interpolation Image...")
    prt(progress)
    autoscroll(False)
    random_seed = get_seed(lcm_interpolation_prefs['seed'])
    generator = torch.Generator(device="cuda").manual_seed(random_seed)
    try:
        images = pipe_lcm_interpolation(
            prompt=images_texts,
            #num_images_per_prompt=lcm_interpolation_prefs['num_images'],
            height=lcm_interpolation_prefs['height'],
            width=lcm_interpolation_prefs['width'],
            num_interpolation_steps=lcm_interpolation_prefs['num_interpolation_steps'],
            num_inference_steps=lcm_interpolation_prefs['steps'],
            guidance_scale=lcm_interpolation_prefs['guidance_scale'],
            embedding_interpolation_type=lcm_interpolation_prefs['embedding_interpolation_type'],
            latent_interpolation_type=lcm_interpolation_prefs['latent_interpolation_type'],
            process_batch_size=lcm_interpolation_prefs['process_batch_size'],
            generator=generator,
            #callback_on_step_end=callback_fnc,
        ).images
    except Exception as e:
        clear_last(2)
        alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    clear_last(2)
    autoscroll(True)
    txt2img_output = stable_dir
    batch_output = prefs['image_output']
    if images is None:
        prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
        return
    idx = 0
    for image in images:
        fname = format_filename(mix_name)
        #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
        fname = f'{lcm_interpolation_prefs["file_prefix"]}{fname}'
        txt2img_output = stable_dir
        if bool(lcm_interpolation_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, lcm_interpolation_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        image_path = available_file(txt2img_output, fname, idx, zfill=4)
        image.save(image_path)
        new_file = image_path.rpartition(slash)[2]
        if not lcm_interpolation_prefs['display_upscaled_image'] or not lcm_interpolation_prefs['apply_ESRGAN_upscale']:
            #prt(Row([Img(src=image_path, width=lcm_interpolation_prefs['width'], height=lcm_interpolation_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            prt(Row([ImageButton(src=image_path, width=lcm_interpolation_prefs['width'], height=lcm_interpolation_prefs['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
        batch_output = os.path.join(prefs['image_output'], lcm_interpolation_prefs['batch_folder_name'])
        if not os.path.exists(batch_output):
            os.makedirs(batch_output)
        if storage_type == "PyDrive Google Drive":
            newFolder = gdrive.CreateFile({'title': lcm_interpolation_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
            newFolder.Upload()
            batch_output = newFolder
        out_path = batch_output# if save_to_GDrive else txt2img_output

        if lcm_interpolation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            upscale_image(image_path, image_path, scale=lcm_interpolation_prefs["enlarge_scale"], face_enhance=lcm_interpolation_prefs["face_enhance"])
            if lcm_interpolation_prefs['display_upscaled_image']:
                time.sleep(0.6)
                prt(Row([Img(src=asset_dir(image_path), width=lcm_interpolation_prefs['width'] * float(lcm_interpolation_prefs["enlarge_scale"]), height=lcm_interpolation_prefs['height'] * float(lcm_interpolation_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        else:
            time.sleep(0.2)
            shutil.copy(image_path, os.path.join(out_path, new_file))
        # TODO: Add Metadata
        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
        idx += 1
    if lcm_interpolation_prefs['save_video']:
        try:
            installer = Installing("Running Google FILM: Frame Interpolation for Large Motion...")
            prt(installer)
            out_file = available_file(out_path, fname, no_num=True, ext="mp4")
            if lcm_interpolation_prefs['interpolate_video']:
                installer.set_message("Saving Frames to Video using FFMPEG with Deflicker...")
                interpolate_video(images, input_fps=lcm_interpolation_prefs['source_fps'], output_fps=lcm_interpolation_prefs['target_fps'], output_video=out_file, installer=installer)
            else:
                pattern = create_pattern(new_file)
                frames_to_video(out_path, pattern=pattern, input_fps=lcm_interpolation_prefs['source_fps'], output_fps=lcm_interpolation_prefs['target_fps'], output_video=out_file, installer=installer, deflicker=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Couldn't interpolate video, but frames still saved...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            pass
        clear_last()
        prt(Markdown(f"Video saved to [{out_file}]({filepath_to_url(out_file)})", on_tap_link=lambda e: e.page.launch_url(e.data)))
    autoscroll(False)
    play_snd(Snd.ALERT, page)
    if lcm_interpolation_prefs['save_video']:
        try:
            prt(Row([VideoContainer(out_file, fps=lcm_interpolation_prefs['target_fps'], width=lcm_interpolation_prefs['width'], height=lcm_interpolation_prefs['height'])], alignment=MainAxisAlignment.CENTER))
        except:
            pass

def run_instaflow(page, from_list=False, with_params=False):
    global instaflow_prefs, pipe_instaflow, prefs, status
    if not check_diffusers(page): return
    instaflow_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            instaflow_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':instaflow_prefs['guidance_scale'], 'num_inference_steps':instaflow_prefs['num_inference_steps'], 'width':instaflow_prefs['width'], 'height':instaflow_prefs['height'], 'num_images':instaflow_prefs['num_images'], 'seed':instaflow_prefs['seed']})
        else:
            instaflow_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(instaflow_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      instaflow_prompts.append({'prompt': instaflow_prefs['prompt'], 'negative_prompt':instaflow_prefs['negative_prompt'], 'guidance_scale':instaflow_prefs['guidance_scale'], 'num_inference_steps':instaflow_prefs['num_inference_steps'], 'width':instaflow_prefs['width'], 'height':instaflow_prefs['height'], 'num_images':instaflow_prefs['num_images'], 'seed':instaflow_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.InstaFlow.controls.append(line)
        if update:
          page.InstaFlow.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.InstaFlow, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.InstaFlow.auto_scroll = scroll
        page.InstaFlow.update()
      else:
        page.InstaFlow.auto_scroll = scroll
        page.InstaFlow.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.InstaFlow.controls = page.InstaFlow.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = instaflow_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing InstaFlow Engine & Models... See console for progress.")
    prt(installer)
    clear_pipes("instaflow")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    cpu_offload = instaflow_prefs['cpu_offload']
    instaflow_model = "XCLIU/instaflow_0_9B_from_sd_1_5" if instaflow_prefs['instaflow_model'] == "instaflow_0_9B_from_sd_1_5" else instaflow_prefs['instaflow_custom_model']
    if 'loaded_instaflow' not in status: status['loaded_instaflow'] = ""
    if instaflow_model != status['loaded_instaflow']:
        clear_pipes()
    #from optimum.intel import OVLatentConsistencyModelPipeline
    #pipe = OVLatentConsistencyModelPipeline.from_pretrained("rupeshs/InstaFlow-dreamshaper-v7-openvino-int8", ov_config={"CACHE_DIR": ""})
    
    from diffusers import DiffusionPipeline
    if pipe_instaflow == None:
        installer.status(f"...initialize InstaFlow Pipeline")
        try:
            pipe_instaflow = DiffusionPipeline.from_pretrained(instaflow_model, torch_dtype=torch.float16, custom_pipeline="instaflow_one_step", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
            #pipe_instaflow = pipeline_scheduler(pipe_instaflow)
            #pipe_instaflow.scheduler = InstaFlowScheduler.from_config(pipe_instaflow.scheduler.config)
            if prefs['vae_slicing']:
                pipe_instaflow.enable_vae_slicing()
            if prefs['vae_tiling']:
                pipe_instaflow.enable_vae_tiling()
            if prefs['enable_torch_compile']:
                installer.status(f"...Torch compiling transformer")
                pipe_instaflow.transformer = torch.compile(pipe_instaflow.transformer, mode="reduce-overhead", fullgraph=True)
                pipe_instaflow = pipe_instaflow.to(torch_device)
            elif cpu_offload:
                pipe_instaflow.enable_model_cpu_offload()
            else:
                pipe_instaflow.to(torch_device)
            pipe_instaflow.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing InstaFlow...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        status['loaded_instaflow'] = instaflow_model
    else:
        clear_pipes('instaflow')

    clear_last()
    s = "" if len(instaflow_prompts) == 1 else "s"
    prt(f"Generating your InstaFlow Image{s}...")
    for pr in instaflow_prompts:
        prt(progress)
        autoscroll(False)
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(int(pr['seed']))
        generator = torch.Generator().manual_seed(random_seed)
        try:
            images = pipe_instaflow(
                prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                num_images_per_prompt=pr['num_images'],
                height=pr['height'],
                width=pr['width'],
                num_inference_steps=pr['num_inference_steps'],
                guidance_scale=0.0,#pr['guidance_scale'],
                generator=generator,
                callback=callback_fnc,
            ).images
        except Exception as e:
            clear_last()
            clear_last()
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        #clear_last()
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(instaflow_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, instaflow_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{instaflow_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            if not instaflow_prefs['display_upscaled_image'] or not instaflow_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, instaflow_prefs, "InstaFlow", instaflow_model, random_seed, extra=pr)
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            batch_output = os.path.join(prefs['image_output'], instaflow_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': instaflow_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)

            if instaflow_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=instaflow_prefs["enlarge_scale"], face_enhance=instaflow_prefs["face_enhance"])
                image_path = upscaled_path
                os.chdir(stable_dir)
                save_metadata(image_path, instaflow_prefs, "InstaFlow", instaflow_model, random_seed, extra=pr)
                if instaflow_prefs['display_upscaled_image']:
                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(instaflow_prefs["enlarge_scale"]), height=pr['height'] * float(instaflow_prefs["enlarge_scale"]), data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], instaflow_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], instaflow_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_pag(page, from_list=False, with_params=False):
    global pag_prefs, pipe_PAG, prefs, status
    if not check_diffusers(page): return
    pag_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            pag_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':pag_prefs['guidance_scale'], 'num_inference_steps':pag_prefs['num_inference_steps'], 'width':pag_prefs['width'], 'height':pag_prefs['height'], 'init_image':pag_prefs['init_image'], 'mask_image': p['mask_image'] if bool(p['mask_image']) else pag_prefs['mask_image'], 'init_image_strength':pag_prefs['init_image_strength'], 'num_images':pag_prefs['num_images'], 'seed':pag_prefs['seed']})
        else:
            pag_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'init_image':p['init_image'], 'mask_image':p['mask_image'], 'init_image_strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(pag_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      pag_prompts.append({'prompt': pag_prefs['prompt'], 'negative_prompt':pag_prefs['negative_prompt'], 'guidance_scale':pag_prefs['guidance_scale'], 'num_inference_steps':pag_prefs['num_inference_steps'], 'width':pag_prefs['width'], 'height':pag_prefs['height'], 'init_image':pag_prefs['init_image'], 'mask_image':pag_prefs['mask_image'], 'init_image_strength':pag_prefs['init_image_strength'], 'num_images':pag_prefs['num_images'], 'seed':pag_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.PAG.controls.append(line)
        if update:
          page.PAG.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.PAG, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.PAG.auto_scroll = scroll
        page.PAG.update()
      else:
        page.PAG.auto_scroll = scroll
        page.PAG.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.PAG.controls = page.PAG.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = pag_prefs['num_inference_steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    cpu_offload = pag_prefs['cpu_offload']
    use_SDXL = pag_prefs['use_SDXL']
    if use_SDXL:
        model = get_SDXL_model(prefs['SDXL_model'])
        pag_model = model['path']
    else:
        model = get_model(prefs['model_ckpt'])
        pag_model = model['path']
    #"SimianLuo/PAG_Dreamshaper_v7" if pag_prefs['pag_model'] == "PAG_Dreamshaper_v7" else "Lykon/dreamshaper-8-pag" if pag_prefs['pag_model'] == "PAG_Dreamshaper_v8" else pag_prefs['pag_custom_model']
    clear_list()
    autoscroll(True)
    installer = Installing(f"Installing PAG Engine & {model['name']} Model...")
    prt(installer)
    if 'loaded_pag' not in status: status['loaded_pag'] = ""
    if 'loaded_pag_mode' not in status: status['loaded_pag_mode'] = ""
    if 'loaded_pag_layers' not in status: status['loaded_pag_layers'] = []
    if pag_model != status['loaded_pag']:
        clear_pipes()
    else:
        clear_pipes('PAG')
    def change_mode(pipe, mode):
        status['loaded_pag_mode'] = mode
        if mode == "inpaint":
            from diffusers import AutoPipelineForInpainting
            return AutoPipelineForInpainting.from_pipe(pipe, enable_pag=True)
        elif mode == "img2img":
            from diffusers import AutoPipelineForImage2Image
            return AutoPipelineForImage2Image.from_pipe(pipe, enable_pag=True)
        else:
            from diffusers import AutoPipelineForText2Image
            return AutoPipelineForText2Image.from_pipe(pipe, enable_pag=True)
    #from optimum.intel import OVLatentConsistencyModelPipeline
    #pipe = OVLatentConsistencyModelPipeline.from_pretrained("rupeshs/PAG-dreamshaper-v7-openvino-int8", ov_config={"CACHE_DIR": ""})
    #from accelerate.utils import set_seed
    #from diffusers import StableDiffusionPipeline
    pag_applied_layers = []
    if pag_prefs['applied_layer_down']: pag_applied_layers.append("down")# if use_SDXL else "d4"
    if pag_prefs['applied_layer_mid']: pag_applied_layers.append("mid")# if use_SDXL else "m0"
    if pag_prefs['applied_layer_up']: pag_applied_layers.append("up")# if use_SDXL else "u0"
    if pipe_PAG == None:
        installer.status(f"...initialize PAG Pipeline")
        try:
            #if use_SDXL:
            from diffusers import AutoPipelineForText2Image
            #pag_applied_layers = "down.block_1" if pag_prefs['pag_applied_layers'] == "down" else "up.block_0.attentions_0" if pag_prefs['pag_applied_layers'] == "up" else "mid"
            pipe_PAG = AutoPipelineForText2Image.from_pretrained(pag_model, enable_pag=True, pag_applied_layers=pag_applied_layers, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            if cpu_offload:
                pipe_PAG.enable_model_cpu_offload()
            else:
                pipe_PAG = pipe_PAG.to(torch_device)
            '''else:
                pipe_PAG = StableDiffusionPipeline.from_pretrained(pag_model, custom_pipeline="hyoungwoncho/sd_perturbed_attention_guidance", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                #pipe_PAG = pipeline_scheduler(pipe_PAG)
                #pipe_PAG.scheduler = PAGScheduler.from_config(pipe_PAG.scheduler.config)
                pipe_PAG = optimize_pipe(pipe_PAG)'''
            pipe_PAG.set_progress_bar_config(disable=True)
            status['loaded_pag_mode'] = "txt2img"
            status['loaded_pag_layers'] = pag_applied_layers
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing PAG...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        status['loaded_pag'] = pag_model
    elif pag_applied_layers != status['loaded_pag_layers'] and use_SDXL:
        pipe_PAG.set_pag_applied_layers(pag_applied_layers)
        status['loaded_pag_layers'] = pag_applied_layers
    ip_adapter_arg = {}
    if pag_prefs['use_ip_adapter']:
        installer.status(f"...initialize IP-Adapter")
        ip_adapter_img = None
        if pag_prefs['ip_adapter_image'].startswith('http'):
          i_response = requests.get(pag_prefs['ip_adapter_image'])
          ip_adapter_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
        else:
          if os.path.isfile(pag_prefs['ip_adapter_image']):
            ip_adapter_img = PILImage.open(pag_prefs['ip_adapter_image'])
          else:
            clear_last()
            prt(f"ERROR: Couldn't find your ip_adapter_image {pag_prefs['ip_adapter_image']}")
        if bool(ip_adapter_img):
          ip_adapter_arg['ip_adapter_image'] = ip_adapter_img
        if bool(ip_adapter_arg):
            if use_SDXL:
              ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == pag_prefs['ip_adapter_SDXL_model'])
            else:
              ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == pag_prefs['ip_adapter_model'])
            pipe_PAG.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
            pipe_PAG.set_ip_adapter_scale(pag_prefs['ip_adapter_strength'])
    clear_last()
    s = "" if len(pag_prompts) == 1 else "s"
    prt(f"Generating your PAG Image{s}...")
    for pr in pag_prompts:
        prt(progress)
        autoscroll(False)
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(pr['seed'])
        #if use_SDXL:
        generator = torch.Generator(device="cpu").manual_seed(random_seed)
        #else:
        #    set_seed(random_seed)
        '''init_img = None
        if bool(pr['init_image']):
            fname = pr['init_image'].rpartition(slash)[2]
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")'''
        init_img = None
        mask_img = None
        if bool(pr['init_image']) and use_SDXL:
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            max_size = max(pr['width'], pr['height'])
            width, height = init_img.size
            width, height = scale_dimensions(width, height, max_size)
            if bool(pag_prefs['alpha_mask']):
                init_img = init_img.convert("RGBA")
            else:
                init_img = init_img.convert("RGB")
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            if not bool(pr['mask_image']) and bool(pag_prefs['alpha_mask']):
                mask_img = init_img.convert('RGBA')
                red, green, blue, alpha = PILImage.Image.split(init_img)
                mask_img = alpha.convert('L')
            elif bool(pr['mask_image']):
                if pr['mask_image'].startswith('http'):
                    mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)
                else:
                    if os.path.isfile(pr['mask_image']):
                        mask_img = PILImage.open(pr['mask_image'])
                    else:
                        alert_msg(page, f"ERROR: Couldn't find your mask_image {pr['mask_image']}")
                        return
                width, height = mask_img.size
                width, height = scale_dimensions(width, height, max_size)
                mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            if pag_prefs['invert_mask'] and not pag_prefs['alpha_mask']:
                from PIL import ImageOps
                mask_img = ImageOps.invert(mask_img.convert('RGB'))
        mode = "inpaint" if init_img != None and mask_img != None else "img2img" if init_img != None else "txt2img"
        if status['loaded_pag_mode'] != mode:
            prt(Installing(f"Switching to PAG {mode} Pipeline..."))
            pipe_PAG = change_mode(pipe_PAG, mode)
            clear_last()
        try:
            #if use_SDXL:
            img_mode = {}
            if mode == "img2img" or mode == "inpaint":
                img_mode = {'strength': pr['init_image_strength'], 'image': init_img}
            if mode == "inpaint":
                img_mode['mask_image'] = mask_img
            images = pipe_PAG(
                prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                num_images_per_prompt=pr['num_images'],
                height=pr['height'],
                width=pr['width'],
                num_inference_steps=pr['num_inference_steps'],
                guidance_scale=pr['guidance_scale'],
                pag_scale=pag_prefs['pag_scale'],
                #pag_applied_layers=[pag_prefs['pag_applied_layers']],
                #pag_applied_layers_index=[pag_prefs['pag_applied_layers_index']],
                pag_adaptive_scale=pag_prefs['pag_adaptive_scaling'],
                #pag_drop_rate=pag_prefs['pag_drop_rate'],
                #latents=latent_input,
                generator=generator,
                callback_on_step_end=callback_fnc,
                **img_mode,
                **ip_adapter_arg,
            ).images
            '''else:
                from diffusers.utils.torch_utils import randn_tensor
                latent_input = randn_tensor(shape=(1,4,64,64), generator=None, device=torch_device, dtype=torch.float16)
                images = pipe_PAG(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    num_images_per_prompt=pr['num_images'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=pr['num_inference_steps'],
                    guidance_scale=pr['guidance_scale'],
                    pag_scale=pag_prefs['pag_scale'],
                    pag_applied_layers_index=pag_applied_layers,#[pag_prefs['pag_applied_layers']],
                    #pag_applied_layers_index=[pag_prefs['pag_applied_layers_index']],
                    pag_adaptive_scaling=pag_prefs['pag_adaptive_scaling'],
                    #pag_drop_rate=pag_prefs['pag_drop_rate'],
                    latents=latent_input,
                    #generator=generator,
                    callback_on_step_end=callback_fnc,
                    **ip_adapter_arg,
                ).images'''
        except Exception as e:
            clear_last(2)
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]), debug_pref=pag_prefs)
            return
        #clear_last()
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(pag_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, pag_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{pag_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            
            batch_output = os.path.join(prefs['image_output'], pag_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': pag_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)

            if pag_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=pag_prefs["enlarge_scale"], face_enhance=pag_prefs["face_enhance"])
                image_path = upscaled_path
                os.chdir(stable_dir)
                save_metadata(image_path, pag_prefs, f"PAG", pag_model, random_seed, extra=pr)
                if pag_prefs['display_upscaled_image']:
                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(pag_prefs["enlarge_scale"]), height=pr['height'] * float(pag_prefs["enlarge_scale"]), data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if not pag_prefs['display_upscaled_image'] or not pag_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, pag_prefs, f"PAG", pag_model, random_seed, extra=pr)
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], pag_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], pag_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            #if not pag_prefs['display_upscaled_image'] or not pag_prefs['apply_ESRGAN_upscale']:
            #    prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_ldm3d(page, from_list=False, with_params=False):
    global ldm3d_prefs, pipe_ldm3d, pipe_ldm3d_upscale, prefs
    if not check_diffusers(page): return
    ldm3d_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            ldm3d_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':ldm3d_prefs['guidance_scale'], 'num_inference_steps':ldm3d_prefs['num_inference_steps'], 'width':ldm3d_prefs['width'], 'height':ldm3d_prefs['height'], 'num_images':ldm3d_prefs['num_images'], 'seed':ldm3d_prefs['seed']})
        else:
            ldm3d_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(ldm3d_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      ldm3d_prompts.append({'prompt': ldm3d_prefs['prompt'], 'negative_prompt':ldm3d_prefs['negative_prompt'], 'guidance_scale':ldm3d_prefs['guidance_scale'], 'num_inference_steps':ldm3d_prefs['num_inference_steps'], 'width':ldm3d_prefs['width'], 'height':ldm3d_prefs['height'], 'num_images':ldm3d_prefs['num_images'], 'seed':ldm3d_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.LDM3D.controls.append(line)
        if update:
          page.LDM3D.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.LDM3D, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.LDM3D.auto_scroll = scroll
        page.LDM3D.update()
      else:
        page.LDM3D.auto_scroll = scroll
        page.LDM3D.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.LDM3D.controls = page.LDM3D.controls[:1]
    progress = ProgressBar(bar_height=8)
    #total_steps = ldm3d_prefs['num_inference_steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):
      #callback_fnc.has_been_called = True
      nonlocal progress
      total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing LDM3D Engine & Models... See console for progress.")
    prt(installer)
    clear_pipes("ldm3d")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    cpu_offload = ldm3d_prefs['cpu_offload']
    ldm3d_model = ldm3d_prefs['ldm3d_model'] if ldm3d_prefs['ldm3d_model'] != "Custom" else ldm3d_prefs['ldm3d_custom_model']
    if 'loaded_ldm3d' not in status: status['loaded_ldm3d'] = ""
    if ldm3d_model != status['loaded_ldm3d']:
        clear_pipes()
    from diffusers import StableDiffusionLDM3DPipeline, LCMScheduler
    if pipe_ldm3d == None:
        installer.status(f"...initialize LDM3D Pipeline")
        try:
            pipe_ldm3d = StableDiffusionLDM3DPipeline.from_pretrained(ldm3d_model, torch_dtype=torch.float16, safety=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_ldm3d = pipeline_scheduler(pipe_ldm3d)
            #pipe_ldm3d.scheduler = LCMScheduler.from_config(pipe_ldm3d.scheduler.config)
            if prefs['enable_torch_compile']:
                installer.status(f"...Torch compiling transformer")
                pipe_ldm3d.transformer = torch.compile(pipe_ldm3d.transformer, mode="reduce-overhead", fullgraph=True)
                pipe_ldm3d = pipe_ldm3d.to(torch_device)
            elif cpu_offload:
                pipe_ldm3d.enable_model_cpu_offload()
            else:
                pipe_ldm3d.to(torch_device)
            pipe_ldm3d.set_progress_bar_config(disable=True)
            if ldm3d_prefs['use_upscale']:
                installer.status(f"...initialize LDM3D Upscale Pipeline")
                from diffusers import DiffusionPipeline#StableDiffusionUpscaleLDM3DPipeline
                pipe_ldm3d_upscale = DiffusionPipeline.from_pretrained("Intel/ldm3d-sr", custom_pipeline="pipeline_stable_diffusion_upscale_ldm3d")#StableDiffusionUpscaleLDM3DPipeline.from_pretrained("Intel/ldm3d-sr")
                pipe_ldm3d_upscale.to("cuda")
                '''low_res_img = Image.open(f"lemons_ldm3d_rgb.jpg").convert("RGB")
                low_res_depth = Image.open(f"lemons_ldm3d_depth.png").convert("L")
                outputs = pipe_ldm3d_upscale(prompt="high quality high resolution uhd 4k image", rgb=low_res_img, depth=low_res_depth, num_inference_steps=50, target_res=[1024, 1024])
                upscaled_rgb, upscaled_depth =outputs.rgb[0], outputs.depth[0]'''
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing LDM3D...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        status['loaded_ldm3d'] = ldm3d_model
    else:
        clear_pipes('ldm3d')
        if prefs['scheduler_mode'] != status['loaded_scheduler']:
            pipe_ldm3d = pipeline_scheduler(pipe_ldm3d)
    ip_adapter_arg = {}
    if ldm3d_prefs['use_ip_adapter']:
      installer.status(f"...initialize IP-Adapter")
      ip_adapter_img = None
      if ldm3d_prefs['ip_adapter_image'].startswith('http'):
        i_response = requests.get(ldm3d_prefs['ip_adapter_image'])
        ip_adapter_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
      else:
        if os.path.isfile(ldm3d_prefs['ip_adapter_image']):
          ip_adapter_img = PILImage.open(ldm3d_prefs['ip_adapter_image'])
        else:
          clear_last()
          prt(f"ERROR: Couldn't find your ip_adapter_image {ldm3d_prefs['ip_adapter_image']}")
      if bool(ip_adapter_img):
        ip_adapter_arg['ip_adapter_image'] = ip_adapter_img
      if bool(ip_adapter_arg):
          ip_adapter_model = next(m for m in ip_adapter_SDXL_models if m['name'] == ldm3d_prefs['ip_adapter_model'])
          pipe_ldm3d.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
          pipe_ldm3d.set_ip_adapter_scale(ldm3d_prefs['ip_adapter_strength'])

    clear_last()
    s = "" if len(ldm3d_prompts) == 1 else "s"
    prt(f"Generating your LDM3D Image{s}...")
    for pr in ldm3d_prompts:
        prt(progress)
        autoscroll(False)
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(pr['seed'])
        generator = torch.Generator(device="cuda").manual_seed(random_seed)
        try:
            images = pipe_ldm3d(
                prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                num_images_per_prompt=pr['num_images'],
                height=pr['height'],
                width=pr['width'],
                num_inference_steps=pr['num_inference_steps'],
                guidance_scale=pr['guidance_scale'],
                generator=generator,
                callback_on_step_end=callback_fnc,
                **ip_adapter_arg,
            )
            if ldm3d_prefs['use_upscale']:
                target_width=pr['width'] * 2
                target_height=pr['height'] * 2
                prt(f"Upscaling Image to {target_width}x{target_height}...")
                images = pipe_ldm3d_upscale(prompt="high quality high resolution uhd 4k image", rgb=images.rgb, depth=images.depth, num_inference_steps=pr['num_inference_steps'], target_res=[target_width, target_height]).images
                clear_last()
        except Exception as e:
            clear_last()
            clear_last()
            alert_msg(page, f"ERROR: Something went wrong generating images...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(ldm3d_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, ldm3d_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        #for image in images:
        for i in range(len(images.rgb)):
            fname = format_filename(pr['prompt'])
            fname = f'{ldm3d_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname+"-rgb", i, no_num=True)
            depth_image_path = available_file(txt2img_output, fname+"-depth", i, no_num=True)
            rgb_image, depth_image = images.rgb[i], images.depth[i]
            rgb_image.save(image_path)
            depth_image.save(depth_image_path)
            output_file = image_path.rpartition(slash)[2]
            depth_output_file = image_path.rpartition(slash)[2]
            if not ldm3d_prefs['display_upscaled_image'] or not ldm3d_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
                prt(Row([ImageButton(src=depth_image_path, width=pr['width'], height=pr['height'], data=depth_image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            batch_output = os.path.join(prefs['image_output'], ldm3d_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': ldm3d_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)
            depth_upscaled_path = os.path.join(out_path, depth_output_file)
            if ldm3d_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=ldm3d_prefs["enlarge_scale"], face_enhance=ldm3d_prefs["face_enhance"])
                upscale_image(depth_image_path, depth_upscaled_path, scale=ldm3d_prefs["enlarge_scale"], face_enhance=ldm3d_prefs["face_enhance"])
                image_path = upscaled_path
                depth_image_path = depth_upscaled_path
                os.chdir(stable_dir)
                if ldm3d_prefs['display_upscaled_image']:
                    prt(Row([ImageButton(src=upscaled_path, width=pr['width'] * float(ldm3d_prefs["enlarge_scale"]), height=pr['height'] * float(ldm3d_prefs["enlarge_scale"]), data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
                    prt(Row([ImageButton(src=depth_upscaled_path, width=pr['width'] * float(ldm3d_prefs["enlarge_scale"]), height=pr['height'] * float(ldm3d_prefs["enlarge_scale"]), data=depth_image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            if prefs['save_image_metadata']:
                imgs = [image_path, depth_image_path]
                for image_file in imgs:
                    img = PILImage.open(image_file)
                    metadata = PngInfo()
                    metadata.add_text("artist", prefs['meta_ArtistName'])
                    metadata.add_text("copyright", prefs['meta_Copyright'])
                    metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {ldm3d_prefs['enlarge_scale']}x with ESRGAN" if ldm3d_prefs['apply_ESRGAN_upscale'] else "")
                    metadata.add_text("pipeline", f"LDM3D")
                    if prefs['save_config_in_metadata']:
                        config_json = ldm3d_prefs.copy()
                        config_json['model_path'] = ldm3d_model
                        config_json['seed'] = random_seed
                        del config_json['num_images']
                        del config_json['display_upscaled_image']
                        del config_json['batch_folder_name']
                        if not config_json['apply_ESRGAN_upscale']:
                            del config_json['enlarge_scale']
                            del config_json['apply_ESRGAN_upscale']
                        metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                    img.save(image_file, pnginfo=metadata)
            if storage_type == "Colab Google Drive":
                new_file = os.path.join(prefs['image_output'], ldm3d_prefs['batch_folder_name'], os.path.basename(image_path))
                depth_new_file = os.path.join(prefs['image_output'], ldm3d_prefs['batch_folder_name'], os.path.basename(depth_image_path))
                out_path = new_file
                shutil.copy(image_path, new_file)
                shutil.copy(depth_image_path, depth_new_file)
            elif bool(prefs['image_output']):
                new_file = os.path.join(prefs['image_output'], ldm3d_prefs['batch_folder_name'], os.path.basename(image_path))
                depth_new_file = os.path.join(prefs['image_output'], ldm3d_prefs['batch_folder_name'], os.path.basename(depth_image_path))
                out_path = new_file
                shutil.copy(image_path, new_file)
                shutil.copy(depth_image_path, depth_new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)
    

def run_task_matrix(page):
    global prefs, status, task_matrix_prefs, pipe_task_matrix
    if not bool(prefs['OpenAI_api_key']):
      alert_msg(page, "You must provide your OpenAI API Key in Settings to use...")
      return
    if not check_diffusers(page): return
    if not bool(task_matrix_prefs['prompt']):
      alert_msg(page, "You must provide a request to process your image generation...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.TaskMatrix.controls.append(line)
      page.TaskMatrix.update()
    def clear_last(lines=1):
      clear_line(page.TaskMatrix, lines=lines)
    def clear_list():
      page.TaskMatrix.controls = page.TaskMatrix.controls[:1]
    def autoscroll(scroll=True):
      page.TaskMatrix.auto_scroll = scroll
      page.TaskMatrix.update()
    #clear_list()
    if 'active_modules' not in status:
        status['active_modules'] = []
    clear_pipes("task_matrix")
    autoscroll(True)
    if status['active_modules'] != task_matrix_prefs['modules']:
        installer = Installing("Installing TaskMatrix Visual ChatGPT Pipeline...")
        prt(installer)
    if pipe_task_matrix is None:
        task_matrix_dir = os.path.join(root_dir, 'TaskMatrix')
        if not os.path.exists(task_matrix_dir):
            installer.status("...cloning microsoft/TaskMatrix")#
            run_sp("git clone https://github.com/microsoft/TaskMatrix", cwd=root_dir, realtime=False)
        if task_matrix_dir not in sys.path:
            sys.path.append(task_matrix_dir)
        try:
            import groundingdino
        except ModuleNotFoundError:
            installer.status("...IDEA-Research/GroundingDINO")
            run_sp("pip install git+https://github.com/IDEA-Research/GroundingDINO.git")
        try:
            import segment_anything
        except ModuleNotFoundError:
            installer.status("...facebookresearch/segment-anything")
            run_sp("pip install git+https://github.com/facebookresearch/segment-anything.git") #langchain==0.0.101
        pip_install("langchain accelerate addict albumentations basicsr controlnet-aux einops gradio imageio imageio-ffmpeg invisible-watermark|imwatermark kornia numpy omegaconf open_clip_torch openai opencv-python prettytable safetensors streamlit test-tube timm torchmetrics webdataset yapf mediapipe", print=False, installer=installer)
        os.environ['OPENAI_API_KEY'] = prefs['OpenAI_api_key']
        makedir(os.path.join(task_matrix_dir, 'checkpoint'))
    try:
        from visual_chatgpt import ConversationBot
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR: Initializing visual_chatgpt ConversationBot...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    if status['active_modules'] != task_matrix_prefs['modules'] or pipe_task_matrix is None:
        #load = "Text2Box_cuda:0,Segmenting_cuda:0,Inpainting_cuda:0,ImageCaptioning_cuda:0"
        #load_dict = {e.split('_')[0].strip(): e.split('_')[1].strip() for e in load.split(',')}
        load_dict = {'ImageCaptioning': 'cuda:0'}
        for m in task_matrix_prefs['modules']:
            load_dict[m] = 'cuda:0'
        clear_task_matrix_pipe()
        pipe_task_matrix = ConversationBot(load_dict=load_dict)
        pipe_task_matrix.init_agent("English")
        clear_last()
    status['active_modules'] = task_matrix_prefs['modules']
    progress = Progress("Getting Response from TaskMatrix Bot...")
    prt(progress)
    class State:
        def __init__(self, initial_value, render: bool = True):
            self._value = initial_value
            self.render = render
            self.callbacks = []
        @property
        def value(self):
            return self._value
        @value.setter
        def value(self, new_value):
            self.set_value(new_value)
        def set_value(self, new_value):
            self._value = new_value
            for callback in self.callbacks:
                callback(new_value)
        def add_callback(self, callback):
            self.callbacks.append(callback)
    def callback(value):
        print("State:", value)
        progress.status(f"...{value}")
    state = State([])
    state.add_callback(callback)
    try:
        if bool(task_matrix_prefs['image_path']):
            from PIL import ImageOps
            image_path = task_matrix_prefs['image_path']
            input_img = None
            fname = image_path.rpartition(slash)[2]
            if image_path.startswith('http'):
                input_img = PILImage.open(requests.get(image_path, stream=True).raw)
            else:
                if os.path.isfile(image_path):
                    input_img = PILImage.open(image_path)
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {image_path}")
                    return
            w, h = input_img.size
            w, h = scale_dimensions(w, h, max=1024, multiple=64)
            input_img = input_img.resize((w, h), resample=PILImage.Resampling.LANCZOS)
            input_img = ImageOps.exif_transpose(input_img).convert("RGB")
            out1, out2, txt = pipe_task_matrix.run_image(input_img, state, task_matrix_prefs['prompt'], 'English')
            print(txt)
        else:
            out1, out2 = pipe_task_matrix.run_text(task_matrix_prefs['prompt'], state)
        clear_last()
        prt(out1)
        prt(out2)
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR: Problem having Conversation with Bot...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_text_to_video(page):
    global text_to_video_prefs, prefs, status, pipe_text_to_video, model_path
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.TextToVideo.controls.append(line)
      page.TextToVideo.update()
    def clear_last(lines=1):
      clear_line(page.TextToVideo, lines=lines)
    def clear_list():
      page.TextToVideo.controls = page.TextToVideo.controls[:1]
    def autoscroll(scroll=True):
      page.TextToVideo.auto_scroll = scroll
      page.TextToVideo.update()
    progress = ProgressBar(bar_height=8)
    total_steps = text_to_video_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    prt(Installing("Installing Text-To-Video Pipeline..."))
    if text_to_video_prefs['model'] == "damo-vilab/text-to-video-ms-1.7b":
        model_id = "damo-vilab/text-to-video-ms-1.7b"
    elif text_to_video_prefs['model'] == "modelscope-damo-text2video-synthesis":
        model_id = "damo-vilab/modelscope-damo-text2video-synthesis"
    elif text_to_video_prefs['model'] == "modelscope-damo-text2video-pruned-weights":
        model_id = "kabachuha/modelscope-damo-text2video-pruned-weights"
    elif text_to_video_prefs['model'] == "cerspense/zeroscope_v2_576w":
        model_id = "cerspense/zeroscope_v2_576w"
    elif text_to_video_prefs['model'] == "cerspense/zeroscope_v2_XL":
        model_id = "cerspense/zeroscope_v2_XL"
    clear_pipes()
    #clear_pipes('text_to_video')
    if pipe_text_to_video is None:
        from diffusers import TextToVideoSDPipeline, DPMSolverMultistepScheduler
        pipe_text_to_video = TextToVideoSDPipeline.from_pretrained(model_id, torch_dtype=torch.float16, variant="fp16", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        #pipe_text_to_video = pipeline_scheduler(pipe_text_to_video)
        #pipe_text_to_video.scheduler = DPMSolverMultistepScheduler.from_config(pipe_text_to_video.scheduler.config)
        if prefs['enable_freeu']:
            pipe_text_to_video.enable_freeu(s1=0.9, s2=0.2, b1=1.1, b2=1.2)
        if text_to_video_prefs['lower_memory']:
            pipe_text_to_video.enable_sequential_cpu_offload()
            #pipe_text_to_video.enable_model_cpu_offload()
            pipe_text_to_video.unet.enable_forward_chunking(chunk_size=1, dim=1)
            pipe_text_to_video.enable_vae_tiling()
            pipe_text_to_video.enable_vae_slicing()
        else:
            pipe_text_to_video = pipe_text_to_video.to(torch_device)
        #pipe_text_to_video = optimize_pipe(pipe_text_to_video, vae_tiling=True, vae=True, to_gpu=False)
        pipe_text_to_video.set_progress_bar_config(disable=True)
    else:
        pipe_text_to_video = pipeline_scheduler(pipe_text_to_video)
    clear_last()
    prt("Generating Text-To-Video of your Prompt...")
    prt(progress)
    autoscroll(False)
    batch_output = os.path.join(stable_dir, text_to_video_prefs['batch_folder_name'])
    makedir(batch_output)
    local_output = batch_output
    batch_output = os.path.join(prefs['image_output'], text_to_video_prefs['batch_folder_name'])
    makedir(batch_output)
    random_seed = get_seed(text_to_video_prefs['seed'])
    generator = torch.Generator(device="cpu").manual_seed(random_seed)
    #generator = torch.manual_seed(random_seed)
    width = text_to_video_prefs['width']
    height = text_to_video_prefs['height']
    try:
      #print(f"prompt={text_to_video_prefs['prompt']}, negative_prompt={text_to_video_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={text_to_video_prefs['edit_momentum_scale']}, edit_mom_beta={text_to_video_prefs['edit_mom_beta']}, num_inference_steps={text_to_video_prefs['num_inference_steps']}, eta={text_to_video_prefs['eta']}, guidance_scale={text_to_video_prefs['guidance_scale']}")
      #, output_type = "pt", width=width, height=height
      frames = pipe_text_to_video(prompt=text_to_video_prefs['prompt'], negative_prompt=text_to_video_prefs['negative_prompt'], num_frames=text_to_video_prefs['num_frames'], num_inference_steps=text_to_video_prefs['num_inference_steps'], eta=text_to_video_prefs['eta'], guidance_scale=text_to_video_prefs['guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).frames
    except Exception as e:
      clear_last(2)
      alert_msg(page, f"ERROR: Couldn't Text-To-Video your image for some reason. Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
      return
    clear_last(2)
    autoscroll(True)
    save_path = os.path.join(prefs['image_output'], text_to_video_prefs['batch_folder_name'])
    filename = f"{prefs['file_prefix']}{format_filename(text_to_video_prefs['prompt'])}"
    filename = filename[:int(prefs['file_max_length'])]
    #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
    autoscroll(True)
    video_path = ""
    
    for images in frames:
        if text_to_video_prefs['export_to_video']:
            from diffusers.utils import export_to_video
            video_path = export_to_video(images)
            shutil.copy(video_path, available_file(local_output, filename, 0, ext="mp4", no_num=True))
            shutil.copy(video_path, available_file(batch_output, filename, 0, ext="mp4", no_num=True))
            #print(f"video_path: {video_path}")
        #video = frames.cpu().numpy()
        #print(f"video: {video}")
        from PIL.PngImagePlugin import PngInfo
        num = 0
        for image in images:
            random_seed += num
            fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
            image_path = available_file(batch_output, fname, num)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            #uint8_image = (image * 255).round().astype("uint8")
            #np_image = image.cpu().numpy()
            #print(f"image: {type(image)}, np_image: {type(np_image)}")
            #print(f"image: {type(image)} to {image_path}")
            cv2.imwrite(image_path, image)
            #PILImage.fromarray(np_image).save(image_path)
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)
            if not text_to_video_prefs['display_upscaled_image'] or not text_to_video_prefs['apply_ESRGAN_upscale']:
                save_metadata(unscaled_path, text_to_video_prefs, f"Text-To-Video", model_id, random_seed, scheduler=True)
                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
            if text_to_video_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=text_to_video_prefs["enlarge_scale"])
                image_path = upscaled_path
                save_metadata(image_path, text_to_video_prefs, f"Text-To-Video", model_id, random_seed, scheduler=True)
                if text_to_video_prefs['display_upscaled_image']:
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(text_to_video_prefs["enlarge_scale"]), height=height * float(text_to_video_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], text_to_video_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], text_to_video_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
            num += 1
        if text_to_video_prefs['interpolate_video']:
            from diffusers.utils import export_to_video
            prt("Running Google FILM: Frame Interpolation for Large Motion...")
            out_file = available_file(batch_output, filename, 0, ext="mp4", no_num=True)
            video_path = interpolate_video(images, input_fps=8, output_fps=30, output_video=out_file)
            clear_last()
            prt(Markdown(f"Video saved to [{out_file}]({filepath_to_url(out_file)})", on_tap_link=lambda e: e.page.launch_url(e.data)))
        if bool(video_path):
            prt(Row([VideoContainer(video_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_text_to_video_zero(page):
    global text_to_video_zero_prefs, prefs, status, pipe_text_to_video_zero, model_path
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.TextToVideo.controls.append(line)
      page.TextToVideo.update()
    def clear_last(lines=1):
      clear_line(page.TextToVideo, lines=lines)
    def clear_list():
      page.TextToVideo.controls = page.TextToVideo.controls[:1]
    def autoscroll(scroll=True):
      page.TextToVideo.auto_scroll = scroll
      page.TextToVideo.update()
      page.TextToVideo.auto_scroll = scroll
      page.TextToVideo.update()
    progress = ProgressBar(bar_height=8)
    total_steps = text_to_video_zero_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    prt(Installing("Installing Text-To-Video Zero Pipeline..."))
    model_id = get_model(prefs['model_ckpt'])['path'] if not text_to_video_zero_prefs['use_SDXL'] else get_SDXL_model(prefs['SDXL_model'])['path']
    if 'loaded_text_to_video_zero' not in status: status['loaded_text_to_video_zero'] = ""
    if model_id != status['loaded_text_to_video_zero']:
        clear_pipes()
    else:
        clear_pipes('text_to_video_zero')
    if pipe_text_to_video_zero is None:
        if not text_to_video_zero_prefs['use_SDXL']:
            from diffusers import TextToVideoZeroPipeline, DPMSolverMultistepScheduler
            pipe_text_to_video_zero = TextToVideoZeroPipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
        else:
            from diffusers import TextToVideoZeroSDXLPipeline, DPMSolverMultistepScheduler
            pipe_text_to_video_zero = TextToVideoZeroSDXLPipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
        #pipe_text_to_video_zero = pipeline_scheduler(pipe_text_to_video_zero)
        if prefs['enable_freeu']:
            pipe_text_to_video_zero.enable_freeu(s1=0.9, s2=0.2, b1=1.1, b2=1.2)
        pipe_text_to_video_zero.scheduler = DPMSolverMultistepScheduler.from_config(pipe_text_to_video_zero.scheduler.config)
        pipe_text_to_video_zero = pipe_text_to_video_zero.to(torch_device)
        #pipe_text_to_video_zero = optimize_pipe(pipe_text_to_video_zero, vae_tiling=True, vae=True, to_gpu=False)
        pipe_text_to_video_zero.set_progress_bar_config(disable=True)
    #else:
    #    pipe_text_to_video_zero = pipeline_scheduler(pipe_text_to_video_zero)
    try:
        import imageio
    except ModuleNotFoundError:
        run_sp("pip install imageio", realtime=False)
        import imageio
        pass
    clear_last()
    prt("Generating Text-To-Video Zero from your Prompt...")
    prt(progress)
    autoscroll(False)
    batch_output = os.path.join(stable_dir, text_to_video_zero_prefs['batch_folder_name'])
    makedir(batch_output)
    local_output = batch_output
    batch_output = os.path.join(prefs['image_output'], text_to_video_zero_prefs['batch_folder_name'])
    makedir(batch_output)
    random_seed = get_seed(text_to_video_zero_prefs['seed'])
    generator = torch.Generator(device="cuda").manual_seed(random_seed)
    #generator = torch.manual_seed(random_seed)
    width = text_to_video_zero_prefs['width']
    height = text_to_video_zero_prefs['height']
    try:
      #print(f"prompt={text_to_video_zero_prefs['prompt']}, negative_prompt={text_to_video_zero_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={text_to_video_zero_prefs['edit_momentum_scale']}, edit_mom_beta={text_to_video_zero_prefs['edit_mom_beta']}, num_inference_steps={text_to_video_zero_prefs['num_inference_steps']}, eta={text_to_video_zero_prefs['eta']}, guidance_scale={text_to_video_zero_prefs['guidance_scale']}")
      #, output_type = "pt", width=width, height=height
      frames = pipe_text_to_video_zero(prompt=text_to_video_zero_prefs['prompt'], negative_prompt=text_to_video_zero_prefs['negative_prompt'], video_length=text_to_video_zero_prefs['num_frames'], num_inference_steps=text_to_video_zero_prefs['num_inference_steps'], eta=text_to_video_zero_prefs['eta'], guidance_scale=text_to_video_zero_prefs['guidance_scale'], motion_field_strength_x=text_to_video_zero_prefs['motion_field_strength_x'], motion_field_strength_y=text_to_video_zero_prefs['motion_field_strength_y'], t0=text_to_video_zero_prefs['t0'], t1=text_to_video_zero_prefs['t1'], generator=generator, callback=callback_fnc, callback_steps=1).images
    except Exception as e:
      clear_last()
      clear_last()
      alert_msg(page, f"ERROR: Couldn't Text-To-Video Zero your image for some reason. Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
      return
    clear_last()
    clear_last()
    save_path = os.path.join(prefs['image_output'], text_to_video_zero_prefs['batch_folder_name'])
    filename = f"{prefs['file_prefix']}{format_filename(text_to_video_zero_prefs['prompt'])}"
    filename = filename[:int(prefs['file_max_length'])]
    #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
    autoscroll(True)
    video_path = ""
    if text_to_video_zero_prefs['export_to_video']:
        #from diffusers.utils import export_to_video
        #video_path = export_to_video(frames)
        local_file = available_file(local_output, filename, 0, ext="mp4", no_num=True)
        save_file = available_file(batch_output, filename, 0, ext="mp4", no_num=True)
        imageio.mimsave(local_file, frames, fps=4)
        shutil.copy(local_file, save_file)
        #print(f"video_path: {video_path}")
    #video = frames.cpu().numpy()
    #print(f"video: {video}")
    #print(f"frames type: {type(frames)} len: {len(frames)}")
    #result = [(r * 255).astype("uint8") for r in result]
    from PIL.PngImagePlugin import PngInfo
    num = 0
    for image in frames:
        random_seed += num
        fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
        image_path = available_file(batch_output, fname, num)
        unscaled_path = image_path
        output_file = image_path.rpartition(slash)[2]
        #uint8_image = (image * 255).round().astype("uint8")
        img = PILImage.fromarray((image * 255).astype("uint8"))
        #print(f"img type: {type(img)}")
        #np_image = image.cpu().numpy()
        #print(f"image: {type(image)}, np_image: {type(np_image)}")
        #print(f"image type: {type(image)} to {image_path}")
        #cv2.imwrite(image_path, image)
        #PILImage.fromarray(image).save(image_path)
        img.save(image_path)
        #image.save(image_path)
        #imageio.imwrite(local_file, image, extension=".png")
        #img = pipe_text_to_video_zero.numpy_to_pil(image)
        #img.save(image_path)
        #imageio.imsave(local_file, img, extension=".png")
        #PILImage.fromarray(img).save(image_path)
        out_path = os.path.dirname(image_path)
        upscaled_path = os.path.join(out_path, output_file)
        if not text_to_video_zero_prefs['display_upscaled_image'] or not text_to_video_zero_prefs['apply_ESRGAN_upscale']:
            save_metadata(unscaled_path, text_to_video_zero_prefs, f"Text-To-Video Zero", model_id, random_seed)
            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
        if text_to_video_zero_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            upscale_image(image_path, upscaled_path, scale=text_to_video_zero_prefs["enlarge_scale"])
            image_path = upscaled_path
            save_metadata(image_path, text_to_video_zero_prefs, f"Text-To-Video Zero", model_id, random_seed)
            if text_to_video_zero_prefs['display_upscaled_image']:
                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(text_to_video_zero_prefs["enlarge_scale"]), height=height * float(text_to_video_zero_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        if storage_type == "Colab Google Drive":
            new_file = available_file(os.path.join(prefs['image_output'], text_to_video_zero_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        elif bool(prefs['image_output']):
            new_file = available_file(os.path.join(prefs['image_output'], text_to_video_zero_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
        num += 1
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_video_to_video(page):
    global video_to_video_prefs, prefs, status, pipe_video_to_video, model_path
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.VideoToVideo.controls.append(line)
      page.VideoToVideo.update()
    def clear_last(lines=1):
      clear_line(page.VideoToVideo, lines=lines)
    def clear_list():
      page.TextToVideo.controls = page.VideoToVideo.controls[:1]
    def autoscroll(scroll=True):
      page.VideoToVideo.auto_scroll = scroll
      page.VideoToVideo.update()
    if not bool(video_to_video_prefs['init_video']):
      alert_msg(page, "You must provide the Input Initial Video Clip to process...")
      return
    progress = ProgressBar(bar_height=8)
    total_steps = video_to_video_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Video-To-Video Pipeline...")
    prt(installer)
    #), dropdown.Option(), dropdown.Option(), dropdown.Option(), dropdown.Option(
    #model_id = "cerspense/zeroscope_v2_XL"
    if video_to_video_prefs['model'] == "damo-vilab/text-to-video-ms-1.7b":
        model_id = "damo-vilab/text-to-video-ms-1.7b"
    elif video_to_video_prefs['model'] == "modelscope-damo-text2video-synthesis":
        model_id = "damo-vilab/modelscope-damo-text2video-synthesis"
    elif video_to_video_prefs['model'] == "modelscope-damo-text2video-pruned-weights":
        model_id = "kabachuha/modelscope-damo-text2video-pruned-weights"
    elif video_to_video_prefs['model'] == "cerspense/zeroscope_v2_576w":
        model_id = "cerspense/zeroscope_v2_576w"
    elif video_to_video_prefs['model'] == "cerspense/zeroscope_v2_XL":
        model_id = "cerspense/zeroscope_v2_XL"
    clear_pipes('video_to_video')
    if pipe_video_to_video is None:
        installer.status("...initializing VideoToVideo SDPipeline")
        from diffusers import VideoToVideoSDPipeline, DPMSolverMultistepScheduler
        pipe_video_to_video = VideoToVideoSDPipeline.from_pretrained(model_id, torch_dtype=torch.float16, variant="fp16", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        #pipe_video_to_video = pipeline_scheduler(pipe_video_to_video)
        pipe_video_to_video.scheduler = DPMSolverMultistepScheduler.from_config(pipe_video_to_video.scheduler.config)
        if video_to_video_prefs['lower_memory']:
            #pipe_video_to_video.enable_sequential_cpu_offload()
            pipe_video_to_video.enable_model_cpu_offload()
            pipe_video_to_video.unet.enable_forward_chunking(chunk_size=1, dim=1)
            #pipe_video_to_video.enable_vae_tiling()
            pipe_video_to_video.enable_vae_slicing()
        else:
            pipe_video_to_video = pipe_video_to_video.to(torch_device)
        #pipe_video_to_video = optimize_pipe(pipe_video_to_video, vae_tiling=True, vae=True, to_gpu=False)
        pipe_video_to_video.set_progress_bar_config(disable=True)
    else:
        pipe_video_to_video = pipeline_scheduler(pipe_video_to_video)
    #clear_last()
    def prep_video(vid):
        nonlocal width, height
        if vid.startswith('http'):
            init_vid = download_file(vid, stable_dir)
            installer.status("...extracting frames from video")
        else:
            if os.path.isfile(vid):
                init_vid = vid
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_video {vid}")
                return
        try:
            start_time = float(controlnet_prefs['start_time'])
            end_time = float(controlnet_prefs['end_time'])
            fps = int(controlnet_prefs['fps'])
            max_size = controlnet_prefs['max_size']
        except Exception:
            alert_msg(page, "Make sure your Numbers are actual numbers...")
            return
        installer.status("Extracting Frames from Video Clip")
        try:
            cap = cv2.VideoCapture(init_vid)
        except Exception as e:
            alert_msg(page, "ERROR Reading Video File. May be Incompatible Format...")
            clear_last()
            return
        count = 0
        video = []
        frames = []
        width = height = 0
        cap.set(cv2.CAP_PROP_FPS, fps)
        video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)
        start_frame = int(start_time * fps)
        if end_time == 0 or end_time == 0.0:
            end_frame = int(video_length)
        else:
            end_frame = int(end_time * fps)
        total = end_frame - start_frame
        for i in range(start_frame, end_frame):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            success, image = cap.read()
            if success:
                #filename = os.path.join(output_dir, f'{file_prefix}{count}.png')
                if width == 0:
                    shape = image.shape
                    width, height = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)
                image = cv2.resize(image, (width, height), interpolation = cv2.INTER_AREA)
                #cv2.imwrite(os.path.join(output_dir, filename), image)
                #image = prep_image(controlnet_prefs['control_task'], PILImage.fromarray(image))
                video.append(image)
                count += 1
        cap.release()
        #clear_last()
        return video
    init_frames = prep_video(video_to_video_prefs['init_video'])
    clear_last()
    prt(f"Generating Video-To-Video on {len(frames)} Frames with your Prompt...")
    prt(progress)
    autoscroll(False)
    batch_output = os.path.join(stable_dir, video_to_video_prefs['batch_folder_name'])
    makedir(batch_output)
    local_output = batch_output
    batch_output = os.path.join(prefs['image_output'], video_to_video_prefs['batch_folder_name'])
    makedir(batch_output)
    random_seed = get_seed(video_to_video_prefs['seed'])
    generator = torch.Generator(device="cpu").manual_seed(random_seed)
    #generator = torch.manual_seed(random_seed)
    width = video_to_video_prefs['width']
    height = video_to_video_prefs['height']
    try:
      #print(f"prompt={video_to_video_prefs['prompt']}, negative_prompt={video_to_video_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={video_to_video_prefs['edit_momentum_scale']}, edit_mom_beta={video_to_video_prefs['edit_mom_beta']}, num_inference_steps={video_to_video_prefs['num_inference_steps']}, eta={video_to_video_prefs['eta']}, guidance_scale={video_to_video_prefs['guidance_scale']}")
      #, output_type = "pt", width=width, height=height
      frames = pipe_video_to_video(prompt=video_to_video_prefs['prompt'], negative_prompt=video_to_video_prefs['negative_prompt'], video=init_frames, num_inference_steps=video_to_video_prefs['num_inference_steps'], eta=video_to_video_prefs['eta'], guidance_scale=video_to_video_prefs['guidance_scale'], strength=video_to_video_prefs['strength'], generator=generator, callback=callback_fnc, callback_steps=1).frames
    except Exception as e:
      clear_last()
      clear_last()
      alert_msg(page, f"ERROR: Couldn't Video-To-Video your image for some reason. Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
      return
    clear_last()
    clear_last()
    autoscroll(True)
    save_path = os.path.join(prefs['image_output'], video_to_video_prefs['batch_folder_name'])
    filename = f"{prefs['file_prefix']}{format_filename(video_to_video_prefs['prompt'])}"
    filename = filename[:int(prefs['file_max_length'])]
    #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
    autoscroll(True)
    video_path = ""
    if video_to_video_prefs['export_to_video']:
        from diffusers.utils import export_to_video
        video_path = export_to_video(frames)
        shutil.copy(video_path, available_file(local_output, filename, 0, ext="mp4", no_num=True))
        shutil.copy(video_path, available_file(batch_output, filename, 0, ext="mp4", no_num=True))
        #print(f"video_path: {video_path}")
    #video = frames.cpu().numpy()
    #print(f"video: {video}")
    from PIL.PngImagePlugin import PngInfo
    num = 0
    for image in frames:
        random_seed += num
        fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
        image_path = available_file(batch_output, fname, num)
        unscaled_path = image_path
        output_file = image_path.rpartition(slash)[2]
        #uint8_image = (image * 255).round().astype("uint8")
        #np_image = image.cpu().numpy()
        #print(f"image: {type(image)}, np_image: {type(np_image)}")
        #print(f"image: {type(image)} to {image_path}")
        cv2.imwrite(image_path, image)
        #PILImage.fromarray(np_image).save(image_path)
        out_path = os.path.dirname(image_path)
        upscaled_path = os.path.join(out_path, output_file)
        if not video_to_video_prefs['display_upscaled_image'] or not video_to_video_prefs['apply_ESRGAN_upscale']:
            save_metadata(unscaled_path, video_to_video_prefs, f"Video-To-Video", model_id, random_seed, scheduler=True)
            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
        if video_to_video_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            upscale_image(image_path, upscaled_path, scale=video_to_video_prefs["enlarge_scale"], face_enhance=video_to_video_prefs["face_enhance"])
            image_path = upscaled_path
            save_metadata(image_path, video_to_video_prefs, f"Video-To-Video", model_id, random_seed, scheduler=True)
            if video_to_video_prefs['display_upscaled_image']:
                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(video_to_video_prefs["enlarge_scale"]), height=height * float(video_to_video_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        if storage_type == "Colab Google Drive":
            new_file = available_file(os.path.join(prefs['image_output'], video_to_video_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        elif bool(prefs['image_output']):
            new_file = available_file(os.path.join(prefs['image_output'], video_to_video_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
        num += 1
    if bool(video_path):
        prt(Row([VideoContainer(video_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_controlnet_temporalnet(page):
    global controlnet_temporalnet_prefs, prefs, status, pipe_controlnet, controlnet
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.TemporalNet_XL.controls.append(line)
      page.TemporalNet_XL.update()
    def clear_last(lines=1):
      clear_line(page.TemporalNet_XL, lines=lines)
    def clear_list():
      page.TextToVideo.controls = page.TemporalNet_XL.controls[:1]
    def autoscroll(scroll=True):
      page.TemporalNet_XL.auto_scroll = scroll
      page.TemporalNet_XL.update()
    if not bool(controlnet_temporalnet_prefs['init_video']):
      alert_msg(page, "You must provide the Input Initial Video Clip to process...")
      return
    progress = ProgressBar(bar_height=8)
    total_steps = controlnet_temporalnet_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Controlnet TemporalNet XL Pipeline...")
    prt(installer)
    from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
    from diffusers.utils import load_image
    import numpy as np
    from PIL import Image as PILImage
    fps = 25
    def split_video_into_frames(video_path, frames_dir):
        nonlocal fps
        vidcap = cv2.VideoCapture(video_path)
        fps = vidcap.get(cv2.CAP_PROP_FPS)
        success, image = vidcap.read()
        count = 0
        while success:
            if max(image.shape[:2]) != controlnet_temporalnet_prefs['max_size']:
                scale_factor = controlnet_temporalnet_prefs['max_size'] / max(image.shape[:2])
                image = cv2.resize(image, (0, 0), fx=scale_factor, fy=scale_factor)
            frame_path = os.path.join(frames_dir, f"frame{count:04d}.png")
            cv2.imwrite(frame_path, image)
            success, image = vidcap.read()
            count += 1
    def frame_number(frame_filename):
        return int(frame_filename[5:-4])
    def count_frame_images(frames_dir):
        frame_files = [f for f in os.listdir(frames_dir) if f.startswith('frame') and f.endswith('.png')]
        return len(frame_files)
    def write_video(video_out, frames_dir, fps):
        frames = [os.path.join(frames_dir, f) for f in os.listdir(frames_dir) if os.path.isfile(os.path.join(frames_dir, f))]
        frame = cv2.imread(frames[0])
        height, width, _ = frame.shape
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(video_out, fourcc, fps, (width, height))
        for frame in frames:
            img = cv2.imread(frame)
            out.write(img)
        out.release()
        cv2.destroyAllWindows()
    
    save_dir = os.path.join(stable_dir, controlnet_temporalnet_prefs['batch_folder_name'])
    frames_dir = os.path.join(save_dir, 'frames')
    makedir(frames_dir)
    batch_output = os.path.join(prefs['image_output'], controlnet_temporalnet_prefs['batch_folder_name'])
    makedir(batch_output)
    output_frames_dir = os.path.join(save_dir, 'output_frames')
    makedir(output_frames_dir)
    if controlnet_temporalnet_prefs['save_canny']:
        canny_frames_dir = os.path.join(batch_output, 'canny_frames')
        makedir(canny_frames_dir)
    if controlnet_temporalnet_prefs['save_frames']:
        save_frames_dir = os.path.join(batch_output, 'frames')
        makedir(save_frames_dir)
    init_image_path = controlnet_temporalnet_prefs['init_image']
    video_path = controlnet_temporalnet_prefs['init_video']
    if video_path.startswith('http'):
        video_path = download_file(video_path, uploads_dir, ext="mp4")
    else:
        if not os.path.isfile(video_path):
            alert_msg(page, f"ERROR: Couldn't find your init_video {video_path}")
            return
    if count_frame_images(frames_dir) == 0:
        installer.status("...split_video_into_frames")
        split_video_into_frames(video_path, frames_dir)
    if bool(init_image_path):
        last_generated_image = load_image(init_image_path)
    else:
        initial_frame_path = os.path.join(frames_dir, "frame0000.png")
        last_generated_image = load_image(initial_frame_path)

    base_model_path = "stabilityai/stable-diffusion-xl-base-1.0"
    controlnet1_path = "CiaraRowles/controlnet-temporalnet-sdxl-1.0"
    controlnet2_path = "xinsir/controlnet-canny-sdxl-1.0"
    #controlnet2_path = "diffusers/controlnet-canny-sdxl-1.0"
    if status['loaded_controlnet_type'] == "TemporalNet":
        clear_pipes('controlnet')
    else:
        clear_pipes()
    if pipe_controlnet is None:
        controlnet = [
            ControlNetModel.from_pretrained(controlnet1_path, torch_dtype=torch.float16,use_safetensors=True),
            ControlNetModel.from_pretrained(controlnet2_path, torch_dtype=torch.float16)
        ]
        pipe_controlnet = StableDiffusionXLControlNetPipeline.from_pretrained(base_model_path, controlnet=controlnet, torch_dtype=torch.float16)
        pipe_controlnet = pipeline_scheduler(pipe_controlnet)
        #pipe_controlnet.scheduler = UniPCMultistepScheduler.from_config(pipe_controlnet.scheduler.config)
        #pipe_controlnet.enable_xformers_memory_efficient_attention()
        if controlnet_temporalnet_prefs['lower_memory']:
            pipe_controlnet.enable_model_cpu_offload()
        else:
            pipe_controlnet.to(torch_device)
        status['loaded_controlnet_type'] = "TemporalNet"
    clear_last()
    prt(f"Generating Controlnet TemporalNet XL on Frames with your Prompt...")
    fname = format_filename(controlnet_temporalnet_prefs['prompt'])
    random_seed = get_seed(controlnet_temporalnet_prefs['seed'])
    generator = torch.manual_seed(random_seed)
    frame_files = sorted(os.listdir(frames_dir), key=frame_number)

    for i, frame_file in enumerate(frame_files):
        # Use the original video frame to create Canny edge-detected image as the conditioning image for the first ControlNetModel
        prt(progress)
        autoscroll(False)
        control_image_path = os.path.join(frames_dir, frame_file)
        control_image = load_image(control_image_path)
        canny_image = np.array(control_image)
        canny_image = cv2.Canny(canny_image, controlnet_temporalnet_prefs['low_threshold'], controlnet_temporalnet_prefs['high_threshold'])
        canny_image = canny_image[:, :, None]
        canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)
        canny_image = PILImage.fromarray(canny_image)
        image = pipe_controlnet(
            controlnet_temporalnet_prefs['prompt'], negative_prompt=controlnet_temporalnet_prefs['negative_prompt'], num_inference_steps=controlnet_temporalnet_prefs['num_inference_steps'], guidance=controlnet_temporalnet_prefs['guidance'], generator=generator, image=[last_generated_image, canny_image], controlnet_conditioning_scale=[controlnet_temporalnet_prefs['temporalnet_strength'], controlnet_temporalnet_prefs['canny_strength']], callback=callback_fnc
        ).images[0]
        w, h = image.size
        output_path = os.path.join(output_frames_dir, f"frame{str(i).zfill(4)}.png")
        image.save(output_path)
        if controlnet_temporalnet_prefs['save_canny']:
            canny_image_path = os.path.join(canny_frames_dir, f"outputcanny{str(i).zfill(4)}.png")
            canny_image.save(canny_image_path)
        last_generated_image = image
        clear_last()
        autoscroll(True)
        prt(Row([Img(src=asset_dir(output_path), fit=ImageFit.CONTAIN, width=w, height=h, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        prt(Row([Text(output_path)], alignment=MainAxisAlignment.CENTER))
    prt(Installing(f"Saving Video File... Frames at {output_frames_dir if not controlnet_temporalnet_prefs['save_frames'] else save_frames_dir}"))
    if controlnet_temporalnet_prefs['save_frames']:
        shutil.copytree(output_frames_dir, save_frames_dir, dirs_exist_ok=True)
    video_out = available_file(batch_output, fname, 0, no_num=True, ext="mp4")
    write_video(video_out, output_frames_dir, fps)
    clear_last
    prt(f"Saved to {video_out}")
    play_snd(Snd.ALERT, page)

def run_infinite_zoom(page):
    global prefs, infinite_zoom_prefs, pipe_infinite_zoom, status
    def prt(line):
        if type(line) == str:
            line = Text(line, size=17)
        page.InfiniteZoom.controls.append(line)
        page.InfiniteZoom.update()
    def clear_last(lines=1):
        clear_line(page.InfiniteZoom, lines=lines)
    def clear_list():
        page.InfiniteZoom.controls = page.InfiniteZoom.controls[:1]
    def autoscroll(scroll=True):
        page.InfiniteZoom.auto_scroll = scroll
        page.InfiniteZoom.update()
    if not check_diffusers(page): return
    if len(infinite_zoom_prefs['animation_prompts']) == 0:
        if bool(infinite_zoom_prefs['prompt']):
            infinite_zoom_prefs['animation_prompts']['0'] = infinite_zoom_prefs['prompt']
        else:
            alert_msg(page, "You need to provide at least one keyframe prompt to animate...")
            return
    if not bool(infinite_zoom_prefs['batch_folder_name']):
        alert_msg(page, "You must give Batch Folder Name to save your project output...")
        return
    clear_list()
    autoscroll(True)
    progress = ProgressBar(bar_height=8)
    total_steps = infinite_zoom_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
        callback_fnc.has_been_called = True
        nonlocal progress, total_steps
        #total_steps = len(latents)
        percent = (step +1)/ total_steps
        progress.value = percent
        progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
        progress.update()
    installer = Installing("Installing Infinite Zoom Stable Diffusion Pipeline...")
    prt(installer)
    pip_install("transformers scipy ftfy accelerate", installer=installer)

    from PIL import Image as PILImage
    from io import BytesIO
    import numpy as np
    os.environ["CUDA_VISIBLE_DEVICES"]="0"
    fname = format_filename(infinite_zoom_prefs['batch_folder_name'])
    max_size = infinite_zoom_prefs['max_size']
    batch_output = os.path.join(stable_dir, infinite_zoom_prefs['batch_folder_name'])
    output_dir = batch_output
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], infinite_zoom_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    #print("3/3: Define helper functions")
    def write_video(file_path, frames, fps, reversed = True, start_frame_dupe_amount = 15, last_frame_dupe_amount = 30, as_gif=False):
        if reversed == True:
            frames.reverse()
        w, h = frames[0].size
        if as_gif:
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
        else:
            fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')
        writer = cv2.VideoWriter(file_path, fourcc, fps, (w, h))
        for x in range(start_frame_dupe_amount):
            np_frame = np.array(frames[0].convert('RGB'))
            cv_frame = cv2.cvtColor(np_frame, cv2.COLOR_RGB2BGR)
            writer.write(cv_frame)
        for frame in frames:
            np_frame = np.array(frame.convert('RGB'))
            cv_frame = cv2.cvtColor(np_frame, cv2.COLOR_RGB2BGR)
            writer.write(cv_frame)
        for x in range(last_frame_dupe_amount):
            np_frame = np.array(frames[len(frames) - 1].convert('RGB'))
            cv_frame = cv2.cvtColor(np_frame, cv2.COLOR_RGB2BGR)
            writer.write(cv_frame)
        writer.release()
    import imageio

    def convert_mp4_to_gif(input_file, output_file, fps=30):
        try:
            video_reader = imageio.get_reader(input_file)
            frames = [frame for frame in video_reader]
            duration = int(1000 * 1/fps)
            imageio.mimsave(output_file, frames, 'GIF', duration=duration)
        except Exception as e:
            print(f"Error with convert_mp4_to_gif: {e}")
    
    def image_grid(imgs, rows, cols):
        assert len(imgs) == rows*cols
        w, h = imgs[0].size
        grid = PILImage.new('RGB', size=(cols*w, rows*h))
        grid_w, grid_h = grid.size
        for i, img in enumerate(imgs):
            grid.paste(img, box=(i%cols*w, i//cols*h))
        return grid

    def shrink_and_paste_on_blank(current_image, mask_width):
        width, height = current_image.size
        #shrink down by mask_width
        prev_image = current_image.resize((width-2*mask_width,height-2*mask_width))
        prev_image = prev_image.convert("RGBA")
        prev_image = np.array(prev_image)
        #create blank non-transparent image
        blank_image = np.array(current_image.convert("RGBA"))*0
        blank_image[:,:,3] = 1
        #paste shrinked onto blank
        blank_image[mask_width:height-mask_width,mask_width:width-mask_width,:] = prev_image
        prev_image = PILImage.fromarray(blank_image)
        return prev_image
        
        new_width = width - 2 * mask_width
        new_height = int(new_width / (width / height))
        prev_image = current_image.resize((new_width, new_height))
        prev_image = prev_image.convert("RGBA")
        blank_image = PILImage.new("RGBA", (width, height))
        paste_x = (width - new_width) // 2
        paste_y = (height - new_height) // 2
        blank_image.paste(prev_image, (paste_x, paste_y))
        return blank_image

    def load_img(address, res=(512, 512)):
        if address.startswith('http'):
            image = PILImage.open(requests.get(address, stream=True).raw)
        else:
            if os.path.isfile(address):
                image = PILImage.open(address)
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {address}")
                return
            image = PILImage.open(address)
        image = image.convert('RGB')
        image = image.resize(res, resample=PILImage.Resampling.LANCZOS)
        return image
    use_SDXL = infinite_zoom_prefs['use_SDXL']
    model_id = infinite_zoom_prefs['inpainting_model'] #param ["stabilityai/stable-diffusion-2-inpainting", "runwayml/stable-diffusion-inpainting", "ImNoOne/f222-inpainting-diffusers","parlance/dreamlike-diffusion-1.0-inpainting","ghunkins/stable-diffusion-liberty-inpainting"] {allow-input: true}
    SDXL_model = get_SDXL_model(prefs['SDXL_model'])
    model_id_SDXL = SDXL_model['path']
    if 'loaded_infinite_zoom' not in status:
        status['loaded_infinite_zoom'] = ''
    if status['loaded_infinite_zoom'] != (model_id_SDXL if use_SDXL else model_id):
        clear_pipes()
    else:
        clear_pipes('infinite_zoom')
        if prefs['scheduler_mode'] != status['loaded_scheduler']:
            pipe_infinite_zoom = pipeline_scheduler(pipe_infinite_zoom)
    if pipe_infinite_zoom == None:
        installer.status(f"...initializing{' SDXL' if use_SDXL else ''} Inpaint Pipeline")
        try:
            if use_SDXL:
                from diffusers import AutoPipelineForInpainting, AutoencoderKL
                #StableDiffusionXLInpaintPipeline
                vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16, force_upcast=False)
                pipe_infinite_zoom = AutoPipelineForInpainting.from_pretrained(
                    #model_id, variant=SDXL_model['revision']
                    "diffusers/stable-diffusion-xl-1.0-inpainting-0.1", variant="fp16",
                    torch_dtype=torch.float16, use_safetensors=True,
                    vae=vae,
                    add_watermarker=False,
                    cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
                    safety_checker=None, requires_safety_checker=False,
                )
                pipe_infinite_zoom = pipeline_scheduler(pipe_infinite_zoom)
                pipe_infinite_zoom = optimize_SDXL(pipe_infinite_zoom)
                pipe_infinite_zoom.set_progress_bar_config(disable=True)
                status['loaded_infinite_zoom'] = model_id_SDXL
            else:
                from diffusers import StableDiffusionInpaintPipeline
                pipe_infinite_zoom = StableDiffusionInpaintPipeline.from_pretrained(
                    infinite_zoom_prefs['inpainting_model'],
                    torch_dtype=torch.float16,
                    cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
                    safety_checker=None, requires_safety_checker=False,
                )
                pipe_infinite_zoom = pipeline_scheduler(pipe_infinite_zoom)
                #pipe_infinite_zoom.scheduler = DPMSolverMultistepScheduler.from_config(pipe_infinite_zoom.scheduler.config)
                #pipe_infinite_zoom = pipe_infinite_zoom.to("cuda")
                def dummy(images, **kwargs):
                    return images, False
                #pipe_infinite_zoom.safety_checker = dummy
                #pipe_infinite_zoom.enable_attention_slicing() #This is useful to save some memory in exchange for a small speed decrease.
                pipe_infinite_zoom = optimize_pipe(pipe_infinite_zoom)
                pipe_infinite_zoom.set_progress_bar_config(disable=True)
                status['loaded_infinite_zoom'] = model_id
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Problem Initializing Pipeline...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    installer.status("...preparing run")
    g_cuda = torch.Generator(device=torch_device)
    animation_prompts = {int(k): v for k, v in infinite_zoom_prefs['animation_prompts'].items()}
    prompt = animation_prompts[0]
    negative_prompt = infinite_zoom_prefs['negative_prompt']
    num_init_images = 1 #TODO: Make multiple to pick from before processing
    random_seed = get_seed(infinite_zoom_prefs['seed'])
    num_inference_steps = infinite_zoom_prefs['num_inference_steps']
    guidance_scale = infinite_zoom_prefs['guidance_scale']
    width = infinite_zoom_prefs['width']
    height = infinite_zoom_prefs['height']#infinite_zoom_prefs['max_size']
    init_image = infinite_zoom_prefs['init_image']#"/content/gdrive/MyDrive/init/image.jpeg"#param {type:"string"}
    num_outpainting_steps = infinite_zoom_prefs['num_outpainting_steps']
    mask_width = infinite_zoom_prefs['mask_width']
    num_interpol_frames = infinite_zoom_prefs['num_interpol_frames']
    init_image_selected = 0 #param
    fps = infinite_zoom_prefs['fps']
    start_frame_dupe_amount = infinite_zoom_prefs['frame_dupe_amount']
    last_frame_dupe_amount = infinite_zoom_prefs['frame_dupe_amount']
    # Since the model was trained on 512 images increasing the resolution to e.g. 1024 will drastically reduce its imagination, so the video will vary a lot less compared to 512

    current_image = PILImage.new(mode="RGBA", size=(height, width))
    mask_image = np.array(current_image)[:,:,3]
    mask_image = PILImage.fromarray(255-mask_image).convert("RGB")
    current_image = current_image.convert("RGB")
    clear_last()
    if not bool(init_image):
        prt(progress)
        try:
            if use_SDXL:
                SDXL_negative_conditions = {'negative_original_size':(512, 512), 'negative_crops_coords_top_left':(0, 0), 'negative_target_size':(1024, 1024)} if not prefs['SDXL_negative_conditions'] else {}
                init_images = pipe_infinite_zoom(prompt=[prompt]*num_init_images,
                                negative_prompt=[negative_prompt]*num_init_images,
                                image=current_image,
                                mask_image=mask_image,
                                #target_size=(width, height),
                                guidance_scale = guidance_scale,
                                generator = g_cuda.manual_seed(random_seed),
                                num_inference_steps=num_inference_steps,
                                callback=callback_fnc, callback_steps=1).images
                                #, **SDXL_negative_conditions
            else:
                init_images =  pipe_infinite_zoom(prompt=[prompt]*num_init_images,
                                    negative_prompt=[negative_prompt]*num_init_images,
                                    image=current_image,
                                    mask_image=mask_image,
                                    guidance_scale = guidance_scale,
                                    height = height,
                                    width = width,
                                    generator = g_cuda.manual_seed(random_seed),
                                    callback=callback_fnc,
                                    num_inference_steps=num_inference_steps).images
        except Exception as e:
                clear_last()
                alert_msg(page, f"ERROR: Something went wrong generating image...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
        #for image in images:
            #prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))

        #image_grid(init_images, rows=1, cols=num_init_images)
        clear_last()
        if num_init_images == 1:
            init_image_selected = 0
        else:
            init_image_selected = init_image_selected - 1
            #TODO: Display numbered init_images and give selection prompt to pick which
        current_image = init_images[init_image_selected]
    else:
        current_image = load_img(init_image,(width,height))
    interpol_path = available_file(output_dir, fname, 0)
    current_image.save(interpol_path)
    prt(Row([ImageButton(src=interpol_path, width=width, height=height, data=interpol_path, page=page)], alignment=MainAxisAlignment.CENTER))
    all_frames = []
    all_frames.append(current_image)

    for i in range(num_outpainting_steps):
        prt(f'Generating Image {i+1} / {num_outpainting_steps} - "{animation_prompts[max(k for k in animation_prompts.keys() if k <= i)]}"')
        autoscroll(False)
        prt(progress)
        prev_image_fix = current_image
        prev_image = shrink_and_paste_on_blank(current_image, mask_width)
        current_image = prev_image
        mask_image = np.array(current_image)[:,:,3]
        mask_image = PILImage.fromarray(255-mask_image).convert("RGB")
        current_image = current_image.convert("RGB")
        try:
            if use_SDXL:
                SDXL_negative_conditions = {'negative_original_size':(512, 512), 'negative_crops_coords_top_left':(0, 0), 'negative_target_size':(1024, 1024)} if not prefs['SDXL_negative_conditions'] else {}
                images = pipe_infinite_zoom(prompt=animation_prompts[max(k for k in animation_prompts.keys() if k <= i)],
                                negative_prompt=negative_prompt,
                                image=current_image,
                                mask_image=mask_image,
                                #target_size=(width, height),
                                guidance_scale = guidance_scale,
                                #generator = g_cuda.manual_seed(random_seed),
                                num_inference_steps=num_inference_steps,
                                callback=callback_fnc, callback_steps=1).images
                                #, **SDXL_negative_conditions
            else:
                images = pipe_infinite_zoom(prompt=animation_prompts[max(k for k in animation_prompts.keys() if k <= i)],
                                negative_prompt=negative_prompt,
                                image=current_image,
                                mask_image=mask_image,
                                guidance_scale = guidance_scale,
                                height = height,
                                width = width,
                                #this can make the whole thing deterministic but the output less exciting
                                #generator = g_cuda.manual_seed(random_seed),
                                num_inference_steps=num_inference_steps,
                                callback=callback_fnc, callback_steps=1).images
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Something went wrong generating image...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        current_image = images[0]
        autoscroll(True)
        clear_last()
        current_image.paste(prev_image, mask=prev_image)
        #interpolation steps bewteen 2 inpainted images (=sequential zoom and crop)
        for j in range(num_interpol_frames - 1):
            interpol_image = current_image
            interpol_width = round((1- ( 1-2*mask_width/width )**( 1-(j+1)/num_interpol_frames ) )*width/2 )
            interpol_height = round((1- ( 1-2*mask_width/height )**( 1-(j+1)/num_interpol_frames ) )*height/2 )
            interpol_image = interpol_image.crop((interpol_width, interpol_height, width - interpol_width, height - interpol_height))
            interpol_image = interpol_image.resize((width, height))
            #paste the higher resolution previous image in the middle to avoid drop in quality caused by zooming
            interpol_width2 = round(( 1 - (width-2*mask_width) / (height-2*interpol_width) ) / 2*height)
            prev_image_fix_crop = shrink_and_paste_on_blank(prev_image_fix, interpol_width2)
            interpol_image.paste(prev_image_fix_crop, mask = prev_image_fix_crop)
            all_frames.append(interpol_image)
            #TODO: Give option to save all Interpolation frames
        all_frames.append(current_image)
        interpol_path = available_file(output_dir, fname, 1)
        interpol_image.save(interpol_path)
        prt(Row([ImageButton(src=interpol_path, width=width, height=height, data=interpol_path, page=page)], alignment=MainAxisAlignment.CENTER))
        if infinite_zoom_prefs['save_frames']:
            save_path = available_file(batch_output, fname, 1)
            shutil.copy(interpol_path, save_path)
        #clear_output(wait=True)
        #interpol_image.show()
    now = datetime.datetime.now()
    date_time = now.strftime("%m-%d-%Y_%H-%M-%S")
    out_vid = os.path.join(batch_output, fname + "_out_"+date_time+".mp4")
    in_vid = os.path.join(batch_output, fname + "_in_"+date_time+".mp4")
    if infinite_zoom_prefs['save_video']:
        prt("Generating Video MP4 File...")
        write_video(in_vid, all_frames, fps, False, start_frame_dupe_amount, last_frame_dupe_amount)
        write_video(out_vid, all_frames, fps, True, start_frame_dupe_amount, last_frame_dupe_amount)
        clear_last()
        prt(f"Saved Videos as {in_vid} and {out_vid}")
        #prt(Row([VideoContainer(out_vid)], alignment=MainAxisAlignment.CENTER))
    if infinite_zoom_prefs['save_gif']:
        prt("Generating Animated GIF File...")
        out_gif = os.path.join(batch_output, fname + "_out_"+date_time+".gif")
        in_gif = os.path.join(batch_output, fname + "_in_"+date_time+".gif")
        convert_mp4_to_gif(out_vid, out_gif, fps)
        convert_mp4_to_gif(in_vid, in_gif, fps)
        #write_video(in_gif, all_frames, fps, False, start_frame_dupe_amount, last_frame_dupe_amount, as_gif=True)
        #write_video(out_gif, all_frames, fps, True, start_frame_dupe_amount, last_frame_dupe_amount, as_gif=True)
        clear_last()
        prt(Row([ImageButton(src=in_gif, width=width, height=height, data=in_gif, page=page)], alignment=MainAxisAlignment.CENTER))
        #prt(Row([ImageButton(src=out_gif, width=width, height=height, data=out_gif, page=page)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_potat1(page):
    global potat1_prefs, prefs, status, pipe_potat1, model_path
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.Potat1.controls.append(line)
      page.Potat1.update()
    def clear_last(lines=1):
      clear_line(page.Potat1, lines=lines)
    def clear_list():
      page.Potat1.controls = page.Potat1.controls[:1]
    def autoscroll(scroll=True):
      page.Potat1.auto_scroll = scroll
      page.Potat1.update()
    progress = ProgressBar(bar_height=8)
    total_steps = potat1_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Potat1 Text-To-Video Pipeline...")
    prt(installer)
    #model_id = "damo-vilab/text-to-video-ms-1.7b"
    clear_pipes()
    potat1_dir = os.path.join(root_dir, "potat1")
    finetuning_dir = os.path.join(root_dir, "Text-To-Video-Finetuning")
    torch_installed = False
    try:
        import torch
        torch_installed = True
    except:
        pass
    if torch_installed:
        if version.parse(torch.__version__).base_version >= version.parse("2.0.1"):
            torch_installed = False
    if not torch_installed:
        installer.status("...installing Torch 1.13.1")
        run_sp("pip install -q torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 torchtext==0.14.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu116 -U", realtime=False)
    run_sp("pip install imageio[ffmpeg] -U einops omegaconf decord", realtime=False)
    if not os.path.exists(finetuning_dir):
        installer.status("...camenduru/Text-To-Video-Finetuning")
        run_sp("git clone -b dev https://github.com/camenduru/Text-To-Video-Finetuning", realtime=False, cwd=root_dir)
    if not os.path.exists(potat1_dir):
        installer.status("...camenduru/potat1")
        run_sp("git clone https://huggingface.co/camenduru/potat1", realtime=False, cwd=root_dir)
    #clear_pipes('potat1')
    clear_last()
    prt("Generating Potat1 of your Prompt...")
    prt(progress)
    autoscroll(False)
    batch_output = os.path.join(stable_dir, potat1_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    local_output = batch_output
    batch_output = os.path.join(prefs['image_output'], potat1_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    random_seed = get_seed(potat1_prefs['seed'])
    #generator = torch.Generator(device="cpu").manual_seed(random_seed)
    #generator = torch.manual_seed(random_seed)
    width = potat1_prefs['width']
    height = potat1_prefs['height']
    x = " -x" if status['installed_xformers'] else ""
    rw = " -rw" if potat1_prefs['remove_watermark'] else ""
    try:
        run_sp(f'python inference.py -m "{potat1_dir}" -p "{potat1_prefs["prompt"]}" -n "{potat1_prefs["negative_prompt"]}" -W {width} -H {height} -o {batch_output} -d cuda{x}{rw} -s {potat1_prefs["num_inference_steps"]} -g {potat1_prefs["guidance_scale"]} -f {potat1_prefs["fps"]} -T {potat1_prefs["num_frames"]}', cwd=finetuning_dir)
      #print(f"prompt={potat1_prefs['prompt']}, negative_prompt={potat1_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={potat1_prefs['edit_momentum_scale']}, edit_mom_beta={potat1_prefs['edit_mom_beta']}, num_inference_steps={potat1_prefs['num_inference_steps']}, eta={potat1_prefs['eta']}, guidance_scale={potat1_prefs['guidance_scale']}")
      #, output_type = "pt", width=width, height=height
      #frames = pipe_potat1(prompt=potat1_prefs['prompt'], negative_prompt=potat1_prefs['negative_prompt'], num_frames=potat1_prefs['num_frames'], num_inference_steps=potat1_prefs['num_inference_steps'], eta=potat1_prefs['eta'], guidance_scale=potat1_prefs['guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).frames
    except Exception as e:
      clear_last()
      clear_last()
      alert_msg(page, f"ERROR: Potat1 Text-To-Video failed for some reason. Possibly out of memory or something wrong with the code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
      return
    clear_last()
    clear_last()
    autoscroll(True)
    filename = f"{format_filename(potat1_prefs['prompt'])}"
    #filename = filename[:int(prefs['file_max_length'])]
    #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
    autoscroll(True)
    video_path = ""
    '''if potat1_prefs['export_to_video']:
        from diffusers.utils import export_to_video
        video_path = export_to_video(frames)
        shutil.copy(video_path, available_file(local_output, filename, 0, ext="mp4", no_num=True))
        shutil.copy(video_path, available_file(batch_output, filename, 0, ext="mp4", no_num=True))
    from PIL.PngImagePlugin import PngInfo
    num = 0
    for image in frames:
        random_seed += num
        fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
        image_path = available_file(batch_output, fname, num)
        unscaled_path = image_path
        output_file = image_path.rpartition(slash)[2]
        #uint8_image = (image * 255).round().astype("uint8")
        #np_image = image.cpu().numpy()
        #print(f"image: {type(image)}, np_image: {type(np_image)}")
        #print(f"image: {type(image)} to {image_path}")
        cv2.imwrite(image_path, image)
        #PILImage.fromarray(np_image).save(image_path)
        out_path = os.path.dirname(image_path)
        upscaled_path = os.path.join(out_path, output_file)
        if not potat1_prefs['display_upscaled_image'] or not potat1_prefs['apply_ESRGAN_upscale']:
            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
        if potat1_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
            upload_folder = 'upload'
            result_folder = 'results'
            if os.path.isdir(upload_folder):
                shutil.rmtree(upload_folder)
            if os.path.isdir(result_folder):
                shutil.rmtree(result_folder)
            os.mkdir(upload_folder)
            os.mkdir(result_folder)
            short_name = f'{fname[:80]}-{num}.png'
            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
            #print(f'Moving {fpath} to {dst_path}')
            #shutil.move(fpath, dst_path)
            shutil.copy(image_path, dst_path)
            #faceenhance = ' --face_enhance' if potat1_prefs["face_enhance"] else ''
            faceenhance = ''
            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {potat1_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
            out_file = short_name.rpartition('.')[0] + '_out.png'
            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
            image_path = upscaled_path
            os.chdir(stable_dir)
            if potat1_prefs['display_upscaled_image']:
                time.sleep(0.6)
                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(potat1_prefs["enlarge_scale"]), height=height * float(potat1_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        if prefs['save_image_metadata']:
            img = PILImage.open(image_path)
            metadata = PngInfo()
            metadata.add_text("artist", prefs['meta_ArtistName'])
            metadata.add_text("copyright", prefs['meta_Copyright'])
            metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {potat1_prefs['enlarge_scale']}x with ESRGAN" if potat1_prefs['apply_ESRGAN_upscale'] else "")
            metadata.add_text("pipeline", "Text-To-Video")
            if prefs['save_config_in_metadata']:
              config_json = potat1_prefs.copy()
              config_json['model_path'] = model_id
              config_json['scheduler_mode'] = prefs['scheduler_mode']
              config_json['seed'] = random_seed
              del config_json['num_frames']
              del config_json['width']
              del config_json['height']
              del config_json['display_upscaled_image']
              del config_json['batch_folder_name']
              del config_json['lower_memory']
              if not config_json['apply_ESRGAN_upscale']:
                del config_json['enlarge_scale']
                del config_json['apply_ESRGAN_upscale']
              metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
            img.save(image_path, pnginfo=metadata)
        #TODO: PyDrive
        if storage_type == "Colab Google Drive":
            new_file = available_file(os.path.join(prefs['image_output'], potat1_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        elif bool(prefs['image_output']):
            new_file = available_file(os.path.join(prefs['image_output'], potat1_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
        num += 1'''
    #prt(Row([VideoContainer(video_path)], alignment=MainAxisAlignment.CENTER))
    prt(f"Done creating video... Check {batch_output}")
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_stable_animation(page):
    global stable_animation_prefs, prefs, status
    if not bool(prefs['Stability_api_key']):
        alert_msg(e.page, "You must have your DreamStudio.ai Stability-API Key to use Stability.  Note that it will cost you tokens.")
        return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.StableAnimation.controls.append(line)
      page.StableAnimation.update()
    def clear_last(lines=1):
      clear_line(page.StableAnimation, lines=lines)
    def clear_list():
      page.StableAnimation.controls = page.StableAnimation.controls[:1]
    def autoscroll(scroll=True):
      page.StableAnimation.auto_scroll = scroll
      page.StableAnimation.update()
    abort_run = False
    def abort_diffusion(e):
      nonlocal abort_run
      abort_run = True
      page.snd_error.play()
      page.snd_delete.play()
    progress = ProgressBar(bar_height=8)
    total_steps = stable_animation_prefs['num_inference_steps']
    def callback_fnc(step: int) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}"
      progress.update()
    clear_list()
    autoscroll(True)
    prt(Installing("Installing Stable Animation Stability.ai Pipeline..."))
    #run_sp('pip install "stability_sdk[anim_ui]"', realtime=True)
    try:
        from stability_sdk import api
    except Exception:
        run_sp('pip install "stability_sdk[anim_ui]"', realtime=True) #
        from stability_sdk import api
        pass
    from tqdm import tqdm
    from PIL.PngImagePlugin import PngInfo

    #batch_output = os.path.join(stable_dir, stable_animation_prefs['batch_folder_name'])
    #if not os.path.isdir(batch_output):
    #  os.makedirs(batch_output)
    #local_output = batch_output
    batch_output = os.path.join(prefs['image_output'], stable_animation_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    random_seed = get_seed(stable_animation_prefs['seed'])
    width = stable_animation_prefs['width']
    height = stable_animation_prefs['height']
    animation_prompts = stable_animation_prefs['animation_prompts']
    '''try:
        prompts = json.loads(animation_prompts)
    except json.JSONDecodeError:
        try:
            prompts = eval(animation_prompts)
        except Exception as e:
            alert_msg(page, "Invalid JSON or Python code for animation_prompts.")
            return'''
    prompts = {int(k): v for k, v in animation_prompts.items()}
    from stability_sdk.api import (ClassifierException, Context, OutOfCreditsException)
    context = Context("grpc.stability.ai:443", prefs['Stability_api_key'])
    try:
        balance, profile_picture = context.get_user_info()
    except:
        alert_msg(page, "Error getting Stability.ai User Info")
        return
    from stability_sdk.animation import (
        AnimationArgs,
        Animator,
        interpolate_frames
    )
    from stability_sdk.utils import (create_video_from_frames, extract_frames_from_video, interpolate_mode_from_string)
    args = AnimationArgs()
    args.width = width
    args.height = height
    args.sampler = stable_animation_prefs['sampler']
    #args_generation.custom_model = stable_animation_prefs['custom_model']
    args.model = stable_animation_prefs['model'].lower()
    args.seed = random_seed
    args.cfg_scale = stable_animation_prefs['guidance_scale']
    args.clip_guidance = stable_animation_prefs['clip_guidance']
    args.init_image = stable_animation_prefs['init_image']
    args.init_sizing = stable_animation_prefs['init_sizing'].lower()
    args.mask_path = stable_animation_prefs['mask_image']
    args.mask_invert = stable_animation_prefs['mask_invert']
    args.preset = stable_animation_prefs['style_preset']
    args.animation_mode = stable_animation_prefs['animation_mode']
    args.max_frames = stable_animation_prefs['max_frames']
    args.border = stable_animation_prefs['border'].lower()
    args.noise_add_curve = stable_animation_prefs['noise_add_curve']
    args.noise_scale_curve = stable_animation_prefs['noise_scale_curve']
    args.strength_curve = stable_animation_prefs['strength_curve']
    args.steps_curve = stable_animation_prefs['steps_curve']
    args.steps_strength_adj = stable_animation_prefs['steps_strength_adj']
    args.interpolate_prompts = stable_animation_prefs['interpolate_prompts']
    args.locked_seed = stable_animation_prefs['locked_seed']
    args.angle = stable_animation_prefs['angle']
    args.zoom = stable_animation_prefs['zoom']
    args.translation_x = stable_animation_prefs['translation_x']
    args.translation_y = stable_animation_prefs['translation_y']
    args.translation_z = stable_animation_prefs['translation_z']
    args.rotation_x = stable_animation_prefs['rotation_x']
    args.rotation_y = stable_animation_prefs['rotation_y']
    args.rotation_z = stable_animation_prefs['rotation_z']
    args.diffusion_cadence_curve = stable_animation_prefs['diffusion_cadence_curve']
    args.cadence_interp = stable_animation_prefs['cadence_interp'].lower()
    args.cadence_spans = stable_animation_prefs['cadence_spans']
    args.color_coherence = stable_animation_prefs['color_coherence']
    args.brightness_curve = stable_animation_prefs['brightness_curve']
    args.contrast_curve = stable_animation_prefs['contrast_curve']
    args.hue_curve = stable_animation_prefs['hue_curve']
    args.saturation_curve = stable_animation_prefs['saturation_curve']
    args.lightness_curve = stable_animation_prefs['lightness_curve']
    args.color_match_animate = stable_animation_prefs['color_match_animate']
    args.depth_model_weight = stable_animation_prefs['depth_model_weight']
    args.near_plane = stable_animation_prefs['near_plane']
    args.far_plane = stable_animation_prefs['far_plane']
    args.fov_curve = stable_animation_prefs['fov_curve']
    args.depth_blur_curve = stable_animation_prefs['depth_blur_curve']
    args.depth_warp_curve = stable_animation_prefs['depth_warp_curve']
    #args_depth.save_depth_maps = stable_animation_prefs['save_depth_maps']
    args.camera_type = stable_animation_prefs['camera_type'].lower()
    args.render_mode = stable_animation_prefs['render_mode'].lower()
    args.mask_power = stable_animation_prefs['mask_power']
    args.use_inpainting_model = stable_animation_prefs['use_inpainting_model']
    args.inpaint_border = stable_animation_prefs['inpaint_border']
    args.mask_min_value = stable_animation_prefs['mask_min_value']
    args.mask_binarization_thr = stable_animation_prefs['mask_binarization_thr']
    #args_inpaint.save_inpaint_masks = False
    args.video_init_path = stable_animation_prefs['video_init_path']
    args.extract_nth_frame = int(stable_animation_prefs['extract_nth_frame'])
    args.video_mix_in_curve = stable_animation_prefs['video_mix_in_curve']
    args.video_flow_warp = stable_animation_prefs['video_flow_warp']
    args.fps = stable_animation_prefs['output_fps']
    args.resume = stable_animation_prefs['resume']
    args.resume_from = int(stable_animation_prefs['resume_from'])
    args.reverse = False
    #arg_objs = AnimationArgs(args_generation, args_animation, args_camera, args_coherence, args_color, args_depth, args_render_3d, args_inpaint, args_vid_in, args_vid_out)
    try:
        animator = Animator(
            api_context=context,
            animation_prompts=prompts,
            negative_prompt=stable_animation_prefs['negative_prompt'],
            negative_prompt_weight=stable_animation_prefs['negative_prompt_weight'],
            args=args,
            #out_dir=batch_output
            #negative_prompt_weight=negative_prompt_weight,
            #resume=resume,
        )
    except ClassifierException as e:
        alert_msg(page, "Animation terminated early due to NSFW classifier. Sorry for the Censorship...")
        return
    except OutOfCreditsException as e:
        alert_msg(page, f"Animation terminated early, out of credits. Refill them Tokens...", content=Text(f"{e.details}"))
        return
    except Exception as e:
        alert_msg(page, f"Animation terminated early due to exception:", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    total_steps = args.max_frames
    filename = f"{prefs['file_prefix']}{format_filename(prompts[0])}"
    filename = filename[:int(prefs['file_max_length'])]
    #fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
    clear_last()
    prt(Row([Text(f"Generating Stable Animation from your Prompts... Available Credits: {round(balance, 1)}"), Container(content=None, expand=True), IconButton(icon=icons.CANCEL, tooltip="Abort Current Run", on_click=abort_diffusion)]))
    prt(progress)
    autoscroll(True)
    try:
      #frames = pipe_stable_animation(prompt=stable_animation_prefs['prompt'], negative_prompt=stable_animation_prefs['negative_prompt'], video_length=stable_animation_prefs['max_frames'], num_inference_steps=stable_animation_prefs['num_inference_steps'], eta=stable_animation_prefs['eta'], guidance_scale=stable_animation_prefs['guidance_scale'], motion_field_strength_x=stable_animation_prefs['motion_field_strength_x'], motion_field_strength_y=stable_animation_prefs['motion_field_strength_y'], t0=stable_animation_prefs['t0'], t1=stable_animation_prefs['t1'], generator=generator, callback=callback_fnc, callback_steps=1).images
      for num, image in enumerate(tqdm(animator.render(), initial=animator.start_frame_idx, total=args.max_frames), start=animator.start_frame_idx):
        if abort_run:
            #clear_last()
            #clear_last()
            prt("🛑  Aborted Current Animation Run")
            return
        callback_fnc(num)
        fname = f"frame_{num:05d}"
        #image_path = available_file(batch_output, fname, num, no_num=True)
        image_path = os.path.join(batch_output, f"{fname}.png")
        unscaled_path = image_path
        output_file = image_path.rpartition(slash)[2]
        image.save(image_path)
        out_path = os.path.dirname(image_path)
        upscaled_path = os.path.join(out_path, output_file)

        if stable_animation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            upscale_image(image_path, upscaled_path, scale=stable_animation_prefs["enlarge_scale"])
            image_path = upscaled_path
            save_metadata(image_path, stable_animation_prefs, f"Stable Animation", model_path, random_seed, scheduler=True)
            if stable_animation_prefs['display_upscaled_image']:
                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(stable_animation_prefs["enlarge_scale"]), height=height * float(stable_animation_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        if not stable_animation_prefs['display_upscaled_image'] or not stable_animation_prefs['apply_ESRGAN_upscale']:
            save_metadata(unscaled_path, stable_animation_prefs, f"Stable Animation", model_path, random_seed, scheduler=True)
            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
        prt(Row([Text(image_path)], alignment=MainAxisAlignment.CENTER))
    except Exception as e:
      #clear_last()
      #clear_last()
      alert_msg(page, f"ERROR: Couldn't Stable Animation your image for some reason. Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
      return
    #clear_last()
    #clear_last()
    save_path = os.path.join(prefs['image_output'], stable_animation_prefs['batch_folder_name'])
    #filename = f"{prefs['file_prefix']}{format_filename(prompts[0])}"
    #filename = filename[:int(prefs['file_max_length'])]
    #autoscroll(True)
    progress = Container(content=None)
    try:
        new_balance, profile_picture = context.get_user_info()
    except:
        alert_msg(page, "Error getting User Info")
        return
    used_balance = balance - new_balance
    cost = f"${round(used_balance * 0.01, 2)}"
    prt(f"🫰  Done Generating Animation... Used {round(used_balance, 2)} Credits Total ~ {cost}")
    if stable_animation_prefs['export_to_video']:
        prt("Exporting Frames to Video")
        #from diffusers.utils import export_to_video
        #video_path = export_to_video(frames)
        #local_file = available_file(local_output, filename, 0, ext="mp4", no_num=True)
        save_file = available_file(batch_output, filename, 0, ext="mp4", no_num=True)
        create_video_from_frames(batch_output, save_file, fps=stable_animation_prefs['output_fps'])
        #imageio.mimsave(local_file, frames, fps=4)
        #shutil.copy(local_file, save_file)
        clear_last()
        try:
            prt(Row([VideoContainer(save_file)], alignment=MainAxisAlignment.CENTER))
            #prt(VideoPlayer(save_file, width, height))
        except Exception as e:
            print(f"Error showing VideoPlayer: {e}")
            pass
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_svd(page):
    global svd_prefs, prefs, status, pipe_svd
    if not check_diffusers(page): return
    if not bool(svd_prefs['init_image']):
      alert_msg(page, "You need to provide an Initial Image to animate before using...")
      return
    if not bool(svd_prefs['batch_folder_name']):
      alert_msg(page, "You need to provide a unique Video Folder Name for your project...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.SVD.controls.append(line)
      page.SVD.update()
    def clear_last(lines=1):
      clear_line(page.SVD, lines=lines)
    def clear_list():
      page.SVD.controls = page.SVD.controls[:1]
    def autoscroll(scroll=True):
      page.SVD.auto_scroll = scroll
      page.SVD.update()
    
    clear_list()
    autoscroll(True)
    installer = Installing("Installing SVD Image-To-Video Pipeline...")
    prt(installer)
    model_id = "stabilityai/stable-video-diffusion-img2vid-xt-1-1" if 'XT 1.1' in svd_prefs['svd_model'] else "stabilityai/stable-video-diffusion-img2vid-xt" if 'XT' in svd_prefs['svd_model'] else "stabilityai/stable-video-diffusion-img2vid"
    svd_prefs['num_frames'] = 25 if 'XT' in svd_prefs['svd_model'] else 14
    from diffusers import StableVideoDiffusionPipeline, EDMEulerScheduler
    if 'loaded_svd' not in status: status['loaded_svd'] = ""
    if model_id != status['loaded_svd']:
        clear_pipes()
    else:
        clear_pipes("svd")
    if pipe_svd == None:
        pipe_svd = StableVideoDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, variant="fp16", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        if svd_prefs['cpu_offload']:
            pipe_svd.enable_model_cpu_offload()
            #pipe_svd.unet.enable_forward_chunking()
        else:
            pipe_svd.to(torch_device)
        if prefs['enable_torch_compile']:
            #pipe_svd.unet.to(memory_format=torch.channels_last)
            pipe_svd.unet = torch.compile(pipe_svd.unet, mode="reduce-overhead", fullgraph=True)
        if prefs['enable_deepcache']:
            try:
                from DeepCache import DeepCacheSDHelper
            except Exception:
                run_sp("pip install DeepCache", realtime=False)
                from DeepCache import DeepCacheSDHelper
                pass
            helper = DeepCacheSDHelper(pipe=pipe_svd)
            helper.set_params(cache_interval=3, cache_branch_id=0)
            helper.enable()
        pipe_svd.scheduler = EDMEulerScheduler.from_pretrained(model_id, subfolder="scheduler")
        status['loaded_svd'] = model_id
    #clear_pipes('svd')
    vid_length = svd_prefs['num_frames'] / svd_prefs['fps']
    if svd_prefs['resume_frame']:
        vid_length = vid_length * (svd_prefs['continue_times'] + 1)
    clear_last()
    progress = Progress(f"Generating Stable Video of your Image... Length: {round(vid_length, 1)} seconds", steps=svd_prefs['num_inference_steps'])
    #progress = ProgressBar(bar_height=8)
    #total_steps = svd_prefs['num_inference_steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):
      nonlocal progress
      total_steps = pipe.num_timesteps
      percent = (step +1)/ total_steps
      progress.progress.value = percent
      progress.progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.progress.update()
    #prt(f"Generating Stable Video of your Image... Length: {round(vid_length, 1)} seconds")
    prt(progress)
    nudge(page.SVD, page=page)
    autoscroll(False)
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    if svd_prefs['init_image'].startswith('http'):
      init_img = PILImage.open(requests.get(svd_prefs['init_image'], stream=True).raw)
    else:
      if os.path.isfile(svd_prefs['init_image']):
        init_img = PILImage.open(svd_prefs['init_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your init_image {svd_prefs['init_image']}")
        return
    width, height = init_img.size
    width, height = scale_dimensions(width, height, svd_prefs['max_size'], multiple=64)
    init_img = init_img.resize((width, height))#, resample=PILImage.Resampling.BICUBIC)
    init_img = ImageOps.exif_transpose(init_img).convert("RGB")
    batch_output = os.path.join(prefs['image_output'], svd_prefs['batch_folder_name'])
    makedir(batch_output)
    #for v in range(svd_prefs['num_videos']):
    frames_batch = None
    random_seed = get_seed(svd_prefs['seed'])
    
    generator = torch.manual_seed(random_seed)
    try: #, callback_on_step_end=callback_fnc , callback_on_step_end=progress.callback_alt
        frames_batch = pipe_svd(init_img, width=width, height=height, num_frames=svd_prefs["num_frames"], decode_chunk_size=svd_prefs["decode_chunk_size"], motion_bucket_id=svd_prefs['motion_bucket_id'], noise_aug_strength=svd_prefs['noise_aug_strength'], num_inference_steps=svd_prefs['num_inference_steps'], min_guidance_scale=svd_prefs['min_guidance_scale'], max_guidance_scale=svd_prefs['max_guidance_scale'], fps=svd_prefs['fps'], generator=generator).frames[0]
        if svd_prefs['resume_frame']:
            #if isinstance(frames_batch[0], list):
            #    frames_batch = frames_batch[0]
            #new_frames = frames_batch
            #for n, f in enumerate(frames_batch):
                #print(f"n: {n} f:{f} {type(f)}")
            #new_frames[n] = []
            last_frame = frames_batch[-1]
            for t in range(svd_prefs['continue_times']):
                progress.status(f"...continue {t + 1}/{svd_prefs['continue_times']}")
                frames_continued = pipe_svd(last_frame, width=width, height=height, num_frames=svd_prefs["num_frames"], decode_chunk_size=svd_prefs["decode_chunk_size"], motion_bucket_id=svd_prefs['motion_bucket_id'], noise_aug_strength=0.0, num_inference_steps=svd_prefs['num_inference_steps'], min_guidance_scale=svd_prefs['min_guidance_scale'], max_guidance_scale=svd_prefs['max_guidance_scale'], fps=svd_prefs['fps'], generator=generator).frames[0]
                frames_batch.extend(frames_continued)
                last_frame = frames_continued[-1]
            #for n, c in enumerate(new_frames):
                #frames_batch[n].append(c)
    except Exception as e:
      if frames_batch != None:
          print(f"SVD {frames_batch} type: {type(frames_batch)}")
      clear_last()
      alert_msg(page, f"ERROR: SVD Image-To-Video failed for some reason. Possibly out of memory or something wrong with the code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
      return
    clear_last()
    autoscroll(True)
    b = 0
    #for frames in frames_batch:
    frames_dir = os.path.join(batch_output, f"frames-{b}")
    exists = True
    while exists:
        if os.path.isdir(frames_dir):
            b += 1
            frames_dir = os.path.join(batch_output, f"frames-{b}")
        else:
            exists = False
    makedir(frames_dir)
    #idx = 0
    for image in frames_batch:
        fname = f"{svd_prefs['file_prefix']}{format_filename(svd_prefs['batch_folder_name'])}-{b}"
        image_path = available_file(frames_dir, fname, 0, zfill=4)
        image.save(image_path)
        new_file = os.path.basename(image_path)
        if not svd_prefs['display_upscaled_image'] or not svd_prefs['apply_ESRGAN_upscale']:
            #prt(Row([Img(src=image_path, width=svd_prefs['width'], height=svd_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            prt(Row([ImageButton(src=image_path, width=width, height=height, data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
        if svd_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            upscale_image(image_path, image_path, scale=svd_prefs["enlarge_scale"], face_enhance=svd_prefs["face_enhance"])
            if svd_prefs['display_upscaled_image']:
                time.sleep(0.6)
                prt(Row([Img(src=asset_dir(image_path), width=width * float(svd_prefs["enlarge_scale"]), height=height * float(svd_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        #else:
        #    time.sleep(0.2)
        #shutil.copy(image_path, os.path.join(frames_dir, new_file))
        # TODO: Add Metadata
        
        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
        #idx += 1
    if svd_prefs['export_to_gif']:
        try:
            installer = Installing("Saving Animated GIF image sequence...")
            prt(installer)
            from diffusers.utils import export_to_gif
            gif_file = available_file(batch_output, fname, no_num=True, ext="gif")
            export_to_gif(frames_batch, gif_file, fps=svd_prefs['fps'])
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Couldn't export gif, but frames still saved...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            pass
        clear_last()
        if not os.path.isfile(gif_file):
            prt(f"Problem creating gif file, but frames still saved...")
        else:
            prt(Row([ImageButton(src=gif_file, width=width, height=height, data=gif_file, page=page)], alignment=MainAxisAlignment.CENTER))
    if svd_prefs['export_to_video']:
        try:
            installer = Installing("Running Google FILM: Frame Interpolation for Large Motion...")
            prt(installer)
            out_file = available_file(batch_output, fname, no_num=True, ext="mp4")
            if svd_prefs['interpolate_video']:
                interpolate_video(frames_dir, input_fps=svd_prefs['fps'], output_fps=svd_prefs['target_fps'], output_video=out_file, installer=installer)
            else:
                installer.set_message("Saving Frames to Video using FFMPEG with Deflicker...")
                pattern = create_pattern(new_file) #fname+"-%04d.png"
                frames_to_video(frames_dir, pattern=pattern, input_fps=svd_prefs['fps'], output_fps=svd_prefs['target_fps'], output_video=out_file, installer=installer, deflicker=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Couldn't interpolate video, but frames still saved...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            pass
        clear_last()
        if not os.path.isfile(out_file):
            prt(f"Problem creating video file, but frames still saved...")
        else:
            prt(Markdown(f"Video saved to [{out_file}]({filepath_to_url(out_file)})", on_tap_link=lambda e: e.page.launch_url(e.data)))
            #prt(Row([VideoContainer(out_file)], alignment=MainAxisAlignment.CENTER))
        b += 1
    #filename = filename[:int(prefs['file_max_length'])]
    #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
    autoscroll(True)
    video_path = ""
    prt(f"Done creating animation... Check {batch_output}")
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_roop(page):
    global roop_prefs, status
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.Roop.controls.append(line)
      page.Roop.update()
    def clear_last(lines=1):
      clear_line(page.Roop, lines=lines)
    def autoscroll(scroll=True):
        page.Roop.auto_scroll = scroll
        page.Roop.update()
    if not bool(roop_prefs['source_image']) or not bool(roop_prefs['target_image']):
        alert_msg(page, "You must provide a source image and target video or image...")
        return
    page.Roop.controls = page.Roop.controls[:1]
    autoscroll()
    installer = Installing("Installing ROOP Libraries...")
    prt(installer)
    roop_dir = os.path.join(root_dir, "roop")
    if not os.path.exists(roop_dir):
        try:
            installer.status("...cloning s0md3v/roop.git")
            run_process("git clone https://github.com/s0md3v/roop.git", cwd=root_dir)
            installer.status("...installing requirements")
            #run_process("pip install -r requirements.txt", cwd=roop_dir)
            installer.status("...downloading roop inswapper")
            download_file("https://huggingface.co/camenduru/roop/resolve/main/inswapper_128.onnx", to=roop_dir)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Point-E Requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    pip_install("ffmpeg opencv-python|cv2 onnx==1.14.0 insightface==0.7.3 tk==0.1.0 customtkinter==5.1.3 onnxruntime-gpu==1.15.0|onnxruntime opennsfw2==0.10.2 protobuf==4.24.3 gfpgan", installer=installer)
    
    clear_pipes()

    from PIL import ImageOps
    if bool(roop_prefs['output_name']):
        fname = format_filename(roop_prefs['output_name'], force_underscore=True)
    elif bool(roop_prefs['batch_folder_name']):
        fname = format_filename(roop_prefs['batch_folder_name'], force_underscore=True)
    else:
        fname = "output"
    #TODO: Add prefix
    if bool(roop_prefs['batch_folder_name']):
        batch_output = os.path.join(stable_dir, roop_prefs['batch_folder_name'])
    else:
        batch_output = stable_dir
    if not os.path.exists(batch_output):
        os.makedirs(batch_output)
    #.rpartition(slash)[0], 'roop'
    output_path = os.path.join(prefs['image_output'], roop_prefs['batch_folder_name'])
    if not os.path.exists(output_path):
        os.makedirs(output_path)
    init_img = None
    if roop_prefs['source_image'].startswith('http'):
        installer.status("...downloading source image")
        init_img = PILImage.open(requests.get(roop_prefs['source_image'], stream=True).raw)
    else:
        if os.path.isfile(roop_prefs['source_image']):
            init_img = PILImage.open(roop_prefs['source_image'])
        else:
            alert_msg(page, f"ERROR: Couldn't find your source_image {roop_prefs['source_image']}")
            return
    source_name = roop_prefs['source_image'].rpartition("/")[2] if "/" in roop_prefs['source_image'] else roop_prefs['source_image'].rpartition(slash)[2]
    source_path = ""
    if init_img != None:
        installer.status("...resizing source image")
        width, height = init_img.size
        width, height = scale_dimensions(width, height, roop_prefs['max_size'])
        init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        source_path = os.path.join(batch_output, f'{source_name.rpartition(".")[0]}.png')
        init_img.save(source_path)
    is_video = roop_prefs['target_image'].endswith('mp4') or roop_prefs['target_image'].endswith('avi')

    target_img = None
    target_name = roop_prefs['target_image'].rpartition("/")[2] if "/" in roop_prefs['target_image'] else roop_prefs['target_image'].rpartition(slash)[2]
    target_path = ""
    if roop_prefs['target_image'].startswith('http'):
        if is_video:
            installer.status("...downloading target video")
            download_file(roop_prefs['target_image'], batch_output)
            target_path = os.path.join(batch_output, target_name)
        else:
            installer.status("...downloading target image")
            target_img = PILImage.open(requests.get(roop_prefs['target_image'], stream=True).raw)
    else:
        if os.path.isfile(roop_prefs['target_image']):
            if is_video:
                target_path = roop_prefs['target_image']
            else:
                target_img = PILImage.open(roop_prefs['target_image'])
        else:
            alert_msg(page, f"ERROR: Couldn't find your target_image {roop_prefs['target_image']}")
            return
    if target_img != None:
        installer.status("...resizing target image")
        width, height = target_img.size
        width, height = scale_dimensions(width, height, roop_prefs['max_size'])
        target_img = target_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        target_img = ImageOps.exif_transpose(target_img).convert("RGB")
        target_path = os.path.join(batch_output, f'{target_name.rpartition(".")[0]}.png')
        target_img.save(target_path)
    clear_last()
    progress = ProgressBar(bar_height=8)
    prt(f"Generating your ROOP Face-Swap {'Video' if is_video else 'Image'}...")
    prt(progress)
    autoscroll(False)
    total_steps = 100 #?
    def callback_fnc(step: int) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}"
      progress.update()
    output_file = available_file(output_path, fname, 0, ext='mp4' if is_video else 'png')
    #output_file = os.path.join(output_path, f"{fname}{'.mp4' if is_video else '.png'}")
    cmd = f'python run.py -s "{source_path}" -t "{target_path}" -o "{output_file}"'
    cmd += f" --frame-processor {roop_prefs['frame_processor']}"
    cmd += f" --execution-provider {torch_device}"
    if roop_prefs['keep_fps'] and is_video: cmd += " --keep-fps"
    if roop_prefs['keep_audio'] and is_video: cmd += " --keep-audio"
    if roop_prefs['keep_frames'] and is_video: cmd += " --keep-frames"
    if roop_prefs['many_faces']: cmd += " --many-faces"
    if is_video:
        cmd += f" --video-encoder {roop_prefs['video_encoder']}"
        cmd += f" --video-quality {roop_prefs['video_quality']}"
    #--max-menory
    prt(f"Running {cmd}")
    try:
        run_process(cmd, cwd=roop_dir, page=page, realtime=True)
    except Exception as e:
        clear_last()
        clear_last()
        alert_msg(page, "Error running Python.", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    clear_last()
    clear_last()
    clear_last()
    autoscroll(True)
    #TODO: Upscale Image
    if os.path.isfile(output_file):
        if is_video:
            prt(Row([VideoContainer(output_file)], alignment=MainAxisAlignment.CENTER))
            #prt(Row([VideoPlayer(video_file=output_file, width=width, height=height)], alignment=MainAxisAlignment.CENTER))
        else:
            prt(Row([ImageButton(src=output_file, data=output_file, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
    else:
        prt("Error Generating Output File! A NSFW Image may have been detected.")
    prt(Row([Text(output_file)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_hallo(page):
    global hallo_prefs, status
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.Hallo.controls.append(line)
      page.Hallo.update()
    def clear_last(lines=1):
      clear_line(page.Hallo, lines=lines)
    def autoscroll(scroll=True):
        page.Hallo.auto_scroll = scroll
        page.Hallo.update()
    if not bool(hallo_prefs['input_audio']) or not bool(hallo_prefs['target_image']):
        alert_msg(page, "You must provide a source audio and target video...")
        return
    page.Hallo.controls = page.Hallo.controls[:1]
    autoscroll()
    installer = Installing("Installing Hallo Packages...")
    prt(installer)
    hallo_dir = os.path.join(root_dir, "hallo")
    hallo_checkpoints = os.path.join(hallo_dir, "pretrained_models")
    if not os.path.exists(hallo_dir):
        try:
            installer.status("...cloning fudan-generative-vision/hallo")
            run_process("git clone https://github.com/fudan-generative-vision/hallo.git", cwd=root_dir)
            installer.status("...installing requirements")
            #run_process("pip install -r requirements.txt", cwd=hallo_dir)
            pip_install("audio-separator==0.17.2 av==12.1.0 bitsandbytes decord==0.6.0 einops==0.8.0 insightface==0.7.3 librosa==0.10.2.post1 mediapipe[vision]|mediapipe mlflow==2.13.1 moviepy numpy omegaconf onnx2torch==1.5.14 onnx onnxruntime-gpu==1.18.0 opencv-contrib-python==4.9.0.80 opencv-python-headless==4.9.0.80 opencv-python==4.9.0.80 pillow setuptools tqdm xformers==0.0.25.post1 isort pylint==3.2.2 pre-commit==3.7.1 gradio", installer=installer)
            get_ffmpeg(installer)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Hallo Requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    elif force_update("hallo"):
        installer.status("...updating fudan-generative-vision/hallo")
        run_sp("git pull origin main", cwd=hallo_dir)
    os.chdir(hallo_dir)
    if not os.path.exists(hallo_checkpoints):
        makedir(hallo_checkpoints)
        installer.status("...downloading pretrained models")
        run_process("git clone https://huggingface.co/fudan-generative-ai/hallo pretrained_models", cwd=hallo_dir)
        
    clear_pipes()
    if bool(hallo_prefs['output_name']):
        fname = format_filename(hallo_prefs['output_name'], force_underscore=True)
    elif bool(hallo_prefs['batch_folder_name']):
        fname = format_filename(hallo_prefs['batch_folder_name'], force_underscore=True)
    else:
        fname = "output"
    #TODO: Add prefix
    results_dir = os.path.join(hallo_dir, "results")
    makedir(results_dir)
    inputs_dir = os.path.join(hallo_dir, "inputs")
    makedir(inputs_dir)
    output_path = os.path.join(prefs['image_output'], hallo_prefs['batch_folder_name'])
    makedir(output_path)
    def center_crop_resize(im):
        width, height = im.size
        d = min(width, height)
        left = (width - d) / 2
        upper = (height - d) / 2
        right = (width + d) / 2
        lower = (height + d) / 2
        return im.crop((left, upper, right, lower)).resize((hallo_prefs['img_size'], hallo_prefs['img_size']))
    original_img = None
    target_name = hallo_prefs['target_image'].rpartition("/")[2] if "/" in hallo_prefs['target_image'] else hallo_prefs['target_image'].rpartition(slash)[2]
    target_path = ""
    installer.status("...preparing image")
    if hallo_prefs['target_image'].startswith('http'):
        installer.status("...downloading target video")
        download_file(hallo_prefs['target_image'], inputs_dir)
        target_path = os.path.join(inputs_dir, target_name)
    else:
        if os.path.isfile(hallo_prefs['target_image']):
            target_path = hallo_prefs['target_image']
            shutil.copy(target_path, os.path.join(inputs_dir, os.path.basename(target_path)))
            target_path = os.path.join(inputs_dir, os.path.basename(target_path))
        else:
            alert_msg(page, f"ERROR: Couldn't find your target_image {hallo_prefs['target_image']}")
            os.chdir(root_dir)
            return
    original_img = PILImage.open(target_path)
    width, height = original_img.size
    if width == height:
        if width != hallo_prefs['img_size']:
            original_img.resize((hallo_prefs['img_size'], hallo_prefs['img_size']))
            original_img.save(target_path)
    else:
        original_img = center_crop_resize(original_img)    
        original_img.save(target_path)
    installer.status("...preparing wav file")
    input_audio = ""
    if hallo_prefs['input_audio'].startswith('http'):
        installer.status("...downloading input audio")
        download_file(hallo_prefs['input_audio'], inputs_dir)
        input_audio = os.path.join(inputs_dir, target_name)
    else:
        if os.path.isfile(hallo_prefs['input_audio']):
            input_audio = hallo_prefs['input_audio']
            shutil.copy(input_audio, os.path.join(inputs_dir, os.path.basename(input_audio)))
            input_audio = os.path.join(inputs_dir, os.path.basename(input_audio))
        else:
            alert_msg(page, f"ERROR: Couldn't find your input_audio {hallo_prefs['input_audio']}")
            os.chdir(root_dir)
            return
    try:
        import ffmpeg
        (
            ffmpeg
            .input(input_audio)
            .output(input_audio, acodec='pcm_s16le', ar=16000)
            .overwrite_output()
            .run(capture_stdout=True, capture_stderr=True)
        )
    except ffmpeg.Error as e:
        alert_msg(page, f"FFmpeg error occurred: {e.stderr.decode()}")
        os.chdir(root_dir)
        return
    except Exception as e:
        alert_msg(page, f"An error occurred: {str(e)}")
        os.chdir(root_dir)
        return
    installer.status("...preparing config")
    yaml_file = os.path.join(hallo_dir, "configs", "sdd.yaml")
    yaml = YAML()
    yaml.preserve_quotes = True
    yaml.indent(mapping=2, sequence=4, offset=2)
    with open(os.path.join(hallo_dir, "configs/inference/default.yaml"), 'r') as file:
        config = yaml.load(file)
    config['inference_steps'] = hallo_prefs['num_inference_steps']
    config['cfg_scale'] = hallo_prefs['guidance_scale']
    #config['save_path'] = './results'
    config['data']['export_video']['fps'] = hallo_prefs['fps']
    if hallo_prefs['img_size'] != 512:
        config['data']['source_image']['width'] = hallo_prefs['img_size']
        config['data']['source_image']['height'] = hallo_prefs['img_size']
    with open(yaml_file, 'w') as file:
        yaml.dump(config, file)
    clear_last()
    progress = ProgressBar(bar_height=8)
    prt(f"Generating your Hallo Video... See Connsole for Progress.")
    prt(progress)
    autoscroll(False)
    video_file = os.path.join(hallo_dir, ".cache", "output.mp4")
    #video_file = os.path.join(results_dir, os.path.basename(output_file))
    output_file = available_file(output_path, fname, 0, ext='mp4', no_num=True)
    output_gif = available_file(output_path, fname, 0, ext='gif', no_num=True)
    
    cmd = f'python scripts/inference.py --source_image inputs/{os.path.basename(target_path)} --driving_audio inputs/{os.path.basename(input_audio)} --config configs/sdd.yaml --pose_weight {hallo_prefs["pose_weight"]} --face_weight {hallo_prefs["face_weight"]} --lip_weight {hallo_prefs["lip_weight"]} --face_expand_ratio {hallo_prefs["face_expand_ratio"]}'
    #f'python inference.py --face "inputs/{os.path.basename(target_path)}"  --audio "inputs/{os.path.basename(input_audio)}"{extras} --outfile "results/{os.path.basename(output_file)}"'
    prt(f"Running {cmd}")
    try:
        run_process(cmd, cwd=hallo_dir, page=page, realtime=True)
    except Exception as e:
        clear_last(2)
        alert_msg(page, "Error running Python inference.", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        os.chdir(root_dir)
        return
    shutil.move(video_file, output_file)
    def convert_mp4_to_gif(input_file, output_file, fps=10):
        try:
            import ffmpeg
            stream = ffmpeg.input(input_file)
            stream = ffmpeg.filter(stream, 'fps', fps=fps)
            #stream = ffmpeg.filter(stream, 'scale', w=scale, h=-1)
            split = ffmpeg.filter(stream, 'split')
            palette = ffmpeg.filter(split[0], 'palettegen')
            stream = ffmpeg.filter([split[1], palette], 'paletteuse')
            output = ffmpeg.output(stream, output_file, loop=0)
            ffmpeg.run(output, overwrite_output=True)
        except ffmpeg.Error as e:
            print(f"An error occurred saving gif: {e.stderr.decode()}")
            pass
    convert_mp4_to_gif(output_file, output_gif, fps=hallo_prefs['fps'])
    clear_last(3)
    autoscroll(True)
    if os.path.isfile(output_gif):
        prt(Row([ImageButton(src=output_gif, width=hallo_prefs['img_size'], height=hallo_prefs['img_size'], data=output_gif, page=page)], alignment=MainAxisAlignment.CENTER))
    if os.path.isfile(output_file):
        prt(Markdown(f"Video saved to [{output_file}]({filepath_to_url(output_file)})", on_tap_link=lambda e: e.page.launch_url(e.data)))
        #prt(Row([Text("Saved to {output_file}")], alignment=MainAxisAlignment.CENTER))
        try:
            prt(Row([VideoContainer(output_file)], alignment=MainAxisAlignment.CENTER))
        except:
            pass
    else:
        prt("💢  Error Generating Output File!")
    autoscroll(False)
    os.chdir(root_dir)
    play_snd(Snd.ALERT, page)


def run_video_retalking(page):
    global video_retalking_prefs, status
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.Video_ReTalking.controls.append(line)
      page.Video_ReTalking.update()
    def clear_last(lines=1):
      clear_line(page.Video_ReTalking, lines=lines)
    def autoscroll(scroll=True):
        page.Video_ReTalking.auto_scroll = scroll
        page.Video_ReTalking.update()
    if not bool(video_retalking_prefs['input_audio']) or not bool(video_retalking_prefs['target_video']):
        alert_msg(page, "You must provide a source audio and target video...")
        return
    page.Video_ReTalking.controls = page.Video_ReTalking.controls[:1]
    autoscroll()
    installer = Installing("Installing Video-ReTalking Packages...")
    prt(installer)
    video_retalking_dir = os.path.join(root_dir, "video-retalking")
    video_retalking_checkpoints = os.path.join(video_retalking_dir, "checkpoints")
    if not os.path.exists(video_retalking_dir):
        try:
            installer.status("...cloning vinthony/video_retalking.git")
            run_process("git clone https://github.com/OpenTalker/video-retalking.git", cwd=root_dir)
            installer.status("...installing requirements")
            #run_process("pip install -r requirements.txt", cwd=video_retalking_dir)
            pip_install("basicsr==1.4.2 kornia==0.5.1 face-alignment==1.3.4 ninja==1.10.2.3 einops facexlib==0.2.5 librosa==0.9.2 dlib==19.24.0 gradio numpy", installer=installer)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Video-ReTalking Requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    makedir(video_retalking_checkpoints)
    if not os.path.isfile(video_retalking_checkpoints, '30_net_gen.pth'):
        installer.status("...downloading checkpoints")
        download_file("https://github.com/vinthony/video-retalking/releases/download/v0.0.1/30_net_gen.pth", to=video_retalking_checkpoints)
        download_file("https://github.com/vinthony/video-retalking/releases/download/v0.0.1/BFM.zip", to=video_retalking_checkpoints)
        download_file("https://github.com/vinthony/video-retalking/releases/download/v0.0.1/DNet.pt", to=video_retalking_checkpoints)
        download_file("https://github.com/vinthony/video-retalking/releases/download/v0.0.1/ENet.pth", to=video_retalking_checkpoints)
        download_file("https://github.com/vinthony/video-retalking/releases/download/v0.0.1/expression.mat", to=video_retalking_checkpoints)
        download_file("https://github.com/vinthony/video-retalking/releases/download/v0.0.1/face3d_pretrain_epoch_20.pth", to=video_retalking_checkpoints)
        download_file("https://github.com/vinthony/video-retalking/releases/download/v0.0.1/GFPGANv1.3.pth", to=video_retalking_checkpoints)
        download_file("https://github.com/vinthony/video-retalking/releases/download/v0.0.1/GPEN-BFR-512.pth", to=video_retalking_checkpoints)
        download_file("https://github.com/vinthony/video-retalking/releases/download/v0.0.1/LNet.pth", to=video_retalking_checkpoints)
        download_file("https://github.com/vinthony/video-retalking/releases/download/v0.0.1/ParseNet-latest.pth", to=video_retalking_checkpoints)
        download_file("https://github.com/vinthony/video-retalking/releases/download/v0.0.1/RetinaFace-R50.pth", to=video_retalking_checkpoints)
        download_file("https://github.com/vinthony/video-retalking/releases/download/v0.0.1/shape_predictor_68_face_landmarks.dat", to=video_retalking_checkpoints)
        installer.status("...unziping BFM")
        run_sp(f"unzip {os.path.join(video_retalking_checkpoints, 'BFM.zip')} -d BFM", realtime=False, cwd=video_retalking_checkpoints)
        os.remove(os.path.join(video_retalking_checkpoints, 'BFM.zip'))
    pip_install("ffmpeg opencv-python|cv2", installer=installer)
    clear_pipes()
    import glob
    from base64 import b64encode
    from PIL import ImageOps
    if bool(video_retalking_prefs['output_name']):
        fname = format_filename(video_retalking_prefs['output_name'], force_underscore=True)
    elif bool(video_retalking_prefs['batch_folder_name']):
        fname = format_filename(video_retalking_prefs['batch_folder_name'], force_underscore=True)
    else:
        fname = "output"
    #TODO: Add prefix
    if bool(video_retalking_prefs['batch_folder_name']):
        batch_output = os.path.join(stable_dir, video_retalking_prefs['batch_folder_name'])
    else:
        batch_output = stable_dir
    makedir(batch_output)
    results_dir = os.path.join(video_retalking_dir, "results")
    makedir(results_dir)
    inputs_dir = os.path.join(video_retalking_dir, "inputs")
    makedir(inputs_dir)
    output_path = os.path.join(prefs['image_output'], video_retalking_prefs['batch_folder_name'])
    makedir(output_path)

    expression_img = None
    target_name = video_retalking_prefs['target_video'].rpartition("/")[2] if "/" in video_retalking_prefs['target_video'] else video_retalking_prefs['target_video'].rpartition(slash)[2]
    target_path = ""
    if video_retalking_prefs['target_video'].startswith('http'):
        installer.status("...downloading target video")
        download_file(video_retalking_prefs['target_video'], inputs_dir)
        target_path = os.path.join(inputs_dir, target_name)
    else:
        if os.path.isfile(video_retalking_prefs['target_video']):
            target_path = video_retalking_prefs['target_video']
            shutil.copy(target_path, os.path.join(inputs_dir, os.path.basename(target_path)))
            target_path = os.path.join(inputs_dir, os.path.basename(target_path))
        else:
            alert_msg(page, f"ERROR: Couldn't find your target_video {video_retalking_prefs['target_video']}")
            return
    input_audio = ""
    if video_retalking_prefs['input_audio'].startswith('http'):
        installer.status("...downloading input audio")
        download_file(video_retalking_prefs['input_audio'], inputs_dir)
        input_audio = os.path.join(inputs_dir, target_name)
    else:
        if os.path.isfile(video_retalking_prefs['input_audio']):
            input_audio = video_retalking_prefs['input_audio']
            shutil.copy(input_audio, os.path.join(inputs_dir, os.path.basename(input_audio)))
            input_audio = os.path.join(inputs_dir, os.path.basename(input_audio))
        else:
            alert_msg(page, f"ERROR: Couldn't find your input_audio {video_retalking_prefs['input_audio']}")
            return
    if video_retalking_prefs['exp_img'] == "Image":
        if video_retalking_prefs['exp_image'].startswith('http'):
            installer.status("...downloading expression image")
            download_file(video_retalking_prefs['exp_image'], inputs_dir)
            expression_img = os.path.join(inputs_dir, target_name)
        else:
            if os.path.isfile(video_retalking_prefs['exp_image']):
                expression_img = video_retalking_prefs['exp_image']
                shutil.copy(expression_img, os.path.join(inputs_dir, os.path.basename(expression_img)))
                expression_img = os.path.join(inputs_dir, os.path.basename(expression_img))
    clear_last()
    progress = ProgressBar(bar_height=8)
    prt(f"Generating your Video-ReTalking Video... See Connsole for Progress.")
    prt(progress)
    autoscroll(False)
    total_steps = 100 #?
    def callback_fnc(step: int) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}"
      progress.update()
    output_file = available_file(output_path, fname, 0, ext='mp4', no_num=True)
    video_file = os.path.join(results_dir, os.path.basename(output_file))
    extras = ""
    if video_retalking_prefs['exp_img'] != 'neutral':
        if video_retalking_prefs['exp_img'] != 'Image':
            if bool(expression_img):
                extras += f' --exp_img "inputs/{os.path.basename(expression_img)}"'
        else:
            extras += f" --exp_img {video_retalking_prefs['exp_img']}"
    if video_retalking_prefs['up_face'] != 'original':
        extras += f" --up_face {video_retalking_prefs['up_face']}"
    if video_retalking_prefs['fps'] != 25:
        extras += f" --fps {video_retalking_prefs['fps']}"
    if video_retalking_prefs['img_size'] != 384:
        extras += f" --img_size {video_retalking_prefs['img_size']}"
    cmd = f'python inference.py --face "inputs/{os.path.basename(target_path)}"  --audio "inputs/{os.path.basename(input_audio)}"{extras} --outfile "results/{os.path.basename(output_file)}"'
    prt(f"Running {cmd}")
    try:
        run_process(cmd, cwd=video_retalking_dir, page=page, realtime=True)
    except Exception as e:
        clear_last(2)
        alert_msg(page, "Error running Python inference.", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    shutil.copy(video_file, output_file)
    clear_last(3)
    autoscroll(True)
    #TODO: Upscale Image
    if os.path.isfile(output_file):
        prt(Markdown(f"Video saved to [{output_file}]({filepath_to_url(output_file)})", on_tap_link=lambda e: e.page.launch_url(e.data)))
        #prt(Row([Text("Saved to {output_file}")], alignment=MainAxisAlignment.CENTER))
        try:
            prt(Row([VideoContainer(output_file)], alignment=MainAxisAlignment.CENTER))
        except:
            pass
    else:
        prt("💢  Error Generating Output File!")
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_live_portrait(page):
    global live_portrait_prefs, status
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.LivePortrait.controls.append(line)
      page.LivePortrait.update()
    def clear_last(lines=1):
      clear_line(page.LivePortrait, lines=lines)
    def autoscroll(scroll=True):
        page.LivePortrait.auto_scroll = scroll
        page.LivePortrait.update()
    if not bool(live_portrait_prefs['input_image']) or not bool(live_portrait_prefs['target_video']):
        alert_msg(page, "You must provide a source image and driving video...")
        return
    page.LivePortrait.controls = page.LivePortrait.controls[:1]
    autoscroll()
    installer = Installing("Installing LivePortrait Packages...")
    prt(installer)
    live_portrait_dir = os.path.join(root_dir, "LivePortrait")
    live_portrait_weights = os.path.join(live_portrait_dir, "pretrained_weights")
    if not os.path.exists(live_portrait_dir):
        try:
            installer.status("...cloning KwaiVGI/LivePortrait")
            run_sp("git clone https://github.com/KwaiVGI/LivePortrait", cwd=root_dir)
            installer.status("...installing requirements")
            get_ffmpeg(installer)
            try:
                import onnxruntime
            except:
                installer.status("...installing onnxruntime-gpu")
                run_sp("pip install onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/")
                pass # onnxruntime-gpu==1.18.0|onnxruntime
            pip_install("numpy pyyaml|yaml opencv-python|cv2 scipy imageio patch-ng lmdb tqdm rich onnx==1.16.1 scikit-image|skimage albumentations==1.4.10 matplotlib imageio-ffmpeg tyro==0.8.5 gradio==4.37.1 colorama gdown", installer=installer)
            #installer.status("...building insightface 3D mesh cython")
            #run_sp("python setup.py builld_ext --inplace", cwd=os.path.join(live_portrait_dir, "src/utils/dependencies/insightface/thirdparty/face3d/mesh/cython"))
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing LivePortrait Requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    elif force_update("liveportrait"):
        installer.status("...updating KwaiVGI/LivePortrait")
        run_sp("git pull origin main", cwd=live_portrait_dir)
    makedir(live_portrait_weights)
    if not os.path.isfile(os.path.join(live_portrait_weights, 'liveportrait', 'landmark.onnx')):
        installer.status("...downloading pretrained weights")
        from gdown import download_folder
        download_folder(id="1UtKgzKjFAOmZkhNK-OYT0caJ_w2XAnib", output=live_portrait_weights, quiet=True)
        # Could also get from https://huggingface.co/camenduru/LivePortrait
    os.chdir(live_portrait_dir)
    clear_pipes()
    if bool(live_portrait_prefs['output_name']):
        fname = format_filename(live_portrait_prefs['output_name'], force_underscore=True)
    elif bool(live_portrait_prefs['batch_folder_name']):
        fname = format_filename(live_portrait_prefs['batch_folder_name'], force_underscore=True)
    else:
        fname = "output"
    #TODO: Add prefix
    if bool(live_portrait_prefs['batch_folder_name']):
        batch_output = os.path.join(stable_dir, live_portrait_prefs['batch_folder_name'])
    else:
        batch_output = stable_dir
    makedir(batch_output)
    results_dir = os.path.join(live_portrait_dir, "animations")
    makedir(results_dir)
    inputs_dir = os.path.join(live_portrait_dir, "assets")
    #makedir(inputs_dir)
    output_path = os.path.join(prefs['image_output'], live_portrait_prefs['batch_folder_name'])
    makedir(output_path)
    target_path = ""
    if live_portrait_prefs['target_video'].startswith('http'):
        installer.status("...downloading target video")
        target_path = download_file(live_portrait_prefs['target_video'], inputs_dir)
    else:
        if os.path.isfile(live_portrait_prefs['target_video']):
            target_path = live_portrait_prefs['target_video']
            shutil.copy(target_path, os.path.join(inputs_dir, os.path.basename(target_path)))
            target_path = os.path.join(inputs_dir, os.path.basename(target_path))
        else:
            alert_msg(page, f"ERROR: Couldn't find your target_video {live_portrait_prefs['target_video']}")
            return
    input_image = ""
    if live_portrait_prefs['input_image'].startswith('http'):
        installer.status("...downloading input image")
        input_image = download_file(live_portrait_prefs['input_image'], inputs_dir)
    else:
        if os.path.isfile(live_portrait_prefs['input_image']):
            input_image = live_portrait_prefs['input_image']
            shutil.copy(input_image, os.path.join(inputs_dir, os.path.basename(input_image)))
            input_image = os.path.join(inputs_dir, os.path.basename(input_image))
        else:
            alert_msg(page, f"ERROR: Couldn't find your input_image {live_portrait_prefs['input_image']}")
            return
    clear_last()
    progress = ProgressBar(bar_height=8)
    prt(f"Generating your LivePortrait Video... See Connsole for Progress.")
    prt(progress)
    autoscroll(False)
    total_steps = 100 #?
    def callback_fnc(step: int) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}"
      progress.update()
    torch.backends.cudnn.benchmark = True
    #output_file = os.path.join(results_dir, os.path.basename(output_file))
    output_file = available_file(output_path, fname, 0, ext='mp4', no_num=True)
    video_file = os.path.join(results_dir, f"{os.path.basename(input_image).rpartition('.')[0]}--{os.path.basename(target_path).rpartition('.')[0]}.mp4")
    extras = ""
    if live_portrait_prefs['disable_pasteback']:
        extras += f" --no_flag_pasteback"
    if live_portrait_prefs['disable_relative']:
        extras += f" --no_flag_relative"
    if live_portrait_prefs['disable_lip_zero']:
        extras += f" --no_flag_lip_zero"
    if live_portrait_prefs['eye_retargeting']:
        extras += f" --flag_eye_retargeting"
    if live_portrait_prefs['lip_retargeting']:
        extras += f" --no_flag_lip_retargeting"
    if not live_portrait_prefs['keep_audio']:
        extras += f" --no_flag_add_sound"
    #if live_portrait_prefs['prepare_video']:
    #    run_process("python inference.py -h", cwd=live_portrait_dir, page=page, realtime=True)
    cmd = f'python inference.py -s assets/{os.path.basename(input_image)} -d assets/{os.path.basename(target_path)} -o {results_dir}{extras}'
    #--face "inputs/{os.path.basename(target_path)}"  --audio "inputs/{os.path.basename(input_image)}"{extras} --outfile "results/{os.path.basename(output_file)}"'
    print(f"Running {cmd}")
    try:#TODO: Use RunConsole UI
        run_sp(cmd, cwd=live_portrait_dir, realtime=True)
    except Exception as e:
        clear_last(2)
        alert_msg(page, "Error running Python inference.", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        os.chdir(root_dir)
        return
    clear_last(2)
    autoscroll(True)
    def apply_audio_to_video(source_video, target_video, output_video):
        import imageio_ffmpeg as ffmpeg
        import tempfile
        audio_file = tempfile.NamedTemporaryFile(delete=False, suffix='.aac')
        audio_file.close()
        ffmpeg.ffmpeg_extract_audio(source_video, audio_file.name)
        stream = ffmpeg.output(ffmpeg.input(target_video), ffmpeg.input(audio_file.name), output_video, vcodec='libx264', acodec='aac', strict='experimental')
        ffmpeg.run(stream, capture_stdout=True, capture_stderr=True)
        os.unlink(audio_file.name)
    #TODO: Upscale Video and keep_audio
    if os.path.isfile(video_file):
        '''if live_portrait_prefs['keep_audio']:
            apply_audio_to_video(target_path, video_file, output_file)
        else:'''
        shutil.copy(video_file, output_file)
        prt(Markdown(f"Video saved to [{output_file}]({filepath_to_url(output_file)})", on_tap_link=lambda e: e.page.launch_url(e.data)))
        #prt(Row([Text("Saved to {output_file}")], alignment=MainAxisAlignment.CENTER))
        try:
            prt(Row([VideoContainer(output_file)], alignment=MainAxisAlignment.CENTER))
        except:
            pass
    else:
        prt("💢  Error Generating Output File. Sorry, probably compatibility issues with onnxruntime-gpu and torch CUDA we're trying to work out...")
    os.chdir(root_dir)
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_style_crafter(page):
    global style_crafter_prefs, prefs, status
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.StyleCrafter.controls.append(line)
      page.StyleCrafter.update()
    def clear_last(lines=1):
      clear_line(page.StyleCrafter, lines=lines)
    def clear_list():
      page.TextToVideo.controls = page.StyleCrafter.controls[:1]
    def autoscroll(scroll=True):
      page.StyleCrafter.auto_scroll = scroll
      page.StyleCrafter.update()
    if not bool(style_crafter_prefs['prompt']):
      alert_msg(page, "You must provide the Stylized Prompt to process...")
      return
    progress = ProgressBar(bar_height=8)

    clear_list()
    autoscroll(True)
    installer = Installing("Installing StyleCrafter Pipeline...")
    prt(installer)
    #pytorch_lightning==1.9.3
    pip_install("decord einops imageio omegaconf pandas pytorch_lightning PyYAML|yaml setuptools moviepy av xformers gradio timm scikit-learn|sklearn open_clip_torch==2.22.0|open_clip kornia", installer=installer, upgrade=True)
    status['installed_xformers'] = True
    style_crafter_dir = os.path.join(root_dir, "StyleCrafter")
    checkpoints_dir = os.path.join(style_crafter_dir, "checkpoints")
    
    if not os.path.exists(style_crafter_dir) or force_update("StyleCrafter"):
        try:
            installer.status("...cloning GongyeLiu/StyleCrafter.git")
            run_sp("git clone https://github.com/GongyeLiu/StyleCrafter.git", cwd=root_dir, realtime=False)
        except Exception as e:
            clear_last()
            alert_msg(page, f"Error Installing github.com/GongyeLiu/StyleCrafter...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    if os.path.join(style_crafter_dir, "scripts", "evaluation") not in sys.path:
        sys.path.append(os.path.join(style_crafter_dir, "scripts", "evaluation"))
    try:
        installer.status("...install open_clip")
        run_sp("git lfs install", cwd=os.path.join(checkpoints_dir, "open_clip"), realtime=False)
        run_sp("git clone https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K", cwd=os.path.join(checkpoints_dir, "open_clip"), realtime=False)
        installer.status("...get Text2Video-512 ckpt")
        download_file("https://huggingface.co/VideoCrafter/Text2Video-512/blob/main/model.ckpt", to=os.path.join(checkpoints_dir, "videocrafter_t2v_320_512"))
        installer.status("...get adapter_v1.pth")
        download_file("https://huggingface.co/liuhuohuo/StyleCrafter/blob/main/adapter_v1.pth", to=os.path.join(checkpoints_dir, "stylecrafter"))
        installer.status("...get temporal_v1.pth")
        download_file("https://huggingface.co/liuhuohuo/StyleCrafter/blob/main/temporal_v1.pth", to=os.path.join(checkpoints_dir, "stylecrafter"))
        installer.status("")
    except Exception as e:
        clear_last()
        alert_msg(page, f"Error Setting up Dependancies...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    import numpy as np
    from PIL import Image as PILImage
    from PIL import ImageOps
    outputs_dir = os.path.join(style_crafter_dir, "outputs")
    if os.path.exists(outputs_dir):
        shutil.rmtree(outputs_dir, ignore_errors=True)
    makedir(outputs_dir)
    save_dir = os.path.join(style_crafter_dir, "my_eval_data")
    if os.path.exists(save_dir):
        shutil.rmtree(save_dir, ignore_errors=True)
    makedir(save_dir)
    prompt_json = os.path.join(save_dir, 'eval_prompt.json')
    #frames_dir = os.path.join(save_dir, 'frames')
    #makedir(frames_dir)
    batch_output = os.path.join(prefs['image_output'], style_crafter_prefs['batch_folder_name'])
    makedir(batch_output)
    output_frames_dir = os.path.join(save_dir, 'output_frames')
    makedir(output_frames_dir)
    data_dir = os.path.join(save_dir, 'data')
    makedir(data_dir)
    style_images = []
    init_images = []
    for fl in page.style_file_list.controls:
        f = fl.title.value
        style_images.append(f)
    if len(style_images) == 0:
        if bool(style_crafter_prefs['init_image']):
            style_images.append(style_crafter_prefs['init_image'])
        else:
            alert_msg(page, f"ERROR: You need to provide Input Style Image(s)")
            return
    #init_image = style_crafter_prefs['init_image']
    #if bool(init_image):
    for init_image in style_images:
        fname = init_image.rpartition(slash)[2]
        init_file = os.path.join(data_dir, fname)
        if init_image.startswith('http'):
            init_img = PILImage.open(requests.get(init_image, stream=True).raw)
        else:
            if os.path.isfile(init_image):
                init_img = PILImage.open(init_image)
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {init_image}")
                return
        init_img = init_img.resize((style_crafter_prefs['max_size'], style_crafter_prefs['max_size']), resample=PILImage.Resampling.LANCZOS)
        init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        init_img.save(init_file)
        init_images.append(f'data/{os.path.basename(init_file)}')
    if len(init_images) == 1:
        init_images = init_images[0]
    clear_last()
    prt(f"Generating StyleCrafter on Frames with your Prompt... See console for progress.")
    fname = format_filename(style_crafter_prefs['prompt'])
    class CustomEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, dict) and len(obj) == 1:
                return [obj]
            return super().default(obj)
    prompt_list = [{'prompt': style_crafter_prefs['prompt'], 'style_path': init_images}]
    with open(prompt_json, "w") as f:
        json.dump(prompt_list, f, ensure_ascii=False, indent=4, cls=CustomEncoder)
    random_seed = get_seed(style_crafter_prefs['seed'])
    config="configs/inference_video_320_512.yaml"
    ckpt="checkpoints/videocrafter_t2v_320_512/model.ckpt"
    adapter_ckpt="checkpoints/stylecrafter/adapter_v1.pth"
    temporal_ckpt="checkpoints/stylecrafter/temporal_v1.pth"
    prompt_dir="my_eval_data"
    filename="eval_prompt.json"
    res_dir="output"
    n_samples=1
    total_steps = style_crafter_prefs['num_inference_steps']#{"video" if style_crafter_prefs["output_video"] else "image"}
    mode = style_crafter_prefs["selected_mode"].split('"')[1]
    cmd = f'python scripts/evaluation/style_inference.py --out_type "{mode}" --adapter_ckpt {adapter_ckpt} --temporal_ckpt {temporal_ckpt} --seed {random_seed} --ckpt_path {ckpt} --base {config} --savedir {res_dir}'
    cmd += f' --n_samples {n_samples} --bs {style_crafter_prefs["batch_size"]} --height {style_crafter_prefs["height"]} --width {style_crafter_prefs["width"]} --unconditional_guidance_scale 15.0 --unconditional_guidance_scale_style {style_crafter_prefs["guidance_scale"]} --ddim_steps {total_steps} --ddim_eta {style_crafter_prefs["eta"]} --prompt_dir {prompt_dir} --filename {filename}'
    prt(f"Running {cmd}")
    try:
        run_sp(cmd, cwd=style_crafter_dir, realtime=True)
    except Exception as e:
        clear_last(2)
        alert_msg(page, "Error running Python.", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    clear_last(2)
    frame_files = [f for f in os.listdir(outputs_dir) if f.endswith('.png')]
    #frame_files = sorted(os.listdir(frames_dir), key=int(frame_filename[5:-4]))
    for i, frame_file in enumerate(frame_files):
        image_path = os.path.join(outputs_dir, frame_file)
        output_path = os.path.join(batch_output, frame_file)#os.path.join(outputs_dir, f"frame{str(i).zfill(4)}.png")
        shutil.copy(image_path, output_path)
        prt(Row([Img(src=asset_dir(output_path), fit=ImageFit.CONTAIN, width=style_crafter_prefs["width"], height=style_crafter_prefs["height"], gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        prt(Row([Text(output_path)], alignment=MainAxisAlignment.CENTER))
    if mode == "video":
        installer = Installing(f"Saving Video File... Frames at {outputs_dir if not style_crafter_prefs['save_frames'] else batch_output}")
        prt(installer)
        #if style_crafter_prefs['save_frames']:
        #    shutil.copytree(output_frames_dir, save_frames_dir, dirs_exist_ok=True)
        video_out = available_file(batch_output, fname, 0, no_num=True, ext="mp4")
        interpolate_video(outputs_dir, output_video=video_out, input_fps=8, output_fps=25, installer=installer)
        clear_last
        prt(Markdown(f"Video saved to [{video_out}]({filepath_to_url(video_out)})", on_tap_link=lambda e: e.page.launch_url(e.data)))
        #prt(f"Saved to {video_out}")
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_rave(page):
    global rave_prefs, status
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.RAVE.controls.append(line)
      page.RAVE.update()
    def clear_last(lines=1):
      clear_line(page.RAVE, lines=lines)
    def autoscroll(scroll=True):
        page.RAVE.auto_scroll = scroll
        page.RAVE.update()
    if not bool(rave_prefs['init_video']):
        alert_msg(page, "You must provide a target init video...")
        return
    if not bool(rave_prefs['prompt']):
        alert_msg(page, "You must provide an interesting prompt to guide the video...")
        return
    if not bool(rave_prefs['batch_folder_name']):
        alert_msg(page, "You must give a unique Batch Folder Name to save to...")
        return
    page.RAVE.controls = page.RAVE.controls[:1]
    autoscroll()
    installer = Installing("Installing RAVE Libraries...")
    prt(installer)
    rave_dir = os.path.join(root_dir, "RAVE")
    if not os.path.exists(rave_dir):
        try:
            installer.status("...cloning rehg-lab/RAVE")
            run_sp("git clone https://github.com/rehg-lab/RAVE.git", cwd=root_dir, realtime=False)
            installer.status("...installing RAVE requirements")
            #run_sp("pip install -r requirements.txt", realtime=True) #pytorch-lightning==1.5.0
            pip_install("addict==2.4.0 basicsr==1.4.2 beautifulsoup4|bs4 caffe2==0.8.1 cityscapesscripts==2.2.2 dominate==2.9.0 einops external==0.0.1 fairscale==0.4.13 ftfy fvcore==0.1.5.post20221221 hydra-core==1.3.2 imageio imutils iopath==0.1.10 kornia==0.7.0 lmdb==1.4.1 matplotlib mc mediapipe mmdet==3.2.0 mmpose==1.2.0 omegaconf onnx==1.15.0 onnxruntime==1.16.3 opencv_python openvino==2023.2.0 packaging pandas parrots==0.1.7 prettytable==3.9.0 Pygments==2.17.2 pytorch_lightning==2.1.2 PyYAML|yaml regex scipy setuptools Shapely==2.0.2 skimage==0.0 std_msgs==0.0.1 tabulate==0.9.0 tensorboardX==2.6.2.2 tensorflow==2.15.0.post1 termcolor==2.4.0 tifffile==2023.7.10 timm tqdm turbojpeg==0.0.2 yapf==0.40.2 watchdog", installer=installer, upgrade=True)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing RAVE Requirements:", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    clear_pipes()
    import yaml
    from PIL import ImageOps
    if bool(rave_prefs['output_name']):
        fname = format_filename(rave_prefs['output_name'], force_underscore=True)
    elif bool(rave_prefs['prompt']):
        fname = format_filename(rave_prefs['prompt'], force_underscore=True)
    elif bool(rave_prefs['batch_folder_name']):
        fname = format_filename(rave_prefs['batch_folder_name'], force_underscore=True)
    else: fname = "output"
    if bool(rave_prefs['file_prefix']):
        fname = f"{rave_prefs['file_prefix']}{fname}"
    data_dir = os.path.join(rave_dir, "data")
    results_dir = os.path.join(rave_dir, "results")
    if os.path.exists(results_dir):
        shutil.rmtree(results_dir, ignore_errors=True)
    makedir(results_dir)
    #if bool(rave_prefs['batch_folder_name']):
    #    batch_output = os.path.join(stable_dir, rave_prefs['batch_folder_name'])
    #else: batch_output = stable_dir
    #if not os.path.exists(batch_output):
    #    os.makedirs(batch_output)
    output_path = os.path.join(prefs['image_output'], rave_prefs['batch_folder_name'])
    makedir(output_path)
    init_vid = rave_prefs['init_video']
    if init_vid.startswith('http'):
        init_vid = download_file(init_vid, uploads_dir, ext="mp4")
    else:
        if not os.path.isfile(init_vid):
            alert_msg(page, f"ERROR: Couldn't find your init_video {init_vid}")
            return
    installer.status("...scaleing video")
    w, h = scale_video(init_vid, os.path.join(data_dir, f"{fname}.mp4"), rave_prefs["max_size"])
    #shutil.copy(init_vid, os.path.join(data_dir, f"{fname}.mp4"))
    #video_out_path = os.path.join(data_dir, rave_prefs['batch_folder_name'])
    #makedir(video_out_path)
    installer.status("...preparing yaml")
    random_seed = get_seed(rave_prefs['seed'])
    output_file = available_file(output_path, fname, 0, ext='mp4', no_num=True)
    config_yaml_file = os.path.join(rave_dir, "configs", "sdd_rave.yaml")
    config_yaml = {
        "video_name": fname,
        "preprocess_name": rave_prefs['control_task'].lower().replace(' ', '_'),
        "batch_size": rave_prefs['batch_size'],
        "batch_size_vae": rave_prefs['batch_size_vae'],
        "cond_step_start": 1 - rave_prefs['controlnet_strength'],
        "controlnet_conditioning_scale": rave_prefs['controlnet_conditioning_scale'],
        "controlnet_guidance_end": rave_prefs['controlnet_guidance_end'],
        "controlnet_guidance_start": rave_prefs['controlnet_guidance_start'],
        "give_control_inversion": rave_prefs['give_control_inversion'],
        "grid_size": 3,
        "sample_size": -1,
        "pad": 1,
        "guidance_scale": rave_prefs['guidance_scale'],
        "inversion_prompt": "",
        "is_ddim_inversion": rave_prefs['is_ddim_inversion'],
        "is_shuffle": rave_prefs['is_shuffle'],
        "negative_prompts": rave_prefs['negative_prompt'],
        "positive_prompts": rave_prefs['prompt'],
        "is_shuffle": rave_prefs['is_shuffle'],
        "num_inference_steps": rave_prefs['num_inference_steps'],
        "num_inversion_steps": rave_prefs['num_inversion_steps'],
        "save_folder": fname,
        "seed": random_seed,
        "model_id": 'None', #['CIVIT_AI/diffusers_models/realisticVisionV60B1_v51VAE']
    }
    with open(config_yaml_file, "w") as outfile:
        yaml.dump(config_yaml, outfile, sort_keys=False)
    #output_file = os.path.join(output_path, f"{fname}{'.mp4' if is_video else '.png'}")
    if not os.path.exists(config_yaml_file):
        print(f"Error creating json file {config_yaml_file}")
    clear_last()
    progress = ProgressBar(bar_height=8)
    prt(f"Generating your RAVE...")
    prt(progress)
    autoscroll(False)
    cmd = f'python scripts/run_experiment.py configs/sdd_rave.yaml'
    img_idx = 0
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
    class Handler(FileSystemEventHandler):
      def __init__(self):
        super().__init__()
      def on_created(self, event):
        nonlocal img_idx, w, h
        if event.is_directory:
          return None
        elif event.event_type == 'created' and (event.src_path.endswith("png") or event.src_path.endswith("jpg") or event.src_path.endswith("gif")):
          autoscroll(True)
          if w == 0:
            time.sleep(0.2)
            try:
              frame = PILImage.open(event.src_path)
              w, h = frame.size
              clear_last()
            except Exception:
              pass
          clear_last()
          if rave_prefs['save_frames']:
            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])
          else:
            fpath = event.src_path
          #prt(Divider(height=6, thickness=2))
          prt(Row([ImageButton(src=event.src_path, data=fpath, width=w, height=h, subtitle=f"Frame {img_idx} - {event.src_path}", center=True, page=page)], alignment=MainAxisAlignment.CENTER))
          prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))
          page.update()
          prt(progress)
          if rave_prefs['save_frames']:
            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])
            shutil.copy(event.src_path, fpath)
          time.sleep(0.2)
          autoscroll(False)
          img_idx += 1
        elif event.event_type == 'created' and (event.src_path.endswith("mp4") or event.src_path.endswith("avi")):
          autoscroll(True)
          #clear_last()
          prt(Divider(height=6, thickness=2))
          fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])
          time.sleep(0.2)
          shutil.copy(event.src_path, fpath)
          prt(Markdown(f"Video saved to [{fpath}]({filepath_to_url(fpath)}) from {event.src_path}", on_tap_link=lambda e: e.page.launch_url(e.data)))
          #prt(Row([VideoContainer(event.src_path)], alignment=MainAxisAlignment.CENTER))
          #prt(Row([VideoPlayer(video_file=event.src_path, width=w, height=h)], alignment=MainAxisAlignment.CENTER))
          #prt(Row([ImageButton(src=event.src_path, data=fpath, width=w, height=h, subtitle=f"Frame {img_idx} - {event.src_path}", center=True, page=page)], alignment=MainAxisAlignment.CENTER))
          #prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))
          #page.update()Image
          #prt(progress)
          time.sleep(0.2)
          autoscroll(False)
    image_handler = Handler()
    observer = Observer()
    observer.schedule(image_handler, results_dir, recursive=True)
    observer.start()
    #console = RunConsole(f"Running {cmd}")
    #console.run_process(cmd, cwd=rave_dir)
    #prt(f"Running {cmd}")
    #prt(progress)
    try:
        #TODO: Parse output to get percent current for progress callback_fnc
        run_sp(cmd, cwd=rave_dir, realtime=True)
    except Exception as e:
        clear_last()
        observer.stop()
        alert_msg(page, "Error running Rave Python", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    #clear_last()
    observer.stop()
    #clear_last()
    autoscroll(True)
    #TODO: Upscale Image
    if os.path.isfile(output_file):
        prt(f"Saved video to {output_file}")
        #prt(Row([VideoContainer(output_file)], alignment=MainAxisAlignment.CENTER))
        #prt(Row([VideoPlayer(video_file=output_file, width=width, height=height)], alignment=MainAxisAlignment.CENTER))
    else:
        prt("Error Generating Output File! Maybe NSFW Image detected or Out of Memory?")
    prt(Row([Text(output_file)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_tokenflow(page):
    global tokenflow_prefs, prefs, status, pipe_tokenflow, model_path
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.TokenFlow.controls.append(line)
      page.TokenFlow.update()
    def clear_last(lines=1):
      clear_line(page.TokenFlow, lines=lines)
    def clear_list():
      page.TokenFlow.controls = page.TokenFlow.controls[:1]
    def autoscroll(scroll=True):
      page.TokenFlow.auto_scroll = scroll
      page.TokenFlow.update()
    progress = ProgressBar(bar_height=8)
    total_steps = tokenflow_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    installer = Installing("Installing TokenFlow Text-To-Video Pipeline...")
    prt(installer)
    #model_id = "damo-vilab/text-to-video-ms-1.7b"
    clear_pipes()
    tokenflow_dir = os.path.join(root_dir, "TokenFlow")
    if not os.path.exists(tokenflow_dir):
        installer.status("...Skquark/TokenFlow") #XmYx/TokenFlow
        run_sp("git clone https://github.com/Skquark/TokenFlow.git", realtime=False, cwd=root_dir)
    data_dir = os.path.join(tokenflow_dir, "data")
    pip_install("ftfy opencv-python|cv2 tqdm numpy pyyaml|yaml xformers tensorboard av kornia", installer=installer)
    #clear_pipes('tokenflow')
    clear_last()
    #prt("Generating TokenFlow of your Video...")
    progressbar = Progress("Generating TokenFlow of your Video... See console for progress.")
    prt(progressbar)
    autoscroll(False)
    #batch_output = os.path.join(stable_dir, tokenflow_prefs['batch_folder_name'])
    #if not os.path.isdir(batch_output):
    #  os.makedirs(batch_output)
    #local_output = batch_output
    batch_output = os.path.join(prefs['image_output'], tokenflow_prefs['batch_folder_name'])
    makedir(batch_output)
    data_folder = format_filename(tokenflow_prefs['batch_folder_name'], use_dash=True)
    makedir(os.path.join(data_dir, data_folder))
    init_vid = tokenflow_prefs['init_video']
    if init_vid.startswith('http'):
        progressbar.status("...Downloading Video")
        init_vid = download_file(init_vid, uploads_dir, ext="mp4")
    else:
        if not os.path.isfile(init_vid):
            alert_msg(page, f"ERROR: Couldn't find your init_video {init_vid}")
            return
    video_file = os.path.basename(init_vid)
    if not video_file.endswith("mp4"):
        video_file += ".mp4"
    progressbar.status("...Scaling Video")
    w, h = scale_video(init_vid, os.path.join(data_dir, data_folder, video_file), tokenflow_prefs["max_size"])
    progressbar.status("...Preparing Run")
    #shutil.copy(init_vid, os.path.join(data_dir, data_folder, video_file))
    random_seed = get_seed(tokenflow_prefs['seed'])
    #width = tokenflow_prefs['width']
    #height = tokenflow_prefs['height']
    selected_mode = tokenflow_prefs['selected_mode']
    cache = prefs["cache_dir"]
    cache_dir = f' --cache_dir "{cache}"' if bool(cache) else '' 
    #x = " -x" if status['installed_xformers'] else ""
    import yaml
    def literal_presenter(dumper, data):
        #return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='"')
        if isinstance(data, str):
            return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='"')
        return dumper.represent_str(data)
    def represent_str(dumper, data):
        if '\n' in data:
            return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')
        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='"')
    def represent_none(dumper, _):
        return dumper.represent_scalar('tag:yaml.org,2002:null', '')
    def save_yaml(config, yaml_out):
        with open(yaml_out, 'w') as file:
            for key, value in config.items():
                if isinstance(value, str):
                    file.write(f'{key}: "{value}"\n')
                else:
                    file.write(f'{key}: {value}\n')

    config_yaml = os.path.join(tokenflow_dir, 'configs', 'sdd_config.yaml')
    config = {'seed': random_seed, 'device': torch_device, 'output_path': 'tokenflow-results', 'data_path': f'data/{data_folder}/{video_file}', 'latents_path': 'latents', 'n_inversion_steps': tokenflow_prefs["num_inversion_steps"], 'n_frames': tokenflow_prefs['num_frames']}
    config['sd_version'] = tokenflow_prefs['sd_version']
    config['guidance_scale'] = tokenflow_prefs['guidance_scale']
    config['n_timesteps'] = tokenflow_prefs["num_inference_steps"]
    config['prompt'] = tokenflow_prefs['prompt']
    config['negative_prompt'] = tokenflow_prefs['negative_prompt']
    config['batch_size'] = tokenflow_prefs['batch_size']
    config['fps'] = tokenflow_prefs['fps']
    if selected_mode == "pnp":
        config['pnp_attn_t'] = tokenflow_prefs['pnp_attn_t']
        config['pnp_f_t'] = tokenflow_prefs['pnp_f_t']
        run_py = "run_tokenflow_pnp.py"
    else:
        config['start'] = tokenflow_prefs['start']
        config['use_ddim_noise'] = tokenflow_prefs['use_ddim_noise']
        run_py = "run_tokenflow_sdedit.py"
    #with open(config_yaml, 'w') as file:
    #    #yaml.add_representer(str, literal_presenter)
    #    yaml.add_representer(str, represent_str)
    #    yaml.add_representer(type(None), represent_none)
    #    yaml.dump(config, file, indent=4, default_flow_style=False, sort_keys=False)
    save_yaml(config, config_yaml)
    preprocess_cmd = f'preprocess.py --data_path "{data_folder}/{video_file}" --inversion_prompt "{tokenflow_prefs["inversion_prompt"]}" --steps {tokenflow_prefs["num_inversion_steps"]} --save_steps {tokenflow_prefs["num_inference_steps"]} --n_frames {tokenflow_prefs["num_frames"]} --sd_version "{tokenflow_prefs["sd_version"]}"{cache_dir}'
    run_cmd = f'{run_py} --config_path "configs/sdd_config.yaml"{cache_dir}'
    print(f'Running {preprocess_cmd} -&- {run_cmd}')
    try:
        os.chdir(tokenflow_dir)
        progressbar.status("...Preprocessing Inverted Video")
        run_sp(f'python {preprocess_cmd}', cwd=tokenflow_dir)
               #-W {width} -H {height} -o {batch_output} -d cuda{x}{rw} -s {tokenflow_prefs["num_inference_steps"]} -g {tokenflow_prefs["guidance_scale"]} -f {tokenflow_prefs["fps"]} -T {tokenflow_prefs["num_frames"]}', cwd=data_dir)
        progressbar.status(f"...Processing TokenFlow {selected_mode}")
        run_sp(f'python {run_cmd}', cwd=tokenflow_dir, realtime=True)
    except Exception as e:
        clear_last()
        tokenflow_prefs['run_cmd'] = f'{preprocess_cmd} -&- {run_cmd}'
        alert_msg(page, f"ERROR: TokenFlow Text-To-Video failed for some reason. Possibly out of memory or something wrong with the code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]), debug_pref=tokenflow_prefs)
        os.chdir(root_dir)
        return
    os.chdir(root_dir)
    clear_last()
    autoscroll(True)
    filename = f"{format_filename(tokenflow_prefs['prompt'])}"
    #filename = filename[:int(prefs['file_max_length'])]
    #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
    autoscroll(True)
    output_path = os.path.join(tokenflow_dir, "tokenflow-results", "tokenflow_PnP.mp4")
    video_path = available_file(batch_output, filename, ext="mp4", no_num=True)
    if os.path.exists(output_path):
        shutil.copy(output_path, video_path)
        prt(f"Done creating video... Check {video_path}")
        prt(Row([VideoContainer(video_path)], alignment=MainAxisAlignment.CENTER))
    else:
        prt("Something went wrong generating video...")
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_animate_diff(page):
    global animate_diff_prefs, prefs, status, pipe_animate_diff, model_path
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.AnimateDiff.controls.append(line)
      page.AnimateDiff.update()
    def clear_last(lines=1):
      clear_line(page.AnimateDiff, lines=lines)
    def autoscroll(scroll=True):
      page.AnimateDiff.auto_scroll = scroll
      page.AnimateDiff.update()
    def clear_list():
      page.AnimateDiff.controls = page.AnimateDiff.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = animate_diff_prefs['steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    installer = Installing("Installing AnimateDiff Requirements...")
    prt(installer)
    try:
        import diffusers
    except ModuleNotFoundError:
        installer.status("...installing diffusers")
        run_process("pip install --upgrade git+https://github.com/Skquark/diffusers.git@main#egg=diffusers[torch]", page=page)
        pass
    try:
        import transformers
    except ModuleNotFoundError:
        installer.status("...installing transformers")
        run_sp("pip install --upgrade transformers==4.30.2", realtime=False) #4.28
        pass
    pip_install("omegaconf einops cmake colorama rich ninja copier==8.1.0 pydantic shellingham typer gdown==4.7.3 black ruff setuptools-scm controlnet_aux mediapipe matplotlib watchdog imageio==2.27.0", installer=installer)

    animatediff_dir = os.path.join(root_dir, 'animatediff-cli-prompt-travel')
    if 'installed_animate_diff' not in status or force_update("animate_diff"):#not os.path.exists(animatediff_dir):
        installer.status("...clone s9roll7/animatediff")
        run_sp("git clone https://github.com/s9roll7/animatediff-cli-prompt-travel", realtime=False, cwd=root_dir)
        #run_sp("git clone https://github.com/Skquark/animatediff-cli", realtime=False, cwd=root_dir) #/neggles
        installer.status("...install animatediff-prompt-travel")
        run_sp("git lfs install", cwd=animatediff_dir, realtime=False)
        status['installed_animate_diff'] = True
    os.chdir(animatediff_dir)
    #if prefs['memory_optimization'] == 'Xformers Mem Efficient Attention':
    try:
        import xformers
    except ModuleNotFoundError:
        installer.status("...installing FaceBook's Xformers")
        #run_sp("pip install --pre -U triton", realtime=False)
        run_sp(f"pip install -U xformers=={'0.0.25' if upgrade_torch else '0.0.22.post7'} --index-url https://download.pytorch.org/whl/cu121", realtime=False)
        status['installed_xformers'] = True
        pass
    pip_install("opencv-python|cv2 onnxruntime-gpu|onnxruntime sentencepiece>=0.1.99 safetensors", installer=installer)
    get_ffmpeg(installer)
    from safetensors import safe_open
    try:
        from animatediff.cli import generate
    except ModuleNotFoundError:
        try:
            installer.status("...installing AnimateDiff Requirements")
            run_sp("pip install -e .[dev]", cwd=animatediff_dir, realtime=False) #'.[dev]'
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Couldn't Install AnimateDiff Requirements for some reason...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    if animate_diff_prefs['save_video']:
        import platform
        rife_dir = os.path.join(animatediff_dir, 'data', 'rife')
        if len(os.listdir(rife_dir)) <= 1:
            installer.status("...downloading RiFE")
            if platform.system() == 'Linux':
                rife_zip = download_file("https://github.com/nihui/rife-ncnn-vulkan/releases/download/20221029/rife-ncnn-vulkan-20221029-ubuntu.zip")
            elif platform.system() == 'Windows':
                rife_zip = download_file("https://github.com/nihui/rife-ncnn-vulkan/releases/download/20221029/rife-ncnn-vulkan-20221029-windows.zip")
            elif platform.system() == 'Darwin':
                rife_zip = download_file("https://github.com/nihui/rife-ncnn-vulkan/releases/download/20221029/rife-ncnn-vulkan-20221029-macos.zip")
            installer.status("...extracting RiFE")
            shutil.unpack_archive(rife_zip, rife_dir)
            os.remove(rife_zip)
            for folder_name in os.listdir(rife_dir):
                if os.path.isdir(os.path.join(rife_dir, folder_name)):
                    rife_folder = os.path.join(rife_dir, folder_name)
                    break
            for file_name in os.listdir(rife_folder):
                shutil.move(os.path.join(rife_folder, file_name), rife_dir)
            run_sp(f"chmod 755 {rife_dir}", realtime=False)
            run_sp(f"chmod 755 {os.path.join(rife_dir, 'rife-ncnn-vulkan')}", realtime=False)
            #TODO: Fix Colab https://github.com/nihui/rife-ncnn-vulkan/issues/46
            os.chmod(os.path.join(rife_dir, 'rife-ncnn-vulkan'), 0o777)
            run_sp("apt-get install libvulkan-dev", realtime=False)
    from pathlib import Path
    output_path = os.path.join(prefs['image_output'], animate_diff_prefs['batch_folder_name'])
    if not os.path.exists(output_path):
        os.makedirs(output_path)
    #run_sp("apt -y install -qq aria2", realtime=False)
    sd_models = os.path.join(animatediff_dir, 'data', 'models', 'StableDiffusion')
    motion_module = os.path.join(animatediff_dir, 'data', 'models', 'motion-module')
    if not os.path.isdir(motion_module):
        os.makedirs(motion_module)
    #run_sp(f"rm -rf {sd_models}", realtime=False)
    if os.path.isdir(sd_models):
        shutil.rmtree(sd_models)
    installer.status("...downloading stable-diffusion-v1-5")
    run_sp(f"git clone -b fp16 https://huggingface.co/runwayml/stable-diffusion-v1-5 {sd_models}", realtime=False, cwd=root_dir)
    mm_model = [mm for mm in animate_diff_motion_modules if mm['name'] == animate_diff_prefs['motion_module']][0]
    mm_path = os.path.join(motion_module, mm_model['file'])
    mm_ckpt = mm_model['file']
    if not os.path.isfile(mm_path):
        installer.status(f"...downloading {mm_model['name']} Motion Module")
        download_file(mm_model['path'], to=motion_module, filename=mm_model['file'])

    #sd_models = "runwayml/stable-diffusion-v1-5"
    lora_model = {'name': 'None', 'file': '', 'path': '', 'weights': None}
    lora_dir = os.path.join(animatediff_dir, 'data', 'models', 'sd')
    #lora_dir = os.path.join(animatediff_dir, 'models', 'DreamBooth_LoRA')
    if not os.path.isdir(lora_dir):
        os.makedirs(lora_dir)
    lora_path = ""
    if animate_diff_prefs['dreambooth_lora'] == "Custom":
        lora = animate_diff_prefs['custom_lora']
        if lora.startswith("http"):
            installer.status(f"...downloading Custom LoRA")
            lora_file = download_file(lora_model['path'], to=lora_dir, ext="safetensors")
            if os.path.isfile(lora_file):
                lora_path = lora_file
        else:
            if os.path.isfile(lora):
                fname = os.path.basename(lora)
                lora_path = os.path.join(lora_dir, fname)
                shutil.copy(lora, lora_path)
    elif animate_diff_prefs['dreambooth_lora'] != "None":
        for lora in animate_diff_loras:
            if lora['name'] == animate_diff_prefs['dreambooth_lora']:
                lora_model = lora
                break
        lora_path = os.path.join(lora_dir, lora_model['file'])
        if not os.path.isfile(lora_path):
            installer.status(f"...downloading {lora_model['name']}")
            download_file(lora_model['path'], to=lora_dir, ext="safetensors")
            #run_sp(f"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {lora_model['path']} -d {lora_dir} -o {lora_model['file']}", realtime=False)
    if bool(lora_path):
        fname = lora_path.rpartition(slash)[2]
        lora_path = f"models{slash}sd{slash}{fname}"
    lora_map = {}
    if len(animate_diff_prefs['lora_map']) > 0:
        lora_map_dir = os.path.join(animatediff_dir, 'data', 'share', 'Lora')
        if not os.path.isdir(lora_map_dir):
            os.makedirs(lora_map_dir)
        for l in animate_diff_prefs['lora_map']:
            file = l['file']
            if l['name'] == "Custom":
                if '/models' in l['path']:
                    file = l['path'].split('/models/')[1].split('?')[0]
                else:
                    file = l['path'].rpartition('/')[1].split('?')[0]
                if ".safetensors" not in file:
                    file += ".safetensors"
            if l['path'].startswith("http"):
                lora_layer_path = os.path.join(lora_map_dir, file)
                if not os.path.isfile(lora_layer_path):
                    installer.status(f"...downloading {file} LoRA")
                    lora_file = download_file(l['path'], to=lora_map_dir, filename=file, ext="safetensors")
                else:
                    lora_file = lora_layer_path
                if os.path.isfile(lora_file):
                    lora_path = lora_file
            elif os.path.isfile(l['path']):
                lora_path = os.path.join(lora_map_dir, file)
                shutil.copy(l['path'], lora_path)
            elif os.path.isfile(os.path.join(lora_map_dir, file)):
                lora_path = os.path.join(lora_map_dir, file)
            lora_map[f"share{slash}Lora{slash}{file}"] = float(l['scale'])
    clear_pipes()

    samples_dir = os.path.join(animatediff_dir, 'samples')
    #batch_output = os.path.join(stable_dir, animate_diff_prefs['batch_folder_name'])
    #if not os.path.isdir(batch_output):
    #  os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], animate_diff_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    editing_prompts = []
    prompt_map = {}
    negative_prompts = []
    seeds = []
    '''if len(animate_diff_prefs['editing_prompts']) == 0:
        if not bool(animate_diff_prefs['prompt']):
            alert_msg(page, "Error: You must provide at least one Prompt to render...")
            return
        else:
            #editing_prompts.append(animate_diff_prefs['prompt'])
            prompt_map["0"] = animate_diff_prefs['prompt']
            negative_prompts.append(animate_diff_prefs['negative_prompt'])
            random_seed = get_seed(animate_diff_prefs['seed'])
            seeds.append(random_seed)
    else:
        num = 0
        for ep in animate_diff_prefs['editing_prompts']:
            #editing_prompts.append(ep['prompt'])
            prompt_map[str(int(num * (animate_diff_prefs['video_length'] / len(animate_diff_prefs['editing_prompts']))))] = animate_diff_prefs['prompt']
            negative_prompts.append(ep['negative_prompt'])
            random_seed = get_seed(ep['seed'])
            seeds.append(random_seed)
            num += 1'''
    if len(animate_diff_prefs['animation_prompts']) == 0:
        if not bool(animate_diff_prefs['prompt']):
            alert_msg(page, "Error: You must provide at least one Prompt to render...")
            return
        else:
            #editing_prompts.append(animate_diff_prefs['prompt'])
            prompt_map["0"] = animate_diff_prefs['prompt']
    else:
        prompt_map = animate_diff_prefs['animation_prompts']
    if not bool(animate_diff_prefs['batch_folder_name']):
        alert_msg(page, "It's highly recommended to give a unique Batch Folder Name to save to...")
        return
    installer.status("...preparing json")
    negative_prompts.append(animate_diff_prefs['negative_prompt'])
    random_seed = get_seed(animate_diff_prefs['seed'])
    seeds.append(random_seed)
    context = min(animate_diff_prefs['video_length'], animate_diff_prefs['context'])

    def extract_frames(video_file, fps, save_dir, start_frame=0):
        vidcap = cv2.VideoCapture(video_file)
        source_fps = vidcap.get(cv2.CAP_PROP_FPS)
        count = 0
        while vidcap.isOpened():
            success, image = vidcap.read()
            if success:
                if (count % (int(source_fps/fps)) == 0):
                    cv2.imwrite(os.path.join(save_dir, f"{str(count + start_frame).zfill(4)}.png"), cv2.resize(image, (animate_diff_prefs['width'], animate_diff_prefs['height']), cv2.INTER_AREA))
                if count >= (context - start_frame):
                    break
                count += 1
            else:
                break
        cv2.destroyAllWindows()
        vidcap.release()
    '''else:
        num = 0
        for ep in animate_diff_prefs['animation_prompts']:
            #editing_prompts.append(ep['prompt'])
            prompt_map[ep[frame]] = animate_diff_prefs['prompt']
            negative_prompts.append(ep['negative_prompt'])
            random_seed = get_seed(ep['seed'])
            seeds.append(random_seed)
            num += 1'''
    prompts_json = {
        'name': 'SDD',
        #'base': "",
        'path': lora_path,
        'motion_module': f"models{slash}motion-module{slash}{mm_ckpt}",
        #'motion_module': os.path.join(motion_module, f"{animate_diff_prefs['motion_module']}.ckpt"),
        'apply_lcm_lora': animate_diff_prefs['apply_lcm_lora'],
        'lcm_lora_scale': 1.0,
        'compile': prefs['enable_torch_compile'],
        'seed': seeds,
        'scheduler': animate_diff_prefs['scheduler'],
        'context_schedule': animate_diff_prefs['context_schedule'].lower(),
        'steps': int(animate_diff_prefs['steps']),
        'guidance_scale': float(animate_diff_prefs['guidance_scale']),
        'clip_skip': int(animate_diff_prefs['clip_skip']),
        #'lora_alpha': float(animate_diff_prefs['lora_alpha']),
        #'prompt': editing_prompts,
        'head_prompt': animate_diff_prefs['head_prompt'],
        'tail_prompt': animate_diff_prefs['tail_prompt'],
        'prompt_map': prompt_map,
        'n_prompt': negative_prompts,
    }
    ref_image = ""
    if bool(animate_diff_prefs['ref_image']):
        ref_file = os.path.join(animate_diff_prefs['ref_image'])
        if os.path.exists(ref_file):
            ref_image = f"ref_image{slash}{os.path.basename(animate_diff_prefs['ref_image'])}"
            shutil.copy(ref_file, os.path.join(animatediff_dir, 'data', 'ref_image', os.path.basename(ref_file)))
    controlnet_map = {
      "input_image_dir" : f"controlnet_image{slash}test",
      "max_samples_on_vram": 200,
      "max_models_on_vram" : 3,
      "save_detectmap": True,
      "preprocess_on_gpu": True,
      "is_loop": animate_diff_prefs['is_loop'],
      "controlnet_tile":{
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "controlnet_ip2p":{
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "controlnet_lineart_anime":{
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "controlnet_openpose":{
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "controlnet_softedge":{
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "controlnet_shuffle": {
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "controlnet_depth": {
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "controlnet_canny": {
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "controlnet_inpaint": {
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "controlnet_lineart": {
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "controlnet_mlsd": {
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "controlnet_normalbae": {
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "controlnet_scribble": {
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "controlnet_seg": {
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "qr_code_monster_v1": {
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "qr_code_monster_v2": {
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "controlnet_mediapipe_face": {
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "animatediff_controlnet": {
        "enable": False,
        "use_preprocessor":True,
        "guess_mode":False,
        "controlnet_conditioning_scale": 1.0,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0,
        "control_scale_list":[0.5,0.4,0.3,0.2,0.1]
      },
      "controlnet_ref": {
        "enable": bool(ref_image),
        "ref_image": ref_image if bool(ref_image) else "ref_image/ref_sample.png",
        "attention_auto_machine_weight": 1.0,
        "gn_auto_machine_weight": 1.0,
        "style_fidelity": 0.5,
        "reference_attn": True,
        "reference_adain": False,
        "scale_pattern":[0.5]
      }
    }
    for l in animate_diff_prefs['controlnet_layers']:
        if l['control_task'].startswith("QR"):
            controlnet_task = l['control_task'].lower()
        elif l['control_task'].startswith("AnimateDiff"):
            controlnet_task = "animatediff_controlnet"
        else:
            controlnet_task = f"controlnet_{l['control_task'].lower()}"
        try:
            scale_list = [float(x.strip()) for x in l['control_scale_list'].split(',')]
        except Exception:
            print(f"Error converting Scale List {l['control_scale_list']} to list of floats. Using default..")
            scale_list = [0.5,0.4,0.3,0.2,0.1]
            pass
        input_image_dir = os.path.join(animatediff_dir, 'data', 'controlnet_image', 'test', controlnet_task)
        for old in os.listdir(input_image_dir):
            installer.status(f"...deleting {old}")
            os.remove(os.path.join(input_image_dir, old))
        control_images = l['control_images']
        for f, ip_image in l['control_images'].items():
            if bool(ip_image):
                if ip_image.endswith('mp4') or ip_image.endswith('avi'):
                    if os.path.isfile(ip_image):
                        installer.status(f"...extracting frames from {os.path.basename(ip_image)}")
                        extract_frames(ip_image, 8, input_image_dir, start_frame=f)
                else:
                    img_path = os.path.join(input_image_dir, f'{str(f).zfill(4)}.png')
                    installer.status(f"...saving {os.path.basename(ip_image)}")
                    #TODO: Resize image
                    if os.path.isfile(ip_image):
                        shutil.copy(ip_image, img_path)
                    elif ip_image.startswith('https://drive'):
                        pip_install("gdown==4.7.3", installer=installer)
                        import gdown
                        gdown.download(url=ip_image, output=img_path, quiet=True)
                    elif ip_image.startswith('http'):
                        download_file(ip_image, img_path)
        controlnet_map[controlnet_task] = {
          "enable": True,
          "use_preprocessor":True,
          "guess_mode":False,
          "controlnet_conditioning_scale": float(l['conditioning_scale']),
          "control_guidance_start": float(l['control_guidance_start']),
          "control_guidance_end": float(l['control_guidance_end']),
          "control_scale_list":scale_list,
        }
    if bool(ip_image):
        ip_adapter_image_dir = os.path.join(animatediff_dir, 'data', 'ip_adapter_image', 'test')
        if os.path.isdir(ip_adapter_image_dir):
            os.rmdir(ip_adapter_image_dir)
        makedir(ip_adapter_image_dir)
        for f, ip_image in animate_diff_prefs['ip_adapter_layers'].items():
            img_path = os.path.join(ip_adapter_image_dir, f'{str(f).zfill(4)}.png')
            installer.status(f"...saving {os.path.basename(ip_image)}")
            #TODO: Resize image
            if os.path.isfile(ip_image):
                shutil.copy(ip_image, img_path)
            elif ip_image.startswith('https://drive'):
                pip_install("gdown==4.7.3", installer=installer)
                import gdown
                gdown.download(url=ip_image, output=img_path, quiet=True)
            elif ip_image.startswith('http'):
                download_file(ip_image, img_path)
    ip_adapter_map = {
      "enable": animate_diff_prefs['use_ip_adapter'],
      "input_image_dir": f"ip_adapter_image{slash}test",
      "save_input_image": False,
      "scale": animate_diff_prefs['ip_adapter_scale'],
      "is_full_face": animate_diff_prefs['ip_adapter_is_full_face'],
      "is_plus_face": animate_diff_prefs['ip_adapter_is_plus_face'],
      "is_plus": animate_diff_prefs['ip_adapter_is_plus'],
      "is_light": animate_diff_prefs['ip_adapter_light']
    }
    if animate_diff_prefs['use_img2img']:
        img2img_image_dir = os.path.join(animatediff_dir, 'data', 'img2img_image', 'test')
        if os.path.isdir(img2img_image_dir):
            os.rmdir(img2img_image_dir)
        makedir(img2img_image_dir)
        for f, img2img_image in animate_diff_prefs['img2img_layers'].items():
            if bool(img2img_image):
                img_path = os.path.join(img2img_image_dir, f'{str(f).zfill(4)}.png')
                installer.status(f"...saving {os.path.basename(img2img_image)}")
                #TODO: Resize image
                if os.path.isfile(img2img_image):
                    shutil.copy(img2img_image, img_path)
                elif img2img_image.startswith('https://drive'):
                    pip_install("gdown==4.7.3", installer=installer)
                    import gdown
                    gdown.download(url=img2img_image, output=img_path, quiet=True)
                elif img2img_image.startswith('http'):
                    download_file(img2img_image, img_path)
        img2img_map = {
            "enable": animate_diff_prefs['use_img2img'],
            "init_image_dir": f"img2img_image{slash}test",
            "save_init_image": True,
            "denoising_strength": animate_diff_prefs['img2img_strength'],
        }
    motion_lora_map = {}
    for m in animate_diff_prefs['motion_loras']:
      for mm in animate_diff_motion_loras:
        if mm['name'] == m:
          mm_path = os.path.join(animatediff_dir, 'data', 'models', 'motion_lora')
          if not os.path.isfile(os.path.join(mm_path, mm['file'])):
              installer.status(f"...downloading {mm['name']}")
              download_file(mm['path'], to=mm_path, filename=mm['file'], ext="ckpt")
          motion_lora_map[f"models{slash}motion_lora{slash}{mm['file']}"] = animate_diff_prefs['motion_loras_strength']
    
    installer.status("...preparing json")
    upscale_config = {
      "scheduler": animate_diff_prefs['scheduler'],
      "steps": animate_diff_prefs['upscale_steps'],
      "strength": animate_diff_prefs['upscale_strength'],
      "guidance_scale": animate_diff_prefs['upscale_guidance_scale'],
      "controlnet_tile": {
        "enable": animate_diff_prefs['upscale_tile'],
        "controlnet_conditioning_scale": 1.0,
        "guess_mode": False,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0
      },
      "controlnet_line_anime": {
        "enable": animate_diff_prefs['upscale_lineart_anime'],
        "controlnet_conditioning_scale": 1.0,
        "guess_mode": False,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0
      },
      "controlnet_ip2p": {
        "enable": animate_diff_prefs['upscale_ip2p'],
        "controlnet_conditioning_scale": 0.5,
        "guess_mode": False,
        "control_guidance_start": 0.0,
        "control_guidance_end": 1.0
      },
      "controlnet_ref": {
        "enable": bool(ref_image),
        "use_frame_as_ref_image": False,
        "use_1st_frame_as_ref_image": True,
        "ref_image": ref_image,
        "attention_auto_machine_weight": 1.0,
        "gn_auto_machine_weight": 1.0,
        "style_fidelity": 0.25,
        "reference_attn": True,
        "reference_adain": False
      }
    }
    prompts_json['lora_map'] = lora_map
    prompts_json['motion_lora_map'] = motion_lora_map
    prompts_json['controlnet_map'] = controlnet_map
    prompts_json['ip_adapter_map'] = ip_adapter_map
    if animate_diff_prefs['use_img2img']:
        prompts_json['img2img_map'] = img2img_map
    prompts_json['upscale_config'] = upscale_config
    #if bool(lora_path):
    #    prompts_json['lora_scale'] = animate_diff_prefs['lora_alpha']
    json_file = os.path.join(animatediff_dir, "config", "prompts", "sdd_prompt.json")
    cli_file = os.path.join(animatediff_dir, "src", "animatediff")
    out_dir = os.path.join(animatediff_dir, "output")
    with open(json_file, "w") as outfile:
        json.dump(prompts_json, outfile, indent=4)
    #cmd = f"python -m scripts.animate --config {yaml_file} --pretrained_model_path {os.path.join(sd_models, 'stable-diffusion-v1-5')}"
    cmd2 = f"python -m cli --config-path {json_file} --pretrained_model_path {os.path.join(sd_models, 'stable-diffusion-v1-5')}"
    cmd2 += f" --L {animate_diff_prefs['video_length']} --W {animate_diff_prefs['width']} --H {animate_diff_prefs['height']}"
    cmd = f"animatediff generate --config-path {json_file} --model-path {sd_models}"
    cmd += f" -L {animate_diff_prefs['video_length']} -W {animate_diff_prefs['width']} -H {animate_diff_prefs['height']} -C {context} -S {animate_diff_prefs['stride']}"
    if animate_diff_prefs['save_gif']:
        cmd += " --save-merged"
    if animate_diff_prefs['is_simple_composite']:
        cmd += " --simple_composite"
    #cmd += f" -O {animate_diff_prefs['overlap']}"
    w = 0
    h = 0
    frame_dir = ""
    output_dir = ""
    output_dirs = []
    img_idx = 0
    from watchdog.observers import Observer
    from watchdog.events import LoggingEventHandler, FileSystemEventHandler
    class Handler(FileSystemEventHandler):
      def __init__(self):
        super().__init__()
      def on_created(self, event):
        nonlocal img_idx, w, h, output_dir
        if event.is_directory:
          output_dir = event.src_path
          output_dirs.append(event.src_path)
          return None
        elif event.event_type == 'created' and (event.src_path.endswith("png") or event.src_path.endswith("gif") or event.src_path.endswith("jpg")):
          autoscroll(True)
          if w == 0:
            time.sleep(0.8)
            try:
              frame = PILImage.open(event.src_path)
              w, h = frame.size
              frame_dir = os.path.dirname(event.src_path)
              #clear_last()
            except Exception:
              frame_dir = os.path.dirname(event.src_path)
              pass
          clear_last()
          if animate_diff_prefs['save_frames']:
            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])
          else:
            fpath = event.src_path
          #prt(Divider(height=6, thickness=2))
          prt(Row([ImageButton(src=event.src_path, data=fpath, width=w, height=h, subtitle=f"Frame {img_idx} - {event.src_path}", center=True, page=page)], alignment=MainAxisAlignment.CENTER))
          prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))
          page.update()
          prt(progress)
          if animate_diff_prefs['save_frames']:
            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])
            shutil.copy(event.src_path, fpath)
          time.sleep(0.2)
          autoscroll(False)
          img_idx += 1
    image_handler = Handler()
    observer = Observer()
    observer.schedule(image_handler, out_dir, recursive=True)
    observer.start()
    #prt(f"Running {cmd}")
    #console = RunConsole(show_progress=False)
    #prt(console)
    try:
      print(f"Running {cmd}")
      clear_last()
      prt("Generating AnimateDiff of your Prompts... See console for progress.")
      prt(progress)
      time.sleep(0.5)
      autoscroll(False)
      run_sp(cmd, cwd=animatediff_dir, realtime=True)
      #    pass
      #console.run_process(cmd, cwd=animatediff_dir)
      #run_sp(cmd, cwd=animatediff_dir, realtime=True)
      #print(f"prompt={animate_diff_prefs['prompt']}, negative_prompt={animate_diff_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={animate_diff_prefs['edit_momentum_scale']}, edit_mom_beta={animate_diff_prefs['edit_mom_beta']}, steps={animate_diff_prefs['steps']}, eta={animate_diff_prefs['eta']}, guidance_scale={animate_diff_prefs['guidance_scale']}")
      #images = pipe_animate_diff(prompt=animate_diff_prefs['prompt'], negative_prompt=animate_diff_prefs['negative_prompt'], editing_prompt=editing_prompts, edit_warmup_steps=edit_warmup_steps, edit_guidance_scale=edit_guidance_scale, edit_threshold=edit_threshold, edit_weights=edit_weights, reverse_editing_direction=reverse_editing_direction, edit_momentum_scale=animate_diff_prefs['edit_momentum_scale'], edit_mom_beta=animate_diff_prefs['edit_mom_beta'], steps=animate_diff_prefs['steps'], eta=animate_diff_prefs['eta'], guidance_scale=animate_diff_prefs['guidance_scale'], width=width, height=height, num_images_per_prompt=animate_diff_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images
    except Exception as e:
      clear_last()
      observer.stop()
      alert_msg(page, f"ERROR: Couldn't run AnimateDiff for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
      return
    if animate_diff_prefs['upscale_tile'] or animate_diff_prefs['upscale_ip2p'] or animate_diff_prefs['upscale_lineart_anime']:
        u_w = int(w * float(animate_diff_prefs['width']))
        u_h = int(h * float(animate_diff_prefs['height']))
        upscale_cmd = f"animatediff tile-upscale {Path(frame_dir)} -c {json_file} -W {u_w} -H {u_h}"
        print(f"Running {upscale_cmd}")
        prt("Upscaling AnimateDiff of your Frames... See console for progress.")
        try:
            run_sp(upscale_cmd, cwd=animatediff_dir, realtime=True)
        except Exception as e:
            clear_last()
            observer.stop()
            alert_msg(page, f"ERROR: Couldn't run Tile-Upscale on AnimateDiff for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    time.sleep(3)
    observer.stop()
    #filename = f"{format_filename(editing_prompts[0]['prompt'])}"
    #filename = filename[:int(prefs['file_max_length'])]
    #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
    autoscroll(True)
    if animate_diff_prefs['save_video']:
        class VidHandler(FileSystemEventHandler):
          def __init__(self):
            super().__init__()
          def on_created(self, event):
            nonlocal w, h, output_dir
            if event.event_type == 'created' and (event.src_path.endswith("mp4") or event.src_path.endswith("avi") or event.src_path.endswith("webm")):
              autoscroll(True)
              #clear_last()
              prt(Divider(height=6, thickness=2))
              fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])
              time.sleep(1)
              shutil.copy(event.src_path, fpath)
              prt(Markdown(f"Video saved to [{fpath}]({filepath_to_url(fpath)}) from {event.src_path}", on_tap_link=lambda e: e.page.launch_url(e.data)))
              #prt(Row([VideoContainer(event.src_path)], alignment=MainAxisAlignment.CENTER))
              #prt(Row([VideoPlayer(video_file=event.src_path, width=w, height=h)], alignment=MainAxisAlignment.CENTER))
              #prt(Row([ImageButton(src=event.src_path, data=fpath, width=w, height=h, subtitle=f"Frame {img_idx} - {event.src_path}", center=True, page=page)], alignment=MainAxisAlignment.CENTER))
              #prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))
              page.update()
              time.sleep(0.2)
              autoscroll(False)
        video_handler = VidHandler()
        vidobserver = Observer()
        vidobserver.schedule(video_handler, out_dir, recursive=True)
        vidobserver.start()
        for dir in output_dirs:
            if len(os.listdir(dir)) < 4:
                continue
            interpolate_cmd = f"animatediff rife interpolate --temporal-tta --uhd {Path(dir)}" #--out_file --codec VideoCodec.h264
            try:
              installer = Installing("Running Google FILM: Frame Interpolation for Large Motion...")
              prt(installer)
              out_file = available_file(out_dir, "interpolated", no_num=True, ext="mp4")
              interpolate_video(dir, input_fps=8, output_fps=30, output_video=out_file, installer=installer)
              installer.set_message("Running RiFE Temporal Video Interpolation...")
              run_sp(interpolate_cmd, cwd=animatediff_dir, realtime=True)
              installer.show_progress(False)
              #print(f"prompt={animate_diff_prefs['prompt']}, negative_prompt={animate_diff_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={animate_diff_prefs['edit_momentum_scale']}, edit_mom_beta={animate_diff_prefs['edit_mom_beta']}, steps={animate_diff_prefs['steps']}, eta={animate_diff_prefs['eta']}, guidance_scale={animate_diff_prefs['guidance_scale']}")
              #images = pipe_animate_diff(prompt=animate_diff_prefs['prompt'], negative_prompt=animate_diff_prefs['negative_prompt'], editing_prompt=editing_prompts, edit_warmup_steps=edit_warmup_steps, edit_guidance_scale=edit_guidance_scale, edit_threshold=edit_threshold, edit_weights=edit_weights, reverse_editing_direction=reverse_editing_direction, edit_momentum_scale=animate_diff_prefs['edit_momentum_scale'], edit_mom_beta=animate_diff_prefs['edit_mom_beta'], steps=animate_diff_prefs['steps'], eta=animate_diff_prefs['eta'], guidance_scale=animate_diff_prefs['guidance_scale'], width=width, height=height, num_images_per_prompt=animate_diff_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images
            except Exception as e:
              #clear_last()
              alert_msg(page, f"ERROR: Couldn't interpolate video with RiFE, but frames & gif still saved...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
              pass
        time.sleep(2)
        vidobserver.stop()

    os.chdir(root_dir)
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_animatediff_img2video(page, from_list=False, with_params=False):
    global animatediff_img2video_prefs, pipe_animatediff_img2video, prefs, status
    if not check_diffusers(page): return
    animatediff_img2video_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            animatediff_img2video_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':animatediff_img2video_prefs['guidance_scale'], 'num_inference_steps':animatediff_img2video_prefs['num_inference_steps'], 'width':animatediff_img2video_prefs['width'], 'height':animatediff_img2video_prefs['height'], 'init_image':animatediff_img2video_prefs['init_image'], 'init_image_strength':animatediff_img2video_prefs['init_image_strength'], 'num_images':animatediff_img2video_prefs['num_images'], 'seed':animatediff_img2video_prefs['seed']})
        else:
            animatediff_img2video_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'init_image':p['init_image'], 'init_image_strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(animatediff_img2video_prefs['init_image']):
        alert_msg(page, "You must provide an Init Image or Video to process your video generation...")
        return
      animatediff_img2video_prompts.append({'prompt': animatediff_img2video_prefs['prompt'], 'negative_prompt':animatediff_img2video_prefs['negative_prompt'], 'guidance_scale':animatediff_img2video_prefs['guidance_scale'], 'num_inference_steps':animatediff_img2video_prefs['num_inference_steps'], 'width':animatediff_img2video_prefs['width'], 'height':animatediff_img2video_prefs['height'], 'init_image':animatediff_img2video_prefs['init_image'], 'init_image_strength':animatediff_img2video_prefs['init_image_strength'], 'num_images':animatediff_img2video_prefs['num_images'], 'seed':animatediff_img2video_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.AnimateDiffImage2Video.controls.append(line)
        if update:
          page.AnimateDiffImage2Video.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.AnimateDiffImage2Video, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.AnimateDiffImage2Video.auto_scroll = scroll
        page.AnimateDiffImage2Video.update()
      else:
        page.AnimateDiffImage2Video.auto_scroll = scroll
        page.AnimateDiffImage2Video.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.AnimateDiffImage2Video.controls = page.AnimateDiffImage2Video.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = animatediff_img2video_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    def callback_step(pipe, step, timestep, callback_kwargs):
      callback_step.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    mode = "Video2Video" if animatediff_img2video_prompts[0]['init_image'].endswith('mp4') or animatediff_img2video_prompts[0]['init_image'].endswith('gif') else "Image2Video"
    installer = Installing(f"Installing AnimateDiff {mode} Engine & Models... See console for progress.")
    prt(installer)
    clear_pipes("animatediff_img2video")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    cpu_offload = animatediff_img2video_prefs['cpu_offload']
    motion_module = "guoyww/animatediff-motion-adapter-v1-5-2" if animatediff_img2video_prefs['motion_module'] == "animatediff-motion-adapter-v1-5-2" else "guoyww/animatediff-motion-adapter-v1-5-2"
    animatediff_img2video_model = "SG161222/Realistic_Vision_V5.1_noVAE" if animatediff_img2video_prefs['animatediff_img2video_model'] == "Realistic_Vision_V5.1_noVAE" else "Lykon/dreamshaper-8" if animatediff_img2video_prefs['animatediff_img2video_model'] == "dreamshaper-8" else "emilianJR/epiCRealism" if animatediff_img2video_prefs['animatediff_img2video_model'] == "epiCRealism" else animatediff_img2video_prefs['animatediff_img2video_custom_model']
    if 'loaded_animatediff_img2video' not in status: status['loaded_animatediff_img2video'] = ""
    if 'loaded_animatediff_img2video_mode' not in status: status['loaded_animatediff_img2video_mode'] = ""
    if animatediff_img2video_model != status['loaded_animatediff_img2video'] or mode != status['loaded_animatediff_img2video_mode']:
        clear_pipes()
    #from optimum.intel import OVLatentConsistencyModelPipeline
    #pipe = OVLatentConsistencyModelPipeline.from_pretrained("rupeshs/AnimateDiffImage2Video-dreamshaper-v7-openvino-int8", ov_config={"CACHE_DIR": ""})
    if mode == "Video2Video":
        pip_install("imageio", installer=installer)
    #from diffusers import AutoPipelineForVideo2Video, AutoPipelineForImage2Video, AnimateDiffImage2VideoScheduler
    from diffusers import MotionAdapter, DiffusionPipeline, AnimateDiffVideoToVideoPipeline, DDIMScheduler
    from diffusers.utils import export_to_gif, load_image
    ie_arg = {}
    if animatediff_img2video_prefs['use_ip_adapter']:
        installer.status(f"...initialize IP-Adapter Image Encoder")
        from transformers import CLIPVisionModelWithProjection
        image_encoder = CLIPVisionModelWithProjection.from_pretrained(
            "h94/IP-Adapter", 
            subfolder="models/image_encoder",
            torch_dtype=torch.float16,
        )
        ie_arg = {'image_encoder': image_encoder}
    if pipe_animatediff_img2video == None:
        installer.status(f"...initialize AnimateDiff {mode} Pipeline")
        try:
            adapter = MotionAdapter.from_pretrained(motion_module)
            if mode == "Image2Video":
                pipe_animatediff_img2video = DiffusionPipeline.from_pretrained(animatediff_img2video_model, motion_adapter=adapter, custom_pipeline="AlanB/pipeline_animatediff_img2video_mod", **ie_arg, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                #pipe_animatediff_img2video = AutoPipelineForImage2Video.from_pretrained(animatediff_img2video_model, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                #pipe_animatediff_img2video.scheduler = DDIMScheduler(beta_schedule="linear", steps_offset=1, clip_sample=False, timestep_spacing="linspace")
                pipe_animatediff_img2video.scheduler = DDIMScheduler.from_pretrained(animatediff_img2video_model, subfolder="scheduler", clip_sample=False, timestep_spacing="linspace", beta_schedule="linear", steps_offset=1)
                status['loaded_animatediff_img2video_mode'] = mode
            else:
                pipe_animatediff_img2video = AnimateDiffVideoToVideoPipeline.from_pretrained(animatediff_img2video_model, motion_adapter=adapter, torch_dtype=torch.float16, **ie_arg, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                #pipe_animatediff_img2video = DiffusionPipeline.from_pretrained(animatediff_img2video_model, motion_adapter=adapter, custom_pipeline="pipeline_animatediff_video2video", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                #pipe_animatediff_img2video = AutoPipelineForVideo2Video.from_pretrained(animatediff_img2video_model, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                pipe_animatediff_img2video.scheduler = DDIMScheduler.from_pretrained(animatediff_img2video_model, subfolder="scheduler", clip_sample=False, timestep_spacing="linspace", beta_schedule="linear", steps_offset=1)
                status['loaded_animatediff_img2video_mode'] = mode
            #pipe_animatediff_img2video = pipeline_scheduler(pipe_animatediff_img2video)
            #pipe_animatediff_img2video.scheduler = AnimateDiffImage2VideoScheduler.from_config(pipe_animatediff_img2video.scheduler.config)
            if prefs['vae_slicing']:
                pipe_animatediff_img2video.enable_vae_slicing()
            if prefs['vae_tiling']:
                pipe_animatediff_img2video.enable_vae_tiling()
            if animatediff_img2video_prefs['free_init']: #Not yet
                installer.status(f"...enable FreeInit")
                pipe_animatediff_img2video.enable_free_init(method="butterworth", use_fast_sampling=True)
            if prefs['enable_freeu']:
                installer.status(f"...enable FreeU")
                pipe_animatediff_img2video.enable_freeu(s1=0.6, s2=0.4, b1=1.1, b2=1.2)
            if prefs['enable_torch_compile']:
                installer.status(f"...Torch compiling transformer")
                pipe_animatediff_img2video.transformer = torch.compile(pipe_animatediff_img2video.transformer, mode="reduce-overhead", fullgraph=True)
                pipe_animatediff_img2video = pipe_animatediff_img2video.to(torch_device)
            elif cpu_offload:
                pipe_animatediff_img2video.enable_model_cpu_offload()
            else:
                pipe_animatediff_img2video = pipe_animatediff_img2video.to(torch_device)
            pipe_animatediff_img2video.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing AnimateDiff Image2Video...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        status['loaded_animatediff_img2video'] = animatediff_img2video_model
    else:
        clear_pipes('animatediff_img2video')
        
    motion_loras = []
    animation_types = []
    adapter_weights = []
    for m in animatediff_img2video_prefs['motion_loras']:
        for mm in animatediff_motion_loras:
            if mm['name'] == m:
                animation_types.append(mm['name'])
                adapter_weights.append(animatediff_img2video_prefs['motion_loras_strength'])
                motion_loras.append(mm)
    if len(motion_loras) > 0:
        for ml in motion_loras:
            pipe_animatediff_img2video.load_lora_weights(ml['path'], adapter_name=ml['name'])
        pipe_animatediff_img2video.set_adapters(animation_types, adapter_weights=adapter_weights)
    #animation_type = ["zoom-out", "tilt-up", "pan-left"]
    #adapter_weight = [0.75]
    #pipe_animatediff_img2video.set_adapters([animation_type], adapter_weights=adapter_weight)
    #pipe_animatediff_img2video.load_lora_weights(f"guoyww/animatediff-motion-lora-{animation_type}", adapter_name=animation_type)
    if len(animatediff_img2video_prefs['lora_map']) > 0:
        adapters = []
        scales = []
        for l in animatediff_img2video_prefs['lora_map']:
            adapters.append(l['name'])
            scales.append(l['scale'])
            weight_args = {}
            if 'weights' in l and bool(l['weights']):
                weight_args['weight_name'] = l['weights']
            pipe_animatediff_img2video.load_lora_weights(l['path'], adapter_name=l['name'], torch_dtype=torch.float16, **weight_args)
        pipe_animatediff_img2video.set_adapters(adapters, adapter_weights=scales)
    #else: p.disable_lora()
    ip_adapter_arg = {}
    if animatediff_img2video_prefs['use_ip_adapter']:
        installer.status(f"...initialize IP-Adapter")
        ip_adapter_img = None
        if animatediff_img2video_prefs['ip_adapter_image'].startswith('http'):
          i_response = requests.get(animatediff_img2video_prefs['ip_adapter_image'])
          ip_adapter_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
        else:
          if os.path.isfile(animatediff_img2video_prefs['ip_adapter_image']):
            ip_adapter_img = PILImage.open(animatediff_img2video_prefs['ip_adapter_image'])
          else:
            clear_last()
            prt(f"ERROR: Couldn't find your ip_adapter_image {animatediff_img2video_prefs['ip_adapter_image']}")
        if bool(ip_adapter_img):
          ip_adapter_arg['ip_adapter_image'] = ip_adapter_img
        if bool(ip_adapter_arg):
            for m in ip_adapter_models:
                if m['name'] == animatediff_img2video_prefs['ip_adapter_model']:
                    ip_adapter_model = m
                    break
            #ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == animatediff_img2video_prefs['ip_adapter_model'])
            pipe_animatediff_img2video.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
            pipe_animatediff_img2video.set_ip_adapter_scale(animatediff_img2video_prefs['ip_adapter_strength'])

    clear_last()
    s = "" if len(animatediff_img2video_prompts) == 1 else "s"
    prt(f"Generating your AnimateDiff Video{s}...")
    for pr in animatediff_img2video_prompts:
        prt(progress)
        nudge(page.imageColumn if from_list else page.AnimateDiffImage2Video, page)
        autoscroll(False)
        mode = "Video2Video" if pr['init_image'].endswith(('mp4', 'gif')) else "Image2Video"
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(pr['seed'])
        generator = torch.Generator().manual_seed(random_seed)
        init_img = None
        if bool(pr['init_image']):
            fname = os.path.basename(pr['init_image'])
            if mode != "Video2Video":
                if pr['init_image'].startswith('http'):
                    init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
                else:
                    if os.path.isfile(pr['init_image']):
                        init_img = PILImage.open(pr['init_image'])
                    else:
                        alert_msg(page, f"ERROR: Couldn't find your init image {pr['init_image']}")
                        return
                init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
                init_img = ImageOps.exif_transpose(init_img).convert("RGB")
            else:
                init_img = []
                import imageio
                if pr['init_image'].startswith(('http://', 'https://')):
                    response = requests.get(pr['init_image'])
                    response.raise_for_status()
                    content = BytesIO(response.content)
                    vid = imageio.get_reader(content)
                else:
                    if os.path.isfile(pr['init_image']):
                        vid = imageio.get_reader(pr['init_image'])
                    else:
                        alert_msg(page, f"ERROR: Couldn't find your init video {pr['init_image']}")
                        return
                for frame in vid:
                    pil_image = PILImage.fromarray(frame)
                    init_img.append(pil_image)
        try:
            if mode == "Image2Video":
                if status['loaded_animatediff_img2video_mode'] != "Image2Video":
                    #pipe_animatediff_img2video = AutoPipelineForImage2Video.from_pipe(pipe_animatediff_img2video)
                    status['loaded_animatediff_img2video_mode'] = "Image2Video"
                output = pipe_animatediff_img2video(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    image=init_img,
                    strength=pr['init_image_strength'],
                    num_frames=animatediff_img2video_prefs['video_length'],
                    num_videos_per_prompt=pr['num_images'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=pr['num_inference_steps'],
                    guidance_scale=pr['guidance_scale'],
                    clip_skip=animatediff_img2video_prefs['clip_skip'],
                    latent_interpolation_method=animatediff_img2video_prefs['latent_interpolation_method'].lower(), # can be lerp, slerp, or your own callback
                    generator=generator,
                    callback=callback_fnc,
                    **ip_adapter_arg,
                ).frames
            else:
                if status['loaded_animatediff_img2video_mode'] != "Video2Video":
                    #pipe_animatediff_img2video = AutoPipelineForVideo2Video.from_pipe(pipe_animatediff_img2video)
                    status['loaded_animatediff_img2video_mode'] = "Video2Video"
                output = pipe_animatediff_img2video(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    video=init_img,
                    strength=pr['init_image_strength'],
                    #num_frames=animatediff_img2video_prefs['video_length'],
                    num_videos_per_prompt=pr['num_images'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=pr['num_inference_steps'],
                    guidance_scale=pr['guidance_scale'],
                    clip_skip=animatediff_img2video_prefs['clip_skip'],
                    #latent_interpolation_method=animatediff_img2video_prefs['latent_interpolation_method'].lower(),
                    generator=generator,
                    callback_on_step_end=callback_step,
                    **ip_adapter_arg,
                ).frames
        except Exception as e:
            clear_last(2)
            alert_msg(page, f"ERROR: Something went wrong generating video...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        #clear_last()
        clear_last()
        autoscroll(True)
        if output is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        batch_output = os.path.join(prefs['image_output'], animatediff_img2video_prefs['batch_folder_name'])
        makedir(batch_output)
        for v, frames_batch in enumerate(output):
            fname = format_filename(pr['prompt'])
            frames_dir = available_folder(batch_output, "frames", v)
            makedir(frames_dir)
            for idx, image in enumerate(frames_batch):
                #fname = f"{animatediff_img2video_prefs['file_prefix']}{format_filename(animatediff_img2video_prefs['batch_folder_name'])}-{b}"
                image_path = available_file(frames_dir, "frame", idx, zfill=4)
                image.save(image_path)
                new_file = os.path.basename(image_path)
                if not animatediff_img2video_prefs['display_upscaled_image'] or not animatediff_img2video_prefs['apply_ESRGAN_upscale']:
                    #prt(Row([Img(src=image_path, width=animatediff_img2video_prefs['width'], height=animatediff_img2video_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                    prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
                if animatediff_img2video_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    upscale_image(image_path, image_path, scale=animatediff_img2video_prefs["enlarge_scale"], face_enhance=animatediff_img2video_prefs["face_enhance"])
                    if animatediff_img2video_prefs['display_upscaled_image']:
                        time.sleep(0.6)
                        prt(Row([Img(src=asset_dir(image_path), width=int(pr['width'] * float(animatediff_img2video_prefs["enlarge_scale"])), height=int(pr['height'] * float(animatediff_img2video_prefs["enlarge_scale"])), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
            gif_file = available_file(batch_output, fname, no_num=True, ext="gif")
            export_to_gif(frames_batch, gif_file, fps=animatediff_img2video_prefs['fps'])
            prt(Row([ImageButton(src=gif_file, width=pr['width'], height=pr['height'], data=gif_file, page=page)], alignment=MainAxisAlignment.CENTER))
            if animatediff_img2video_prefs['export_to_video']:
                try:
                    installer = Installing("Running Google FILM: Frame Interpolation for Large Motion...")
                    prt(installer)
                    out_file = available_file(batch_output, fname, no_num=True, ext="mp4")
                    if animatediff_img2video_prefs['interpolate_video']:
                        interpolate_video(frames_dir, input_fps=animatediff_img2video_prefs['fps'], output_fps=animatediff_img2video_prefs['target_fps'], output_video=out_file, installer=installer)
                    else:
                        installer.set_message("Saving Frames to Video using FFMPEG with Deflicker...")
                        pattern = create_pattern(new_file) #fname+"-%04d.png"
                        frames_to_video(frames_dir, pattern=pattern, input_fps=animatediff_img2video_prefs['fps'], output_fps=animatediff_img2video_prefs['target_fps'], output_video=out_file, installer=installer, deflicker=True)
                except Exception as e:
                    clear_last()
                    alert_msg(page, f"ERROR: Couldn't interpolate video, but frames still saved...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                    pass
                clear_last()
                if not os.path.isfile(out_file):
                    prt(f"Problem creating video file, but frames still saved...")
                else:
                    prt(Markdown(f"Video saved to [{out_file}]({filepath_to_url(out_file)})", on_tap_link=lambda e: e.page.launch_url(e.data)))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_animatediff_sdxl(page, from_list=False, with_params=False):
    global animatediff_sdxl_prefs, pipe_animatediff_sdxl, prefs, status
    if not check_diffusers(page): return
    animatediff_sdxl_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            animatediff_sdxl_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':animatediff_sdxl_prefs['guidance_scale'], 'num_inference_steps':animatediff_sdxl_prefs['num_inference_steps'], 'width':animatediff_sdxl_prefs['width'], 'height':animatediff_sdxl_prefs['height'], 'init_image':animatediff_sdxl_prefs['init_image'], 'init_image_strength':animatediff_sdxl_prefs['init_image_strength'], 'num_images':animatediff_sdxl_prefs['num_images'], 'seed':animatediff_sdxl_prefs['seed']})
        else:
            animatediff_sdxl_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'init_image':p['init_image'], 'init_image_strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      animatediff_sdxl_prompts.append({'prompt': animatediff_sdxl_prefs['prompt'], 'negative_prompt':animatediff_sdxl_prefs['negative_prompt'], 'guidance_scale':animatediff_sdxl_prefs['guidance_scale'], 'num_inference_steps':animatediff_sdxl_prefs['num_inference_steps'], 'width':animatediff_sdxl_prefs['width'], 'height':animatediff_sdxl_prefs['height'], 'init_image':animatediff_sdxl_prefs['init_image'], 'init_image_strength':animatediff_sdxl_prefs['init_image_strength'], 'num_images':animatediff_sdxl_prefs['num_images'], 'seed':animatediff_sdxl_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.AnimateDiffSDXL.controls.append(line)
        if update:
          page.AnimateDiffSDXL.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.AnimateDiffSDXL, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.AnimateDiffSDXL.auto_scroll = scroll
        page.AnimateDiffSDXL.update()
      else:
        page.AnimateDiffSDXL.auto_scroll = scroll
        page.AnimateDiffSDXL.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.AnimateDiffSDXL.controls = page.AnimateDiffSDXL.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = animatediff_sdxl_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    def callback_step(pipe, step, timestep, callback_kwargs):
      callback_step.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    mode = "Video2Video" if animatediff_sdxl_prompts[0]['init_image'].endswith('mp4') or animatediff_sdxl_prompts[0]['init_image'].endswith('gif') else "SDXL"
    installer = Installing(f"Installing AnimateDiff SDXL Engine & Models...")
    prt(installer)
    clear_pipes("animatediff_sdxl")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    cpu_offload = animatediff_sdxl_prefs['cpu_offload']
    motion_module = "a-r-r-o-w/animatediff-motion-adapter-sdxl-beta"
    #"guoyww/animatediff-motion-adapter-v1-5-2" if animatediff_sdxl_prefs['motion_module'] == "animatediff-motion-adapter-v1-5-2" else "guoyww/animatediff-motion-adapter-v1-5-2"
    model_SDXL = get_SDXL_model(prefs['SDXL_model'])
    animatediff_sdxl_model = model_SDXL['path']
    variant = {'variant': model_SDXL['variant']} if 'variant' in model_SDXL else {}
    
    #"SG161222/Realistic_Vision_V5.1_noVAE" if animatediff_sdxl_prefs['animatediff_sdxl_model'] == "Realistic_Vision_V5.1_noVAE" else "Lykon/dreamshaper-8" if animatediff_sdxl_prefs['animatediff_sdxl_model'] == "dreamshaper-8" else "emilianJR/epiCRealism" if animatediff_sdxl_prefs['animatediff_sdxl_model'] == "epiCRealism" else animatediff_sdxl_prefs['animatediff_sdxl_custom_model']
    if 'loaded_animatediff_sdxl' not in status: status['loaded_animatediff_sdxl'] = ""
    if animatediff_sdxl_model != status['loaded_animatediff_sdxl']:
        clear_pipes()
    #from optimum.intel import OVLatentConsistencyModelPipeline
    #pipe = OVLatentConsistencyModelPipeline.from_pretrained("rupeshs/AnimateDiffSDXL-dreamshaper-v7-openvino-int8", ov_config={"CACHE_DIR": ""})
    # if mode == "Video2Video":
    #     pip_install("imageio", installer=installer)
    #from diffusers import AutoPipelineForVideo2Video, AutoPipelineForSDXL, AnimateDiffSDXLScheduler
    from diffusers import MotionAdapter, DiffusionPipeline, AnimateDiffSDXLPipeline, DDIMScheduler
    from diffusers.utils import export_to_gif, load_image
    ie_arg = {}
    if animatediff_sdxl_prefs['use_ip_adapter']:
        installer.status(f"...initialize IP-Adapter Image Encoder")
        from transformers import CLIPVisionModelWithProjection
        image_encoder = CLIPVisionModelWithProjection.from_pretrained(
            "h94/IP-Adapter", 
            subfolder="models/image_encoder",
            torch_dtype=torch.float16,
        )
        ie_arg = {'image_encoder': image_encoder}
    if pipe_animatediff_sdxl == None:
        installer.status(f"...initialize AnimateDiff SDXL with {model_SDXL['name']} Model")
        scheduler = DDIMScheduler.from_pretrained(
          animatediff_sdxl_model,
          subfolder="scheduler",
          clip_sample=False,
          timestep_spacing="linspace",
          beta_schedule="linear",
          steps_offset=1,
        )
        try:
            adapter = MotionAdapter.from_pretrained(motion_module, torch_dtype=torch.float16)
            pipe_animatediff_sdxl = AnimateDiffSDXLPipeline.from_pretrained(animatediff_sdxl_model, motion_adapter=adapter, scheduler=scheduler, torch_dtype=torch.float16, **variant, **ie_arg, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_animatediff_sdxl = DiffusionPipeline.from_pretrained(animatediff_sdxl_model, motion_adapter=adapter, custom_pipeline="pipeline_animatediff_video2video", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_animatediff_sdxl = AutoPipelineForVideo2Video.from_pretrained(animatediff_sdxl_model, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_animatediff_sdxl.scheduler = model_scheduler(animatediff_sdxl_model, clip_sample=False, timestep_spacing="linspace", beta_schedule="linear", steps_offset=1)
            #pipe_animatediff_sdxl.scheduler = DDIMScheduler.from_pretrained(animatediff_sdxl_model, subfolder="scheduler", clip_sample=False, timestep_spacing="linspace", beta_schedule="linear", steps_offset=1)
            #pipe_animatediff_sdxl = pipeline_scheduler(pipe_animatediff_sdxl)
            if prefs['vae_slicing']:
                pipe_animatediff_sdxl.enable_vae_slicing()
            if prefs['vae_tiling']:
                pipe_animatediff_sdxl.enable_vae_tiling()
            if animatediff_sdxl_prefs['free_init']:
                installer.status(f"...enable FreeInit")
                pipe_animatediff_sdxl.enable_free_init(method="butterworth", use_fast_sampling=True)
            '''if prefs['enable_freeu']:
                installer.status(f"...enable FreeU")
                pipe_animatediff_sdxl.enable_freeu(s1=0.6, s2=0.4, b1=1.1, b2=1.2)'''
            if prefs['enable_torch_compile']:
                installer.status(f"...Torch compiling transformer")
                pipe_animatediff_sdxl.transformer = torch.compile(pipe_animatediff_sdxl.transformer, mode="reduce-overhead", fullgraph=True)
                pipe_animatediff_sdxl = pipe_animatediff_sdxl.to(torch_device)
            elif cpu_offload:
                pipe_animatediff_sdxl.enable_model_cpu_offload()
            else:
                pipe_animatediff_sdxl = pipe_animatediff_sdxl.to(torch_device)
            pipe_animatediff_sdxl.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing AnimateDiff SDXL...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        status['loaded_animatediff_sdxl'] = animatediff_sdxl_model
    status['loaded_scheduler'] = prefs['scheduler_mode']
    '''
    motion_loras = []
    animation_types = []
    adapter_weights = []
    for m in animatediff_sdxl_prefs['motion_loras']:
        for mm in animatediff_motion_loras:
            if mm['name'] == m:
                animation_types.append(mm['name'])
                adapter_weights.append(animatediff_sdxl_prefs['motion_loras_strength'])
                motion_loras.append(mm)
    if len(motion_loras) > 0:
        for ml in motion_loras:
            pipe_animatediff_sdxl.load_lora_weights(ml['path'], adapter_name=ml['name'])
        pipe_animatediff_sdxl.set_adapters(animation_types, adapter_weights=adapter_weights)
    '''
    #animation_type = ["zoom-out", "tilt-up", "pan-left"]
    #adapter_weight = [0.75]
    #pipe_animatediff_sdxl.set_adapters([animation_type], adapter_weights=adapter_weight)
    #pipe_animatediff_sdxl.load_lora_weights(f"guoyww/animatediff-motion-lora-{animation_type}", adapter_name=animation_type)
    if len(animatediff_sdxl_prefs['lora_map']) > 0:
        adapters = []
        scales = []
        for l in animatediff_sdxl_prefs['lora_map']:
            adapters.append(l['name'])
            scales.append(l['scale'])
            weight_args = {}
            if 'weights' in l and bool(l['weights']):
                weight_args['weight_name'] = l['weights']
            pipe_animatediff_sdxl.load_lora_weights(l['path'], adapter_name=l['name'], torch_dtype=torch.float16, **weight_args)
        pipe_animatediff_sdxl.set_adapters(adapters, adapter_weights=scales)
    #else: p.disable_lora()
    ip_adapter_arg = {}
    if animatediff_sdxl_prefs['use_ip_adapter']:
        installer.status(f"...initialize IP-Adapter")
        ip_adapter_img = None
        if animatediff_sdxl_prefs['ip_adapter_image'].startswith('http'):
          i_response = requests.get(animatediff_sdxl_prefs['ip_adapter_image'])
          ip_adapter_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
        else:
          if os.path.isfile(animatediff_sdxl_prefs['ip_adapter_image']):
            ip_adapter_img = PILImage.open(animatediff_sdxl_prefs['ip_adapter_image'])
          else:
            clear_last()
            prt(f"ERROR: Couldn't find your ip_adapter_image {animatediff_sdxl_prefs['ip_adapter_image']}")
        if bool(ip_adapter_img):
          ip_adapter_arg['ip_adapter_image'] = ip_adapter_img
        if bool(ip_adapter_arg):
            ip_adapter_SDXL_model = next((m for m in ip_adapter_SDXL_models if m['name'] == animatediff_sdxl_prefs['ip_adapter_SDXL_model']), None)
            pipe_animatediff_sdxl.load_ip_adapter(ip_adapter_SDXL_model['path'], subfolder=ip_adapter_SDXL_model['subfolder'], weight_name=ip_adapter_SDXL_model['weight_name'])
            pipe_animatediff_sdxl.set_ip_adapter_scale(animatediff_sdxl_prefs['ip_adapter_strength'])
    clear_last()
    s = "" if len(animatediff_sdxl_prompts) == 1 else "s"
    prt(f"Generating your AnimateDiff Video{s}...")
    for pr in animatediff_sdxl_prompts:
        prt(progress)
        nudge(page.imageColumn if from_list else page.AnimateDiffSDXL, page)
        autoscroll(False)
        mode = "Video2Video" if pr['init_image'].endswith(('mp4', 'gif')) else "SDXL"
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(pr['seed'])
        generator = torch.Generator().manual_seed(random_seed)
        try:
            output = pipe_animatediff_sdxl(
                prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                num_frames=animatediff_sdxl_prefs['video_length'],
                num_videos_per_prompt=pr['num_images'],
                height=pr['height'],
                width=pr['width'],
                num_inference_steps=pr['num_inference_steps'],
                guidance_scale=pr['guidance_scale'],
                clip_skip=animatediff_sdxl_prefs['clip_skip'],
                #latent_interpolation_method=animatediff_sdxl_prefs['latent_interpolation_method'].lower(), # can be lerp, slerp, or your own callback
                generator=generator,
                callback_on_step_end=callback_step,
                #callback=callback_fnc,
                **ip_adapter_arg,
            ).frames
        except Exception as e:
            clear_last(2)
            alert_msg(page, f"ERROR: Something went wrong generating video...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        #clear_last()
        clear_last()
        autoscroll(True)
        if output is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        batch_output = os.path.join(prefs['image_output'], animatediff_sdxl_prefs['batch_folder_name'])
        makedir(batch_output)
        for v, frames_batch in enumerate(output):
            fname = format_filename(pr['prompt'])
            frames_dir = available_folder(batch_output, "frames", v)
            makedir(frames_dir)
            for idx, image in enumerate(frames_batch):
                #fname = f"{animatediff_sdxl_prefs['file_prefix']}{format_filename(animatediff_sdxl_prefs['batch_folder_name'])}-{b}"
                image_path = available_file(frames_dir, "frame", idx, zfill=4)
                image.save(image_path)
                new_file = os.path.basename(image_path)
                if not animatediff_sdxl_prefs['display_upscaled_image'] or not animatediff_sdxl_prefs['apply_ESRGAN_upscale']:
                    #prt(Row([Img(src=image_path, width=animatediff_sdxl_prefs['width'], height=animatediff_sdxl_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                    prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
                if animatediff_sdxl_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    upscale_image(image_path, image_path, scale=animatediff_sdxl_prefs["enlarge_scale"], face_enhance=animatediff_sdxl_prefs["face_enhance"])
                    if animatediff_sdxl_prefs['display_upscaled_image']:
                        time.sleep(0.6)
                        prt(Row([Img(src=asset_dir(image_path), width=int(pr['width'] * float(animatediff_sdxl_prefs["enlarge_scale"])), height=int(pr['height'] * float(animatediff_sdxl_prefs["enlarge_scale"])), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
            gif_file = available_file(batch_output, fname, no_num=True, ext="gif")
            export_to_gif(frames_batch, gif_file, fps=animatediff_sdxl_prefs['fps'])
            prt(Row([ImageButton(src=gif_file, width=pr['width'], height=pr['height'], data=gif_file, page=page)], alignment=MainAxisAlignment.CENTER))
            if animatediff_sdxl_prefs['export_to_video']:
                try:
                    installer = Installing("Running Google FILM: Frame Interpolation for Large Motion...")
                    prt(installer)
                    out_file = available_file(batch_output, fname, no_num=True, ext="mp4")
                    if animatediff_sdxl_prefs['interpolate_video']:
                        interpolate_video(frames_dir, input_fps=animatediff_sdxl_prefs['fps'], output_fps=animatediff_sdxl_prefs['target_fps'], output_video=out_file, installer=installer)
                    else:
                        installer.set_message("Saving Frames to Video using FFMPEG with Deflicker...")
                        pattern = create_pattern(new_file) #fname+"-%04d.png"
                        frames_to_video(frames_dir, pattern=pattern, input_fps=animatediff_sdxl_prefs['fps'], output_fps=animatediff_sdxl_prefs['target_fps'], output_video=out_file, installer=installer, deflicker=True)
                except Exception as e:
                    clear_last()
                    alert_msg(page, f"ERROR: Couldn't interpolate video, but frames still saved...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                    pass
                clear_last()
                if not os.path.isfile(out_file):
                    prt(f"Problem creating video file, but frames still saved...")
                else:
                    prt(Markdown(f"Video saved to [{out_file}]({filepath_to_url(out_file)})", on_tap_link=lambda e: e.page.launch_url(e.data)))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

class ProgressBar_gr:
    def __init__(self, progress_bar_object):
        self.progress_bar = progress_bar_object
        self.current_progress = 0
    def progress(self, percent):
        self.progress_bar.value = float(percent)
        self.progress_bar.update()

def run_diffsynth(page, from_list=False, with_params=False):
    global diffsynth_prefs, prefs, status, pipe_diffsynth, pipe_diffsynth_image, diffsynth_model_manager
    if not check_diffusers(page):
        return
    '''if not bool(diffsynth_prefs['init_image']):
        alert_msg(page, "You need to provide an Initial Image to animate before using...")
        return
    if not bool(diffsynth_prefs['batch_folder_name']):
        alert_msg(page, "You need to provide a unique Video Folder Name for your project...")
        return'''
    diffsynth_prompts = []
    if from_list:
        if len(prompts) < 1:
            alert_msg(page, "You need to add Prompts to your List first... ")
            return
        for p in prompts:
            if with_params:
                diffsynth_prompts.append({'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'guidance_scale': diffsynth_prefs['guidance_scale'], 'num_inference_steps': diffsynth_prefs['num_inference_steps'], 'width': diffsynth_prefs['width'], 'height': diffsynth_prefs['height'], 'init_image': diffsynth_prefs['init_image'], 'controlnet_strength': diffsynth_prefs['controlnet_strength'], 'num_videos': diffsynth_prefs['num_videos'], 'seed': diffsynth_prefs['seed']})
            else:
                diffsynth_prompts.append({'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'guidance_scale': p['guidance_scale'], 'num_inference_steps': p['steps'], 'width': p['width'], 'height': p['height'], 'init_image': p['init_image'], 'controlnet_strength': p['init_image_strength'], 'num_videos': p['batch_size'], 'seed': p['seed']})
    else:
        '''if not bool(diffsynth_prefs['prompt']):
            alert_msg(page, "You must provide a text prompt to process your image generation...")
            return'''
        diffsynth_prompts.append({'prompt': diffsynth_prefs['prompt'], 'negative_prompt': diffsynth_prefs['negative_prompt'], 'guidance_scale': diffsynth_prefs['guidance_scale'], 'num_inference_steps': diffsynth_prefs['num_inference_steps'], 'width': diffsynth_prefs['width'], 'height': diffsynth_prefs['height'], 'init_image': diffsynth_prefs['init_image'], 'controlnet_strength': diffsynth_prefs['controlnet_strength'], 'num_videos': diffsynth_prefs['num_videos'], 'seed': diffsynth_prefs['seed']})
    def prt(line, update=True):
        if type(line) == str:
            line = Text(line, size=17)
        page.DiffSynth.controls.append(line)
        page.DiffSynth.update()
        if from_list:
            page.imageColumn.controls.append(line)
            if update:
                page.imageColumn.update()
            else:
                page.DiffSynth.controls.append(line)
                if update:
                    page.DiffSynth.update()
    def clear_last(lines=1):
        if from_list:
            clear_line(page.imageColumn, lines=lines)
        else:
            clear_line(page.DiffSynth, lines=lines)
    def clear_list():
        if from_list:
            page.imageColumn.controls.clear()
        else:
            page.DiffSynth.controls = page.DiffSynth.controls[:1]
    def autoscroll(scroll=True):
        if from_list:
            page.imageColumn.auto_scroll = scroll
            page.imageColumn.update()
        else:
            page.DiffSynth.auto_scroll = scroll
            page.DiffSynth.update()
    if from_list:
        page.tabs.selected_index = 4
        page.tabs.update()
    modes = {"video_rerender": "Video ReRender", "exvideo_svd": "ExVideo SVD", "sd_text_to_video": "SD Text-to-Video", "sdxl_text_to_video": "SDXL Text-to-Video", "svd_text_to_video": "SVD Text-to-Video", "diffutoon": "Diffutoon Shading", "sd_toon_shading": "SD Toon Shading"}
    diffsynth_mode = switchcase(diffsynth_prefs["diffsynth_mode"], modes)
    clear_list()
    autoscroll(True)
    installer = Installing("Installing DiffSynth Image-To-Video Pipeline...")
    prt(installer)
    pip_install("mediapipe modelscope cupy-cuda12x|cupy controlnet-aux imageio imageio[ffmpeg]|imageio safetensors einops sentencepiece", installer=installer)
    diffsynth_dir = os.path.join(root_dir, "DiffSynth-Studio")
    models = os.path.join(diffsynth_dir, "models")
    if not os.path.exists(diffsynth_dir):
        installer.status("...cloning modelscope/DiffSynth-Studio")
        run_sp("git clone https://github.com/modelscope/DiffSynth-Studio.git", cwd=root_dir, realtime=False)
    elif force_update("diffsynth"):
        installer.status("...updating modelscope/DiffSynth-Studio")
        run_sp("git pull origin main", cwd=diffsynth_dir, realtime=False)
    if diffsynth_dir not in sys.path:
        sys.path.append(diffsynth_dir)
    os.chdir(diffsynth_dir)
    mode = diffsynth_prefs["diffsynth_mode"]
    status.setdefault("loaded_diffsynth", "")
    if mode != status['loaded_diffsynth']:
        clear_pipes()
    else:
        clear_pipes("diffsynth")
    try:
        from diffsynth import save_video, ModelManager, SDVideoPipelineRunner, SVDVideoPipeline, HunyuanDiTImagePipeline, save_video, download_models
        from diffsynth.extensions.RIFE import RIFEInterpolater
        if mode == "video_rerender":
            from diffsynth import SDVideoPipeline, ControlNetConfigUnit, VideoData, download_models
            from diffsynth.processors.FastBlend import FastBlendSmoother
            from diffsynth.processors.PILEditor import ContrastEditor, SharpnessEditor
            from diffsynth.processors.sequencial_processor import SequencialProcessor
            if pipe_diffsynth is None:
                installer.status("...downloading models")
                download_models([
                    "ControlNet_v11f1p_sd15_depth",
                    "ControlNet_v11p_sd15_softedge",
                    "DreamShaper_8"
                ])#, downloading_priority = ["HuggingFace", "ModelScope"])
                diffsynth_model_manager = ModelManager(torch_dtype=torch.float16, device="cuda",
                                                       file_path_list=[
                                                           f"models{slash}stable_diffusion{slash}dreamshaper_8.safetensors",
                                                           f"models{slash}ControlNet{slash}control_v11f1p_sd15_depth.pth",
                                                           f"models{slash}ControlNet{slash}control_v11p_sd15_softedge.pth",
                                                       ]
                                                       )
                installer.status("...loading SDVideoPipeline")
                pipe_diffsynth = SDVideoPipeline.from_model_manager(
                    diffsynth_model_manager,
                    [
                        ControlNetConfigUnit(
                            processor_id="depth",
                            model_path=f"models{slash}ControlNet{slash}control_v11f1p_sd15_depth.pth",
                            scale=0.5
                        ),
                        ControlNetConfigUnit(
                            processor_id="softedge",
                            model_path=f"models{slash}ControlNet{slash}control_v11p_sd15_softedge.pth",
                            scale=0.5
                        )
                    ]
                )
            smoother = SequencialProcessor([FastBlendSmoother(), ContrastEditor(rate=1.1), SharpnessEditor(rate=1.1)])
        elif mode == "exvideo_svd":
            os.environ["TOKENIZERS_PARALLELISM"] = "True"
            if pipe_diffsynth is None:
                installer.status("...downloading HunyuanDiT models")
                os.environ["TOKENIZERS_PARALLELISM"] = "True"
                download_models(["HunyuanDiT"])#, downloading_priority = ["HuggingFace", "ModelScope"])
                diffsynth_model_manager = ModelManager(torch_dtype=torch.float16, device="cuda",
                                                       file_path_list=[
                                                           f"models{slash}HunyuanDiT{slash}t2i{slash}clip_text_encoder{slash}pytorch_model.bin",
                                                           f"models{slash}HunyuanDiT{slash}t2i{slash}mt5{slash}pytorch_model.bin",
                                                           f"models{slash}HunyuanDiT{slash}t2i{slash}model{slash}pytorch_model_ema.pt",
                                                           f"models{slash}HunyuanDiT{slash}t2i{slash}sdxl-vae-fp16-fix{slash}diffusion_pytorch_model.bin",
                                                       ])
                installer.status("...loading HunyuanDiTImagePipeline")
                pipe_diffsynth_image = HunyuanDiTImagePipeline.from_model_manager(diffsynth_model_manager)
                installer.status("...downloading svd-img2vid-xt & ExVideo-SVD")
                download_models(["stable-video-diffusion-img2vid-xt", "ExVideo-SVD-128f-v1"])
                model_manager = ModelManager(torch_dtype=torch.float16, device="cuda",
                                             file_path_list=[
                                                 f"models{slash}stable_video_diffusion{slash}svd_xt.safetensors",
                                                 f"models{slash}stable_video_diffusion{slash}model.fp16.safetensors",
                                             ])
                installer.status("...loading SVDVideoPipeline")
                pipe_diffsynth = SVDVideoPipeline.from_model_manager(model_manager)
        elif mode == "sd_text_to_video":
            from diffsynth import SDImagePipeline
            if pipe_diffsynth is None:
                installer.status("...downloading dreamshaper_8 mm_sd_v15_v2 & flownet")
                download_models(["DreamShaper_8", "AnimateDiff_v2", "RIFE"])#, downloading_priority = ["HuggingFace", "ModelScope"])
                diffsynth_model_manager = ModelManager(torch_dtype=torch.float16, device="cuda")
                diffsynth_model_manager.load_models([
                    f"models{slash}stable_diffusion{slash}dreamshaper_8.safetensors",
                    f"models{slash}AnimateDiff{slash}mm_sd_v15_v2.ckpt",
                    f"models{slash}RIFE{slash}flownet.pkl",
                ])
                installer.status("...loading SDImagePipeline")
                pipe_diffsynth_image = SDImagePipeline.from_model_manager(diffsynth_model_manager)
                installer.status("...loading SDVideoPipeline")
                pipe_diffsynth = SDVideoPipeline.from_model_manager(diffsynth_model_manager)
        elif mode == "sdxl_text_to_video":
            from diffsynth import SDXLVideoPipeline
            if pipe_diffsynth is None:
                installer.status("...downloading mm_sdxl models")
                download_models(["StableDiffusionXL_v1", "AnimateDiff_xl_beta"])#, downloading_priority = ["HuggingFace", "ModelScope"])
                diffsynth_model_manager = ModelManager(torch_dtype=torch.float16, device="cuda")
                diffsynth_model_manager.load_models([
                    f"models{slash}stable_diffusion_xl{slash}sd_xl_base_1.0.safetensors",
                    f"models{slash}AnimateDiff{slash}mm_sdxl_v10_beta.ckpt",
                ])
                installer.status("...loading SDXLVideoPipeline")
                pipe_diffsynth = SDXLVideoPipeline.from_model_manager(diffsynth_model_manager)
        elif mode == "svd_text_to_video":
            from diffsynth import SDXLImagePipeline, SVDVideoPipeline
            if pipe_diffsynth is None:
                installer.status("...downloading img2vid models")
                download_models(["StableDiffusionXL_v1", "stable-video-diffusion-img2vid-xt"])#, downloading_priority = ["HuggingFace", "ModelScope"])
                diffsynth_model_manager = ModelManager(torch_dtype=torch.float16, device="cuda")
                diffsynth_model_manager.load_models([f"models{slash}stable_diffusion_xl{slash}sd_xl_base_1.0.safetensors"])
                installer.status("...loading SDXLImagePipeline")
                pipe_diffsynth_image = SDXLImagePipeline.from_model_manager(diffsynth_model_manager)
                diffsynth_model_manager.to("cpu")
                installer.status("...downloading svd_xt models")
                diffsynth_model_manager = ModelManager()
                diffsynth_model_manager.load_models([f"models{slash}stable_video_diffusion{slash}svd_xt.safetensors"])
                installer.status("...loading SVDVideoPipeline")
                pipe_diffsynth = SVDVideoPipeline.from_model_manager(diffsynth_model_manager)
        elif mode == "diffutoon":
            installer.status("...downloading Diffutoon models")
            download_models([
                "AingDiffusion_v12",
                "AnimateDiff_v2",
                "ControlNet_v11p_sd15_lineart",
                "ControlNet_v11f1e_sd15_tile",
                "TextualInversion_VeryBadImageNegative_v1.3"
            ])#, downloading_priority = ["HuggingFace", "ModelScope"])
        elif mode == "sd_toon_shading":
            if pipe_diffsynth is None:
                installer.status("...downloading sd_toon_shading models")
                download_models([
                    "Flat2DAnimerge_v45Sharp",
                    "AnimateDiff_v2",
                    "ControlNet_v11p_sd15_lineart",
                    "ControlNet_v11f1e_sd15_tile",
                    "TextualInversion_VeryBadImageNegative_v1.3"
                ])#, downloading_priority = ["HuggingFace", "ModelScope"])
                diffsynth_model_manager = ModelManager(torch_dtype=torch.float16, device="cuda")
                diffsynth_model_manager.load_textual_inversions(f"models{slash}textual_inversion")
                diffsynth_model_manager.load_models([
                    f"models{slash}stable_diffusion{slash}flat2DAnimerge_v45Sharp.safetensors",
                    f"models{slash}AnimateDiff{slash}mm_sd_v15_v2.ckpt",
                    f"models{slash}ControlNet{slash}control_v11p_sd15_lineart.pth",
                    f"models{slash}ControlNet{slash}control_v11f1e_sd15_tile.pth",
                ])
                installer.status("...loading SDVideoPipeline")
                pipe_diffsynth = SDVideoPipeline.from_model_manager(
                    diffsynth_model_manager,
                    [
                        ControlNetConfigUnit(
                            processor_id="lineart",
                            model_path=f"models{slash}ControlNet{slash}control_v11p_sd15_lineart.pth",
                            scale=0.5
                        ),
                        ControlNetConfigUnit(
                            processor_id="tile",
                            model_path=f"models{slash}ControlNet{slash}control_v11f1e_sd15_tile.pth",
                            scale=0.5
                        )
                    ]
                )
        else:
            print(f"Invalid DiffSynth mode: {mode}")
        status['loaded_diffsynth'] = mode
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR: DiffSynth failed initializing. Possibly out of memory or something wrong with the code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        os.chdir(root_dir)
        return
    vid_length = diffsynth_prefs['num_frames'] / diffsynth_prefs['fps']
    clear_last()

    def upscale_video(image, video):
        # Load models
        download_models(["stable-video-diffusion-img2vid-xt", "ExVideo-SVD-128f-v1"])
        model_manager = ModelManager(torch_dtype=torch.float16, device="cuda",
                                     file_path_list=[
                                         f"models{slash}stable_video_diffusion{slash}svd_xt.safetensors",
                                         f"models{slash}stable_video_diffusion{slash}model.fp16.safetensors",
                                     ])
        pipe = SVDVideoPipeline.from_model_manager(model_manager)
        torch.manual_seed(2)
        video = pipe(
            input_image=image.resize((1024, 1024)),
            input_video=[frame.resize((1024, 1024)) for frame in video], denoising_strength=0.5,
            num_frames=128, fps=30, height=1024, width=1024,
            motion_bucket_id=127,
            num_inference_steps=25,
            min_cfg_scale=2, max_cfg_scale=2, contrast_enhance_scale=1.2
        )
        model_manager.to("cpu")
        return video
    def callback_fnc(pipe, step, timestep, callback_kwargs):
        nonlocal progress
        total_steps = pipe.num_timesteps
        percent = (step + 1) / total_steps
        progress.progress.value = percent
        progress.progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
        progress.progress.update()
        
    for pr in diffsynth_prompts:
        progress = Progress(f"Generating {diffsynth_mode}... Length: {round(vid_length, 1)} seconds", steps=diffsynth_prefs['num_inference_steps'])
        prt(progress)
        pb = ProgressBar_gr(progress.progress)
        nudge(page.DiffSynth, page=page)
        autoscroll(False)
        from io import BytesIO
        from PIL.PngImagePlugin import PngInfo
        from PIL import ImageOps
        init_img, init_video = [None] * 2
        if bool(pr['init_image']):
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            width, height = init_img.size
            width, height = scale_dimensions(width, height, diffsynth_prefs['max_size'], multiple=64)
            init_img = init_img.resize((width, height))  # , resample=PILImage.Resampling.BICUBIC)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        if bool(diffsynth_prefs['init_video']):
            if diffsynth_prefs['init_video'].startswith('http'):
                progress.status("...downloading target video")
                init_video = download_file(diffsynth_prefs['init_video'], uploads_dir)
                progress.status()
            else:
                if os.path.isfile(diffsynth_prefs['init_video']):
                    init_video = diffsynth_prefs['init_video']
                    # shutil.copy(target_path, os.path.join(inputs_dir, os.path.basename(target_path)))
                    # target_path = os.path.join(inputs_dir, os.path.basename(target_path))
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_video {diffsynth_prefs['init_video']}")
                    return
        batch_output = os.path.join(prefs['image_output'], diffsynth_prefs['batch_folder_name'])
        makedir(batch_output)
        # for v in range(diffsynth_prefs['num_videos']):
        frames_batch = None
        random_seed = get_seed(pr['seed'])
        torch.manual_seed(random_seed)
        fname = f"{diffsynth_prefs['file_prefix']}{format_filename(pr['prompt'] if bool(pr['prompt']) else diffsynth_prefs['batch_folder_name'])}"
        out_file = available_file(batch_output, fname, no_num=True, ext="mp4")
        RIFE_file = available_file(batch_output, fname+"-RIFE", no_num=True, ext="mp4")
        video = None
        try:
            if mode == "video_rerender":
                video = VideoData(video_file=init_video, height=pr["height"], width=pr["width"])
                input_video = [video[i] for i in range(128)]
                video = pipe_diffsynth(
                    prompt=pr["prompt"],
                    negative_prompt=pr["negative_prompt"], cfg_scale=pr["guidance_scale"],
                    input_frames=input_video, controlnet_frames=input_video, num_frames=len(input_video),
                    num_inference_steps=pr["num_inference_steps"], height=pr["height"], width=pr["width"],
                    animatediff_batch_size=diffsynth_prefs["batch_size"], animatediff_stride=diffsynth_prefs["stride"], unet_batch_size=diffsynth_prefs["batch_size"],
                    cross_frame_attention=True,
                    smoother=smoother, smoother_progress_ids=[4, 9, 14, 19],
                    progress_bar_st=pb
                )
            elif mode == "exvideo_svd":
                if init_img != None:
                    image = init_img
                else:
                    image = pipe_diffsynth_image(
                        prompt=pr["prompt"],
                        negative_prompt=pr["negative_prompt"],
                        num_inference_steps=pr["num_inference_steps"], height=pr["height"], width=pr["width"],
                    )
                    diffsynth_model_manager.to("cpu")
                vram = vram_free()
                if vram < 12:
                    print(f"May need to clear pipeline, available VRAM {vram}")
                video = pipe_diffsynth(
                    input_image=image.resize((pr["width"], pr["height"])),
                    num_frames=diffsynth_prefs["num_frames"], fps=diffsynth_prefs["target_fps"], height=pr["height"], width=pr["width"],
                    motion_bucket_id=diffsynth_prefs["motion_bucket_id"],
                    num_inference_steps=pr["num_inference_steps"],
                    min_cfg_scale=diffsynth_prefs["min_guidance_scale"], max_cfg_scale=diffsynth_prefs["max_guidance_scale"], contrast_enhance_scale=1.2,
                    progress_bar_st=pb
                )
                diffsynth_model_manager.to("cpu")
            elif mode == "sd_text_to_video":
                if init_img != None:
                    image = init_img
                else:
                    image = pipe_diffsynth_image(
                        prompt=pr["prompt"],
                        negative_prompt=pr["negative_prompt"],
                        cfg_scale=pr["guidance_scale"],
                        num_inference_steps=pr["num_inference_steps"], height=pr["height"], width=pr["width"],
                        progress_bar_st=pb
                    )
                if vram < 8:
                    print(f"May need to clear pipeline, available VRAM {vram}")
                # Text + Image -> Video (6GB VRAM is enough!)
                # pipe_diffsynth = SDVideoPipeline.from_model_manager(diffsynth_model_manager)
                video = pipe_diffsynth(
                    prompt=pr["prompt"],
                    negative_prompt=pr["negative_prompt"],
                    cfg_scale=pr["guidance_scale"],
                    num_frames=diffsynth_prefs["num_frames"],
                    num_inference_steps=pr["num_inference_steps"], height=pr["height"], width=pr["width"],
                    animatediff_batch_size=diffsynth_prefs["batch_size"], animatediff_stride=diffsynth_prefs["stride"], input_frames=[image] * diffsynth_prefs["num_frames"], denoising_strength=0.9,
                    vram_limit_level=0,
                    progress_bar_st=pb
                )
                # Video -> Video with high fps
                interpolater = RIFEInterpolater.from_model_manager(diffsynth_model_manager)
                video = interpolater.interpolate(video, num_iter=3)
            elif mode == "sdxl_text_to_video":
                video = pipe_diffsynth(
                    prompt=pr["prompt"],
                    negative_prompt=pr["negative_prompt"],
                    cfg_scale=pr["guidance_scale"],
                    height=pr["height"], width=pr["width"], num_frames=diffsynth_prefs["num_frames"],
                    num_inference_steps=pr["num_inference_steps"],
                    progress_bar_st=pb
                )
            elif mode == "svd_text_to_video":
                if init_img != None:
                    image = init_img
                else:
                    image = pipe_diffsynth_image(
                        prompt=pr["prompt"],
                        negative_prompt=pr["negative_prompt"],
                        cfg_scale=pr["guidance_scale"],
                        height=pr["height"], width=pr["width"], num_inference_steps=pr["num_inference_steps"],
                        progress_bar_st=pb
                    )
                vram = vram_free()
                if vram < 12:
                    print(f"May need to clear pipeline, available VRAM {vram}")
                diffsynth_model_manager.to("cpu")
                video = pipe_diffsynth(
                    input_image=image,
                    num_frames=diffsynth_prefs["num_frames"], fps=diffsynth_prefs["fps"], height=pr["height"], width=pr["width"],
                    motion_bucket_id=diffsynth_prefs["motion_bucket_id"],
                    num_inference_steps=pr["num_inference_steps"],
                    progress_bar_st=pb
                )
            elif mode == "diffutoon":
                config = {
                    "models": {
                        "model_list": [
                            "models/stable_diffusion/aingdiffusion_v12.safetensors",
                            "models/AnimateDiff/mm_sd_v15_v2.ckpt",
                            "models/ControlNet/control_v11f1e_sd15_tile.pth",
                            "models/ControlNet/control_v11p_sd15_lineart.pth"
                        ],
                        "textual_inversion_folder": "models/textual_inversion",
                        "device": "cuda",
                        "lora_alphas": [],
                        "controlnet_units": [
                            {
                                "processor_id": "tile",
                                "model_path": "models/ControlNet/control_v11f1e_sd15_tile.pth",
                                "scale": diffsynth_prefs["controlnet_strength"],
                            },
                            {
                                "processor_id": "lineart",
                                "model_path": "models/ControlNet/control_v11p_sd15_lineart.pth",
                                "scale": diffsynth_prefs["controlnet_strength"],
                            }
                        ]
                    },
                    "data": {
                        "input_frames": {
                            "video_file": init_video,
                            "image_folder": None,
                            "height": 1536,
                            "width": 1536,
                            "start_frame_id": 0,
                            "end_frame_id": 30,
                        },
                        "controlnet_frames": [
                            {
                                "video_file": init_video,
                                "image_folder": None,
                                "height": 1536,
                                "width": 1536,
                                "start_frame_id": 0,
                                "end_frame_id": 30,
                            },
                            {
                                "video_file": init_video,
                                "image_folder": None,
                                "height": 1536,
                                "width": 1536,
                                "start_frame_id": 0,
                                "end_frame_id": 30,
                            }
                        ],
                        "output_folder": batch_output,
                        "fps": diffsynth_prefs["target_fps"]
                    },
                    "pipeline": {
                        "seed": random_seed,
                        "pipeline_inputs": {
                            "prompt": pr["prompt"],
                            "negative_prompt": pr["negative_prompt"],
                            "cfg_scale": pr["guidance_scale"],
                            "clip_skip": diffsynth_prefs["clip_skip"],
                            "denoising_strength": 1.0,
                            "num_inference_steps": pr["num_inference_steps"],
                            "animatediff_batch_size": diffsynth_prefs["batch_size"],
                            "animatediff_stride": diffsynth_prefs["stride"],
                            "unet_batch_size": 1,
                            "controlnet_batch_size": 1,
                            "cross_frame_attention": False,
                            # The following parameters will be overwritten. You don't need to modify them.
                            "input_frames": [],
                            "num_frames": diffsynth_prefs["num_frames"],
                            "width": pr["width"],
                            "height": pr["height"],
                            "controlnet_frames": [],
                        }
                    }
                }
                runner = SDVideoPipelineRunner()
                runner.run(config)
            elif mode == "sd_toon_shading":
                video = VideoData(
                    video_file=init_video,
                    height=pr["height"], width=pr["width"])
                input_video = [video[i] for i in range(40 * 60, 41 * 60)]
                # Toon shading (20G VRAM)
                video = pipe_diffsynth(
                    prompt=pr["prompt"],
                    negative_prompt=pr["negative_prompt"],
                    cfg_scale=pr["guidance_scale"], clip_skip=diffsynth_prefs["clip_skip"],
                    controlnet_frames=input_video, num_frames=len(input_video),
                    num_inference_steps=pr["num_inference_steps"], height=pr["height"], width=pr["width"],
                    animatediff_batch_size=diffsynth_prefs["batch_size"], animatediff_stride=diffsynth_prefs["stride"],
                    vram_limit_level=0,
                )
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: DiffSynth failed for some reason. Possibly out of memory or something wrong with the code...", debug_pref=diffsynth_prefs, content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            os.chdir(root_dir)
            return
        clear_last()
        autoscroll(True)
        progress.status("...saving video")
        save_video(video, out_file, fps=diffsynth_prefs["target_fps"])
        if diffsynth_prefs["interpolate_video"]:
            progress.status("...RIFE Interpolater")
            diffsynth_model_manager.load_models(["models/RIFE/flownet.pkl"])
            interpolater = RIFEInterpolater.from_model_manager(diffsynth_model_manager)
            video = interpolater.interpolate(video, num_iter=3)
            save_video(video, RIFE_file, fps=diffsynth_prefs["target_fps"])
        if diffsynth_prefs['export_to_gif']:
            try:
                installer = Installing("Saving Animated GIF image sequence...")
                prt(installer)
                from diffusers.utils import export_to_gif
                gif_file = available_file(batch_output, fname, no_num=True, ext="gif")
                export_to_gif(video, gif_file, fps=diffsynth_prefs['fps'])
            except Exception as e:
                clear_last()
                alert_msg(page, f"ERROR: Couldn't export gif, but frames still saved...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                pass
            clear_last()
            if not os.path.isfile(gif_file):
                prt(f"Problem creating gif file, but frames still saved...")
            else:
                prt(Row([ImageButton(src=gif_file, width=width, height=height, data=gif_file, page=page)], alignment=MainAxisAlignment.CENTER))
        '''if diffsynth_prefs['export_to_video']:
            try:
                installer = Installing("Running Google FILM: Frame Interpolation for Large Motion...")
                prt(installer)
                out_file = available_file(batch_output, fname, no_num=True, ext="mp4")
                if diffsynth_prefs['interpolate_video']:
                    interpolate_video(video, input_fps=diffsynth_prefs['fps'], output_fps=diffsynth_prefs['target_fps'], output_video=out_file, installer=installer)
                else:
                    installer.set_message("Saving Frames to Video using FFMPEG with Deflicker...")
                    pattern = create_pattern(new_file)  # fname+"-%04d.png"
                    frames_to_video(video, pattern=pattern, input_fps=diffsynth_prefs['fps'], output_fps=diffsynth_prefs['target_fps'], output_video=out_file, installer=installer, deflicker=True)
            except Exception as e:
                clear_last()
                alert_msg(page, f"ERROR: Couldn't interpolate video, but frames still saved...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                pass
            clear_last()'''
        autoscroll(True)
        if not os.path.isfile(out_file):
            prt(f"Problem creating video file...")
        else:
            interpolate = f" and [{os.path.basename(RIFE_file)}]({filepath_to_url(RIFE_file)})" if diffsynth_prefs["interpolate_video"] else ""
            prt(Markdown(f"Video saved to [{out_file}]({filepath_to_url(out_file)}){interpolate}", on_tap_link=lambda e: e.page.launch_url(e.data)))
            # prt(Row([VideoContainer(out_file)], alignment=MainAxisAlignment.CENTER))
            b += 1
    # filename = filename[:int(prefs['file_max_length'])]
    # if prefs['file_suffix_seed']: filename += f"-{random_seed}"
    os.chdir(root_dir)
    video_path = ""
    #prt(f"Done creating animation... Check {batch_output}")
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_pia(page, from_list=False, with_params=False):
    global pia_prefs, pipe_pia, prefs, status
    if not check_diffusers(page): return
    pia_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            pia_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':pia_prefs['guidance_scale'], 'num_inference_steps':pia_prefs['num_inference_steps'], 'width':pia_prefs['width'], 'height':pia_prefs['height'], 'init_image':pia_prefs['init_image'], 'init_image_strength':pia_prefs['init_image_strength'], 'num_images':pia_prefs['num_images'], 'seed':pia_prefs['seed']})
        else:
            pia_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'init_image':p['init_image'], 'init_image_strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(pia_prefs['init_image']):
        alert_msg(page, "You must provide an Init Image to process your video generation...")
        return
      pia_prompts.append({'prompt': pia_prefs['prompt'], 'negative_prompt':pia_prefs['negative_prompt'], 'guidance_scale':pia_prefs['guidance_scale'], 'num_inference_steps':pia_prefs['num_inference_steps'], 'width':pia_prefs['width'], 'height':pia_prefs['height'], 'init_image':pia_prefs['init_image'], 'init_image_strength':pia_prefs['init_image_strength'], 'num_images':pia_prefs['num_images'], 'seed':pia_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.PIA.controls.append(line)
        if update:
          page.PIA.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.PIA, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.PIA.auto_scroll = scroll
        page.PIA.update()
      else:
        page.PIA.auto_scroll = scroll
        page.PIA.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.PIA.controls = page.PIA.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = pia_prefs['num_inference_steps']
    def callback_step(pipe, step, timestep, callback_kwargs):
      #callback_step.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    #mode = "Video2Video" if pia_prompts[0]['init_image'].endswith('mp4') or pia_prompts[0]['init_image'].endswith('gif') else "Image2Video"
    installer = Installing(f"Installing Personalized Image Animator Engine & Models... See console for progress.")
    prt(installer)
    clear_pipes("pia")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    cpu_offload = pia_prefs['cpu_offload']
    motion_module = "openmmlab/PIA-condition-adapter"
    pia_model = "SG161222/Realistic_Vision_V6.0_B1_noVAE" if pia_prefs['pia_model'] == "Realistic_Vision_V6.0_B1_noVAE" else "SG161222/Realistic_Vision_V5.1_noVAE" if pia_prefs['pia_model'] == "Realistic_Vision_V5.1_noVAE" else "Lykon/dreamshaper-8" if pia_prefs['pia_model'] == "dreamshaper-8" else pia_prefs['pia_custom_model']
    if 'loaded_pia' not in status: status['loaded_pia'] = ""
    if pia_model != status['loaded_pia']:
        clear_pipes()
    #from diffusers import AutoPipelineForVideo2Video, AutoPipelineForImage2Video, PIAScheduler
    from diffusers import MotionAdapter, EulerDiscreteScheduler, PIAPipeline, DDIMScheduler
    from diffusers.utils import export_to_gif, load_image

    if pipe_pia == None:
        installer.status(f"...initialize PIA Pipeline")
        try:
            adapter = MotionAdapter.from_pretrained(motion_module)
            pipe_pia = PIAPipeline.from_pretrained(pia_model, motion_adapter=adapter, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_pia = AutoPipelineForImage2Video.from_pretrained(pia_model, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_pia.scheduler = DDIMScheduler(beta_schedule="linear", steps_offset=1, clip_sample=False, timestep_spacing="linspace")
            #pipe_pia = pipeline_scheduler(pipe_pia)
            pipe_pia.scheduler = DDIMScheduler.from_config(pipe_pia.scheduler.config)
            #pipe_pia.scheduler = EulerDiscreteScheduler.from_config(pipe_pia.scheduler.config)
            if prefs['vae_slicing']:
                pipe_pia.enable_vae_slicing()
            if prefs['vae_tiling']:
                pipe_pia.enable_vae_tiling()
            if pia_prefs['free_init']: #Not yet
                installer.status(f"...enable FreeInit")
                pipe_pia.enable_free_init(method="butterworth", use_fast_sampling=True)
            if prefs['enable_freeu']:
                installer.status(f"...enable FreeU")
                pipe_pia.enable_freeu(s1=0.6, s2=0.4, b1=1.1, b2=1.2)
            if prefs['enable_torch_compile']:
                installer.status(f"...Torch compiling transformer")
                pipe_pia.transformer = torch.compile(pipe_pia.transformer, mode="reduce-overhead", fullgraph=True)
                pipe_pia = pipe_pia.to(torch_device)
            elif cpu_offload:
                pipe_pia.enable_model_cpu_offload()
            else:
                pipe_pia = pipe_pia.to(torch_device)
            pipe_pia.scheduler = DDIMScheduler.from_config(pipe_pia.scheduler.config)
            pipe_pia.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Personalized Image Animator...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        status['loaded_pia'] = pia_model
    else:
        clear_pipes('pia')
    if len(pia_prefs['lora_map']) > 0:
        installer.status(f"...loading LoRAs")
        adapters = []
        scales = []
        for l in pia_prefs['lora_map']:
            adapters.append(l['name'])
            scales.append(l['scale'])
            weight_args = {}
            if 'weights' in l and bool(l['weights']):
                weight_args['weight_name'] = l['weights']
            pipe_pia.load_lora_weights(l['path'], adapter_name=l['name'], torch_dtype=torch.float16, **weight_args)
        pipe_pia.set_adapters(adapters, adapter_weights=scales)
    ip_adapter_arg = {}
    if pia_prefs['use_ip_adapter']:
        installer.status(f"...initialize IP-Adapter")
        ip_adapter_img = None
        if pia_prefs['ip_adapter_image'].startswith('http'):
          i_response = requests.get(pia_prefs['ip_adapter_image'])
          ip_adapter_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
        else:
          if os.path.isfile(pia_prefs['ip_adapter_image']):
            ip_adapter_img = PILImage.open(pia_prefs['ip_adapter_image'])
          else:
            clear_last()
            prt(f"ERROR: Couldn't find your ip_adapter_image {pia_prefs['ip_adapter_image']}")
        if bool(ip_adapter_img):
          ip_adapter_arg['ip_adapter_image'] = ip_adapter_img
        if bool(ip_adapter_arg):
            ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == pia_prefs['ip_adapter_model'])
            pipe_pia.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
            pipe_pia.set_ip_adapter_scale(pia_prefs['ip_adapter_strength'])

    clear_last()
    s = "" if len(pia_prompts) == 1 else "s"
    prt(f"Generating your PIA Video{s}...")
    for pr in pia_prompts:
        prt(progress)
        nudge(page.imageColumn if from_list else page.PIA, page)
        autoscroll(False)
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(pr['seed'])
        generator = torch.Generator("cpu").manual_seed(random_seed)
        init_img = None
        fname = os.path.basename(pr['init_image'])
        if pr['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
        else:
            if os.path.isfile(pr['init_image']):
                init_img = PILImage.open(pr['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init image {pr['init_image']}")
                return
        init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
        init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        try:
            output = pipe_pia(
                prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                image=init_img,
                strength=pr['init_image_strength'],
                num_frames=pia_prefs['video_length'],
                num_videos_per_prompt=pr['num_images'],
                height=pr['height'],
                width=pr['width'],
                num_inference_steps=pr['num_inference_steps'],
                guidance_scale=pr['guidance_scale'],
                motion_scale=pia_prefs['motion_scale'],
                clip_skip=pia_prefs['clip_skip'],
                generator=generator,
                # TODO: Put this back. Was getting error 'NoneType' object has no attribute 'pop' for callback_outputs
                callback_on_step_end=callback_step,
                **ip_adapter_arg,
            ).frames
        except Exception as e:
            clear_last(2)
            alert_msg(page, f"ERROR: Something went wrong generating video...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        #clear_last()
        clear_last()
        autoscroll(True)
        if output is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        batch_output = os.path.join(prefs['image_output'], pia_prefs['batch_folder_name'])
        makedir(batch_output)
        for v, frames_batch in enumerate(output):
            fname = format_filename(pr['prompt'])
            frames_dir = available_folder(batch_output, "frames", v)
            makedir(frames_dir)
            for idx, image in enumerate(frames_batch):
                #fname = f"{pia_prefs['file_prefix']}{format_filename(pia_prefs['batch_folder_name'])}-{b}"
                image_path = available_file(frames_dir, "frame", idx, zfill=4)
                image.save(image_path)
                new_file = os.path.basename(image_path)
                if not pia_prefs['display_upscaled_image'] or not pia_prefs['apply_ESRGAN_upscale']:
                    #prt(Row([Img(src=image_path, width=pia_prefs['width'], height=pia_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                    prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
                if pia_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    upscale_image(image_path, image_path, scale=pia_prefs["enlarge_scale"], face_enhance=pia_prefs["face_enhance"])
                    if pia_prefs['display_upscaled_image']:
                        time.sleep(0.6)
                        prt(Row([Img(src=asset_dir(image_path), width=int(pr['width'] * float(pia_prefs["enlarge_scale"])), height=int(pr['height'] * float(pia_prefs["enlarge_scale"])), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
            gif_file = available_file(batch_output, fname, no_num=True, ext="gif")
            export_to_gif(frames_batch, gif_file, fps=pia_prefs['fps'])
            prt(Row([ImageButton(src=gif_file, width=pr['width'], height=pr['height'], data=gif_file, page=page)], alignment=MainAxisAlignment.CENTER))
            if pia_prefs['export_to_video']:
                try:
                    installer = Installing("Running Google FILM: Frame Interpolation for Large Motion...")
                    prt(installer)
                    out_file = available_file(batch_output, fname, no_num=True, ext="mp4")
                    if pia_prefs['interpolate_video']:
                        interpolate_video(frames_dir, input_fps=pia_prefs['fps'], output_fps=pia_prefs['target_fps'], output_video=out_file, installer=installer)
                    else:
                        installer.set_message("Saving Frames to Video using FFMPEG with Deflicker...")
                        pattern = create_pattern(new_file) #fname+"-%04d.png"
                        frames_to_video(frames_dir, pattern=pattern, input_fps=pia_prefs['fps'], output_fps=pia_prefs['target_fps'], output_video=out_file, installer=installer, deflicker=True)
                except Exception as e:
                    clear_last()
                    alert_msg(page, f"ERROR: Couldn't interpolate video, but frames still saved...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                    pass
                clear_last()
                if not os.path.isfile(out_file):
                    prt(f"Problem creating video file, but frames still saved...")
                else:
                    prt(Markdown(f"Video saved to [{out_file}]({filepath_to_url(out_file)})", on_tap_link=lambda e: e.page.launch_url(e.data)))
    nudge(page.imageColumn if from_list else page.PIA, page=page)
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_easyanimate(page, from_list=False, with_params=False):
    global easyanimate_prefs, pipe_easyanimate, easyanimate_transformer, prefs, status
    if not check_diffusers(page): return
    easyanimate_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            easyanimate_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':easyanimate_prefs['guidance_scale'], 'num_inference_steps':easyanimate_prefs['num_inference_steps'], 'width':easyanimate_prefs['width'], 'height':easyanimate_prefs['height'], 'init_image':easyanimate_prefs['init_image'], 'num_images':easyanimate_prefs['num_images'], 'seed':easyanimate_prefs['seed']})
        else:
            easyanimate_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'init_image':p['init_image'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      '''if not bool(easyanimate_prefs['init_image']):
        alert_msg(page, "You must provide an Init Image to process your video generation...")
        return'''
      easyanimate_prompts.append({'prompt': easyanimate_prefs['prompt'], 'negative_prompt':easyanimate_prefs['negative_prompt'], 'guidance_scale':easyanimate_prefs['guidance_scale'], 'num_inference_steps':easyanimate_prefs['num_inference_steps'], 'width':easyanimate_prefs['width'], 'height':easyanimate_prefs['height'], 'init_image':easyanimate_prefs['init_image'], 'num_images':easyanimate_prefs['num_images'], 'seed':easyanimate_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.EasyAnimate.controls.append(line)
        if update:
          page.EasyAnimate.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.EasyAnimate, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.EasyAnimate.auto_scroll = scroll
        page.EasyAnimate.update()
      else:
        page.EasyAnimate.auto_scroll = scroll
        page.EasyAnimate.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.EasyAnimate.controls = page.EasyAnimate.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = easyanimate_prefs['num_inference_steps']
    def callback_fn(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      nonlocal progress, total_steps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    def callback_step(pipe, step, timestep, callback_kwargs):
      callback_step.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    #mode = "Video2Video" if easyanimate_prompts[0]['init_image'].endswith('mp4') or easyanimate_prompts[0]['init_image'].endswith('gif') else "Image2Video"
    installer = Installing(f"Installing EasyAnimate Engine & Models... See console for progress.")
    prt(installer)
    easyanimate_dir = os.path.join(root_dir, "EasyAnimate")
    if not os.path.exists(easyanimate_dir):
        try:
            installer.status("...cloning aigc-apps/EasyAnimate")
            run_sp("git clone https://github.com/aigc-apps/EasyAnimate.git", cwd=root_dir)
            installer.status("...installing requirements")
            get_ffmpeg(installer)
            pip_install("einops safetensors timm tomesd torchdiffeq torchsde xformers decord datasets numpy scikit-image|skimage opencv-python|cv2 omegaconf SentencePiece albumentations imageio[pyav]|imageio imageio[ffmpeg]|imageio tensorboard gradio beautifulsoup4 ftfy", installer=installer)
            #installer.status("...building insightface 3D mesh cython")
            #run_sp("python setup.py builld_ext --inplace", cwd=os.path.join(easyanimate_dir, "src/utils/dependencies/insightface/thirdparty/face3d/mesh/cython"))
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing EasyAnimate Requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    elif force_update("easyanimate"):
        installer.status("...fetching aigc-apps/EasyAnimate")
        run_sp("git pull origin main", cwd=easyanimate_dir)
    transformer_dir = os.path.join(easyanimate_dir, "models", "Diffusion_Transformer")
    if not os.path.exists(transformer_dir):
        makedir(transformer_dir)
        makedir(os.path.join(easyanimate_dir, "models", "Motion_Module"))
        makedir(os.path.join(easyanimate_dir, "models", "Personalized_Model"))
    easyanimate_model = easyanimate_prefs['custom_model'] if easyanimate_prefs['easyanimate_model'] == "Custom" else f"alibaba-pai/{easyanimate_prefs['easyanimate_model']}"
    model_name = easyanimate_prefs['custom_model'] if easyanimate_prefs['easyanimate_model'] == "Custom" else f"models/Diffusion_Transformer/{easyanimate_prefs['easyanimate_model']}"
    #model_name = "models/Diffusion_Transformer/EasyAnimateV3-XL-2-InP-512x512"
    if not os.path.exists(os.path.join(transformer_dir, easyanimate_prefs['easyanimate_model'])) and (easyanimate_prefs['easyanimate_model'] != "Custom" or easyanimate_prefs['custom_model'].count('/') == 1):
        installer.status("...cloning pretrained weights")
        from huggingface_hub import Repository
        model_dir = os.path.join(transformer_dir, easyanimate_prefs['easyanimate_model'])
        makedir(model_dir)
        repo = Repository(local_dir=model_dir, clone_from=easyanimate_model)
    '''if not os.path.exists(transformer_dir):
        installer.status("...downloading pretrained weights")
        makedir(transformer_dir)
        makedir(os.path.join(easyanimate_dir, "models", "Motion_Module"))
        makedir(os.path.join(easyanimate_dir, "models", "Personalized_Model"))
        download_file("https://pai-aigc-photog.oss-cn-hangzhou.aliyuncs.com/easyanimate/Diffusion_Transformer/EasyAnimateV3-XL-2-InP-512x512.tar", to=transformer_dir)
        import tarfile
        with tarfile.open(os.path.join(transformer_dir, "EasyAnimateV3-XL-2-InP-512x512.tar"), 'r') as tar:
            tar.extractall(path=transformer_dir)'''
    if easyanimate_dir not in sys.path:
        sys.path.append(easyanimate_dir)
    os.chdir(easyanimate_dir)
    import numpy as np
    from diffusers import (AutoencoderKL, DDIMScheduler, DPMSolverMultistepScheduler,
                        EulerAncestralDiscreteScheduler, EulerDiscreteScheduler, PNDMScheduler)
    from omegaconf import OmegaConf
    from PIL import Image as PILImage
    from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection
    from easyanimate.models.autoencoder_magvit import AutoencoderKLMagvit
    from easyanimate.models.transformer3d import Transformer3DModel
    from easyanimate.pipeline.pipeline_easyanimate import EasyAnimatePipeline
    from easyanimate.pipeline.pipeline_easyanimate_inpaint import EasyAnimateInpaintPipeline
    from easyanimate.utils.lora_utils import merge_lora, unmerge_lora
    from easyanimate.utils.utils import get_image_to_video_latent, save_videos_grid
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    config_path = os.path.join(easyanimate_dir, "config/easyanimate_video_slicevae_motion_module_v3.yaml")
    #model_name = "models/Diffusion_Transformer/EasyAnimateV3-XL-2-InP-512x512"
    #easyanimate_model = "models/Diffusion_Transformer/EasyAnimateV3-XL-2-InP-512x512" if easyanimate_prefs['easyanimate_model'] == "EasyAnimateV3-XL-2-InP-512x512" else easyanimate_prefs['custom_model']
    cpu_offload = easyanimate_prefs['cpu_offload']
    status.setdefault("loaded_easyanimate", "")
    if easyanimate_model != status['loaded_easyanimate']:
        clear_pipes()
    else:
        clear_pipes("easyanimate")
    config = OmegaConf.load(config_path)
    weight_dtype = torch.bfloat16

    if pipe_easyanimate == None:
        installer.status(f"...initialize EasyAnimate Pipeline")
        try:
            easyanimate_transformer = Transformer3DModel.from_pretrained_2d(
                model_name, 
                subfolder="transformer",
                transformer_additional_kwargs=OmegaConf.to_container(config['transformer_additional_kwargs'])
            ).to(weight_dtype)
            if OmegaConf.to_container(config['vae_kwargs'])['enable_magvit']:
                Choosen_AutoencoderKL = AutoencoderKLMagvit
            else:
                Choosen_AutoencoderKL = AutoencoderKL
            vae = Choosen_AutoencoderKL.from_pretrained(
                model_name, 
                subfolder="vae"
            ).to(weight_dtype)
            if easyanimate_transformer.config.in_channels == 12:
                clip_image_encoder = CLIPVisionModelWithProjection.from_pretrained(
                    model_name, subfolder="image_encoder"
                ).to("cuda", weight_dtype)
                clip_image_processor = CLIPImageProcessor.from_pretrained(
                    model_name, subfolder="image_encoder"
                )
            else:
                clip_image_encoder = None
                clip_image_processor = None
            Choosen_Scheduler = scheduler_dict = {
                "Euler": EulerDiscreteScheduler,
                "Euler A": EulerAncestralDiscreteScheduler,
                "DPM++": DPMSolverMultistepScheduler, 
                "PNDM": PNDMScheduler,
                "DDIM": DDIMScheduler,
            }[easyanimate_prefs['sampler']]
            scheduler = Choosen_Scheduler(**OmegaConf.to_container(config['noise_scheduler_kwargs']))
            if easyanimate_transformer.config.in_channels == 12:
                pipe_easyanimate = EasyAnimateInpaintPipeline.from_pretrained(
                    model_name,
                    vae=vae,
                    transformer=easyanimate_transformer,
                    scheduler=scheduler,
                    torch_dtype=weight_dtype,
                    clip_image_encoder=clip_image_encoder,
                    clip_image_processor=clip_image_processor,
                )
            else:
                pipe_easyanimate = EasyAnimatePipeline.from_pretrained(
                    model_name,
                    vae=vae,
                    transformer=easyanimate_transformer,
                    scheduler=scheduler,
                    torch_dtype=weight_dtype
                )
            if cpu_offload:
                pipe_easyanimate.enable_sequential_cpu_offload()
            else:
                pipe_easyanimate.enable_model_cpu_offload()
            #if lora_path is not None:
            #    pipe_easyanimate = merge_lora(pipe_easyanimate, lora_path, lora_weight)
            #pipe_easyanimate = EasyAnimatePipeline.from_pretrained(easyanimate_model, torch_dtype=torch.float16, variant="fp16", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_easyanimate.scheduler = EulerDiscreteScheduler.from_config(pipe_easyanimate.scheduler.config)
            #pipe_easyanimate.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing EasyAnimate...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            autoscroll(False)
            return
        status['loaded_easyanimate'] = easyanimate_model
    else:
        clear_pipes('easyanimate')
    if len(easyanimate_prefs['lora_map']) > 0:
        installer.status(f"...loading LoRAs")
        adapters = []
        scales = []
        for l in easyanimate_prefs['lora_map']:
            adapters.append(l['name'])
            scales.append(l['scale'])
            weight_args = {}
            if 'weights' in l and bool(l['weights']):
                weight_args['weight_name'] = l['weights']
            pipe_easyanimate.load_lora_weights(l['path'], adapter_name=l['name'], torch_dtype=torch.float16, **weight_args)
        pipe_easyanimate.set_adapters(adapters, adapter_weights=scales)
    clear_last()
    s = "" if len(easyanimate_prompts) == 1 else "s"
    prt(f"Generating your EasyAnimate Video{s}...")
    for pr in easyanimate_prompts:
        init_img = None
        end_img = None
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(pr['seed'])
        fname = os.path.basename(pr['init_image'])
        if bool(pr['init_image']):
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init image {pr['init_image']}")
                    return
            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        if bool(easyanimate_prefs['end_image']):
            if easyanimate_prefs['end_image'].startswith('http'):
                end_img = PILImage.open(requests.get(easyanimate_prefs['end_image'], stream=True).raw)
            else:
                if os.path.isfile(easyanimate_prefs['end_image']):
                    end_img = PILImage.open(easyanimate_prefs['end_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init image {easyanimate_prefs['end_image']}")
                    return
            end_img = end_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
            end_img = ImageOps.exif_transpose(end_img).convert("RGB")
        if end_img is not None and init_img is None:
            alert_msg(page, f"ERROR: Can't have an ending image without an init image.")
            return
        for n in range(pr['num_images']):
            progress.value = None
            prt(progress)
            nudge(page.imageColumn if from_list else page.EasyAnimate, page)
            autoscroll(False)
            generator = torch.Generator("cuda").manual_seed(random_seed + n)
            video_length = easyanimate_prefs['video_length']
            if easyanimate_prefs['generate_image']:
                video_length = 1
            try:
                with torch.no_grad():
                    if easyanimate_transformer.config.in_channels == 12:
                        video_length = int(video_length // pipe_easyanimate.vae.mini_batch_encoder * pipe_easyanimate.vae.mini_batch_encoder) if video_length != 1 else 1
                        input_video, input_video_mask, clip_image = get_image_to_video_latent(init_img, end_img, video_length=video_length, sample_size=[pr['height'], pr['width']])
                        output = pipe_easyanimate(
                            pr['prompt'], 
                            video_length = video_length,
                            negative_prompt = pr['negative_prompt'],
                            height = pr['height'],
                            width = pr['width'],
                            generator = generator,
                            guidance_scale = pr['guidance_scale'],
                            num_inference_steps = pr['num_inference_steps'],
                            video = input_video,
                            mask_video = input_video_mask,
                            clip_image = clip_image,
                            callback = callback_fn,
                        ).videos
                    else:
                        output = pipe_easyanimate(
                            pr['prompt'], 
                            video_length = video_length,
                            negative_prompt = pr['negative_prompt'],
                            height = pr['height'],
                            width = pr['width'],
                            generator = generator,
                            guidance_scale = pr['guidance_scale'],
                            num_inference_steps = pr['num_inference_steps'],
                            callback = callback_fn,
                            #callback_on_step_end=callback_step,
                        ).videos
            except Exception as e:
                clear_last(2)
                alert_msg(page, f"ERROR: Something went wrong generating video...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            #clear_last()
            clear_last()
            autoscroll(True)
            if output is None:
                prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
                return
            batch_output = os.path.join(prefs['image_output'], easyanimate_prefs['batch_folder_name'])
            makedir(batch_output)
            fname = format_filename(pr['prompt'])
            try:
                if video_length == 1:
                    video_path = available_file(batch_output, fname, no_num=True)
                    image = output[0, :, 0]
                    image = image.transpose(0, 1).transpose(1, 2)
                    image = (image * 255).numpy().astype(np.uint8)
                    image = PILImage.fromarray(image)
                    image.save(video_path)
                    prt(Row([ImageButton(src=video_path, width=pr['width'], height=pr['height'], data=video_path, page=page)], alignment=MainAxisAlignment.CENTER))
                else:
                    gif_path = available_file(batch_output, fname, no_num=True, ext="gif")
                    save_videos_grid(output, gif_path, fps=easyanimate_prefs['fps'])
                    if os.path.isfile(gif_path):
                        prt(Row([ImageButton(src=gif_path, width=pr['width'], height=pr['height'], data=gif_path, page=page)], alignment=MainAxisAlignment.CENTER))
                    video_path = available_file(batch_output, fname, no_num=True, ext="mp4")
                    save_videos_grid(output, video_path, fps=easyanimate_prefs['fps'])
                    if not os.path.isfile(video_path):
                        prt(f"Problem creating video file, but frames still saved...")
                    else:
                        prt(Markdown(f"Video saved to [{video_path}]({filepath_to_url(video_path)})", on_tap_link=lambda e: e.page.launch_url(e.data)))
            except Exception as e:
                alert_msg(page, f"ERROR: Something went wrong saving video frames...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            #gif_file = available_file(batch_output, fname, no_num=True, ext="gif")
            #export_to_gif(frames_batch, gif_file, fps=easyanimate_prefs['fps'])
            '''if easyanimate_prefs['export_to_video']:
                try:
                    installer = Installing("Running Google FILM: Frame Interpolation for Large Motion...")
                    prt(installer)
                    out_file = available_file(batch_output, fname, no_num=True, ext="mp4")
                    if easyanimate_prefs['interpolate_video']:
                        interpolate_video(frames_dir, input_fps=easyanimate_prefs['fps'], output_fps=easyanimate_prefs['target_fps'], output_video=out_file, installer=installer)
                    else:
                        installer.set_message("Saving Frames to Video using FFMPEG with Deflicker...")
                        pattern = create_pattern(new_file) #fname+"-%04d.png"
                        frames_to_video(frames_dir, pattern=pattern, input_fps=easyanimate_prefs['fps'], output_fps=easyanimate_prefs['target_fps'], output_video=out_file, installer=installer, deflicker=True)
                except Exception as e:
                    clear_last()
                    alert_msg(page, f"ERROR: Couldn't interpolate video, but frames still saved...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                    pass'''
            nudge(page.imageColumn if from_list else page.EasyAnimate, page=page)
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_i2vgen_xl(page, from_list=False, with_params=False):
    global i2vgen_xl_prefs, pipe_i2vgen_xl, prefs, status
    if not check_diffusers(page): return
    i2vgen_xl_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            i2vgen_xl_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':i2vgen_xl_prefs['guidance_scale'], 'num_inference_steps':i2vgen_xl_prefs['num_inference_steps'], 'width':i2vgen_xl_prefs['width'], 'height':i2vgen_xl_prefs['height'], 'init_image':i2vgen_xl_prefs['init_image'], 'num_images':i2vgen_xl_prefs['num_images'], 'seed':i2vgen_xl_prefs['seed']})
        else:
            i2vgen_xl_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps':p['steps'], 'width':p['width'], 'height':p['height'], 'init_image':p['init_image'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(i2vgen_xl_prefs['init_image']):
        alert_msg(page, "You must provide an Init Image to process your video generation...")
        return
      i2vgen_xl_prompts.append({'prompt': i2vgen_xl_prefs['prompt'], 'negative_prompt':i2vgen_xl_prefs['negative_prompt'], 'guidance_scale':i2vgen_xl_prefs['guidance_scale'], 'num_inference_steps':i2vgen_xl_prefs['num_inference_steps'], 'width':i2vgen_xl_prefs['width'], 'height':i2vgen_xl_prefs['height'], 'init_image':i2vgen_xl_prefs['init_image'], 'num_images':i2vgen_xl_prefs['num_images'], 'seed':i2vgen_xl_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.I2VGenXL.controls.append(line)
        if update:
          page.I2VGenXL.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.I2VGenXL, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.I2VGenXL.auto_scroll = scroll
        page.I2VGenXL.update()
      else:
        page.I2VGenXL.auto_scroll = scroll
        page.I2VGenXL.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.I2VGenXL.controls = page.I2VGenXL.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = i2vgen_xl_prefs['num_inference_steps']
    def callback_step(pipe, step, timestep, callback_kwargs):
      callback_step.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    #mode = "Video2Video" if i2vgen_xl_prompts[0]['init_image'].endswith('mp4') or i2vgen_xl_prompts[0]['init_image'].endswith('gif') else "Image2Video"
    installer = Installing(f"Installing I2VGen-XL Engine & Models... See console for progress.")
    prt(installer)
    clear_pipes("i2vgen_xl")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    cpu_offload = i2vgen_xl_prefs['cpu_offload']
    i2vgen_xl_model = "ali-vilab/i2vgen-xl" if i2vgen_xl_prefs['i2vgen_xl_model'] == "I2VGen-XL" else i2vgen_xl_prefs['custom_model']
    if 'loaded_i2vgen_xl' not in status: status['loaded_i2vgen_xl'] = ""
    if i2vgen_xl_model != status['loaded_i2vgen_xl']:
        clear_pipes()
    #from diffusers import AutoPipelineForVideo2Video, AutoPipelineForImage2Video, I2VGenXLScheduler
    from diffusers import I2VGenXLPipeline, DDIMScheduler
    from diffusers.utils import export_to_gif, load_image

    if pipe_i2vgen_xl == None:
        installer.status(f"...initialize I2VGen-XL Pipeline")
        try:
            pipe_i2vgen_xl = I2VGenXLPipeline.from_pretrained(i2vgen_xl_model, torch_dtype=torch.float16, variant="fp16", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_i2vgen_xl = AutoPipelineForImage2Video.from_pretrained(i2vgen_xl_model, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_i2vgen_xl.scheduler = DDIMScheduler(beta_schedule="linear", steps_offset=1, clip_sample=False, timestep_spacing="linspace")
            #pipe_i2vgen_xl.scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", clip_sample=False, set_alpha_to_one=False)
            #pipe_i2vgen_xl = pipeline_scheduler(pipe_i2vgen_xl)
            #pipe_i2vgen_xl.scheduler = EulerDiscreteScheduler.from_config(pipe_i2vgen_xl.scheduler.config)
            if prefs['vae_slicing']:
                pipe_i2vgen_xl.enable_vae_slicing()
            if prefs['vae_tiling']:
                pipe_i2vgen_xl.enable_vae_tiling()
            #if i2vgen_xl_prefs['free_init']: #Not yet
            #    installer.status(f"...enable FreeInit")
            #    pipe_i2vgen_xl.enable_free_init(method="butterworth", use_fast_sampling=True)
            if prefs['enable_freeu']:
                installer.status(f"...enable FreeU")
                pipe_i2vgen_xl.enable_freeu(s1=0.6, s2=0.4, b1=1.1, b2=1.2)
            if prefs['enable_torch_compile']:
                installer.status(f"...Torch compiling transformer")
                pipe_i2vgen_xl.transformer = torch.compile(pipe_i2vgen_xl.transformer, mode="reduce-overhead", fullgraph=True)
                pipe_i2vgen_xl = pipe_i2vgen_xl.to(torch_device)
            elif cpu_offload:
                pipe_i2vgen_xl.enable_model_cpu_offload()
            else:
                pipe_i2vgen_xl = pipe_i2vgen_xl.to(torch_device)
            #pipe_i2vgen_xl.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing I2VGen-XL...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            autoscroll(False)
            return
        status['loaded_i2vgen_xl'] = i2vgen_xl_model
    else:
        clear_pipes('i2vgen_xl')
    if len(i2vgen_xl_prefs['lora_map']) > 0:
        installer.status(f"...loading LoRAs")
        adapters = []
        scales = []
        for l in i2vgen_xl_prefs['lora_map']:
            adapters.append(l['name'])
            scales.append(l['scale'])
            weight_args = {}
            if 'weights' in l and bool(l['weights']):
                weight_args['weight_name'] = l['weights']
            pipe_i2vgen_xl.load_lora_weights(l['path'], adapter_name=l['name'], torch_dtype=torch.float16, **weight_args)
        pipe_i2vgen_xl.set_adapters(adapters, adapter_weights=scales)
    clear_last()
    s = "" if len(i2vgen_xl_prompts) == 1 else "s"
    prt(f"Generating your I2VGen-XL Video{s}... See console for progress.")
    for pr in i2vgen_xl_prompts:
        prt(progress)
        nudge(page.imageColumn if from_list else page.I2VGenXL, page)
        autoscroll(False)
        total_steps = pr['num_inference_steps']
        random_seed = get_seed(pr['seed'])
        generator = torch.Generator("cpu").manual_seed(random_seed)
        init_img = None
        fname = os.path.basename(pr['init_image'])
        if pr['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
        else:
            if os.path.isfile(pr['init_image']):
                init_img = PILImage.open(pr['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init image {pr['init_image']}")
                return
        init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
        init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        try:
            output = pipe_i2vgen_xl(
                prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                image=init_img,
                num_frames=i2vgen_xl_prefs['video_length'],
                num_videos_per_prompt=pr['num_images'],
                target_fps=i2vgen_xl_prefs['fps'],
                height=pr['height'],
                width=pr['width'],
                num_inference_steps=pr['num_inference_steps'],
                guidance_scale=pr['guidance_scale'],
                clip_skip=i2vgen_xl_prefs['clip_skip'],
                generator=generator,
                #callback_on_step_end=callback_step,
            ).frames
        except Exception as e:
            clear_last(2)
            alert_msg(page, f"ERROR: Something went wrong generating video...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        #clear_last()
        clear_last()
        autoscroll(True)
        if output is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        batch_output = os.path.join(prefs['image_output'], i2vgen_xl_prefs['batch_folder_name'])
        makedir(batch_output)
        for v, frames_batch in enumerate(output):
            fname = format_filename(pr['prompt'])
            frames_dir = available_folder(batch_output, "frames", v)
            makedir(frames_dir)
            for idx, image in enumerate(frames_batch):
                #fname = f"{i2vgen_xl_prefs['file_prefix']}{format_filename(i2vgen_xl_prefs['batch_folder_name'])}-{b}"
                image_path = available_file(frames_dir, "frame", idx, zfill=4)
                image.save(image_path)
                new_file = os.path.basename(image_path)
                if not i2vgen_xl_prefs['display_upscaled_image'] or not i2vgen_xl_prefs['apply_ESRGAN_upscale']:
                    #prt(Row([Img(src=image_path, width=i2vgen_xl_prefs['width'], height=i2vgen_xl_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                    prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
                if i2vgen_xl_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    upscale_image(image_path, image_path, scale=i2vgen_xl_prefs["enlarge_scale"], face_enhance=i2vgen_xl_prefs["face_enhance"])
                    if i2vgen_xl_prefs['display_upscaled_image']:
                        time.sleep(0.6)
                        prt(Row([Img(src=asset_dir(image_path), width=int(pr['width'] * float(i2vgen_xl_prefs["enlarge_scale"])), height=int(pr['height'] * float(i2vgen_xl_prefs["enlarge_scale"])), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
            gif_file = available_file(batch_output, fname, no_num=True, ext="gif")
            export_to_gif(frames_batch, gif_file, fps=i2vgen_xl_prefs['fps'])
            prt(Row([ImageButton(src=gif_file, width=pr['width'], height=pr['height'], data=gif_file, page=page)], alignment=MainAxisAlignment.CENTER))
            if i2vgen_xl_prefs['export_to_video']:
                try:
                    installer = Installing("Running Google FILM: Frame Interpolation for Large Motion...")
                    prt(installer)
                    out_file = available_file(batch_output, fname, no_num=True, ext="mp4")
                    if i2vgen_xl_prefs['interpolate_video']:
                        interpolate_video(frames_dir, input_fps=i2vgen_xl_prefs['fps'], output_fps=i2vgen_xl_prefs['target_fps'], output_video=out_file, installer=installer)
                    else:
                        installer.set_message("Saving Frames to Video using FFMPEG with Deflicker...")
                        pattern = create_pattern(new_file) #fname+"-%04d.png"
                        frames_to_video(frames_dir, pattern=pattern, input_fps=i2vgen_xl_prefs['fps'], output_fps=i2vgen_xl_prefs['target_fps'], output_video=out_file, installer=installer, deflicker=True)
                except Exception as e:
                    clear_last()
                    alert_msg(page, f"ERROR: Couldn't interpolate video, but frames still saved...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                    pass
                clear_last()
                if not os.path.isfile(out_file):
                    prt(f"Problem creating video file, but frames still saved...")
                else:
                    prt(Markdown(f"Video saved to [{out_file}]({filepath_to_url(out_file)})", on_tap_link=lambda e: e.page.launch_url(e.data)))
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_hotshot_xl(page):
    global hotshot_xl_prefs, prefs, status, pipe_hotshot_xl, model_path
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.HotshotXL.controls.append(line)
      page.HotshotXL.update()
    def clear_last(lines=1):
      clear_line(page.HotshotXL, lines=lines)
    def clear_list():
      page.HotshotXL.controls = page.HotshotXL.controls[:1]
    def autoscroll(scroll=True):
      page.HotshotXL.auto_scroll = scroll
      page.HotshotXL.update()
    progress = ProgressBar(bar_height=8)
    total_steps = hotshot_xl_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Hotshot-XL Text-To-Video Pipeline...")
    prt(installer)
    #model_id = "damo-vilab/text-to-video-ms-1.7b"
    clear_pipes()
    hotshot_xl_dir = os.path.join(root_dir, "Hotshot-XL")
    if not os.path.exists(hotshot_xl_dir):
        installer.status("...hotshotco/Hotshot-XL")
        run_sp("git clone https://github.com/hotshotco/Hotshot-XL", realtime=False, cwd=root_dir)
        #run_sp("git clone https://github.com/Skquark/Hotshot-XL", realtime=False, cwd=root_dir)
    try:
        pip_install("appdirs==1.4.4 certifi==2023.7.22 charset-normalizer==3.3.0 click==8.1.7 cmake decorator==4.4.2 docker-pycreds==0.4.0 einops filelock==3.12.4 fsspec==2023.9.2 gitdb==4.0.10 GitPython==3.1.37 idna==3.4 imageio imageio-ffmpeg importlib-metadata==6.8.0 Jinja2==3.1.3 lit==17.0.2 MarkupSafe==2.1.3 moviepy mpmath==1.3.0 networkx==3.1 numpy pathtools proglog==0.1.10 protobuf==4.24.3 psutil PyYAML regex safetensors sentry-sdk==1.31.0 setproctitle==1.3.3 six==1.16.0 smmap==5.0.1 sympy==1.12 tokenizers==0.14.0 tqdm transformers triton typing_extensions urllib3 wandb zipp==3.17.0", installer=installer)
        #pip_install("nvidia-cublas-cu11==11.10.3.66 nvidia-cuda-cupti-cu11==11.7.101 nvidia-cuda-nvrtc-cu11==11.7.99 nvidia-cuda-runtime-cu11==11.7.99 nvidia-cudnn-cu11==8.5.0.96 nvidia-cufft-cu11==10.9.0.58 nvidia-curand-cu11==10.2.10.91 nvidia-cusolver-cu11==11.4.0.1 nvidia-cusparse-cu11==11.7.4.91 nvidia-nccl-cu11==2.14.3 nvidia-nvtx-cu11==11.7.91")
        run_sp("apt-get install git-lfs", realtime=False)
        run_sp("git lfs install", realtime=False)
        #installer.status("...huggingface hotshotco/Hotshot-XL")
        #run_sp("git clone https://huggingface.co/hotshotco/Hotshot-XL", realtime=False, cwd=root_dir)
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR: Hotshot-XL Text-To-Video requirements failed for some reason...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    try:
        import RealESRGAN
    except:
        installer.status("...istalling Real-ESRGAN")
        run_sp("pip install git+https://github.com/sberbank-ai/Real-ESRGAN.git", realtime=False)
        pass
    hotshot_lora = os.path.join(hotshot_xl_dir, 'lora')
    hotshot_input = os.path.join(hotshot_xl_dir, 'input')
    hotshot_output = os.path.join(hotshot_xl_dir, 'output')
    os.makedirs(hotshot_lora, exist_ok=True)
    os.makedirs(hotshot_input, exist_ok=True)
    os.makedirs(hotshot_output, exist_ok=True)
    #hsxl = "hsxl_temporal_layers.f16.safetensors"
    #hotshot_model = os.path.join(hotshot_xl_dir, hsxl)
    #if not os.path.isfile(hotshot_model):
    #    installer.status("...downloading hsxl_temporal_layers")
    #    hotshot_model = download_file("https://huggingface.co/hotshotco/Hotshot-XL/resolve/main/hsxl_temporal_layers.f16.safetensors?download=true", to=hotshot_xl_dir, filename=hsxl)
    lora = hotshot_xl_prefs['lora_layer']
    lora_path = ""
    if lora != "None":
        lora_model = get_SDXL_LoRA_model(lora_model)
        installer.status(f"...getting LoRA {lora_model['name']}")
        if lora == "Custom":
            lora_model['name'] = "Custom SDXL LoRA"
            lora_model['path'] = hotshot_xl_prefs['custom_lora_layer']
            if lora_model['path'].count('/') == 1:
                from huggingface_hub import HfFileSystem
                fs = HfFileSystem()
                files = fs.ls(lora_model['path'], detail=False)
                lora_model['weights'] = [file.rpartition('/')[2] for file in files if file.endswith(".safetensors")]
        if lora_model['path'].count('/') == 1:
            lora_path = download_file(f"https://huggingface.co/{lora_model['path']}/blob/main/{lora_model['weights']}", to=hotshot_lora)
        elif os.path.isfile(lora_model['path']):
            lora_path = lora_model['path']
        elif lora_model['path'].startswith("http"):
            lora_path = download_file(lora_model['path'], to=hotshot_lora, ext="safetensors")
        else:
            print(f"Couldn't download LoRA {lora_model['path']}")
    
    local_output = os.path.join(stable_dir, hotshot_xl_prefs['batch_folder_name'])
    if not os.path.isdir(local_output):
        os.makedirs(local_output)
    batch_output = os.path.join(prefs['image_output'], hotshot_xl_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
        os.makedirs(batch_output)
    width = hotshot_xl_prefs['width']
    height = hotshot_xl_prefs['height']
    filename = format_filename(hotshot_xl_prefs["prompt"])
    out_file = f"{filename}.{'mp4' if hotshot_xl_prefs['export_to_video'] else 'gif'}"
    x = " --xformers" if status['installed_xformers'] else ""
    lora_arg = f' --lora "lora/{os.path.basename(lora_path)}"' if bool(lora_path) else ""
    upscale = f' --upscale {int(hotshot_xl_prefs["enlarge_scale"])}' if hotshot_xl_prefs["apply_ESRGAN_upscale"] else ''
    scale = int(hotshot_xl_prefs["enlarge_scale"]) if hotshot_xl_prefs["apply_ESRGAN_upscale"] else 1
    if bool(hotshot_xl_prefs["gif"]):
        if os.path.isfile(hotshot_xl_prefs["gif"]):
            shutil.copy(hotshot_xl_prefs["gif"], os.path.join(hotshot_input, os.path.basename(hotshot_xl_prefs["gif"])))
    gif = f' --control_type {hotshot_xl_prefs["controlnet_type"].lower()} --controlnet_conditioning_scale {hotshot_xl_prefs["conditioning_scale"]} --control_guidance_start {hotshot_xl_prefs["control_guidance_start"]} --control_guidance_end {hotshot_xl_prefs["control_guidance_end"]} --gif "input/{os.path.basename(hotshot_xl_prefs["gif"])}"' if bool(hotshot_xl_prefs["gif"]) else ''
    clear_last()
    for num in range(hotshot_xl_prefs["num_images"]):
        random_seed = get_seed(int(hotshot_xl_prefs['seed']) + num)
        prt("Generating Hotshot-XL of your Prompt...")
        prt(progress)
        autoscroll(False)
        try: # --pretrained_path "{os.path.basename(hotshot_model)}"
            run_sp(f'python inference.py --prompt "{hotshot_xl_prefs["prompt"]}" --negative_prompt "{hotshot_xl_prefs["negative_prompt"]}" --width {width} --height {height} --seed {random_seed} --steps {hotshot_xl_prefs["num_inference_steps"]} --video_length {hotshot_xl_prefs["video_length"]} --video_duration {hotshot_xl_prefs["video_duration"]} --scheduler {hotshot_xl_prefs["scheduler"]}{gif}{" --low_vram_mode" if not prefs["higher_vram_mode"] else ""}{x}{lora_arg}{upscale} --output "output/{out_file}"', realtime=True, cwd=hotshot_xl_dir)
            #print(f"prompt={hotshot_xl_prefs['prompt']}, negative_prompt={hotshot_xl_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={hotshot_xl_prefs['edit_momentum_scale']}, edit_mom_beta={hotshot_xl_prefs['edit_mom_beta']}, num_inference_steps={hotshot_xl_prefs['num_inference_steps']}, eta={hotshot_xl_prefs['eta']}, guidance_scale={hotshot_xl_prefs['guidance_scale']}")
        except Exception as e:
            clear_last()
            clear_last()
            alert_msg(page, f"ERROR: Hotshot-XL Text-To-Video failed for some reason. Possibly out of memory or something wrong with the code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        clear_last()
        clear_last()
        autoscroll(True)
        video_path = os.path.join(hotshot_output, out_file)
        if not os.path.isfile(video_path):
            prt(f"Problem creating file {video_path}")
            return
        output_path = os.path.join(prefs['image_output'], rerender_a_video_prefs['batch_folder_name'])
        if not os.path.exists(output_path):
            os.makedirs(output_path)
        output_file = available_file(output_path, out_file, 0, ext="mp4" if hotshot_xl_prefs['export_to_video'] else 'gif', no_num=True)
        shutil.copy(video_path, output_file)
        if hotshot_xl_prefs['export_to_video']:
            prt(f"Saved Video file to {output_file}")
        else:
            prt(Row([ImageButton(src=output_file, data=output_file, width=width * scale, height=height * scale, show_subtitle=True, page=page)], alignment=MainAxisAlignment.CENTER))
    #prt(Row([VideoContainer(video_path)], alignment=MainAxisAlignment.CENTER))
    prt(f"Done creating video... Check {batch_output}")
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_rerender_a_video(page):
    global rerender_a_video_prefs, status
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.Rerender_a_video.controls.append(line)
      page.Rerender_a_video.update()
    def clear_last(lines=1):
      clear_line(page.Rerender_a_video, lines=lines)
    def autoscroll(scroll=True):
        page.Rerender_a_video.auto_scroll = scroll
        page.Rerender_a_video.update()
    if not bool(rerender_a_video_prefs['init_video']):
        alert_msg(page, "You must provide a target init video...")
        return
    if not bool(rerender_a_video_prefs['prompt']):
        alert_msg(page, "You must provide an interesting prompt to guide the video...")
        return
    if not bool(rerender_a_video_prefs['batch_folder_name']):
        alert_msg(page, "You must give a unique Batch Folder Name to save to...")
        return
    page.Rerender_a_video.controls = page.Rerender_a_video.controls[:1]
    autoscroll()
    installer = Installing("Installing Rerender-a-Video Libraries...")
    prt(installer)
    rerender_a_video_dir = os.path.join(root_dir, "Rerender_A_Video")
    if not os.path.exists(rerender_a_video_dir):
        try:
            installer.status("...cloning williamyang1991/Rerender_A_Video")
            run_sp("git clone https://github.com/williamyang1991/Rerender_A_Video.git --recursive", cwd=root_dir, realtime=False)
            installer.status("...installing Rerender_A_Video requirements")
            #run_sp("pip install -r requirements.txt", realtime=True) #pytorch-lightning==1.5.0
            pip_install("addict==2.4.0 albumentations==1.4.10 basicsr==1.4.2 blendmodes einops gradio imageio imageio-ffmpeg invisible-watermark kornia==0.6 numba omegaconf open_clip_torch prettytable==3.6.0 pytorch-lightning safetensors streamlit==1.12.1 streamlit-drawable-canvas==0.8.0 test-tube==0.7.5 timm torchmetrics transformers webdataset yapf==0.32.0 watchdog", installer=installer, upgrade=True)
            installer.status("...downloading SD models")
            run_sp("python install.py", cwd=rerender_a_video_dir, realtime=True)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Rerender_A_Video Requirements:", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    #import logging
    #logging.set_verbosity_error()
    clear_pipes()

    from PIL import ImageOps
    if bool(rerender_a_video_prefs['output_name']):
        fname = format_filename(rerender_a_video_prefs['output_name'], force_underscore=True)
    elif bool(rerender_a_video_prefs['prompt']):
        fname = format_filename(rerender_a_video_prefs['prompt'], force_underscore=True)
    elif bool(rerender_a_video_prefs['batch_folder_name']):
        fname = format_filename(rerender_a_video_prefs['batch_folder_name'], force_underscore=True)
    else: fname = "output"
    if bool(rerender_a_video_prefs['file_prefix']):
        fname = f"{rerender_a_video_prefs['file_prefix']}{fname}"
    video_dir = os.path.join(rerender_a_video_dir, "videos")
    #if bool(rerender_a_video_prefs['batch_folder_name']):
    #    batch_output = os.path.join(stable_dir, rerender_a_video_prefs['batch_folder_name'])
    #else: batch_output = stable_dir
    #if not os.path.exists(batch_output):
    #    os.makedirs(batch_output)
    output_path = os.path.join(prefs['image_output'], rerender_a_video_prefs['batch_folder_name'])
    makedir(output_path)
    init_vid = rerender_a_video_prefs['init_video']
    if init_vid.startswith('http'):
        init_vid = download_file(init_vid, uploads_dir, ext="mp4")
    else:
        if not os.path.isfile(init_vid):
            alert_msg(page, f"ERROR: Couldn't find your init_video {init_vid}")
            return
    video_file = os.path.basename(init_vid)
    if not video_file.endswith("mp4"):
        video_file += ".mp4"
    shutil.copy(init_vid, os.path.join(video_dir, video_file))
    video_out_path = os.path.join(video_dir, rerender_a_video_prefs['batch_folder_name'])
    makedir(video_out_path)
    lora_dir = os.path.join(rerender_a_video_dir, 'models')
    makedir(lora_dir)
    lora_path = ""
    if rerender_a_video_prefs['dreambooth_lora'] == "Custom":
        lora = rerender_a_video_prefs['custom_lora']
        if lora.startswith("http"):
            installer.status(f"...downloading Custom LoRA")
            lora_file = download_file(lora_model['path'], to=lora_dir, ext="safetensors")
            if os.path.isfile(lora_file):
                lora_path = lora_file
        else:
            if os.path.isfile(lora):
                fname = os.path.basename(lora)
                lora_path = os.path.join(lora_dir, fname)
                shutil.copy(lora, lora_path)
    else:
        for lora in animate_diff_loras:
            if lora['name'] == rerender_a_video_prefs['dreambooth_lora']:
                lora_model = lora
                break
        lora_path = os.path.join(lora_dir, lora_model['file'])
        if not os.path.isfile(lora_path):
            installer.status(f"...downloading {lora_model['name']}")
            download_file(lora_model['path'], to=lora_dir, ext="safetensors")
    sd_model = os.path.basename(lora_path)
    installer.status("...preparing json")
    random_seed = get_seed(rerender_a_video_prefs['seed'])
    '''total_steps = rerender_a_video_prefs['steps']
    def callback_fnc(step: int) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}"
      progress.update()'''
    output_file = available_file(output_path, fname, 0, ext='mp4', no_num=True)
    config_json_file = os.path.join(rerender_a_video_dir, "config", "sdd_rerender.json")
    config_json = {
        "input": f"videos/{video_file}",
        "output": f"videos/{rerender_a_video_prefs['batch_folder_name']}/{os.path.basename(output_file)}",
        "work_dir": f"videos/{rerender_a_video_prefs['batch_folder_name']}",
        "key_subdir": "keys",
        "sd_model": f"models/{sd_model}", #realisticVisionV20_v20.safetensors",
        "frame_count": rerender_a_video_prefs['frame_count'],
        "interval": rerender_a_video_prefs['interval'],
        "crop": [
            rerender_a_video_prefs['crop']['left'],
            rerender_a_video_prefs['crop']['right'],
            rerender_a_video_prefs['crop']['top'],
            rerender_a_video_prefs['crop']['bottom']
        ],
        "prompt": rerender_a_video_prefs['prompt'],
        "a_prompt": rerender_a_video_prefs['a_prompt'],#"RAW photo, subject, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3",
        "n_prompt": rerender_a_video_prefs['negative_prompt'],
        "x0_strength": rerender_a_video_prefs['x0_strength'],
        "control_type": rerender_a_video_prefs['control_task'] if rerender_a_video_prefs['control_task'] == "HED" else rerender_a_video_prefs['control_task'].lower(),
        "canny_low": rerender_a_video_prefs['low_threshold'],
        "canny_high": rerender_a_video_prefs['high_threshold'],
        "control_strength": rerender_a_video_prefs['controlnet_strength'],
        "style_update_freq": rerender_a_video_prefs['style_update_freq'],
        "loose_cfattn": rerender_a_video_prefs['loose_cfattn'],
        "inner_strength": rerender_a_video_prefs['inner_strength'],
        "smooth_boundary": rerender_a_video_prefs['smooth_boundary'],
        "color_preserve": rerender_a_video_prefs['color_preserve'],
        "loose_cfattn": rerender_a_video_prefs['loose_cfattn'],
        "seed": random_seed,
        "image_resolution": rerender_a_video_prefs['max_size'],
        "warp_period": [0, 0.1],
        "ada_period": [0.8, 1],
    }
    if rerender_a_video_prefs['enable_freeu']:
        config_json['freeu_args'] = [rerender_a_video_prefs['freeu_args']['b1'], rerender_a_video_prefs['freeu_args']['b2'], rerender_a_video_prefs['freeu_args']['s1'], rerender_a_video_prefs['freeu_args']['s2']]
    with open(config_json_file, "w") as outfile:
        json.dump(config_json, outfile, indent=4)
    #output_file = os.path.join(output_path, f"{fname}{'.mp4' if is_video else '.png'}")
    if not os.path.exists(config_json_file):
        print(f"Error creating json file {config_json_file}")
    clear_last()
    progress = ProgressBar(bar_height=8)
    prt(f"Generating your Rerender-a-Video...")
    prt(progress)
    autoscroll(False)
    cmd = f'python rerender.py --cfg config/sdd_rerender.json -{"nb" if rerender_a_video_prefs["first_frame"] else "nr"}'
    w = 0
    h = 0
    img_idx = 0
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
    class Handler(FileSystemEventHandler):
      def __init__(self):
        super().__init__()
      def on_created(self, event):
        nonlocal img_idx, w, h
        if event.is_directory:
          return None
        elif event.event_type == 'created' and event.src_path.endswith("png"):
          autoscroll(True)
          if w == 0:
            time.sleep(0.8)
            try:
              frame = PILImage.open(event.src_path)
              w, h = frame.size
              clear_last()
            except Exception:
              pass
          clear_last()
          if rerender_a_video_prefs['save_frames']:
            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])
          else:
            fpath = event.src_path
          #prt(Divider(height=6, thickness=2))
          prt(Row([ImageButton(src=event.src_path, data=fpath, width=w, height=h, subtitle=f"Frame {img_idx} - {event.src_path}", center=True, page=page)], alignment=MainAxisAlignment.CENTER))
          prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))
          page.update()
          prt(progress)
          if rerender_a_video_prefs['save_frames']:
            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])
            shutil.copy(event.src_path, fpath)
          time.sleep(0.2)
          autoscroll(False)
          img_idx += 1
        elif event.event_type == 'created' and (event.src_path.endswith("mp4") or event.src_path.endswith("avi")):
          autoscroll(True)
          #clear_last()
          prt(Divider(height=6, thickness=2))
          fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])
          time.sleep(1)
          shutil.copy(event.src_path, fpath)
          prt(Markdown(f"Video saved to [{fpath}]({filepath_to_url(fpath)}) from {event.src_path}", on_tap_link=lambda e: e.page.launch_url(e.data)))
          #prt(Row([VideoContainer(event.src_path)], alignment=MainAxisAlignment.CENTER))
          #prt(Row([VideoPlayer(video_file=event.src_path, width=w, height=h)], alignment=MainAxisAlignment.CENTER))
          #prt(Row([ImageButton(src=event.src_path, data=fpath, width=w, height=h, subtitle=f"Frame {img_idx} - {event.src_path}", center=True, page=page)], alignment=MainAxisAlignment.CENTER))
          #prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))
          #page.update()Image
          #prt(progress)
          time.sleep(0.2)
          autoscroll(False)
    image_handler = Handler()
    observer = Observer()
    observer.schedule(image_handler, video_out_path, recursive=True)
    observer.start()
    #prt(f"Running {cmd}")
    #prt(progress)
    try:
        #os.system(f'cd {rerender_a_video_dir};{cmd}')
        #if is_Colab:
        #  os.chdir(rerender_a_video_dir)
        #  $cmd
        #  os.chdir(root_dir)
        #else:
        #TODO: Parse output to get percent current for progress callback_fnc
        run_sp(cmd, cwd=rerender_a_video_dir, realtime=True)
    except Exception as e:
        clear_last()
        observer.stop()
        alert_msg(page, "Error running rerender.py!", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    #clear_last()
    observer.stop()
    #clear_last()
    autoscroll(True)
    #TODO: Upscale Image
    if os.path.isfile(output_file):
        prt(f"Saved video to {output_file}")
        #prt(Row([VideoContainer(output_file)], alignment=MainAxisAlignment.CENTER))
        #prt(Row([VideoPlayer(video_file=output_file, width=width, height=height)], alignment=MainAxisAlignment.CENTER))
    else:
        prt("Error Generating Output File! Maybe NSFW Image detected or Out of Memory?")
    prt(Row([Text(output_file)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_fresco(page):
    global fresco_prefs, prefs, status, model_path
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.Fresco.controls.append(line)
      page.Fresco.update()
    def clear_last(lines=1):
      clear_line(page.Fresco, lines=lines)
    def clear_list():
      page.Fresco.controls = page.Fresco.controls[:1]
    def autoscroll(scroll=True):
      page.Fresco.auto_scroll = scroll
      page.Fresco.update()
    progress = ProgressBar(bar_height=8)
    total_steps = fresco_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    installer = Installing("Installing FRESCO Video-To-Video Pipeline...")
    prt(installer)
    #model_id = "damo-vilab/text-to-video-ms-1.7b"
    clear_pipes()
    fresco_dir = os.path.join(root_dir, "FRESCO")
    if not os.path.exists(fresco_dir):
        installer.status("...cloning williamyang1991/FRESCO") #XmYx/Fresco
        run_sp("git clone https://github.com/williamyang1991/FRESCO.git", realtime=False, cwd=root_dir)
    #data_dir = os.path.join(fresco_dir, "data")
    pip_install("einops opencv-python|cv2 timm matplotlib av kornia basicsr==1.4.2 numba==0.57.0 imageio-ffmpeg gradio", installer=installer)
    if fresco_dir not in sys.path:
        sys.path.append(fresco_dir)
    try:
        from webUI import process1, process2
    except Exception:
        try:
            from FRESCO.webUI import process1, process2
            print(f"Importing FRESCO.webUI")
            pass
        except Exception:
            alert_msg(page,"ERROR Importing FRESCO webUI process. Working on it...")
            return
    installer.status("...downloading Models & Ebsynth")
    os.chdir(fresco_dir)
    run_sp("python install.py", realtime=False, cwd=fresco_dir)
    clear_last()
    progressbar = Progress("Generating FRESCO of your Video... See console for progress.")
    prt(progressbar)
    autoscroll(False)
    batch_output = os.path.join(prefs['image_output'], fresco_prefs['batch_folder_name'])
    makedir(batch_output)
    #data_folder = format_filename(fresco_prefs['batch_folder_name'], use_dash=True)
    #makedir(os.path.join(data_dir, data_folder))
    init_vid = fresco_prefs['init_video']
    if init_vid.startswith('http'):
        progressbar.status("...Downloading Video")
        init_vid = download_file(init_vid, uploads_dir, ext="mp4")
    else:
        if not os.path.isfile(init_vid):
            alert_msg(page, f"ERROR: Couldn't find your init_video {init_vid}")
            return
    video_file = os.path.basename(init_vid)
    if not video_file.endswith("mp4"):
        video_file += ".mp4"
    #progressbar.status("...Scaling Video")
    #w, h = scale_video(init_vid, os.path.join(data_dir, data_folder, video_file), fresco_prefs["max_size"])
    progressbar.status("...Preparing Run")
    sd_model = fresco_models[fresco_prefs['sd_model']]
    random_seed = get_seed(fresco_prefs['seed'])
    inputs = {
        'input_path':video_file,
        'prompt':fresco_prefs['prompt'],
        'sd_model':sd_model,
        'seed':random_seed,
        'image_resolution':fresco_prefs['image_resolution'],
        'control_strength':fresco_prefs['control_strength'],
        'x0_strength':fresco_prefs['x0_strength'],
        'control_type':fresco_prefs['control_type'].lower(),
        'low_threshold':fresco_prefs['low_threshold'],
        'high_threshold':fresco_prefs['high_threshold'],
        'ddpm_steps':fresco_prefs['num_inference_steps'],
        'scale':fresco_prefs['guidance_scale'],
        'a_prompt':fresco_prefs['a_prompt'],
        'n_prompt':fresco_prefs['n_prompt'],
        'frame_count':fresco_prefs['frame_count'],
        'batch_size':fresco_prefs['batch_size'],
        'mininterv':fresco_prefs['mininterv'],
        'maxinterv':fresco_prefs['maxinterv'],
        'use_constraints': [c.lower() for c in fresco_prefs['use_constraints']],
        'bg_smooth':fresco_prefs['bg_smooth'],
        'use_poisson':fresco_prefs['use_poisson'],
        'max_process':fresco_prefs['max_process'],
        'b1':fresco_prefs['b1'], 'b2':fresco_prefs['b2'], 's1':fresco_prefs['s1'], 's2':fresco_prefs['s2']
    }
    try:
        progressbar.status("...Preprocessing Video Keyframes")
        result_keyframe = process1(**inputs)
        progressbar.status("...Processing Full Video Interpolation")
        output_path = process2(**inputs)
        '''run_sp(f'python {preprocess_cmd}', cwd=fresco_dir)
               #-W {width} -H {height} -o {batch_output} -d cuda{x}{rw} -s {fresco_prefs["num_inference_steps"]} -g {fresco_prefs["guidance_scale"]} -f {fresco_prefs["fps"]} -T {fresco_prefs["num_frames"]}', cwd=data_dir)
        progressbar.status(f"...Processing Fresco {selected_mode}")
        run_sp(f'python {run_cmd}', cwd=fresco_dir, realtime=True)'''
    except Exception as e:
        clear_last()
        #fresco_prefs['run_cmd'] = f'{preprocess_cmd} -&- {run_cmd}'
        alert_msg(page, f"ERROR: Fresco Video-To-Video failed for some reason. Possibly out of memory or something wrong with the code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]), debug_pref=fresco_prefs)
        os.chdir(root_dir)
        return
    os.chdir(root_dir)
    clear_last()
    autoscroll(True)
    filename = f"{format_filename(fresco_prefs['prompt'])}"
    #filename = filename[:int(prefs['file_max_length'])]
    #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
    autoscroll(True)
    #output_path = os.path.join(fresco_dir, "fresco-results", "fresco_PnP.mp4")
    keyframe_path = available_file(batch_output, filename+"-keyframes", ext="mp4", no_num=True)
    video_path = available_file(batch_output, filename, ext="mp4", no_num=True)
    if os.path.exists(output_path):
        shutil.move(result_keyframe, keyframe_path)
        shutil.move(output_path, video_path)
        prt(Row([Markdown(f"Done creating video... Check {and_list([filepath_to_url(keyframe_path), filepath_to_url(video_path)])}", on_tap_link=lambda e: e.page.launch_url(e.data))], alignment=MainAxisAlignment.CENTER))
        prt(Row([VideoContainer(video_path)], alignment=MainAxisAlignment.CENTER))
    else:
        prt("Something went wrong generating video...")
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_fresco_v2v(page):
    global fresco_v2v_prefs, prefs, status, pipe_fresco_v2v, model_path
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.Fresco.controls.append(line)
      page.Fresco.update()
    def clear_last(lines=1):
      clear_line(page.Fresco, lines=lines)
    def clear_list():
      page.Fresco.controls = page.Fresco.controls[:1]
    def autoscroll(scroll=True):
      page.Fresco.auto_scroll = scroll
      page.Fresco.update()
    progress = ProgressBar(bar_height=8)
    total_steps = fresco_v2v_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    installer = Installing("Installing FRESCO Video-To-Video Pipeline...")
    prt(installer)
    #model_id = "damo-vilab/text-to-video-ms-1.7b"
    clear_pipes()
    gmflow_dir = os.path.join(root_dir, "gmflow")
    if not os.path.exists(gmflow_dir):
        installer.status("...cloning haofeixu/gmflow") #XmYx/Fresco
        run_sp("git clone https://github.com/haofeixu/gmflow.git", realtime=False, cwd=root_dir)
    #data_dir = os.path.join(fresco_v2v_dir, "data")
    pip_install("controlnet-aux einops opencv-python|cv2 imageio-ffmpeg", installer=installer)
    if gmflow_dir not in sys.path:
        sys.path.append(gmflow_dir)
    batch_output = os.path.join(prefs['image_output'], fresco_v2v_prefs['batch_folder_name'])
    #data_folder = format_filename(fresco_v2v_prefs['batch_folder_name'], use_dash=True)
    #makedir(os.path.join(data_dir, data_folder))
    init_vid = fresco_v2v_prefs['init_video']
    if init_vid.startswith('http'):
        installer.status("...Downloading Video")
        init_vid = download_file(init_vid, uploads_dir, ext="mp4")
    else:
        if not os.path.isfile(init_vid):
            alert_msg(page, f"ERROR: Couldn't find your init_video {init_vid}")
            return
    video_file = os.path.basename(init_vid)
    if not video_file.endswith("mp4"):
        video_file += ".mp4"
    #installer.status("...Scaling Video")
    #w, h = scale_video(init_vid, os.path.join(data_dir, data_folder, video_file), fresco_v2v_prefs["max_size"])
    
    from diffusers import ControlNetModel, DDIMScheduler, DiffusionPipeline
    model = get_model(prefs['model_ckpt'])
    sd_model = model['path']
    frames = video_to_frame(video_file, fresco_v2v_prefs['source_fps'])
    control_frames = []
    installer.status(f"...Preparing Frames with {fresco_v2v_prefs['control_type']}")
    total_frames = len(frames)
    if fresco_v2v_prefs['control_type'].lower() == "canny":
        for f, frame in enumerate(frames):
            installer.progress.value = f / total_frames
            installer.progress.update()
            image = cv2.Canny(frame, fresco_v2v_prefs['low_threshold'], fresco_v2v_prefs['high_threshold'])
            np_image = np.array(image)
            np_image = np_image[:, :, None]
            np_image = np.concatenate([np_image, np_image, np_image], axis=2)
            canny_image = PILImage.fromarray(np_image)
            control_frames.append(canny_image)
        controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny").to('cuda')
    elif fresco_v2v_prefs['control_type'].lower() == "depth":
        from transformers import DPTFeatureExtractor, DPTForDepthEstimation
        depth_estimator = DPTForDepthEstimation.from_pretrained("Intel/dpt-hybrid-midas").to("cuda")
        feature_extractor = DPTFeatureExtractor.from_pretrained("Intel/dpt-hybrid-midas")
        for f, frame in enumerate(frames):
            installer.progress.value = f / total_frames
            installer.progress.update()
            frame = feature_extractor(images=frame, return_tensors="pt").pixel_values.to("cuda")
            with torch.no_grad(), torch.autocast("cuda"):
                depth_map = depth_estimator(frame).predicted_depth
            depth_map = torch.nn.functional.interpolate(
                depth_map.unsqueeze(1),
                size=(1024, 1024),
                mode="bicubic",
                align_corners=False,
            )
            depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)
            depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)
            depth_map = (depth_map - depth_min) / (depth_max - depth_min)
            frame = torch.cat([depth_map] * 3, dim=1)
            frame = frame.permute(0, 2, 3, 1).cpu().numpy()[0]
            depth_image = PILImage.fromarray((frame * 255.0).clip(0, 255).astype(np.uint8))
            control_frames.append(depth_image)
        controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-depth").to('cuda')
    elif fresco_v2v_prefs['control_type'].lower() == "hed":
        from controlnet_aux import HEDdetector
        hed = HEDdetector.from_pretrained('lllyasviel/Annotators')
        for f, frame in enumerate(frames):
            installer.progress.value = f / total_frames
            installer.progress.update()
            hed_image = hed(frame, safe=True)
            control_frames.append(hed_image)
        controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-hed").to('cuda')
    installer.progress.value = None
    installer.progress.update()
    if sd_model != status['loaded_model'] or status['loaded_controlnet'] != fresco_v2v_prefs['control_type'].lower():
      clear_pipes()
    else:
      clear_pipes("fresco")
      
    if pipe_fresco_v2v is None:
        installer.status(f"...Initializing Pipeline with {model['name']}")
        pipe_fresco_v2v = DiffusionPipeline.from_pretrained(model_path, controlnet=controlnet, custom_pipeline='fresco_v2v').to('cuda')
        status['loaded_model'] = sd_model
        status['loaded_controlnet'] = fresco_v2v_prefs['control_type'].lower()
        pipe_fresco_v2v = pipeline_scheduler(pipe_fresco_v2v)
        #pipe_fresco_v2v.scheduler = DDIMScheduler.from_config(pipe_fresco_v2v.scheduler.config)
    else:
        if prefs['scheduler_mode'] != status['loaded_scheduler']:
            pipe_fresco_v2v = pipeline_scheduler(pipe_fresco_v2v)
    ip_adapter_arg = {}
    if fresco_v2v_prefs['use_ip_adapter']:
        installer.status(f"...initialize IP-Adapter")
        ip_adapter_img = None
        if fresco_v2v_prefs['ip_adapter_image'].startswith('http'):
          from io import BytesIO
          i_response = requests.get(fresco_v2v_prefs['ip_adapter_image'])
          ip_adapter_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
        else:
          if os.path.isfile(fresco_v2v_prefs['ip_adapter_image']):
            ip_adapter_img = PILImage.open(fresco_v2v_prefs['ip_adapter_image'])
          else:
            clear_last()
            prt(f"ERROR: Couldn't find your ip_adapter_image {fresco_v2v_prefs['ip_adapter_image']}")
        if bool(ip_adapter_img):
          ip_adapter_arg['ip_adapter_image'] = ip_adapter_img
        if bool(ip_adapter_arg):
            ip_adapter_model = next(m for m in ip_adapter_models if m['name'] == fresco_v2v_prefs['ip_adapter_model'])
            pipe_fresco_v2v.load_ip_adapter(ip_adapter_model['path'], subfolder=ip_adapter_model['subfolder'], weight_name=ip_adapter_model['weight_name'])
            pipe_fresco_v2v.set_ip_adapter_scale(fresco_v2v_prefs['ip_adapter_strength'])
    clear_last()
    progressbar = Progress("Generating FRESCO of your Video... See console for progress.")
    prt(progressbar)
    autoscroll(False)
    random_seed = get_seed(fresco_v2v_prefs['seed'])
    generator = torch.manual_seed(random_seed)
    frames = [PILImage.fromarray(frame) for frame in frames]
    makedir(batch_output)
    try:
        progressbar.status("...Generating Frames")
        output_frames = pipe_fresco_v2v(
            fresco_v2v_prefs['prompt'] + ' ' + fresco_v2v_prefs['a_prompt'],
            frames,
            control_frames,
            negative_prompt=fresco_v2v_prefs['n_prompt'],
            num_inference_steps=fresco_v2v_prefs['num_inference_steps'],
            guidance_scale=fresco_v2v_prefs['guidance_scale'],
            strength=fresco_v2v_prefs['control_strength'],
            controlnet_conditioning_scale=fresco_v2v_prefs['x0_strength'],
            guess_mode=fresco_v2v_prefs['guess_mode'],
            control_guidance_start=fresco_v2v_prefs['control_guidance_start'],
            control_guidance_end=fresco_v2v_prefs['control_guidance_end'],
            end_opt_step=fresco_v2v_prefs['end_opt_step'],#15
            num_intraattn_steps=fresco_v2v_prefs['num_intraattn_steps'],#1,
            step_interattn_end=fresco_v2v_prefs['step_interattn_end'],#350,
            #ip_adapter_image
            generator=generator,
            callback_on_step_end=progress.callback_step,
            **ip_adapter_arg,
        ).images

    except Exception as e:
        clear_last()
        #fresco_v2v_prefs['run_cmd'] = f'{preprocess_cmd} -&- {run_cmd}'
        alert_msg(page, f"ERROR: Fresco Video-To-Video failed for some reason. Possibly out of memory or something wrong with the code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]), debug_pref=fresco_v2v_prefs)
        os.chdir(root_dir)
        return
    os.chdir(root_dir)
    
    filename = f"{format_filename(fresco_v2v_prefs['prompt'])}"
    #filename = filename[:int(prefs['file_max_length'])]
    #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
    clear_last()
    autoscroll(True)
    installer = Installing("Saving Video File...")
    prt(installer)
    #output_path = os.path.join(fresco_v2v_dir, "fresco_v2v-results", "fresco_v2v_PnP.mp4")
    #keyframe_path = available_file(batch_output, filename+"-keyframes", ext="mp4", no_num=True)
    video_path = available_file(batch_output, filename, ext="mp4", no_num=True)
    out_file = available_file(batch_output, filename+"-interpolated", ext="mp4", no_num=True)
    if fresco_v2v_prefs['save_frames']:
        installer.status("...Saving Frames")
        imgs = []
        for i, img in enumerate(output_frames):
            img_file = os.path.join(batch_output, f'{filename}_{i}.png')
            img.save(img_file)
            imgs.append(img_file)
    installer.status("...Saving Output Video")
    duration = int((len(output_frames) / fresco_v2v_prefs['target_fps']) * 100)
    output_frames[0].save(video_path, save_all=True, append_images=output_frames[1:], duration=duration, loop=0)
    outputs = [filepath_to_url(video_path)]
    if fresco_v2v_prefs['interpolate_video']:
        installer.set_message("Saving Frames to Video using FFMPEG with Deflicker...")
        interpolate_video(output_frames, input_fps=fresco_v2v_prefs['source_fps'], output_fps=fresco_v2v_prefs['target_fps'], output_video=out_file, installer=installer)
        outputs.append(filepath_to_url(out_file))
    clear_last()
    play_snd(Snd.ALERT, page)
    if os.path.exists(video_path):
        #shutil.move(result_keyframe, keyframe_path)
        #shutil.move(output_path, video_path)
        prt(Row([Markdown(f"Done creating video... Check {and_list(outputs)}", on_tap_link=lambda e: e.page.launch_url(e.data))], alignment=MainAxisAlignment.CENTER))
        prt(Row([VideoContainer(video_path)], alignment=MainAxisAlignment.CENTER))
    else:
        prt("Something went wrong generating video...")
    autoscroll(False)

def run_latte(page):
    global latte_prefs, prefs, status, pipe_latte
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.Latte.controls.append(line)
      page.Latte.update()
    def clear_last(lines=1):
      clear_line(page.Latte, lines=lines)
    def clear_list():
      page.Latte.controls = page.Latte.controls[:1]
    def autoscroll(scroll=True):
      page.Latte.auto_scroll = scroll
      page.Latte.update()
    progress = ProgressBar(bar_height=8)
    total_steps = latte_prefs['num_inference_steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Latte Text-To-Video Pipeline...")
    prt(installer)
    model_id = "maxin-cn/Latte-1"
    from diffusers import LattePipeline
    from diffusers.utils import export_to_gif, export_to_video
    clear_pipes('latte')
    if pipe_latte == None:
        installer.status(f"...initialize Pipeline")
        try:
            pipe_latte = LattePipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")
            if latte_prefs['cpu_offload']:
                pipe_latte.enable_model_cpu_offload()
            else:
                pipe_latte = pipe_latte.to(torch_device)
                if prefs['enable_torch_compile']:
                    installer.status(f"...Torch compiling unet")
                    pipe_latte.transformer.to(memory_format=torch.channels_last)
                    pipe_latte.vae.to(memory_format=torch.channels_last)
                    pipe_latte.transformer = torch.compile(pipe_latte.transformer)
                    pipe_latte.vae.decode = torch.compile(pipe_latte.vae.decode)
            #pipe_latte.scheduler = DDIMScheduler.from_config(pipe_latte.scheduler.config)
            pipe_latte.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Personalized Image Animator...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    if latte_prefs['clean_caption']:
        pip_install("beautifulsoup4|bs4 ftfy", installer=installer)
    clear_last()
    #batch_output = os.path.join(stable_dir, latte_prefs['batch_folder_name'])
    #if not os.path.isdir(batch_output):
    #  os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], latte_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    random_seed = get_seed(latte_prefs['seed'])
    for n in range(latte_prefs['num_images']):
        prt("Generating Latte Video from your Prompt...")
        prt(progress)
        autoscroll(False)
        generator = torch.Generator(device="cpu").manual_seed(random_seed + n)
        #generator = torch.manual_seed(random_seed)
        width = latte_prefs['width']
        height = latte_prefs['height']
        try:
            videos = pipe_latte(latte_prefs['prompt'], negative_prompt=latte_prefs['negative_prompt'], video_length=latte_prefs['num_frames'], num_inference_steps=latte_prefs['num_inference_steps'], guidance_scale=latte_prefs['guidance_scale'], clean_caption=latte_prefs['clean_caption'], width=width, height=height, generator=generator, callback_on_step_end=callback_fnc).frames[0]
        except Exception as e:
          clear_last()
          clear_last()
          alert_msg(page, f"ERROR: Latte Text-To-Video failed for some reason. Possibly out of memory or something wrong with the code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
          return
        clear_last()
        clear_last()
        autoscroll(True)
        fname = f"{format_filename(latte_prefs['prompt'])}"
        gif_file = available_file(batch_output, fname, no_num=True, ext="gif")
        video_file = available_file(batch_output, fname, no_num=True, ext="mp4")
        export_to_gif(videos, gif_file, fps=latte_prefs['fps'])
        prt(Row([ImageButton(src=gif_file, width=width, height=height, data=gif_file, page=page)], alignment=MainAxisAlignment.CENTER))
        #filename = filename[:int(prefs['file_max_length'])]
        #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
        autoscroll(True)
        export_to_video(videos, video_file, fps=latte_prefs['fps'])
        #prt(Row([VideoContainer(video_file)], alignment=MainAxisAlignment.CENTER))
        prt(Markdown(f"Video saved to [{video_file}]({filepath_to_url(video_file)})", on_tap_link=lambda e: e.page.launch_url(e.data)))
    #prt(f"Done creating video... Check {batch_output}")
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_open_sora_plan(page):
    global open_sora_plan_prefs, prefs, status, pipe_open_sora_plan
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.OpenSoraPlan.controls.append(line)
      page.OpenSoraPlan.update()
    def clear_last(lines=1):
      clear_line(page.OpenSoraPlan, lines=lines)
    def clear_list():
      page.OpenSoraPlan.controls = page.OpenSoraPlan.controls[:1]
    def autoscroll(scroll=True):
      page.OpenSoraPlan.auto_scroll = scroll
      page.OpenSoraPlan.update()
    progress = ProgressBar(bar_height=8)
    total_steps = open_sora_plan_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, callback_kwargs) -> None: #(pipe, step, timestep, callback_kwargs):#
      nonlocal progress, total_steps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Open-Sora-Plan Text-To-Video Pipeline...")
    prt(installer)
    open_sora_plan_dir = os.path.join(root_dir, "Open-Sora-Plan")#"Open-Sora-Plan-v1.0.0-hf"
    if not os.path.exists(open_sora_plan_dir):
        try:
            installer.status("...cloning Open-Sora-Plan")
            #run_sp("git clone -b dev https://github.com/camenduru/Open-Sora-Plan-v1.0.0-hf", cwd=root_dir, realtime=False)
            run_sp("git clone -b dev https://github.com/PKU-YuanGroup/Open-Sora-Plan", cwd=root_dir, realtime=False)
            installer.status("...installing Open-Sora-Plan requirements")
            #run_sp("pip install -r requirements.txt", realtime=True) #pytorch-lightning==1.5.0
            pip_install("albumentations==1.4.0 av decord==0.6.0 einops fastapi==0.110.0 gdown h5py==3.10.0 idna==3.6 imageio matplotlib numpy omegaconf==2.1.1 opencv-python==4.9.0.80 opencv-python-headless==4.9.0.80 pandas pydub pytorch-lightning==1.4.2 pytorchvideo==0.1.5 PyYAML|yaml regex==2023.12.25 scikit-learn|sklearn scipy six==1.16.0 tensorboard==2.14.0 test-tube==0.7.5 timm torchdiffeq==0.2.3 torchmetrics==0.5.0 tqdm urllib3==2.2.1 uvicorn==0.27.1 scikit-video==1.1.11 triton", installer=installer, upgrade=True)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Open-Sora Requirements:", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    elif force_update("open-sora"):
        installer.status("...updating Open-Sora-Plan")
        run_sp("git pull origin main", cwd=open_sora_plan_dir)
    if open_sora_plan_dir not in sys.path:
        sys.path.append(open_sora_plan_dir)
    os.chdir(open_sora_plan_dir)
    import imageio
    from diffusers import ConsistencyDecoderVAE, DPMSolverMultistepScheduler, Transformer2DModel, AutoencoderKL, SASolverScheduler
    from typing import Tuple
    import numpy as np
    from opensora.models import CausalVAEModelWrapper
    #from opensora.models.causalvideovae import ae_stride_config, ae_channel_config
    #from opensora.models.causalvideovae import ae_norm, ae_denorm
    from transformers import T5EncoderModel, T5Tokenizer, AutoTokenizer, MT5EncoderModel
    from opensora.models.causalvideovae import ae_stride_config
    from transformers import AutoTokenizer, MT5EncoderModel
    from diffusers import DPMSolverMultistepScheduler, SASolverScheduler
    from diffusers.schedulers import DDIMScheduler, DDPMScheduler, PNDMScheduler, EulerDiscreteScheduler, DPMSolverMultistepScheduler, EulerAncestralDiscreteScheduler, DEISMultistepScheduler
    from opensora.models.diffusion.opensora.modeling_opensora import OpenSoraT2V
    from opensora.sample.pipeline_opensora import OpenSoraPipeline
    #from opensora.serve.gradio_utils import DESCRIPTION, MAX_SEED, style_list, randomize_seed_fn, save_video
    #from opensora.serve.gradio_utils import block_css, title_markdown, examples, DESCRIPTION
    status.setdefault("loaded_open_sora", "")
    schedule = open_sora_plan_prefs['scheduler']
    ae = 'CausalVAEModel_4x8x8'
    ae_path = "/path/to/causalvideovae"
    model_path = "/path/to/checkpoint-xxx/model_ema"
    force_images = open_sora_plan_prefs['num_frames'] == 1
    model_id = "LanguageBind/Open-Sora-Plan-v1.2.0"
    text_encoder_name = 'google/mt5-xxl'
    weight_dtype = torch.bfloat16
    T5_token_max_length = 512
    #version = '221x512x512' if open_sora_plan_prefs['num_frames'] == "221" else '65x512x512'
    if status['loaded_open_sora'] != model_id:
        clear_pipes()
    else:
        clear_pipes("open_sora_plan")
    cache_dir = prefs['cache_dir'] if bool(prefs['cache_dir']) else "cache_dir"
    if pipe_open_sora_plan == None:
        installer.status(f"...initialize Pipeline VAE")
        try:
            vae = CausalVAEModelWrapper(model_id, subfolder="vae", cache_dir=cache_dir).eval()
            vae.vae = vae.vae.to(device=torch_device, dtype=weight_dtype)
            vae.vae.enable_tiling()
            vae.vae.tile_overlap_factor = 0.125
            vae.vae.tile_sample_min_size = 256
            vae.vae.tile_latent_min_size = 32
            vae.vae.tile_sample_min_size_t = 29
            vae.vae.tile_latent_min_size_t = 8
            vae.vae_scale_factor = ae_stride_config[ae]
            installer.status(f"...loading Text Encoder (really big)")
            text_encoder = MT5EncoderModel.from_pretrained(text_encoder_name, cache_dir=cache_dir, low_cpu_mem_usage=True, torch_dtype=weight_dtype)
            installer.status(f"...loading Tokenizer")
            tokenizer = AutoTokenizer.from_pretrained(text_encoder_name, cache_dir=cache_dir)
            installer.status(f"...loading Transformer")
            transformer = OpenSoraT2V.from_pretrained(model_id, subfolder="93x720p", cache_dir=cache_dir, low_cpu_mem_usage=False,  device_map=None, torch_dtype=weight_dtype)
            scheduler = EulerAncestralDiscreteScheduler()
            installer.status(f"...loading OpenSora Pipeline")
            pipe_open_sora_plan = OpenSoraPipeline(vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, scheduler=scheduler, transformer=transformer)
            pipe_open_sora_plan.to(torch_device)
            if pipe_open_sora_plan['speed_up_T5']:
                installer.status(f"...speeding up T5")
                pipe_open_sora_plan.text_encoder.to_bettertransformer()
            if prefs['enable_torch_compile']:
                installer.status(f"...torch Compile Transformer")
                pipe_open_sora_plan.transformer = torch.compile(pipe_open_sora_plan.transformer, mode="reduce-overhead", fullgraph=True)
            #status['video_length'] = transformer_model.config.video_length if not force_images else 1
            status['loaded_open_sora'] = model_id
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Open-Sora...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    #if open_sora_plan_prefs['clean_caption']:
    #    pip_install("beautifulsoup4|bs4 ftfy", installer=installer)
    if schedule == 'DPM-Solver':
        if not isinstance(pipe_open_sora_plan.scheduler, DPMSolverMultistepScheduler):
            pipe_open_sora_plan.scheduler = DPMSolverMultistepScheduler()
    elif schedule == "PNDM-Solver":
        if not isinstance(pipe_open_sora_plan.scheduler, PNDMScheduler):
            pipe_open_sora_plan.scheduler = PNDMScheduler()
    elif schedule == "DDIM-Solver":
        if not isinstance(pipe_open_sora_plan.scheduler, DDIMScheduler):
            pipe_open_sora_plan.scheduler = DDIMScheduler()
    elif schedule == "Euler-Solver":
        if not isinstance(pipe_open_sora_plan.scheduler, EulerDiscreteScheduler):
            pipe_open_sora_plan.scheduler = EulerDiscreteScheduler()
    elif schedule == "DDPM-Solver":
        if not isinstance(pipe_open_sora_plan.scheduler, DDPMScheduler):
            pipe_open_sora_plan.scheduler = DDPMScheduler()
    elif schedule == "EulerA-Solver":
        if not isinstance(pipe_open_sora_plan.scheduler, EulerAncestralDiscreteScheduler):
            pipe_open_sora_plan.scheduler = EulerAncestralDiscreteScheduler()
    elif schedule == "DEISM-Solver":
        if not isinstance(pipe_open_sora_plan.scheduler, DEISMultistepScheduler):
            pipe_open_sora_plan.scheduler = DEISMultistepScheduler()
    elif schedule == "SA-Solver":
        if not isinstance(pipe_open_sora_plan.scheduler, SASolverScheduler):
            pipe_open_sora_plan.scheduler = SASolverScheduler.from_config(pipe.scheduler.config, algorithm_type='data_prediction', tau_func=lambda t: 1 if 200 <= t <= 800 else 0, predictor_order=2, corrector_order=2)
    else:
        raise ValueError(f"Unknown schedule: {schedule}")

    clear_last()
    batch_output = os.path.join(prefs['image_output'], open_sora_plan_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    random_seed = get_seed(open_sora_plan_prefs['seed'])
    for n in range(open_sora_plan_prefs['num_images']):
        prt("Generating Open-Sora-Plan Video from your Prompt...")
        prt(progress)
        autoscroll(False)
        width = open_sora_plan_prefs['width']
        height = open_sora_plan_prefs['height']
        generator = torch.Generator().manual_seed(random_seed + n)
        #height, width = int(args.version.split('x')[1]), int(args.version.split('x')[2])
        #num_frames = 1 if force_images else int(open_sora_plan_prefs['num_frames'])
        try:
            with torch.no_grad():
              with torch.inference_mode():
                videos = pipe_open_sora_plan(
                    prompt=open_sora_plan_prefs['prompt'],
                    negative_prompt=open_sora_plan_prefs['negative_prompt'] if bool(open_sora_plan_prefs['negative_prompt']) else None,
                    num_frames=open_sora_plan_prefs['num_frames'],#93,
                    width=width,
                    height=height,
                    guidance_scale=open_sora_plan_prefs['guidance_scale'],
                    num_inference_steps=open_sora_plan_prefs['num_inference_steps'],
                    generator=generator,
                    num_images_per_prompt=open_sora_plan_prefs['batch_size'],
                    max_sequence_length=T5_token_max_length,
                    #callback=callback_fnc,
                ).images
                '''videos = pipe_open_sora_plan(open_sora_plan_prefs['prompt'],
                                        negative_prompt=open_sora_plan_prefs['negative_prompt'],
                                        video_length=status['video_length'],
                                        height=height,
                                        width=width,
                                        num_inference_steps=open_sora_plan_prefs['num_inference_steps'],
                                        guidance_scale=open_sora_plan_prefs['guidance_scale'],
                                        enable_temporal_attentions=not force_images,
                                        num_images_per_prompt=1,
                                        mask_feature=True,
                                        callback=callback_fnc,
                                        ).video[0]'''
            #videos = pipe_open_sora_plan(open_sora_plan_prefs['prompt'], negative_prompt=open_sora_plan_prefs['negative_prompt'], video_length=open_sora_plan_prefs['num_frames'], num_inference_steps=open_sora_plan_prefs['num_inference_steps'], guidance_scale=open_sora_plan_prefs['guidance_scale'], clean_caption=open_sora_plan_prefs['clean_caption'], width=width, height=height, generator=generator, callback_on_step_end=callback_fnc).frames[0]
        except Exception as e:
            clear_last(2)
            flush()
            alert_msg(page, f"ERROR: Open-Sora-Plan Text-To-Video failed for some reason. Possibly out of memory or something wrong with the code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        flush()
        clear_last(2)
        autoscroll(True)
        #display_model_info = f"Video size: {num_frames}×{height}×{width}, \nSampling Step: {sample_steps}, \nGuidance Scale: {scale}"
        for video in videos:
            fname = f"{format_filename(open_sora_plan_prefs['prompt'])}"
            gif_file = available_file(batch_output, fname, no_num=True, ext="gif")
            video_file = available_file(batch_output, fname, no_num=True, ext="mp4")
            imageio.mimwrite(video_file, video, fps=open_sora_plan_prefs['fps'], quality=9)  # highest quality is 10, lowest is 0
            if open_sora_plan_prefs['export_to_gif']:
                from diffusers.utils import export_to_gif
                export_to_gif(video, gif_file, fps=open_sora_plan_prefs['fps'])
                prt(Row([ImageButton(src=gif_file, width=width, height=height, data=gif_file, page=page)], alignment=MainAxisAlignment.CENTER))
            #filename = filename[:int(prefs['file_max_length'])]
            #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
            autoscroll(True)
            #export_to_video(videos, video_file, fps=open_sora_plan_prefs['fps'])
            #prt(Row([VideoContainer(video_file)], alignment=MainAxisAlignment.CENTER))
            prt(Markdown(f"Video saved to [{video_file}]({filepath_to_url(video_file)})", on_tap_link=lambda e: e.page.launch_url(e.data)))
    #prt(f"Done creating video... Check {batch_output}")
    os.chdir(root_dir)
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_video_infinity(page):
    global video_infinity_prefs, prefs, status, pipe_video_infinity
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.VideoInfinity.controls.append(line)
      page.VideoInfinity.update()
    def clear_last(lines=1):
      clear_line(page.VideoInfinity, lines=lines)
    def clear_list():
      page.VideoInfinity.controls = page.VideoInfinity.controls[:1]
    def autoscroll(scroll=True):
      page.VideoInfinity.auto_scroll = scroll
      page.VideoInfinity.update()
    progress = ProgressBar(bar_height=8)
    total_steps = video_infinity_prefs['num_inference_steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Video-Infinity Text-To-Video Pipeline...")
    prt(installer)
    video_infinity_dir = os.path.join(root_dir, "Video-Infinity")
    if not os.path.exists(video_infinity_dir):
        try:
            installer.status("...cloning Yuanshi9815/Video-Infinity")
            run_sp("git clone https://github.com/Yuanshi9815/Video-Infinity", cwd=root_dir, realtime=False)
            installer.status("...installing Video-Infinity requirements")
            pip_install("absl-py==2.1.0 addict==2.4.0 aiofiles==23.2.1 aiohttp==3.9.3 aiosignal==1.3.1 aliyun-python-sdk-core==2.15.1 aliyun-python-sdk-kms==2.16.2 altair==5.3.0 annotated-types==0.6.0 antlr4-python3-runtime==4.9.3 anyio==4.3.0 anykeystore==0.2 apex==0.9.10.dev0 appdirs==1.4.4 async-timeout==4.0.3 attrs==23.2.0 av==12.0.0 bcrypt==4.1.2 beartype==0.18.5 beautifulsoup4==4.12.3 blessed==1.20.0 causal-conv1d==1.2.0.post2 certifi==2024.2.2 cffi==1.16.0 cfgv==3.4.0 charset-normalizer==3.3.2 click==8.1.7 colossalai==0.3.6 contexttimer==0.3.3 contourpy==1.2.1 crcmod==1.7 cryptacular==1.6.2 cryptography==42.0.5 cycler==0.12.1 datasets==2.18.0 decord==0.6.0 deepspeed==0.14.2 defusedxml==0.7.1 Deprecated==1.2.14 diffusers==0.27.2 dill==0.3.8 distlib==0.3.8 docker-pycreds==0.4.0 einops==0.7.0 fabric fastapi==0.110.1 ffmpy==0.3.2 filelock==3.13.3 fonttools==4.51.0 frozenlist==1.4.1 fsspec==2024.2.0 ftfy==6.2.0 gast==0.5.4 gdown gitdb==4.0.11 GitPython==3.1.43 google==3.0.0 gpustat==1.1.1 gradio==4.25.0 greenlet==3.0.3 grpcio==1.62.2 h11==0.14.0 hjson==3.1.0 httpcore==1.0.5 httpx==0.27.0 hupper==1.12.1 identify==2.5.36 idna==3.6 imageio==2.34.0 imageio-ffmpeg==0.4.9 importlib_resources==6.4.0 invoke==2.2.0 Jinja2==3.1.3 jmespath==0.10.0 joblib==1.3.2 jsonschema==4.21.1 jsonschema-specifications==2023.12.1 kiwisolver==1.4.5 kornia==0.7.2 kornia_rs==0.1.3 lightning-utilities==0.11.2 mamba-ssm==1.2.0.post1 Markdown==3.6 markdown-it-py==3.0.0 MarkupSafe==2.1.5 matplotlib==3.8.4 mdurl==0.1.2 mmengine==0.10.4 modelscope==1.13.3 moviepy mpmath==1.3.0 msgpack==1.0.8 multidict==6.0.5 multiprocess==0.70.16 networkx==3.3 ninja==1.11.1.1 nodeenv==1.8.0 numpy nvidia-cublas-cu12==12.1.3.1 nvidia-cuda-cupti-cu12==12.1.105 nvidia-cuda-nvrtc-cu12==12.1.105 nvidia-cuda-runtime-cu12==12.1.105 nvidia-cudnn-cu12==8.9.2.26 nvidia-cufft-cu12==11.0.2.54 nvidia-curand-cu12==10.3.2.106 nvidia-cusolver-cu12==11.4.5.107 nvidia-cusparse-cu12==12.1.0.106 nvidia-ml-py==12.550.52 nvidia-nccl-cu12==2.19.3 nvidia-nvjitlink-cu12==12.4.127 nvidia-nvtx-cu12==12.1.105 oauthlib==3.2.2 omegaconf==2.3.0 open-clip-torch==2.24.0 opencv-python==4.9.0.80 orjson==3.10.0 oss2==2.18.4 pandarallel==1.6.5 pandas==2.2.1 paramiko==3.4.0 PasteDeploy==3.1.0 pbkdf2==1.3 plaster==1.1.2 plaster-pastedeploy==1.0.1 pre-commit==3.7.0 proglog==0.1.10 protobuf==4.25.3 py-cpuinfo==9.0.0 pyarrow==15.0.2 pyarrow-hotfix==0.6 pyav==12.0.5 pycparser==2.22 pycryptodome==3.20.0 pydantic==2.6.4 pydantic_core==2.16.3 pydub==0.25.1 PyNaCl==1.5.0 pynvml==11.5.0 pyparsing==3.1.2 pyramid==2.0.2 pyramid-mailer==0.15.1 PySocks==1.7.1 python-multipart==0.0.9 python3-openid==3.2.0 pytorch-lightning==1.9.0 pytz==2024.1 PyYAML==6.0.1 ray==2.11.0 referencing==0.34.0 regex==2023.12.25 repoze.sendmail==4.4.1 requests==2.31.0 requests-oauthlib==2.0.0 rich==13.7.1 rotary-embedding-torch==0.5.3 rpds-py==0.18.0 ruff==0.3.5 safetensors==0.4.2 scikit-learn==1.4.1.post1 scipy==1.13.0 semantic-version==2.10.0 sentencepiece==0.2.0 sentry-sdk==1.45.0 setproctitle==1.3.3 shellingham==1.5.4 simplejson==3.19.2 smmap==5.0.1 sniffio==1.3.1 sortedcontainers==2.4.0 soupsieve==2.5 SQLAlchemy==2.0.29 starlette==0.37.2 sympy==1.12 tensorboard==2.16.2 tensorboard-data-server==0.7.2 termcolor==2.4.0 threadpoolctl==3.4.0 timm==0.9.16 tokenizers==0.15.2 tomli==2.0.1 tomlkit==0.12.0 toolz==0.12.1 torchmetrics==1.3.2 torchvision==0.17.2 tqdm==4.66.2 transaction==4.0 translationstring==1.4 triton==2.2.0 typer==0.12.1 tzdata==2024.1 urllib3==2.2.1 uvicorn==0.29.0 velruse==1.1.1 venusian==3.1.0 virtualenv==20.26.0 wandb==0.16.6 WebOb==1.8.7 websockets==11.0.3 Werkzeug==3.0.2 wrapt==1.16.0 WTForms==3.1.2 wtforms-recaptcha==0.3.2 xformers==0.0.25.post1 xxhash==3.4.1 yapf==0.40.2 yarl==1.9.4 zope.deprecation==5.0 zope.interface==6.3 zope.sqlalchemy==3.1", installer=installer, upgrade=True)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Video Infinity Requirements:", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    elif force_update("video_infinity"):
        installer.status("...updating Yuanshi9815/Video-Infinity")
        run_sp("git pull origin main", cwd=video_infinity_dir)
    if video_infinity_dir not in sys.path:
        sys.path.append(video_infinity_dir)
    os.chdir(video_infinity_dir)
    installer.status("...preparing config")
    import json
    config = json.load(open(os.path.join(video_infinity_dir, "examples", "single_gpu.json")))
    model_id = "adamdad/videocrafterv2_diffusers"
    config['seed'] = video_infinity_prefs['seed']
    num_gpus = torch.cuda.device_count()
    config["devices"] = list(range(num_gpus))
    size = len(config["devices"])
    import torch.distributed as dist
    import torch.multiprocessing as mp
    import json
    from src.video_crafter import VideoCrafterPipeline, UNetVideoCrafter
    from diffusers.schedulers import DPMSolverMultistepScheduler
    from src.tools import DistController
    from src.video_infinity.wrapper import DistWrapper
    clear_pipes('video_infinity')
    if pipe_video_infinity == None:
        installer.status(f"...initialize VideoCrafter2 Pipeline")
        try:
            with torch.inference_mode():
                pipe_video_crafter = VideoCrafterPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
                pipe_video_crafter.enable_model_cpu_offload(
                    gpu_id=config["devices"][dist.get_rank() % len(config["devices"])],
                )
                pipe_video_crafter.enable_vae_slicing()
                dist_controller = DistController(0, 1, config)
            pipe_video_infinity = DistWrapper(pipe_video_crafter, dist_controller, config)
            #pipe_video_infinity.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Video-Infinity...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    
    def run_inference(rank, world_size, config):
        #dist_pipe = DistWrapper(pipe_video_infinity, dist_controller, config)
        start = time.time()
        pipe_configs=config['pipe_configs']
        plugin_configs=config['plugin_configs']
        prompt_id = int(rank / world_size * len(pipe_configs["prompts"]))
        prompt = pipe_configs["prompts"][prompt_id]
        start = time.time()
        with torch.inference_mode():
            output = pipe_video_infinity.inference(
                prompt,
                config,
                pipe_configs,
                plugin_configs,
                additional_info={
                    "full_config": config,
                }
            )
        prt(f"Rank {rank} finished. Time: {time.time() - start}")
    clear_last()
    batch_output = os.path.join(prefs['image_output'], video_infinity_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    if not os.path.exists(config["base_path"]):
        os.makedirs(config["base_path"])
    random_seed = get_seed(video_infinity_prefs['seed'])
    prompt_list = [video_infinity_prefs['prompt']] if len(video_infinity_prefs['prompts']) == 0 and bool(video_infinity_prefs['prompt']) else video_infinity_prefs['prompts']
    for n in range(video_infinity_prefs['num_images']):
        prt("Generating Video-Infinity from your Prompt... See console for progress.")
        prt(progress)
        autoscroll(False)
        fname = f"{format_filename(video_infinity_prefs['prompt'])}"
        width = video_infinity_prefs['width']
        height = video_infinity_prefs['height']
        config['seed'] = random_seed + n
        config['pipe_configs']['prompts'] = prompt_list #[video_infinity_prefs['prompt']]
        config['pipe_configs']['steps'] = video_infinity_prefs['num_inference_steps']
        config['pipe_configs']['guidance_scale'] = video_infinity_prefs['guidance_scale']
        config['pipe_configs']['fps'] = video_infinity_prefs['target_fps']
        config['pipe_configs']['export_fps'] = video_infinity_prefs['fps']
        config['pipe_configs']['num_frames'] = video_infinity_prefs['num_frames']
        config['pipe_configs']['width'] = width
        config['pipe_configs']['height'] = height
        config['pipe_configs']['file_name'] = fname
        processes = []
        try:
            for rank, _ in enumerate(config["devices"]):
                p = mp.Process(target=run_inference, args=(rank, size, config))
                p.start()
                processes.append(p)
            for p in processes:
                p.join()
        except Exception as e:
          clear_last(2)
          alert_msg(page, f"ERROR: Video-Infinity Text-To-Video failed for some reason. Possibly out of memory or something wrong with the code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
          os.chdir(root_dir)
          return
        clear_last(2)
        autoscroll(True)
        gif_file = available_file(batch_output, fname, no_num=True, ext="gif")
        video_file = available_file(batch_output, fname, no_num=True, ext="mp4")
        base_path = config["base_path"]
        mp4_files = [f for f in os.listdir(base_path) if f.endswith(".mp4")]
        if not mp4_files:
            prt("No generated video file found.  Something went wrong...")
            os.chdir(root_dir)
            return
        latest_file = max(mp4_files, key=lambda f: os.path.getmtime(os.path.join(base_path, f)))
        generated_file = os.path.join(base_path, latest_file)
        shutil.move(generated_file, video_file)
        #export_to_gif(videos, gif_file, fps=video_infinity_prefs['fps'])
        #prt(Row([ImageButton(src=gif_file, width=width, height=height, data=gif_file, page=page)], alignment=MainAxisAlignment.CENTER))
        #filename = filename[:int(prefs['file_max_length'])]
        #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
        autoscroll(True)
        #export_to_video(videos, video_file, fps=video_infinity_prefs['fps'])
        #prt(Row([VideoContainer(video_file)], alignment=MainAxisAlignment.CENTER))
        prt(Markdown(f"Video saved to [{video_file}]({filepath_to_url(video_file)})", on_tap_link=lambda e: e.page.launch_url(e.data)))
    #prt(f"Done creating video... Check {batch_output}")
    os.chdir(root_dir)
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_materialdiffusion(page):
    global materialdiffusion_prefs, prefs
    if not bool(materialdiffusion_prefs['material_prompt']):
      alert_msg(page, "You must provide a text prompt to process your material...")
      return
    if not bool(prefs['Replicate_api_key']):
      alert_msg(page, "You must provide your Replicate API Token in Settings to process your material...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.MaterialDiffusion.controls.append(line)
      page.MaterialDiffusion.update()
    def clear_last(lines=1):
      clear_line(page.MaterialDiffusion, lines=lines)
    def clear_list():
      page.MaterialDiffusion.controls = page.MaterialDiffusion.controls[:1]
    def autoscroll(scroll=True):
      page.MaterialDiffusion.auto_scroll = scroll
      page.MaterialDiffusion.update()
    progress = ProgressBar(bar_height=8)
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress
      total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    '''try:
      run_sp("pip install git+https://github.com/TomMoore515/material_stable_diffusion.git@main#egg=predict", realtime=False)
    except Exception as e:
        print(f"Error: {e}")
        #alert_msg(page, f"Error installing Material Diffusion from TomMoore515...", content=Text(str(e)))
        pass'''
    prt(Installing("Installing Replicate Material Diffusion Pipeline..."))
    try:
        import replicate
    except ModuleNotFoundError as e:
        run_process("pip install replicate -qq", realtime=False)
        import replicate
        pass
    os.environ["REPLICATE_API_TOKEN"] = prefs['Replicate_api_key']
    #export REPLICATE_API_TOKEN=
    try:
        rep_model = replicate.models.get("tommoore515/material_stable_diffusion")
        rep_version = rep_model.versions.get("3b5c0242f8925a4ab6c79b4c51e9b4ce6374e9b07b5e8461d89e692fd0faa449")
    except Exception as e:
        alert_msg(page, f"Seems like your Replicate API Token is Invalid. Check it again...", content=Text(str(e)))
        return
    import requests
    from io import BytesIO
    from PIL import ImageOps
    init_img = None
    if bool(materialdiffusion_prefs['init_image']):
        if materialdiffusion_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(materialdiffusion_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(materialdiffusion_prefs['init_image']):
                init_img = PILImage.open(materialdiffusion_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {materialdiffusion_prefs['init_image']}")
                return
        #width, height = init_img.size
        #width, height = scale_dimensions(materialdiffusion_prefs['width'], materialdiffusion_prefs['height'])
        init_img = init_img.resize((materialdiffusion_prefs['width'], materialdiffusion_prefs['height']), resample=PILImage.Resampling.LANCZOS)
        init_img = ImageOps.exif_transpose(init_img).convert("RGB")
    mask_img = None
    if bool(materialdiffusion_prefs['mask_image']):
        if materialdiffusion_prefs['mask_image'].startswith('http'):
            mask_img = PILImage.open(requests.get(materialdiffusion_prefs['mask_image'], stream=True).raw)
        else:
            if os.path.isfile(materialdiffusion_prefs['mask_image']):
                mask_img = PILImage.open(materialdiffusion_prefs['mask_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your mask_image {materialdiffusion_prefs['mask_image']}")
                return
            if materialdiffusion_prefs['invert_mask']:
                mask_img = ImageOps.invert(mask_img.convert('RGB'))
                mask_img = mask_img.resize((materialdiffusion_prefs['width'], materialdiffusion_prefs['height']), resample=PILImage.NEAREST)
                mask_img = ImageOps.exif_transpose(mask_img).convert("RGB")
    #print(f'Resize to {width}x{height}')
    clear_pipes()
    clear_last()
    prt("Generating your Material Diffusion Image...")
    prt(progress)
    autoscroll(False)
    random_seed = get_seed(materialdiffusion_prefs['seed'])
    #input = {'prompt':materialdiffusion_prefs['material_prompt'], 'width':materialdiffusion_prefs['width'], 'height':materialdiffusion_prefs['height'], 'init_image':init_img, 'mask':mask_img, 'prompt_strength':materialdiffusion_prefs['prompt_strength'], 'num_outputs':materialdiffusion_prefs['num_outputs'], 'num_inference_steps':materialdiffusion_prefs['steps'], 'guidance_scale':materialdiffusion_prefs['guidance_scale'], 'seed':random_seed}
    try:
        if init_img != None and mask_img != None:
            images = rep_version.predict(prompt=materialdiffusion_prefs['material_prompt'], width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], init_image=init_img, mask=mask_img, prompt_strength=materialdiffusion_prefs['prompt_strength'], num_outputs=materialdiffusion_prefs['num_outputs'], num_inference_steps=materialdiffusion_prefs['steps'], guidance_scale=materialdiffusion_prefs['guidance_scale'], seed=random_seed)
        elif init_img != None:
            images = rep_version.predict(prompt=materialdiffusion_prefs['material_prompt'], width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], init_image=init_img, prompt_strength=materialdiffusion_prefs['prompt_strength'], num_outputs=materialdiffusion_prefs['num_outputs'], num_inference_steps=materialdiffusion_prefs['steps'], guidance_scale=materialdiffusion_prefs['guidance_scale'], seed=random_seed)
            #images = version.predict(prompt=materialdiffusion_prefs['material_prompt'], width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], init_image=init_img if init_img != None else "", mask=mask_img if mask_img != None else "", prompt_strength=materialdiffusion_prefs['prompt_strength'], num_outputs=materialdiffusion_prefs['num_outputs'], num_inference_steps=materialdiffusion_prefs['steps'], guidance_scale=materialdiffusion_prefs['guidance_scale'], seed=random_seed)
        else:
            images = rep_version.predict(prompt=materialdiffusion_prefs['material_prompt'], width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], prompt_strength=materialdiffusion_prefs['prompt_strength'], num_outputs=materialdiffusion_prefs['num_outputs'], num_inference_steps=materialdiffusion_prefs['steps'], guidance_scale=materialdiffusion_prefs['guidance_scale'], seed=random_seed)
    except Exception as e:
        clear_last(2)
        alert_msg(page, f"ERROR: Couldn't create your image for some reason.  Possibly out of memory or something wrong with my code...", content=Text(str(e)))
        return
    autoscroll(True)
    clear_last(2)
    txt2img_output = stable_dir
    batch_output = prefs['image_output']
    #print(str(images))
    if images is None:
        prt(f"ERROR: Problem generating images, check your settings and run above blocks again, or report the error to Skquark if it really seems broken.")
        return
    idx = 0
    for image in images:
        random_seed += idx
        fname = format_filename(materialdiffusion_prefs['material_prompt'])
        seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
        fname = f'{materialdiffusion_prefs["file_prefix"]}{fname}{seed_suffix}'
        txt2img_output = stable_dir
        if bool(materialdiffusion_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, materialdiffusion_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        image_path = available_file(txt2img_output, fname, 1)
        #image.save(image_path)
        response = requests.get(image, stream=True)
        with open(image_path, "wb") as f:
          f.write(response.content)
        new_file = image_path.rpartition(slash)[2]
        if not materialdiffusion_prefs['display_upscaled_image'] or not materialdiffusion_prefs['apply_ESRGAN_upscale']:
            save_metadata(image_path, materialdiffusion_prefs, "Material Diffusion", seed=random_seed)
            #prt(Row([Img(src=image_path, width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            prt(Row([ImageButton(src=image_path, width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], page=page)], alignment=MainAxisAlignment.CENTER))

        #if save_to_GDrive:
        batch_output = os.path.join(prefs['image_output'], materialdiffusion_prefs['batch_folder_name'])
        if not os.path.exists(batch_output):
            os.makedirs(batch_output)
        if storage_type == "PyDrive Google Drive":
            newFolder = gdrive.CreateFile({'title': materialdiffusion_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
            newFolder.Upload()
            batch_output = newFolder
        out_path = batch_output# if save_to_GDrive else txt2img_output
        new_path = os.path.join(out_path, new_file)
        if materialdiffusion_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            upscaled_path = os.path.join(out_path, new_file)
            upscale_image(image_path, upscaled_path, scale=materialdiffusion_prefs["enlarge_scale"])
            image_path = upscaled_path
            save_metadata(image_path, materialdiffusion_prefs, "Material Diffusion", seed=random_seed)
            if materialdiffusion_prefs['display_upscaled_image']:
                prt(Row([Img(src=asset_dir(upscaled_path), width=materialdiffusion_prefs['width'] * float(materialdiffusion_prefs["enlarge_scale"]), height=materialdiffusion_prefs['height'] * float(materialdiffusion_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        else:
            new_path
            try:
              shutil.copy(image_path, new_path)
            except shutil.SameFileError: pass
        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
    del rep_model, rep_version
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_materialdiffusion_sdxl(page):
    global materialdiffusion_sdxl_prefs, prefs
    if not bool(materialdiffusion_sdxl_prefs['material_prompt']):
      alert_msg(page, "You must provide a text prompt to process your material...")
      return
    if not bool(prefs['Replicate_api_key']):
      alert_msg(page, "You must provide your Replicate API Token in Settings to process your material...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.MaterialDiffusion.controls.append(line)
      page.MaterialDiffusion.update()
    def clear_last(lines=1):
      clear_line(page.MaterialDiffusion, lines=lines)
    def clear_list():
      page.MaterialDiffusion.controls = page.MaterialDiffusion.controls[:1]
    def autoscroll(scroll=True):
      page.MaterialDiffusion.auto_scroll = scroll
      page.MaterialDiffusion.update()
    progress = ProgressBar(bar_height=8)
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress
      total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    prt(Installing("Installing Replicate.com API..."))
    #run_sp("pip install --upgrade pydantic==2.0.3", realtime=True)
    try:
        import replicate
    except ModuleNotFoundError as e:
        run_process("pip install --upgrade git+https://github.com/replicate/replicate-python.git", realtime=False)
        #run_process("pip install replicate -qq", realtime=False)
        import replicate
        pass
    os.environ["REPLICATE_API_TOKEN"] = prefs['Replicate_api_key']
    #export REPLICATE_API_TOKEN=
    '''try:
        
    except Exception as e:
        alert_msg(page, f"Seems like your Replicate API Token is Invalid. Check it again...", content=Text(str(e)))
        return'''
    import requests
    clear_last()
    prt("Generating your Material Diffusion SDXL Image...")
    prt(progress)
    autoscroll(False)
    random_seed = get_seed(materialdiffusion_sdxl_prefs['seed'])
    #input = {'prompt':materialdiffusion_sdxl_prefs['material_prompt'], 'width':materialdiffusion_sdxl_prefs['width'], 'height':materialdiffusion_sdxl_prefs['height'], 'init_image':init_img, 'mask':mask_img, 'prompt_strength':materialdiffusion_sdxl_prefs['prompt_strength'], 'num_outputs':materialdiffusion_sdxl_prefs['num_outputs'], 'num_inference_steps':materialdiffusion_sdxl_prefs['steps'], 'guidance_scale':materialdiffusion_sdxl_prefs['guidance_scale'], 'seed':random_seed}
    try:
        images = replicate.run("pwntus/material-diffusion-sdxl:ce888cbe17a7c04d4b9c4cbd2b576715d480c55b2ba8f9f3d33f2ad70a26cd99",
            input={
                "width": int(materialdiffusion_sdxl_prefs["width"]),
                "height": int(materialdiffusion_sdxl_prefs["height"]),
                "prompt": materialdiffusion_sdxl_prefs["material_prompt"],
                "refine": "expert_ensemble_refiner" if materialdiffusion_sdxl_prefs["refine"] else "no_refiner",
                "scheduler": materialdiffusion_sdxl_prefs["scheduler"],
                "num_outputs": int(materialdiffusion_sdxl_prefs["num_outputs"]),
                "guidance_scale": float(materialdiffusion_sdxl_prefs["guidance_scale"]),
                "apply_watermark": prefs['SDXL_watermark'],
                "high_noise_frac": float(materialdiffusion_sdxl_prefs["high_noise_frac"]),
                "negative_prompt": materialdiffusion_sdxl_prefs["negative_prompt"],
                "num_inference_steps": int(materialdiffusion_sdxl_prefs["steps"])
            }
        )
        print(images)
        #images = output['output']
        #rep_version.predict(prompt=materialdiffusion_sdxl_prefs['material_prompt'], width=materialdiffusion_sdxl_prefs['width'], height=materialdiffusion_sdxl_prefs['height'], prompt_strength=materialdiffusion_sdxl_prefs['prompt_strength'], num_outputs=materialdiffusion_sdxl_prefs['num_outputs'], num_inference_steps=materialdiffusion_sdxl_prefs['steps'], guidance_scale=materialdiffusion_sdxl_prefs['guidance_scale'], seed=random_seed)
    except Exception as e:
        clear_last(2)
        alert_msg(page, f"ERROR: Couldn't create your image for some reason.  Possibly out of memory or something wrong with my code...", content=Text(str(e), selectable=True))
        return
    autoscroll(True)
    clear_last(2)
    txt2img_output = stable_dir
    batch_output = prefs['image_output']
    #print(str(images))
    if images is None:
        prt(f"ERROR: Problem generating images, check your settings and run above blocks again, or report the error to Skquark if it really seems broken.")
        return
    idx = 0
    for image in images:
        random_seed += idx
        fname = format_filename(materialdiffusion_sdxl_prefs['material_prompt'])
        seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
        fname = f'{materialdiffusion_sdxl_prefs["file_prefix"]}{fname}{seed_suffix}'
        txt2img_output = stable_dir
        if bool(materialdiffusion_sdxl_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, materialdiffusion_sdxl_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        image_path = available_file(txt2img_output, fname, 1)
        #image.save(image_path)
        response = requests.get(image, stream=True)
        with open(image_path, "wb") as f:
            f.write(response.content)
        new_file = image_path.rpartition(slash)[2]
        if not materialdiffusion_sdxl_prefs['display_upscaled_image'] or not materialdiffusion_sdxl_prefs['apply_ESRGAN_upscale']:
            #prt(Row([Img(src=image_path, width=materialdiffusion_sdxl_prefs['width'], height=materialdiffusion_sdxl_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            save_metadata(image_path, materialdiffusion_sdxl_prefs, "Material Diffusion SDXL", seed=random_seed)
            prt(Row([ImageButton(src=image_path, width=int(materialdiffusion_sdxl_prefs['width']), height=int(materialdiffusion_sdxl_prefs['height']), page=page)], alignment=MainAxisAlignment.CENTER))
        batch_output = os.path.join(prefs['image_output'], materialdiffusion_sdxl_prefs['batch_folder_name'])
        if not os.path.exists(batch_output):
            os.makedirs(batch_output)
        if storage_type == "PyDrive Google Drive":
            newFolder = gdrive.CreateFile({'title': materialdiffusion_sdxl_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
            newFolder.Upload()
            batch_output = newFolder
        out_path = batch_output# if save_to_GDrive else txt2img_output
        new_path = os.path.join(out_path, new_file)
        if materialdiffusion_sdxl_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            upscaled_path = os.path.join(out_path, new_file)
            upscale_image(image_path, upscaled_path, scale=materialdiffusion_sdxl_prefs["enlarge_scale"])
            image_path = upscaled_path
            save_metadata(image_path, materialdiffusion_sdxl_prefs, "Material Diffusion SDXL", seed=random_seed)
            if materialdiffusion_sdxl_prefs['display_upscaled_image']:
                prt(Row([Img(src=asset_dir(upscaled_path), width=int(materialdiffusion_sdxl_prefs['width']) * float(materialdiffusion_sdxl_prefs["enlarge_scale"]), height=int(materialdiffusion_sdxl_prefs['height']) * float(materialdiffusion_sdxl_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        else:
            new_path
            try:
              shutil.copy(image_path, new_path)
            except shutil.SameFileError: pass
        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_DiT(page, from_list=False):
    global DiT_prefs, pipe_DiT
    if not check_diffusers(page): return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      page.DiT.controls.append(line)
      if update:
        page.DiT.update()
    def clear_last(lines=1):
      clear_line(page.DiT, lines=lines)
    def clear_list():
      page.DiT.controls = page.DiT.controls[:1]
    def autoscroll(scroll=True):
      page.DiT.auto_scroll = scroll
      page.DiT.update()
    progress = ProgressBar(bar_height=8)
    total_steps = DiT_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    DiT_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        DiT_prompts.append(p.prompt)
    else:
      if not bool(DiT_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      DiT_prompts.append(DiT_prefs['prompt'])
    clear_list()
    autoscroll(True)
    from PIL.PngImagePlugin import PngInfo
    clear_pipes('DiT')
    if pipe_DiT == None:
        from diffusers import DiTPipeline
        prt(Installing("Downloading DiT Pipeline..."))
        try:
            pipe_DiT = DiTPipeline.from_pretrained("facebook/DiT-XL-2-512", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            pipe_DiT.to(torch_device)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Downloading DiT Pipeline", content=Text(str(e)))
            return
        pipe_DiT.set_progress_bar_config(disable=True)
        clear_last()
    s = "s" if DiT_prefs['num_images'] > 1 else ""
    prt(f"Generating DiT{s} of your Image...")
    batch_output = os.path.join(stable_dir, DiT_prefs['batch_folder_name'])
    makedir(batch_output)
    batch_output = os.path.join(prefs['image_output'], DiT_prefs['batch_folder_name'])
    makedir(batch_output)
    for pr in DiT_prompts:
        for num in range(DiT_prefs['num_images']):
            prt(progress)
            autoscroll(False)
            random_seed = get_seed(int(DiT_prefs['seed']) + num)
            generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            words = [w.strip() for w in pr.split(',')]
            try:
                ids = pipe_DiT.get_label_ids(words)
            except ValueError as e:
                clear_last()
                alert_msg(page, "ImageNet Token not found...", content=Text(str(e)))
                pass
            if len(ids) == 0:
                clear_last()
                alert_msg("No ImageNet class phases found in your prompt list. See full 1000 list for classifications.", page)
                return
            try:
                images = pipe_DiT(ids, num_inference_steps=DiT_prefs['num_inference_steps'], guidance_scale=DiT_prefs['guidance_scale'], generator=generator).images #, callback=callback_fnc, callback_steps=1
            except Exception as e:
                clear_last()
                alert_msg(page, "Error running DiT Pipeline", content=Text(str(e)))
                return
            clear_last()
            autoscroll(True)
            fname = format_filename(pr)

            if prefs['file_suffix_seed']: fname += f"-{random_seed}"
            for image in images:
                image_path = available_file(os.path.join(stable_dir, DiT_prefs['batch_folder_name']), fname, num)
                unscaled_path = image_path
                output_file = image_path.rpartition(slash)[2]
                image.save(image_path)
                out_path = os.path.dirname(image_path)
                upscaled_path = os.path.join(out_path, output_file)
                if not DiT_prefs['display_upscaled_image'] or not DiT_prefs['apply_ESRGAN_upscale']:
                    save_metadata(unscaled_path, DiT_prefs, "DiT", "facebook/DiT-XL-2-512", random_seed, prompt=pr)
                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if DiT_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    upscale_image(image_path, upscaled_path, scale=DiT_prefs["enlarge_scale"], face_enhance=DiT_prefs["face_enhance"])
                    image_path = upscaled_path
                    save_metadata(upscaled_path, DiT_prefs, "DiT", "facebook/DiT-XL-2-512", random_seed, prompt=pr)
                    if DiT_prefs['display_upscaled_image']:
                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(DiT_prefs["enlarge_scale"]), height=512 * float(DiT_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], DiT_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], DiT_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                time.sleep(0.2)
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def get_dreamfusion(page):
    os.chdir(root_dir)
    run_process("git clone https://github.com/ashawkey/stable-dreamfusion.git -q", page=page)
    os.chdir(os.path.join(root_dir, "stable-dreamfusion"))
    run_process("pip install -r requirements.txt -q", page=page)
    run_process("pip install git+https://github.com/NVlabs/nvdiffrast/ -q", page=page)
    os.chdir(root_dir)

def run_dreamfusion(page):
    global dreamfusion_prefs, status
    def add_to_dreamfusion_output(o):
      page.dreamfusion_output.controls.append(o)
      page.dreamfusion_output.update()
    def clear_last(lines=1):
      clear_line(page.dreamfusion_output, lines=lines)
    if not check_diffusers(page): return
    if not bool(dreamfusion_prefs["prompt_text"].strip()):
      alert_msg(page, "You must enter a simple prompt to generate 3D model from...")
      return
    page.dreamfusion_output.controls = []
    page.dreamfusion_output.update()
    if not status['installed_dreamfusion']:
      add_to_dreamfusion_output(Installing("Installing Stable DreamFusion 3D Pipeline..."))
      get_dreamfusion(page)
      status['installed_dreamfusion'] = True
      clear_last()
    def convert(seconds):
      seconds = seconds % (24 * 3600)
      hour = seconds // 3600
      seconds %= 3600
      minutes = seconds // 60
      seconds %= 60
      return "%d:%02d" % (hour, minutes)
    estimate = convert(int(dreamfusion_prefs["training_iters"] * 0.7))
    add_to_dreamfusion_output(Text("Generating your 3D model, this'll take a while...  Estimating " + estimate))
    add_to_dreamfusion_output(ProgressBar())
    df_path = os.path.join(root_dir, "stable-dreamfusion")
    os.chdir(df_path)
    run_str = f'python main.py -O --text "{dreamfusion_prefs["prompt_text"]}" --workspace {dreamfusion_prefs["workspace"]} --iters {dreamfusion_prefs["training_iters"]} --lr {dreamfusion_prefs["learning_rate"]} --w {dreamfusion_prefs["training_nerf_resolution"]} --h {dreamfusion_prefs["training_nerf_resolution"]} --seed {dreamfusion_prefs["seed"]} --lambda_entropy {dreamfusion_prefs["lambda_entropy"]} --ckpt {dreamfusion_prefs["checkpoint"]} --save_mesh --max_steps {dreamfusion_prefs["max_steps"]}'
    print(run_str)
    torch.cuda.empty_cache()
    try:
      run_process(run_str, page=page)
    except:
      clear_last()
      alert_msg(page, "Error running DreamFusion, probably Out of Memory. Adjust settings & try again.")
      return
    clear_last()
    add_to_dreamfusion_output(Text("Finished generating obj model, texture and video... Hope it's good."))
    df_out = os.path.join(df_path, dreamfusion_prefs["workspace"])
    if storage_type == "Colab Google Drive":
      dreamfusion_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'dreamfusion_out', dreamfusion_prefs["workspace"])
      #os.makedirs(dreamfusion_out, exist_ok=True)
      if os.path.exists(dreamfusion_out):
        dreamfusion_out = available_folder(os.path.join(prefs['image_output'].rpartition(slash)[0], 'dreamfusion_out'), dreamfusion_prefs["workspace"], 1)
      shutil.copytree(df_out, dreamfusion_out)
      add_to_dreamfusion_output(Text(f"Saved to {dreamfusion_out}"))
    else:
      add_to_dreamfusion_output(Text(f"Saved to {df_out}"))
    # TODO: PyDrive2
    play_snd(Snd.ALERT, page)
    os.chdir(root_dir)

base_diffusion = upsampler_diffusion = point_sampler = None

def run_point_e(page):
    global point_e_prefs, status, base_diffusion, upsampler_diffusion, point_sampler
    def add_to_point_e_output(o):
      page.point_e_output.controls.append(o)
      page.point_e_output.update()
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.point_e_output.controls.append(line)
      page.point_e_output.update()
    def prt_status(text):
        nonlocal status_txt
        status_txt.value = text
        status_txt.update()
    def clear_last(lines=1):
      clear_line(page.point_e_output, lines=lines)
    if not check_diffusers(page): return
    if not bool(point_e_prefs["prompt_text"].strip()):
      alert_msg(page, "You must enter a simple prompt to generate 3D model from...")
      return
    page.point_e_output.controls = []
    page.point_e_output.update()
    point_e_dir = os.path.join(root_dir, "point-e")
    if not os.path.exists(point_e_dir):
        add_to_point_e_output(Installing("Installing OpenAI Point-E 3D Library..."))
        try:
            run_process("pip install -U scikit-image")
            run_process("git clone https://github.com/openai/point-e.git", cwd=root_dir)
            run_process("pip install .", cwd=point_e_dir)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Point-E Requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        clear_last()
    clear_pipes()
    from tqdm.auto import tqdm
    from point_e.diffusion.configs import DIFFUSION_CONFIGS, diffusion_from_config
    from point_e.util.pc_to_mesh import marching_cubes_mesh
    from point_e.diffusion.sampler import PointCloudSampler
    from point_e.models.download import load_checkpoint
    from point_e.models.configs import MODEL_CONFIGS, model_from_config
    from point_e.util.plotting import plot_point_cloud
    from point_e.util.point_cloud import PointCloud
    import skimage.measure
    from PIL import ImageOps

    if bool(point_e_prefs['batch_folder_name']):
        fname = format_filename(point_e_prefs['batch_folder_name'], force_underscore=True)
    else:
        if bool(point_e_prefs['prompt_text']):
            fname = format_filename(point_e_prefs['prompt_text'])
        else:
            alert_msg(page, "If you're not using Prompt Text, provide a name for your 3D Model.")
            return
    filename = format_filename(point_e_prefs['prompt_text'])
    #fname = f"{point_e_prefs['file_prefix']}{fname}"
    if bool(point_e_prefs['batch_folder_name']):
        point_e_out = os.path.join(point_e_dir, point_e_prefs['batch_folder_name'])
    else:
        point_e_out = point_e_dir
    if not os.path.exists(point_e_out):
        os.makedirs(point_e_out)
    #point_e_out = os.path.join(point_e_out, fname)
    #estimate = convert(int(point_e_prefs["training_iters"] * 0.7))
    init_img = None
    if bool(point_e_prefs['init_image']):
        if point_e_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(point_e_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(point_e_prefs['init_image']):
                init_img = PILImage.open(point_e_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {point_e_prefs['init_image']}")
                if not bool(point_e_prefs['prompt_text']):
                    return
        if init_img != None:
            width, height = init_img.size
            width, height = scale_dimensions(width, height, 960)
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")

    status_txt = Text("Generating your 3D model, may take a while...")
    progress = ProgressBar(bar_height=8)
    total_steps = 130
    def callback_fnc(step: int) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}"
      progress.update()
    add_to_point_e_output(status_txt)
    add_to_point_e_output(progress)
    base_name = point_e_prefs['base_model']
    base_model = model_from_config(MODEL_CONFIGS[base_name], torch_device)
    base_model.eval()
    base_diffusion = diffusion_from_config(DIFFUSION_CONFIGS[base_name])

    prt_status('Creating Upsample model...')
    upsampler_model = model_from_config(MODEL_CONFIGS['upsample'], torch_device)
    upsampler_model.eval()
    upsampler_diffusion = diffusion_from_config(DIFFUSION_CONFIGS['upsample'])

    prt_status('Downloading Base Checkpoint...')
    base_model.load_state_dict(load_checkpoint(base_name, torch_device))

    prt_status('Downloading Upsampler Checkpoint...')
    upsampler_model.load_state_dict(load_checkpoint('upsample', torch_device))
    point_sampler = PointCloudSampler(
        device=torch_device,
        models=[base_model, upsampler_model],
        diffusions=[base_diffusion, upsampler_diffusion],
        num_points=[1024, 4096 - 1024],
        aux_channels=['R', 'G', 'B'],
        guidance_scale=[point_e_prefs['guidance_scale'], 0.0],
        model_kwargs_key_filter=('texts', ''), # Do not condition the upsampler at all
    )
    samples = None
    prt_status("Generating Point-E Samples...") #images=[img]
    step = 0
    if init_img != None:
        for x in tqdm(point_sampler.sample_batch_progressive(batch_size=point_e_prefs['batch_size'], model_kwargs=dict(images=[init_img]))):
            samples = x
            step += 1
            callback_fnc(step)
    else:
        for x in tqdm(point_sampler.sample_batch_progressive(batch_size=point_e_prefs['batch_size'], model_kwargs=dict(texts=[point_e_prefs['prompt_text']]))):
            samples = x
            step += 1
            callback_fnc(step)
    #print(f"Total steps: {step}")
    pc = point_sampler.output_to_point_clouds(samples)[0]
    fig = plot_point_cloud(pc, grid_size=3, fixed_bounds=((-0.75, -0.75, -0.75),(0.75, 0.75, 0.75)))

    prt_status('Saving PointCloud NPZ file...')
    pc_file = os.path.join(point_e_out, f'{filename}.npz')
    PointCloud.save(pc, pc_file)
    sdf = 'sdf'
    model = model_from_config(MODEL_CONFIGS[sdf], torch_device)
    model.eval()

    prt_status('Loading SDF model...')
    model.load_state_dict(load_checkpoint(sdf, torch_device))
    torch.cuda.empty_cache()
    #pc = PointCloud.load('example_data/pc_corgi.npz')
    # Plot the point cloud as a sanity check.
    #fig = plot_point_cloud(pc, grid_size=2)
    #fig = plot_point_cloud(pc, grid_size=3, fixed_bounds=((-0.75, -0.75, -0.75),(0.75, 0.75, 0.75)))
    # Produce a mesh (with vertex colors)
    try:
        mesh = marching_cubes_mesh(
            pc=pc,
            model=model,
            batch_size=4096,
            grid_size=32, # increase to 128 for resolution used in evals
            progress=True,
        )
    except Exception as e:
      clear_last()
      alert_msg(page, "Error running Point-E marching_cubes_mesh.", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
      return
    # Write the mesh to a PLY file to import into some other program.
    ply_file = os.path.join(point_e_out, f'{filename}.ply')
    with open(ply_file, 'wb') as f:
        mesh.write_ply(f)

    #with open("mesh.ply", 'r') as file:
    #    print(file.name)
    #https://colab.research.google.com/drive/1Ok3ye2xWsuYOcmbAU3INN7AHy5gvvq5m
    del point_sampler
    flush()
    point_sampler = None
    clear_last()
    clear_last()
    prt("Finished generating Point Cloud and Mesh... Hope it's good.")
    if storage_type == "Colab Google Drive":
      point_e_save = os.path.join(prefs['image_output'].rpartition(slash)[0], 'point_e', point_e_prefs['batch_folder_name'])
      #os.makedirs(point_e_out, exist_ok=True)
      #if os.path.exists(point_e_save):
      #  point_e_save = available_folder(os.path.join(prefs['image_output'].rpartition(slash)[0], 'point_e'), fname, 1)
      shutil.copytree(point_e_out, point_e_save)
      prt(Text(f"Saved npz & ply files to {point_e_save}"))
    else:
      prt(Text(f"Saved npz & ply files to {point_e_out}"))
    # TODO: PyDrive2
    play_snd(Snd.ALERT, page)
    os.chdir(root_dir)

def run_shap_e(page):
    global shap_e_prefs, status
    if not check_diffusers(page): return
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.shap_e_output.controls.append(line)
      page.shap_e_output.update()
    def prt_status(text):
        nonlocal status_txt
        status_txt.value = text
        status_txt.update()
    def clear_last(lines=1, update=True):
      clear_line(page.shap_e_output, lines, update)
    if not bool(shap_e_prefs["prompt_text"].strip()):
      alert_msg(page, "You must enter a simple prompt to generate 3D model from...")
      return
    def callback_fnc(step: int) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}"
      progress.update()
    page.shap_e_output.controls = []
    page.shap_e_output.update()
    installer = Installing("Installing OpenAI Shap-E 3D Libraries...")
    prt(installer)
    
    shap_e_dir = os.path.join(root_dir, "shap-e")
    if not os.path.exists(shap_e_dir):
        try:
            #run_process("pip install -U scikit-image")
            run_process("git clone https://github.com/openai/shap-e.git", cwd=root_dir)
            run_process("pip install .", cwd=shap_e_dir)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Shap-E Requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    clear_pipes()
    from shap_e.diffusion.sample import sample_latents
    from shap_e.diffusion.gaussian_diffusion import diffusion_from_config
    from shap_e.models.download import load_model, load_config
    from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget
    from shap_e.util.image_util import load_image
    from PIL import ImageOps
    try:
      import imageio
    except Exception:
      run_sp("pip install imageio", realtime=False)
      import imageio
      pass
    try:
        xm = load_model('transmitter', device=torch_device)
        shap_e_model = load_model('image300M' if bool(shap_e_prefs['init_image']) else 'text300M' , device=torch_device)
        diffusion = diffusion_from_config(load_config('diffusion'))
    except Exception as e:
        clear_last()
        alert_msg(page, "Error downloading Shap-E models.", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    if bool(shap_e_prefs['prompt_text']):
        fname = format_filename(shap_e_prefs['prompt_text'])
    elif bool(shap_e_prefs['init_image']):
        fname = format_filename(shap_e_prefs['init_image'].rpartition(slash)[1].rparition('.')[0])
    elif bool(shap_e_prefs['batch_folder_name']):
        fname = format_filename(shap_e_prefs['batch_folder_name'], force_underscore=True)
    else:
        alert_msg(page, "If you're not using Prompt Text, provide a name for your 3D Model.")
        return

    #filename = format_filename(shap_e_prefs['prompt_text'])
    #fname = f"{shap_e_prefs['file_prefix']}{fname}"
    shap_e_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'shap_e', shap_e_prefs['batch_folder_name'])
    #if bool(shap_e_prefs['batch_folder_name']):
    #    shap_e_out = os.path.join(shap_e_dir, shap_e_prefs['batch_folder_name'])
    #else:
    #    shap_e_out = shap_e_dir
    if not os.path.exists(shap_e_out):
        os.makedirs(shap_e_out)
    #shap_e_out = os.path.join(shap_e_out, fname)
    #estimate = convert(int(shap_e_prefs["training_iters"] * 0.7))
    init_img = None
    if bool(shap_e_prefs['init_image']):
        if shap_e_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(shap_e_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(shap_e_prefs['init_image']):
                init_img = PILImage.open(shap_e_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {shap_e_prefs['init_image']}")
                if not bool(shap_e_prefs['prompt_text']):
                    return
        if init_img != None:
            width, height = init_img.size
            width, height = scale_dimensions(width, height, 512)
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")

    status_txt = Text("Generating your 3D model... See console for progress.")
    progress = ProgressBar(bar_height=8)
    total_steps = shap_e_prefs['karras_steps']
    
    clear_last(update=False)
    prt(status_txt)
    prt(progress)
    if init_img == None:
        model_kwargs = dict(texts=[shap_e_prefs['prompt_text']] * shap_e_prefs['batch_size'])
    else:
        model_kwargs = dict(images=[init_img] * shap_e_prefs['batch_size'])
    try:
        latents = sample_latents(
            batch_size=shap_e_prefs['batch_size'],
            model=shap_e_model,
            diffusion=diffusion,
            guidance_scale=shap_e_prefs['guidance_scale'],
            model_kwargs=model_kwargs,
            progress=True,
            clip_denoised=True,
            use_fp16=True,
            use_karras=shap_e_prefs['use_karras'],
            karras_steps=shap_e_prefs['karras_steps'],
            sigma_min=1e-3,
            sigma_max=160,
            s_churn=0,
        )
    except Exception as e:
        clear_last()
        alert_msg(page, "Error running Shap-E sample_latents.", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    prt_status("Generating Shap-E 3D Models...") #images=[img]
    step = 0
    try:
        cameras = create_pan_cameras(shap_e_prefs['size'], torch_device)
        for i, latent in enumerate(latents):
            img_file = os.path.join(shap_e_out, f'{fname}_{i}.png')
            images = decode_latent_images(xm, latent, cameras, rendering_mode=shap_e_prefs['render_mode'].lower())
            #images.save(img_file)
            if is_Colab:
                IPython.display(gif_widget(images))
            #callback_fnc(i)
    except Exception as e:
        clear_last()
        alert_msg(page, "Error running Shap-E decode_latent_images.", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    if shap_e_prefs['save_frames']:
        imgs = []
        for i, img in enumerate(images):
            img_file = os.path.join(shap_e_out, f'{fname}_{i}.png')
            img.save(img_file)
            #imgs.append(imageio.imread(np.asarray(img)))
    gif_file = os.path.join(shap_e_out, f'{fname}.gif')
    #imageio.mimsave(gif_file, imgs, 'GIF', duration=100, loop=0)
    images[0].save(gif_file, save_all=True, append_images=images[1:], duration=100, loop=0)

    prt_status('Saving PLY mesh file...')
    from shap_e.util.notebooks import decode_latent_mesh
    try:
        for i, latent in enumerate(latents):
            pc_file = os.path.join(shap_e_out, f'{fname}_{i}.ply')
            with open(pc_file, 'wb') as f:
                decode_latent_mesh(xm, latent).tri_mesh().write_ply(f)
    except Exception as e:
      clear_last()
      alert_msg(page, "Error running Shap-E decode_latent_mesh.", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
      return
    del xm
    del shap_e_model
    del diffusion
    del latents
    flush()
    clear_last(update=False)
    clear_last(update=False)
    prt(ImageButton(src=gif_file, width=shap_e_prefs['size'], height=shap_e_prefs['size'], data=gif_file, subtitle=pc_file, page=page))
    prt("Finished generating Shap-E Mesh... Hope it's good.")
    play_snd(Snd.ALERT, page)
    os.chdir(root_dir)

def run_shap_e2(page):
    global shap_e_prefs, pipe_shap_e, status
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.shap_e_output.controls.append(line)
      page.shap_e_output.update()
    def prt_status(text):
        nonlocal status_txt
        status_txt.value = text
        status_txt.update()
    def clear_last(lines=1, update=True):
      clear_line(page.shap_e_output, lines, update)
    if not bool(shap_e_prefs["prompt_text"].strip()):
      alert_msg(page, "You must enter a simple prompt to generate 3D model from...")
      return
    page.shap_e_output.controls = []
    page.shap_e_output.update()
    shap_e_dir = os.path.join(root_dir, "shap-e")
    installer = Installing("Installing OpenAI Shap-E 3D Libraries...")
    prt(installer)
    from diffusers import DiffusionPipeline
    from diffusers.utils import export_to_ply
    if 'loaded_shap_e' not in status: status['loaded_shap_e'] = ""
    shap_e_type = "text"
    if bool(shap_e_prefs['prompt_text']):
        fname = format_filename(shap_e_prefs['prompt_text'])
    elif bool(shap_e_prefs['init_image']):
        shap_e_type = "img2img"
        fname = format_filename(shap_e_prefs['init_image'].rpartition(slash)[1].rparition('.')[0])
    elif bool(shap_e_prefs['batch_folder_name']):
        fname = format_filename(shap_e_prefs['batch_folder_name'], force_underscore=True)
    else:
        alert_msg(page, "If you're not using Prompt Text, provide a name for your 3D Model.")
        return
    repo = "openai/shap-e"
    if status['loaded_shap_e'] == shap_e_type:
        clear_pipes("shap_e")
    else:
        clear_pipes()
    if pipe_shap_e == None:
        try:
            installer.status(f"...loading Shap-E {shap_e_type} pipeline")
            if shap_e_type == "img2img":
                from diffusers import ShapEImg2ImgPipeline
                pipe_shap_e = ShapEImg2ImgPipeline.from_pretrained("openai/shap-e-img2img", torch_dtype=torch.float16, variant="fp16", use_safetensors=True)
            elif shap_e_type == "text":
                from diffusers import ShapEPipeline
                pipe_shap_e = ShapEPipeline.from_pretrained("openai/shap-e", torch_dtype=torch.float16, variant="fp16", use_safetensors=True)
            pipe_shap_e = pipe_shap_e.to(torch_device)
            status['loaded_shap_e'] = shap_e_type
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Shap-E Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    from PIL import ImageOps
    try:
      import trimesh
    except Exception:
      installer.status("...installing trimesh")
      run_sp("pip install trimesh", realtime=False)
      import trimesh
      pass
    
    #filename = format_filename(shap_e_prefs['prompt_text'])
    #fname = f"{shap_e_prefs['file_prefix']}{fname}"
    shap_e_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'shap_e', shap_e_prefs['batch_folder_name'])
    #if bool(shap_e_prefs['batch_folder_name']):
    #    shap_e_out = os.path.join(shap_e_dir, shap_e_prefs['batch_folder_name'])
    #else:
    #    shap_e_out = shap_e_dir
    if not os.path.exists(shap_e_out):
        os.makedirs(shap_e_out)
    #shap_e_out = os.path.join(shap_e_out, fname)
    #estimate = convert(int(shap_e_prefs["training_iters"] * 0.7))
    init_img = None
    if bool(shap_e_prefs['init_image']):
      # TODO: Make Multi-Image List
        if shap_e_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(shap_e_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(shap_e_prefs['init_image']):
                init_img = PILImage.open(shap_e_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {shap_e_prefs['init_image']}")
                if not bool(shap_e_prefs['prompt_text']):
                    return
        if init_img != None:
            width, height = init_img.size
            width, height = scale_dimensions(width, height, 512)
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")

    status_txt = Text("Generating your 3D model... See console for progress.")
    progress = ProgressBar(bar_height=8)
    total_steps = shap_e_prefs['karras_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"[{int(percent * 100)}%] {step +1} / {total_steps}"
      progress.update()
    clear_last(update=False)
    prt(status_txt)
    prt(progress)
    if shap_e_type == "text":
        model_kwargs = dict(texts=shap_e_prefs['prompt_text'])
    else:
        model_kwargs = dict(images=init_img)
    try:
        images = pipe_shap_e(**model_kwargs, num_images_per_prompt=shap_e_prefs['batch_size'], guidance_scale=shap_e_prefs['guidance_scale'], num_inference_steps=shap_e_prefs['karras_steps'], frame_size=shap_e_prefs['size'], callback=callback_fnc)
    except Exception as e:
        clear_last()
        alert_msg(page, "Error running Shap-E sample_latents.", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    #images = pipe_shap_e(shap_e_prefs['prompt'], guidance_scale=shap_e_prefs['guidance_scale'], num_inference_steps=shap_e_prefs['karras_steps'], frame_size=shap_e_prefs['size'], output_type="mesh").images
    
    prt_status("Generating Shap-E 3D Models...") #images=[img]
    step = 0
    if shap_e_prefs['save_frames']:
        imgs = []
        for i, img in enumerate(images):
            img_file = os.path.join(shap_e_out, f'{fname}_{i}.png')
            img.save(img_file)
            #imgs.append(imageio.imread(np.asarray(img)))
    gif_file = os.path.join(shap_e_out, f'{fname}.gif')
    ply_file = os.path.join(shap_e_out, f'{fname}.ply')
    glb_file = os.path.join(shap_e_out, f'{fname}.glb')
    #imageio.mimsave(gif_file, imgs, 'GIF', duration=100, loop=0)
    images[0].save(gif_file, save_all=True, append_images=images[1:], duration=100, loop=0)

    prt_status('Saving PLY mesh file...')
    ply_path = export_to_ply(images[0], ply_file)
    print(f"saved to folder: {ply_path}")
    mesh = trimesh.load(ply_path)
    rot = trimesh.transformations.rotation_matrix(-np.pi / 2, [1, 0, 0])
    mesh = mesh.apply_transform(rot)
    mesh.export(glb_file, file_type="glb")

    flush()
    clear_last(update=False)
    clear_last(update=False)
    prt(ImageButton(src=gif_file, width=shap_e_prefs['size'], height=shap_e_prefs['size'], data=gif_file, subtitle=ply_path, page=page))
    prt("Finished generating Shap-E Mesh... Hope it's good.")
    play_snd(Snd.ALERT, page)
    os.chdir(root_dir)

def run_zoe_depth(page):
    global zoe_depth_prefs, pipe_zoe_depth
    def prt(line):
        if type(line) == str:
            line = Text(line, size=17)
        page.ZoeDepth.controls.append(line)
        page.ZoeDepth.update()
    def clear_last(lines=1):
      clear_line(page.ZoeDepth, lines=lines)
    def clear_list():
      page.ZoeDepth.controls = page.ZoeDepth.controls[:1]
    def autoscroll(scroll=True):
      page.ZoeDepth.auto_scroll = scroll
      page.ZoeDepth.update()
    if not bool(zoe_depth_prefs['init_image']):
        alert_msg(page, f"ERROR: You must provide an init image to prrocess.")
        return
    file_name = "zoedepth"
    if zoe_depth_prefs['init_image'].startswith('http'):
        image = PILImage.open(requests.get(zoe_depth_prefs['init_image'], stream=True).raw)
        file_name = zoe_depth_prefs['init_image'].rpartition('/')[2]
    else:
        if os.path.isfile(zoe_depth_prefs['init_image']):
            image = PILImage.open(zoe_depth_prefs['init_image'])
            file_name = os.path.basename(zoe_depth_prefs['init_image'])
        else:
            alert_msg(page, f"ERROR: Couldn't find your init_image {zoe_depth_prefs['init_image']}")
            return
    if '.' in file_name:
        file_name = file_name.rpartition('.')[0]
    file_name = format_filename(file_name)
    progress = ProgressBar(bar_height=8)
    installer = Installing("Installing ZoeDepth Image-to-3D Pipeline...")
    clear_list()
    prt(installer)
    pip_install("timm trimesh h5py hdf5 matplotlib matplotlib-base opencv scipy", installer=installer, upgrade=True)
    zoe_depth_dir = os.path.join(root_dir, "ZoeDepth")
    if not os.path.exists(zoe_depth_dir):
        try:
            installer.status("...cloning isl-org/ZoeDepth.git")
            run_sp("git clone https://github.com/isl-org/ZoeDepth.git", cwd=root_dir, realtime=False)
        except Exception as e:
            clear_last()
            alert_msg(page, f"Error Installing github.com/isl-org/ZoeDepth...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    if zoe_depth_dir not in sys.path:
        sys.path.append(zoe_depth_dir)
    try:
        installer.status("...running sanity check")
        run_sp("python sanity.py", cwd=zoe_depth_dir, realtime=False)
    except Exception as e:
        clear_last()
        alert_msg(page, f"Error Running ZoeDepth sanity.py...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    from zoedepth.utils.misc import get_image_from_url, colorize, save_raw_16bit
    import matplotlib.pyplot as plt
    import trimesh
    from zoedepth.utils.geometry import depth_to_points, create_triangles
    from functools import partial

    def depth_edges_mask(depth):
        depth_dx, depth_dy = np.gradient(depth)
        depth_grad = np.sqrt(depth_dx ** 2 + depth_dy ** 2)
        mask = depth_grad > 0.05
        return mask
    def pano_depth_to_world_points(depth):
        radius = depth.flatten()
        lon = np.linspace(-np.pi, np.pi, depth.shape[1])
        lat = np.linspace(-np.pi/2, np.pi/2, depth.shape[0])
        lon, lat = np.meshgrid(lon, lat)
        lon = lon.flatten()
        lat = lat.flatten()
        # Convert to cartesian coordinates
        x = radius * np.cos(lat) * np.cos(lon)
        y = radius * np.cos(lat) * np.sin(lon)
        z = radius * np.sin(lat)
        pts3d = np.stack([x, y, z], axis=1)
        return pts3d
    
    if zoe_depth_prefs['zoe_model'] == zoe_depth_prefs['loaded_model']:
        clear_pipes('zoe_depth')
    else:
        clear_pipes()
    if pipe_zoe_depth == None:
        installer.status("...loading isl-org/ZoeDepth")#'isl-org/ZoeDepth'
        pipe_zoe_depth = torch.hub.load(zoe_depth_dir, zoe_depth_prefs["zoe_model"], pretrained=True).to(torch_device).eval()
        zoe_depth_prefs['loaded_model'] = zoe_depth_prefs['zoe_model']
    batch_output = os.path.join(prefs['image_output'], zoe_depth_prefs['batch_folder_name'])
    makedir(batch_output)
    installer.set_message("Running ZoeDepth on your Image...")
    installer.show_progress(False)
    installer.status("...infering depth")
    prt(progress)
    glb_path = available_file(batch_output, file_name, ext="glb", no_num=True)
    depth_path = available_file(batch_output, file_name, no_num=True)
    colored_path = available_file(batch_output, f"{file_name}-colored", no_num=True)
    image.thumbnail((zoe_depth_prefs['max_size'],zoe_depth_prefs['max_size']))  # limit the size of the input image
    depth = pipe_zoe_depth.infer_pil(image)
    installer.status("...save depth")
    save_raw_16bit(depth, depth_path)
    if zoe_depth_prefs['pano_360']:
        installer.status("...pano depth to world points")
        pts3d = pano_depth_to_world_points(depth)
    else:
        installer.status("...depth to points")
        pts3d = depth_to_points(depth[None])
    verts = pts3d.reshape(-1, 3)
    if zoe_depth_prefs['colorize']:
        installer.status("...colorize depth")
        colored = colorize(depth)
        PILImage.fromarray(colored).save(colored_path)
    image = np.array(image)
    installer.status("...create triangles")
    if zoe_depth_prefs['keep_edges']:
        triangles = create_triangles(image.shape[0], image.shape[1])
    else:
        triangles = create_triangles(image.shape[0], image.shape[1], mask=~depth_edges_mask(depth))
    colors = image.reshape(-1, 3)
    installer.status("...create trimesh")
    mesh = trimesh.Trimesh(vertices=verts, faces=triangles, vertex_colors=colors)
    mesh.export(glb_path)
    autoscroll(True)
    clear_last(2)
    prt(ImageButton(src=depth_path, width=depth.shape[1], height=depth.shape[0], data=depth_path, subtitle=depth_path, page=page))
    if zoe_depth_prefs['colorize']:
        prt(ImageButton(src=colored_path, width=colored.shape[1], height=colored.shape[0], data=colored_path, subtitle=colored_path, page=page))
    
    prt(f"Saved to {glb_path}")
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_marigold_depth(page):
    global marigold_depth_prefs, pipe_marigold_depth
    def prt(line):
        if type(line) == str:
            line = Text(line, size=17)
        page.MarigoldDepth.controls.append(line)
        page.MarigoldDepth.update()
    def clear_last(lines=1):
      clear_line(page.MarigoldDepth, lines=lines)
    def clear_list():
      page.MarigoldDepth.controls = page.MarigoldDepth.controls[:1]
    def autoscroll(scroll=True):
      page.MarigoldDepth.auto_scroll = scroll
      page.MarigoldDepth.update()
    file_name = "Marigold"
    if marigold_depth_prefs['init_image'].startswith('http'):
        image = PILImage.open(requests.get(marigold_depth_prefs['init_image'], stream=True).raw)
        file_name = marigold_depth_prefs['init_image'].rpartition('/')[2]
    else:
        if os.path.isfile(marigold_depth_prefs['init_image']):
            image = PILImage.open(marigold_depth_prefs['init_image'])
            file_name = os.path.basename(marigold_depth_prefs['init_image'])
        else:
            alert_msg(page, f"ERROR: Couldn't find your init_image {marigold_depth_prefs['init_image']}")
            return
    if '.' in file_name:
        file_name = file_name.rpartition('.')[0]
    file_name = format_filename(file_name)
    progress = ProgressBar(bar_height=8)
    installer = Installing("Installing Marigold Depth Pipeline...")
    clear_list()
    prt(installer)
    pip_install("matplotlib", installer=installer)
    from diffusers import DiffusionPipeline, MarigoldDepthPipeline, MarigoldNormalsPipeline
    import diffusers
    if 'loaded_marigold' not in status: status['loaded_marigold'] = ""
    if marigold_depth_prefs['create_normals']:
        model_id = "prs-eth/marigold-normals-v0-1" if not marigold_depth_prefs['use_LCM'] else "prs-eth/marigold-normals-lcm-v0-1"
    else:
        model_id = "prs-eth/marigold-v1-0" if not marigold_depth_prefs['use_LCM'] else "prs-eth/marigold-depth-lcm-v1-0"
    if model_id != status['loaded_marigold']:
        clear_pipes()
    else:
        clear_pipes('marigold_depth')
    if pipe_marigold_depth == None:
        installer.status(f"...loading {model_id}")
        mem_kwargs = {} if prefs['higher_vram_mode'] else {'torch_dtype': torch.float16, 'variant': 'fp16'}
        if marigold_depth_prefs['create_normals']:
            pipe_marigold_depth = MarigoldNormalsPipeline.from_pretrained(model_id, **mem_kwargs)
        else:
            pipe_marigold_depth = MarigoldDepthPipeline.from_pretrained(model_id, **mem_kwargs)
        pipe_marigold_depth.to(torch_device)
        pipe_marigold_depth.vae = diffusers.AutoencoderTiny.from_pretrained("madebyollin/taesd", torch_dtype=torch.float16).cuda()
        if prefs['enable_torch_compile']:
            installer.status(f"...Torch compiling unet")
            pipe_marigold_depth.unet = torch.compile(pipe.unet, mode="reduce-overhead", fullgraph=True)
        if marigold_depth_prefs['use_LCM']:
            pipe_marigold_depth = pipeline_scheduler(pipe_marigold_depth, scheduler="LCM")
        else:
            pipe_marigold_depth = pipeline_scheduler(pipe_marigold_depth, scheduler="DDIM")
        status['loaded_marigold'] = model_id
    batch_output = os.path.join(prefs['image_output'], marigold_depth_prefs['batch_folder_name'])
    makedir(batch_output)
    clear_last()
    prt("Generating Marigold Depth Estimation Images...")
    prt(progress)
    random_seed = get_seed(marigold_depth_prefs['seed'], max=1000000)
    generator = torch.Generator(device=torch_device).manual_seed(random_seed)
    marigold_output = pipe_marigold_depth(
        image,                  # Input image.
        num_inference_steps=marigold_depth_prefs['denoising_steps'],     # (optional) Number of denoising steps of each inference pass. Default: 10.
        ensemble_size=marigold_depth_prefs['ensemble_size'],       # (optional) Number of inference passes in the ensemble. Default: 10.
        processing_res=marigold_depth_prefs['processing_res'],     # (optional) Maximum resolution of processing. If set to 0: will not resize at all. Defaults to 768.
        match_input_res=marigold_depth_prefs['match_input_res'],   # (optional) Resize depth prediction to match input resolution.
        #seed=random_seed,
        generator=generator,
        # batch_size=0,           # (optional) Inference batch size, no bigger than `num_ensemble`. If set to 0, the script will automatically decide the proper batch size. Defaults to 0.
        #color_map=marigold_depth_prefs['color_map'] if marigold_depth_prefs['color_map'] != 'None' else None,   # (optional) Colormap used to colorize the depth map. Defaults to "Spectral".
        show_progress_bar=True, # (optional) If true, will show progress bars of the inference progress.
    )
    depth_path = available_file(batch_output, file_name, no_num=True)
    colored_path = available_file(batch_output, f"{file_name}-colored", no_num=True)
    if marigold_depth_prefs['create_normals']:
        depth_colored = None
        vis = pipe_marigold_depth.image_processor.visualize_normals(marigold_output.prediction)
    else:
        depth_colored = pipe_marigold_depth.image_processor.visualize_depth(marigold_output.prediction, color_map=marigold_depth_prefs['color_map'] if marigold_depth_prefs['color_map'] != 'None' else None)
        vis = pipe_marigold_depth.image_processor.export_depth_to_16bit_png(marigold_output.prediction)
    '''depth: np.ndarray = marigold_output.depth_np                    # Predicted depth map
    depth_colored = marigold_output.depth_colored      # Colorized prediction
    depth_uint16 = (depth * 65535.0).astype(np.uint16) # Save as uint16 PNG
    PILImage.fromarray(depth_uint16).save(depth_path, mode="I;16")'''
    w, h = vis[0].size
    vis[0].save(depth_path)
    if depth_colored is not None:
        depth_colored[0].save(colored_path)
    autoscroll(True)
    clear_last(2)
    prt(Row([ImageButton(src=depth_path, width=w, height=h, data=depth_path, subtitle=depth_path, page=page)], alignment=MainAxisAlignment.CENTER))
    if depth_colored is not None:
        prt(Row([ImageButton(src=colored_path, width=w, height=h, data=colored_path, subtitle=colored_path, page=page)], alignment=MainAxisAlignment.CENTER))
    prt(ImageButton(src=depth_path, width=w, height=h, data=depth_path, subtitle=depth_path, page=page))
    #if depth_colored is not None:
    #    prt(ImageButton(src=colored_path, width=depth.shape[1], height=depth.shape[0], data=colored_path, subtitle=colored_path, page=page))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_tripo(page):
    global tripo_prefs, pipe_tripo, status
    if not bool(tripo_prefs['init_image']):
        alert_msg(page, f"ERROR: You must provide an init image to prrocess.")
        return
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.Tripo.controls.append(line)
      page.Tripo.update()
    def prt_status(text):
        nonlocal status_txt
        status_txt.value = text
        status_txt.update()
    def clear_last(lines=1):
      clear_line(page.Tripo, lines=lines)
    page.Tripo.controls = page.Tripo.controls[:1]
    installer = Installing("Installing Tripo 3D Libraries...")
    prt(installer)
    tripo_dir = os.path.join(root_dir, "TripoSR")
    if not os.path.exists(tripo_dir):
        installer.status("...VAST-AI-Research/TripoSR")
        run_sp("git clone https://github.com/VAST-AI-Research/TripoSR.git", cwd=root_dir)
    if tripo_dir not in sys.path:
        sys.path.append(tripo_dir)
    pip_install("omegaconf==2.3.0 einops==0.7.0 trimesh==4.0.5 rembg huggingface-hub gradio")
    try:
      import torchmcubes
    except Exception:
      installer.status("...installing torchmcubes (slow)")
      run_sp("pip install git+https://github.com/tatsy/torchmcubes.git", realtime=False)
      pass
    name = tripo_prefs['title'] if bool(tripo_prefs['title']) else tripo_prefs['init_image'].rpartition(slash)[1].rparition('.')[0]
    fname = format_filename(name)
    import numpy as np
    from PIL import ImageOps
    import rembg
    from functools import partial
    from tsr.system import TSR
    from tsr.utils import remove_background, resize_foreground, to_gradio_3d_orientation
    clear_pipes("tripo")
    if pipe_tripo == None:
        try:
            installer.status(f"...loading Tripo 3D pipeline")
            pipe_tripo = TSR.from_pretrained("stabilityai/TripoSR", config_name="config.yaml", weight_name="model.ckpt",)
            pipe_tripo.renderer.set_chunk_size(tripo_prefs['chunk_size'])
            pipe_tripo.to(torch_device)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Tripo Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    rembg_session = rembg.new_session()
    tripo_out = os.path.join(prefs['image_output'], tripo_prefs['batch_folder_name'])
    if not os.path.exists(tripo_out):
        os.makedirs(tripo_out)
    init_img = None
    if bool(tripo_prefs['init_image']):
        if tripo_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(tripo_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(tripo_prefs['init_image']):
                init_img = PILImage.open(tripo_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {tripo_prefs['init_image']}")
                if not bool(tripo_prefs['prompt_text']):
                    return
        if init_img != None:
            width, height = init_img.size
            width, height = scale_dimensions(width, height, tripo_prefs['max_size'])
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
    def fill_background(image):
        image = np.array(image).astype(np.float32) / 255.0
        image = image[:, :, :3] * image[:, :, 3:4] + (1 - image[:, :, 3:4]) * 0.5
        image = PILImage.fromarray((image * 255.0).astype(np.uint8))
        return image

    if tripo_prefs['remove_background']:
        init_img = init_img.convert("RGB")
        init_img = remove_background(init_img, rembg_session)
        init_img = resize_foreground(init_img, float(tripo_prefs['foreground_ratio']))
        init_img = fill_background(init_img)
        #TODO: Display image with no background
        src_base64 = pil_to_base64(init_img)
        prt(Row([Img(src_base64=src_base64, width=width, height=height, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
    else:
        if init_img.mode == "RGBA":
            init_img = fill_background(init_img)
    status_txt = Text("Generating your 3D model... See console for progress.")
    progress = ProgressBar(bar_height=8)
    clear_last()
    prt(status_txt)
    prt(progress)
    obj_path = available_file(tripo_out, fname, ext='obj', no_num=True)
    ply_path = available_file(tripo_out, fname, ext='ply', no_num=True)
    glb_path = available_file(tripo_out, fname, ext='glb', no_num=True)
    model_names = and_list([f"[obj]({filepath_to_url(obj_path)})", f"[ply]({filepath_to_url(ply_path)})", f"[glb]({filepath_to_url(glb_path)})"])
    try:
        scene_codes = pipe_tripo(init_img, device=torch_device)
        mesh = pipe_tripo.extract_mesh(scene_codes, resolution=tripo_prefs['mesh_resolution'], threshold=tripo_prefs['mesh_threshold'])[0]
        mesh = to_gradio_3d_orientation(mesh)
        prt_status("Exporting Extracted Mesh to 3D Files...")
        mesh.export(obj_path)
        mesh.export(ply_path)
        mesh.export(glb_path)
    except Exception as e:
        clear_last()
        alert_msg(page, "Error running Tripo pipeline.", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    clear_last(2)
    prt(Row([Markdown(f"Saved Models as {model_names}", on_tap_link=lambda e: e.page.launch_url(e.data))], alignment=MainAxisAlignment.CENTER))
    flush()
    #prt(ImageButton(src=gif_file, width=tripo_prefs['size'], height=tripo_prefs['size'], data=gif_file, subtitle=ply_path, page=page))
    #prt("Finished generating Tripo Mesh... Hope it's good.")
    play_snd(Snd.ALERT, page)
    os.chdir(root_dir)

def run_instantmesh(page):
    global instantmesh_prefs, pipe_instantmesh, instantmesh_model, instantmesh_rembg_session, status
    if not check_diffusers(page): return
    if int(status['cpu_memory']) < 12:
        alert_msg(page, f"Sorry, you need at least 12GB CPU RAM to run this. {'Change Runtime to High-RAM and try again.' if is_Colab else 'Upgrade your memory if you want to use it.'}")
        return
    if not bool(instantmesh_prefs['init_image']):
        alert_msg(page, f"ERROR: You must provide an init image to prrocess.")
        return
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.InstantMesh.controls.append(line)
      page.InstantMesh.update()
    def prt_status(text):
        nonlocal status_txt
        status_txt.value = text
        status_txt.update()
    def clear_last(lines=1):
      clear_line(page.InstantMesh, lines=lines)
    page.InstantMesh.controls = page.InstantMesh.controls[:1]
    installer = Installing("Installing InstantMesh 3D Libraries... See console for progress.")
    prt(installer)
    instantmesh_dir = os.path.join(root_dir, "InstantMesh")
    if not os.path.exists(instantmesh_dir):
        installer.status("...cloning TencentARC/InstantMesh")
        run_sp("git clone https://github.com/TencentARC/InstantMesh.git", cwd=root_dir)
    if instantmesh_dir not in sys.path:
        sys.path.append(instantmesh_dir)
    try:
        import triton
    except ModuleNotFoundError:
        installer.status("...installing Triton")
        if sys.platform.startswith("win"):
            run_sp("pip install https://huggingface.co/r4ziel/xformers_pre_built/resolve/main/triton-2.0.0-cp310-cp310-win_amd64.whl", realtime=False)
        else:
            run_sp("pip install triton", realtime=False)
        pass
    try:
        #os.environ['LD_LIBRARY_PATH'] += "/usr/lib/wsl/lib:$LD_LIBRARY_PATH"
        import bitsandbytes
    except ModuleNotFoundError:
        installer.status("...installing bitsandbytes")
        run_sp("pip install bitsandbytes", realtime=False)
        pass
    try:
        import xformers
    except ModuleNotFoundError:
        installer.status("...installing FaceBook's Xformers (slow)")
        run_sp(f"pip install -U xformers=={'0.0.25' if upgrade_torch else '0.0.22.post7'} --index-url https://download.pytorch.org/whl/cu121", realtime=False)
        status['installed_xformers'] = True
        pass
    pip_install("pytorch-lightning==2.1.2 einops omegaconf torchmetrics webdataset tensorboard PyMCubes|mcubes rembg imageio[ffmpeg]|imageio xatlas plyfile git+https://github.com/NVlabs/nvdiffrast|nvdiffrast jax==0.4.19 jaxlib==0.4.19 ninja trimesh", installer=installer)
    os.chdir(instantmesh_dir)
    name = instantmesh_prefs['title'] if bool(instantmesh_prefs['title']) else instantmesh_prefs['init_image'].rpartition(slash)[1].rparition('.')[0]
    fname = format_filename(name)
    clear_pipes("instantmesh")

    import numpy as np
    import rembg
    from PIL import ImageOps
    from pytorch_lightning import seed_everything
    from einops import rearrange #, repeat
    from diffusers import DiffusionPipeline, EulerAncestralDiscreteScheduler
    from huggingface_hub import hf_hub_download
    from src.utils.infer_util import remove_background, resize_foreground
    from torchvision.transforms import v2
    from omegaconf import OmegaConf
    import imageio
    from src.utils.train_util import instantiate_from_config
    from src.utils.camera_util import (FOV_to_intrinsics, get_zero123plus_input_cameras,get_circular_camera_poses,)
    from src.utils.mesh_util import save_obj, save_glb, save_obj_with_mtl
    
    instantmesh_out = os.path.join(prefs['image_output'], instantmesh_prefs['batch_folder_name'])
    if not os.path.exists(instantmesh_out):
        os.makedirs(instantmesh_out)
    def get_render_cameras(batch_size=1, M=120, radius=2.5, elevation=10.0, is_flexicubes=False):
        c2ws = get_circular_camera_poses(M=M, radius=radius, elevation=elevation)
        if is_flexicubes:
            cameras = torch.linalg.inv(c2ws)
            cameras = cameras.unsqueeze(0).repeat(batch_size, 1, 1, 1)
        else:
            extrinsics = c2ws.flatten(-2)
            intrinsics = FOV_to_intrinsics(50.0).unsqueeze(0).repeat(M, 1, 1).float().flatten(-2)
            cameras = torch.cat([extrinsics, intrinsics], dim=-1)
            cameras = cameras.unsqueeze(0).repeat(batch_size, 1, 1)
        return cameras
    def images_to_video(images, output_path, fps=30):
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        frames = []
        for i in range(images.shape[0]):
            frame = (images[i].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8).clip(0, 255)
            assert frame.shape[0] == images.shape[2] and frame.shape[1] == images.shape[3], \
                f"Frame shape mismatch: {frame.shape} vs {images.shape}"
            assert frame.min() >= 0 and frame.max() <= 255, \
                f"Frame value out of range: {frame.min()} ~ {frame.max()}"
            frames.append(frame)
        imageio.mimwrite(output_path, np.stack(frames), fps=fps, codec='h264')
    def find_cuda(): # Check if CUDA_HOME or CUDA_PATH environment variables are set
        cuda_home = os.environ.get('CUDA_HOME') or os.environ.get('CUDA_PATH')
        if cuda_home and os.path.exists(cuda_home):
            return cuda_home
        nvcc_path = shutil.which('nvcc') # Search for the nvcc executable in the system's PATH
        if nvcc_path: # Remove the 'bin/nvcc' part to get the CUDA installation path
            cuda_path = os.path.dirname(os.path.dirname(nvcc_path))
            return cuda_path
        return None
    cuda_path = find_cuda()
    if not cuda_path:
        alert_msg(page, "CUDA installation not found")
        return
    config_path = 'configs/instant-mesh-large.yaml'
    config = OmegaConf.load(config_path)
    config_name = os.path.basename(config_path).replace('.yaml', '')
    model_config = config.model_config
    infer_config = config.infer_config
    IS_FLEXICUBES = True if config_name.startswith('instant-mesh') else False
    device = torch.device('cuda')
    if pipe_instantmesh == None:
        try:
            installer.status(f"...loading InstantMesh Pipeline")
            pipe_instantmesh = DiffusionPipeline.from_pretrained("sudo-ai/zero123plus-v1.2", custom_pipeline="zero123plus",torch_dtype=torch.float16,)
            pipe_instantmesh.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe_instantmesh.scheduler.config, timestep_spacing='trailing')
            installer.status(f"...downloading TencentARC/InstantMesh")
            unet_ckpt_path = hf_hub_download(repo_id="TencentARC/InstantMesh", filename="diffusion_pytorch_model.bin", repo_type="model")
            state_dict = torch.load(unet_ckpt_path, map_location='cpu')
            pipe_instantmesh.unet.load_state_dict(state_dict, strict=True)
            device = torch.device('cuda')
            pipe_instantmesh = pipe_instantmesh.to(device)
            #print('Loading reconstruction model ...')
            model_ckpt_path = hf_hub_download(repo_id="TencentARC/InstantMesh", filename="instant_mesh_large.ckpt", repo_type="model")
            instantmesh_model = instantiate_from_config(model_config)
            state_dict = torch.load(model_ckpt_path, map_location='cpu')['state_dict']
            state_dict = {k[14:]: v for k, v in state_dict.items() if k.startswith('lrm_generator.') and 'source_camera' not in k}
            instantmesh_model.load_state_dict(state_dict, strict=True)
            instantmesh_model = instantmesh_model.to(device)
        except Exception as e:
            #clear_last()
            alert_msg(page, "Error Installing InstantMesh Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            os.chdir(root_dir)
            return

    init_img = None
    if bool(instantmesh_prefs['init_image']):
        if instantmesh_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(instantmesh_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(instantmesh_prefs['init_image']):
                init_img = PILImage.open(instantmesh_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {instantmesh_prefs['init_image']}")
                if not bool(instantmesh_prefs['prompt_text']):
                    return
        if init_img != None:
            width, height = init_img.size
            width, height = scale_dimensions(width, height, instantmesh_prefs['max_size'])
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
    if instantmesh_rembg_session == None:
        instantmesh_rembg_session = rembg.new_session() if instantmesh_prefs["remove_background"] else None
    if instantmesh_prefs["remove_background"]:
        init_img = remove_background(init_img, instantmesh_rembg_session)
        init_img = resize_foreground(init_img, instantmesh_prefs["foreground_ratio"])
    status_txt = Text("Generating your 3D model... See console for progress.")
    progress = ProgressBar(bar_height=8)
    clear_last()
    prt(status_txt)
    prt(progress)
    random_seed = get_seed(instantmesh_prefs['seed'], max=1000000)
    obj_path = available_file(instantmesh_out, fname, ext='obj', no_num=True)
    glb_path = available_file(instantmesh_out, fname, ext='glb', no_num=True)
    video_path = available_file(instantmesh_out, fname, ext='mp4', no_num=True)
    image_path = available_file(instantmesh_out, fname)
    xyz_path = available_file(instantmesh_out+"-xyz", fname)
    model_names = and_list([f"[obj]({filepath_to_url(obj_path)})", f"[glb]({filepath_to_url(glb_path)})"])
    try:
        seed_everything(random_seed)
        #z123_image
        images = pipe_instantmesh(init_img, num_inference_steps=instantmesh_prefs["steps"]).images[0]
        show_image = np.asarray(images, dtype=np.uint8)
        show_image = torch.from_numpy(show_image)   # (960, 640, 3)
        show_image = rearrange(show_image, '(n h) (m w) c -> (n m) h w c', n=3, m=2)
        show_image = rearrange(show_image, '(n m) h w c -> (n h) (m w) c', n=2, m=3)
        show_image = PILImage.fromarray(show_image.numpy())
        prt_status("Exporting Mesh to 3D Files...")
        if IS_FLEXICUBES:
            instantmesh_model.init_flexicubes_geometry(device, use_renderer=False)
        instantmesh_model = instantmesh_model.eval()
        images = np.asarray(images, dtype=np.float32) / 255.0
        images = torch.from_numpy(images).permute(2, 0, 1).contiguous().float()     # (3, 960, 640)
        images = rearrange(images, 'c (n h) (m w) -> (n m) c h w', n=3, m=2)        # (6, 3, 320, 320)
        input_cameras = get_zero123plus_input_cameras(batch_size=1, radius=4.0).to(device)
        render_cameras = get_render_cameras(batch_size=1, radius=2.5, is_flexicubes=IS_FLEXICUBES).to(device)
        images = images.unsqueeze(0).to(device)
        images = v2.functional.resize(images, (320, 320), interpolation=3, antialias=True).clamp(0, 1)
        #mesh_basename = os.path.basename(obj_path).split('.')[0]
        #mesh_dirname = os.path.dirname(obj_path)
        #video_fpath = os.path.join(mesh_dirname, f"{mesh_basename}.mp4")
        #mesh_glb_fpath = os.path.join(mesh_dirname, f"{mesh_basename}.glb")
        with torch.no_grad(): # get triplane
            planes = instantmesh_model.forward_planes(images, input_cameras)
            if instantmesh_prefs["save_video"]:
                prt_status("Rendering 3D to Video File...")
                chunk_size = 20 if IS_FLEXICUBES else 1
                render_size = 384
                frames = []
                for i in range(0, render_cameras.shape[1], chunk_size):
                    if IS_FLEXICUBES:
                        frame = instantmesh_model.forward_geometry(
                            planes,
                            render_cameras[:, i:i+chunk_size],
                            render_size=render_size,
                        )['img']
                    else:
                        frame = instantmesh_model.synthesizer(
                            planes,
                            cameras=render_cameras[:, i:i+chunk_size],
                            render_size=render_size,
                        )['images_rgb']
                    frames.append(frame)
                frames = torch.cat(frames, dim=1)
                images_to_video(
                    frames[0],
                    video_path,
                    fps=30,
                )
                #print(f"Video saved to {video_path}")
            prt_status("Saving Mesh OBJ and GLB 3D Files...")
            mesh_out = instantmesh_model.extract_mesh(
                planes,
                use_texture_map=False,
                **infer_config,
            )
            vertices, faces, vertex_colors = mesh_out
            vertices = vertices[:, [1, 2, 0]]
            save_glb(vertices, faces, vertex_colors, glb_path)
            save_obj(vertices, faces, vertex_colors, obj_path)
        #shutil.copy(glb, glb_path)
        #shutil.copy(obj, obj_path)
        #imgs = PILImage.fromarray(show_image)
        show_image.save(image_path)
        save_metadata(image_path, instantmesh_prefs, f"InstantMesh 3D", "TencentARC/InstantMesh", random_seed)
        xyz = PILImage.fromarray(images)
        xyz.save(xyz_path)
        width, height = show_image.size
        width_x, height_x = xyz.size
        #return Image.fromarray(np_imgs), Image.fromarray(np_xyzs), glb_path, obj_path
    except Exception as e:
        clear_last()
        alert_msg(page, "Error running InstantMesh pipeline.", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        os.chdir(root_dir)
        return
    clear_last(2)
    prt(Row([ImageButton(src=image_path, width=width, height=height, data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
    prt(Row([ImageButton(src=xyz_path, width=width_x, height=height_x, data=xyz_path, page=page)], alignment=MainAxisAlignment.CENTER))
    prt(Row([Markdown(f"Saved Models as {model_names}{f'. Video saved to [video_path]({filepath_to_url(video_path)})' if instantmesh_prefs['save_video'] else ''}", on_tap_link=lambda e: e.page.launch_url(e.data))], alignment=MainAxisAlignment.CENTER))
    flush()
    #prt(ImageButton(src=gif_file, width=instantmesh_prefs['size'], height=instantmesh_prefs['size'], data=gif_file, subtitle=ply_path, page=page))
    #prt("Finished generating InstantMesh Mesh... Hope it's good.")
    play_snd(Snd.ALERT, page)
    os.chdir(root_dir)

def run_splatter_image(page):
    global splatter_image_prefs, pipe_splatter_image, splatter_image_model, splatter_image_rembg_session, status
    if not check_diffusers(page): return
    if int(status['cpu_memory']) < 12:
        alert_msg(page, f"Sorry, you need at least 12GB CPU RAM to run this. {'Change Runtime to High-RAM and try again.' if is_Colab else 'Upgrade your memory if you want to use it.'}")
        return
    if not bool(splatter_image_prefs['init_image']):
        alert_msg(page, f"ERROR: You must provide an init image to prrocess.")
        return
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.SplatterImage.controls.append(line)
      page.SplatterImage.update()
    def clear_last(lines=1):
      clear_line(page.SplatterImage, lines=lines)
    page.SplatterImage.controls = page.SplatterImage.controls[:1]
    installer = Installing("Installing SplatterImage 3D Libraries... See console for progress.")
    prt(installer)
    splatter_image_dir = os.path.join(root_dir, "splatter-image")
    gaussian_splatting_dir = os.path.join(root_dir, "gaussian-splatting")
    if not os.path.exists(gaussian_splatting_dir):
        installer.status("...cloning graphdeco-inria/gaussian-splatting")
        run_sp("git clone --recursive https://github.com/graphdeco-inria/gaussian-splatting.git", cwd=root_dir)
    try:
        import diff_gaussian_rasterization
    except ModuleNotFoundError:
        installer.status("...installing diff-gaussian-rasterization")
        run_sp("pip install submodules/diff-gaussian-rasterization", cwd=gaussian_splatting_dir)
        pass
    if not os.path.exists(splatter_image_dir):
        installer.status("...cloning szymanowiczs/splatter-image")
        run_sp("git clone https://github.com/szymanowiczs/splatter-image.git", cwd=root_dir)
    if splatter_image_dir not in sys.path:
        sys.path.append(splatter_image_dir)
    pip_install("ema-pytorch tqdm hydra-core omegaconf lpips plyfile timm einops imageio wandb moviepy lightning==2.0 markupsafe==2.0.1 trimesh diff_gaussian_rasterization rembg", installer=installer)
    os.chdir(splatter_image_dir)
    name = splatter_image_prefs['title'] if bool(splatter_image_prefs['title']) else splatter_image_prefs['init_image'].rpartition(slash)[1].rparition('.')[0]
    fname = format_filename(name)
    clear_pipes("splatter_image")
    import torchvision
    import numpy as np
    from omegaconf import OmegaConf
    from utils.app_utils import (
        remove_background, 
        resize_foreground, 
        set_white_background,
        resize_to_128,
        to_tensor,
        get_source_camera_v2w_rmo_and_quats,
        get_target_cameras,
        export_to_obj)
    import imageio
    import trimesh
    from scene.gaussian_predictor import GaussianSplatPredictor
    from gaussian_renderer import render_predicted
    import rembg
    from huggingface_hub import hf_hub_download
    from PIL import ImageOps
    splatter_image_out = os.path.join(prefs['image_output'], splatter_image_prefs['batch_folder_name'])
    if not os.path.exists(splatter_image_out):
        os.makedirs(splatter_image_out)
    os.environ["CUDA_VISIBLE_DEVICES"] = "1"
    device = torch.device("cuda:0")
    torch.cuda.set_device(device)
    if pipe_splatter_image == None:
        try:
            installer.status(f"...downloading SplatterImage model")
            model_cfg = OmegaConf.load(os.path.join(splatter_image_dir, "gradio_config.yaml"))
            model_ckpt_path = hf_hub_download(repo_id="szymanowiczs/splatter-image-multi-category-v1",  filename="model_latest.pth")
            #TODO: Change more settings in config
            installer.status('...loading Gaussian Splat Predictor')
            splatter_image_model = GaussianSplatPredictor(model_cfg)
            pipe_splatter_image = torch.load(model_ckpt_path, map_location=device)
            splatter_image_model.load_state_dict(pipe_splatter_image["model_state_dict"])
            splatter_image_model.to(device)
        except Exception as e:
            #clear_last()
            alert_msg(page, "Error Installing SplatterImage Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            os.chdir(root_dir)
            return
    init_img = None
    if bool(splatter_image_prefs['init_image']):
        if splatter_image_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(splatter_image_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(splatter_image_prefs['init_image']):
                init_img = PILImage.open(splatter_image_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {splatter_image_prefs['init_image']}")
                if not bool(splatter_image_prefs['prompt_text']):
                    return
        if init_img != None:
            width, height = init_img.size
            width, height = scale_dimensions(width, height, splatter_image_prefs['max_size'])
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
    if splatter_image_rembg_session == None:
        splatter_image_rembg_session = rembg.new_session() if splatter_image_prefs["remove_background"] else None
    if splatter_image_prefs["remove_background"]:
        init_img = remove_background(init_img, splatter_image_rembg_session)
        init_img = resize_foreground(init_img, splatter_image_prefs["foreground_ratio"])
        init_img = set_white_background(init_img)
    else:
        if init_img.mode == "RGBA":
            init_img = set_white_background(init_img)
    init_img = resize_to_128(init_img)
    progress = Progress("Generating your 3D model... See console for progress.")
    #status_txt = Text("Generating your 3D model... See console for progress.")
    #progress = ProgressBar(bar_height=8)
    clear_last()
    #prt(status_txt)
    prt(progress)
    #random_seed = get_seed(splatter_image_prefs['seed'], max=1000000)
    ply_path = available_file(splatter_image_out, fname, ext='ply', no_num=True)
    obj_path = available_file(splatter_image_out, fname, ext='obj', no_num=True)
    mtl_path = available_file(splatter_image_out, fname, ext='mtl', no_num=True)
    video_path = available_file(splatter_image_out, fname, ext='mp4', no_num=True)
    image_path = available_file(splatter_image_out, fname)
    model_names = and_list([f"[ply]({filepath_to_url(ply_path)})", f"[obj]({filepath_to_url(obj_path)})", f"[mtl]({filepath_to_url(mtl_path)})"])
    try:
        progress.status("...Saving Reconstructed 3D Files")
        #image = to_tensor(init_img).to(device)
        image = torch.tensor(init_img, dtype=torch.float32).permute(2, 0, 1) / 255.0
        image = image.to(device)
        view_to_world_source, rot_transform_quats = get_source_camera_v2w_rmo_and_quats()
        view_to_world_source = view_to_world_source.to(device)
        rot_transform_quats = rot_transform_quats.to(device)
        reconstruction_unactivated = splatter_image_model(
            image.unsqueeze(0).unsqueeze(0),
            view_to_world_source,
            rot_transform_quats,
            None,
            activate_output=False)
        reconstruction = {k: v[0].contiguous() for k, v in reconstruction_unactivated.items()}
        reconstruction["scaling"] = splatter_image_model.scaling_activation(reconstruction["scaling"])
        reconstruction["opacity"] = splatter_image_model.opacity_activation(reconstruction["opacity"])
        progress.status("...Rendering Images in a Loop")
        world_view_transforms, full_proj_transforms, camera_centers = get_target_cameras()
        background = torch.tensor([1, 1, 1] , dtype=torch.float32, device=device)
        loop_renders = []
        t_to_512 = torchvision.transforms.Resize(512, interpolation=torchvision.transforms.InterpolationMode.NEAREST)
        for r_idx in range(world_view_transforms.shape[0]):
            image = render_predicted(reconstruction,
                                        world_view_transforms[r_idx].to(device),
                                        full_proj_transforms[r_idx].to(device), 
                                        camera_centers[r_idx].to(device),
                                        background,
                                        model_cfg,
                                        focals_pixels=None)["render"]
            image = t_to_512(image)
            loop_renders.append(torch.clamp(image * 255, 0.0, 255.0).detach().permute(1, 2, 0).cpu().numpy().astype(np.uint8))
        imageio.mimsave(video_path, loop_renders, fps=25)
        progress.status("...Saving Mesh PLY 3D Files")
        export_to_obj(reconstruction_unactivated, ply_path)
        progress.status("...Saving Mesh OBJ 3D File")
        mesh = trimesh.open(ply_path)
        mesh.export(obj_path)
        progress.status("...Saving Texture MTL File")
        textures = mesh.visual.material.image
        with open(mtl_path, 'w') as f:
            for i, texture in enumerate(textures):
                f.write(f"newmtl material_{i}\n")
                f.write(f"map_Kd {texture}\n")
        #shutil.copy(glb, glb_path)
        #shutil.copy(obj, obj_path)
        #save_metadata(image_path, splatter_image_prefs, f"SplatterImage 3D", "TencentARC/SplatterImage", random_seed)
        #width, height = imgs.size
        #return Image.fromarray(np_imgs), Image.fromarray(np_xyzs), glb_path, obj_path
    except Exception as e:
        clear_last()
        alert_msg(page, "Error running SplatterImage pipeline.", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        os.chdir(root_dir)
        return
    clear_last()
    #prt(Row([ImageButton(src=image_path, width=width, height=height, data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
    prt(Row([Markdown(f"Saved Models as {model_names}. Video saved to [{video_path}]({filepath_to_url(video_path)})", on_tap_link=lambda e: e.page.launch_url(e.data))], alignment=MainAxisAlignment.CENTER))
    flush()
    #prt(ImageButton(src=gif_file, width=splatter_image_prefs['size'], height=splatter_image_prefs['size'], data=gif_file, subtitle=ply_path, page=page))
    #prt("Finished generating SplatterImage Mesh... Hope it's good.")
    play_snd(Snd.ALERT, page)
    os.chdir(root_dir)

def run_crm(page):
    global crm_prefs, pipe_crm, crm_rembg_session, status
    if not check_diffusers(page): return
    if int(status['cpu_memory']) < 16:
        alert_msg(page, f"Sorry, you need at least 16GB CPU RAM to run this. {'Change Runtime to High-RAM and try again.' if is_Colab else 'Upgrade your memory if you want to use it.'}")
        return
    if not bool(crm_prefs['init_image']):
        alert_msg(page, f"ERROR: You must provide an init image to prrocess.")
        return
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.CRM.controls.append(line)
      page.CRM.update()
    def prt_status(text):
        nonlocal status_txt
        status_txt.value = text
        status_txt.update()
    def clear_last(lines=1):
      clear_line(page.CRM, lines=lines)
    page.CRM.controls = page.CRM.controls[:1]
    installer = Installing("Installing CRM 3D Libraries... See console for progress.")
    prt(installer)
    crm_dir = os.path.join(root_dir, "CRM")
    if not os.path.exists(crm_dir):
        installer.status("...cloning thu-ml/CRM (big)")
        run_sp("git clone https://github.com/thu-ml/CRM.git", cwd=root_dir)
    if crm_dir not in sys.path:
        sys.path.append(crm_dir)
    pip_install("omegaconf einops==0.7.0 trimesh rembg huggingface-hub open-clip-torch==2.7.0|open_clip opencv-contrib-python-headless==4.9.0.80|cv2 opencv-python-headless==4.9.0.80 git+https://github.com/NVlabs/nvdiffrast|nvdiffrast pygltflib kiui xatlas ninja pymeshlab", installer=installer)
    try:
        import xformers
    except ModuleNotFoundError:
        installer.status("...installing FaceBook's Xformers (slow)")
        run_sp(f"pip install -U xformers=={'0.0.25' if upgrade_torch else '0.0.22.post7'} --index-url https://download.pytorch.org/whl/cu121", realtime=False)
        status['installed_xformers'] = True
        pass
    name = crm_prefs['title'] if bool(crm_prefs['title']) else crm_prefs['init_image'].rpartition(slash)[1].rparition('.')[0]
    fname = format_filename(name)
    import numpy as np
    from omegaconf import OmegaConf
    from PIL import ImageOps
    from pipelines import TwoStagePipeline
    from huggingface_hub import hf_hub_download
    import rembg
    from typing import Any
    import json
    from model import CRM
    from inference import generate3d
    clear_pipes("crm")
    os.chdir(crm_dir)
    if pipe_crm == None:
        try:
            installer.status(f"...downloading Zhengyi/CRM")
            crm_path = hf_hub_download(repo_id="Zhengyi/CRM", filename="CRM.pth")
            specs = json.load(open(os.path.join(crm_dir, "configs/specs_objaverse_total.json")))
            installer.status(f"...loading CRM specs")
            crm_model = CRM(specs).to(torch_device)
            installer.status(f"...loading OmegaConf")
            crm_model.load_state_dict(torch.load(crm_path, map_location = torch_device), strict=False)
            stage1_config = OmegaConf.load(os.path.join(crm_dir, "configs", "nf7_v3_SNR_rd_size_stroke.yaml")).config
            stage2_config = OmegaConf.load(os.path.join(crm_dir, "configs", "stage2-v2-snr.yaml")).config
            stage2_sampler_config = stage2_config.sampler
            stage1_sampler_config = stage1_config.sampler
            stage1_model_config = stage1_config.models
            stage2_model_config = stage2_config.models
            installer.status(f"...downloading ccm-diffusion")
            xyz_path = hf_hub_download(repo_id="Zhengyi/CRM", filename="ccm-diffusion.pth")
            installer.status(f"...downloading pixel-diffusion")
            pixel_path = hf_hub_download(repo_id="Zhengyi/CRM", filename="pixel-diffusion.pth")
            stage1_model_config.resume = pixel_path
            stage2_model_config.resume = xyz_path
            installer.status(f"...loading CRM 3D pipeline")
            pipe_crm = TwoStagePipeline(
                stage1_model_config,
                stage2_model_config,
                stage1_sampler_config,
                stage2_sampler_config,
                device=torch_device,
                dtype=torch.float16
            )
            installer.status(f"...creating rembg session")
            crm_rembg_session = rembg.new_session()
        except Exception as e:
            #clear_last()
            alert_msg(page, "Error Installing CRM Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            os.chdir(root_dir)
            return
    crm_out = os.path.join(prefs['image_output'], crm_prefs['batch_folder_name'])
    if not os.path.exists(crm_out):
        os.makedirs(crm_out)
    def expand_to_square(image, bg_color=(0, 0, 0, 0)): # expand image to 1:1
        width, height = image.size
        if width == height:
            return image
        new_size = (max(width, height), max(width, height))
        new_image = PILImage.new("RGBA", new_size, bg_color)
        paste_position = ((new_size[0] - width) // 2, (new_size[1] - height) // 2)
        new_image.paste(image, paste_position)
        return new_image
    def remove_background(image: PIL.Image.Image, rembg_session: Any = None, force: bool = False, **rembg_kwargs) -> PIL.Image.Image:
        do_remove = True
        if image.mode == "RGBA" and image.getextrema()[3][0] < 255:
            print("alhpa channl not enpty, skip remove background, using alpha channel as mask")
            background = PILImage.new("RGBA", image.size, (0, 0, 0, 0))
            image = PILImage.alpha_composite(background, image)
            do_remove = False
        do_remove = do_remove or force
        if do_remove:
            image = rembg.remove(image, session=rembg_session, **rembg_kwargs)
        return image
    def do_resize_content(original_image: PILImage, scale_rate):
        if scale_rate != 1:
            new_size = tuple(int(dim * scale_rate) for dim in original_image.size)
            resized_image = original_image.resize(new_size)
            padded_image = PILImage.new("RGBA", original_image.size, (0, 0, 0, 0))
            paste_position = ((original_image.width - resized_image.width) // 2, (original_image.height - resized_image.height) // 2)
            padded_image.paste(resized_image, paste_position)
            return padded_image
        else:
            return original_image
    def add_background(image, bg_color=(255, 255, 255)):
        background = PILImage.new("RGBA", image.size, bg_color)
        return PILImage.alpha_composite(background, image)
    def preprocess_image(image, foreground_ratio, backgroud_color):
        if not crm_prefs['remove_background']:
            background = PILImage.new("RGBA", image.size, (0, 0, 0, 0))
            image = PILImage.alpha_composite(background, image)
        else:
            image = remove_background(image, crm_rembg_session, force_remove=True)
        image = do_resize_content(image, foreground_ratio)
        image = expand_to_square(image)
        image = add_background(image, backgroud_color)
        return image.convert("RGB")
    init_img = None
    if bool(crm_prefs['init_image']):
        if crm_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(crm_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(crm_prefs['init_image']):
                init_img = PILImage.open(crm_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {crm_prefs['init_image']}")
                if not bool(crm_prefs['prompt_text']):
                    return
        if init_img != None:
            width, height = init_img.size
            width, height = scale_dimensions(width, height, crm_prefs['max_size'])
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
    init_img = preprocess_image(init_img, crm_prefs["foreground_ratio"], crm_prefs["background_color"])
    status_txt = Text("Generating your 3D model... See console for progress.")
    progress = ProgressBar(bar_height=8)
    clear_last()
    prt(status_txt)
    prt(progress)
    random_seed = get_seed(crm_prefs['seed'], max=1000000)
    obj_path = available_file(crm_out, fname, ext='obj', no_num=True)
    glb_path = available_file(crm_out, fname, ext='glb', no_num=True)
    image_path = available_file(crm_out, fname)
    xyz_path = available_file(crm_out+"-xyz", fname)
    model_names = and_list([f"[obj]({filepath_to_url(obj_path)})", f"[glb]({filepath_to_url(glb_path)})"])
    try:
        pipe_crm.set_seed(random_seed)
        rt_dict = pipe_crm(init_img, scale=crm_prefs["guidance_scale"], step=crm_prefs['steps'])
        prt_status("Exporting Mesh to 3D Files...")
        stage1_images = rt_dict["stage1_images"]
        stage2_images = rt_dict["stage2_images"]
        np_imgs = np.concatenate(stage1_images, 1)
        np_xyzs = np.concatenate(stage2_images, 1)
        glb, obj = generate3d(crm_model, np_imgs, np_xyzs, torch_device)
        shutil.copy(glb, glb_path)
        shutil.copy(obj, obj_path)
        imgs = PILImage.fromarray(np_imgs)
        imgs.save(image_path)
        save_metadata(image_path, crm_prefs, f"CRM 3D", "Zhengyi/CRM", random_seed)
        xyz = PILImage.fromarray(np_xyzs)
        xyz.save(xyz_path)
        width, height = imgs.size
        width_x, height_x = xyz.size
        #return Image.fromarray(np_imgs), Image.fromarray(np_xyzs), glb_path, obj_path
    except Exception as e:
        clear_last()
        alert_msg(page, "Error running CRM pipeline.", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        os.chdir(root_dir)
        return
    clear_last(2)
    prt(Row([ImageButton(src=image_path, width=width, height=height, data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
    prt(Row([ImageButton(src=xyz_path, width=width_x, height=height_x, data=xyz_path, page=page)], alignment=MainAxisAlignment.CENTER))
    prt(Row([Markdown(f"Saved Models as {model_names}", on_tap_link=lambda e: e.page.launch_url(e.data))], alignment=MainAxisAlignment.CENTER))
    flush()
    #prt(ImageButton(src=gif_file, width=crm_prefs['size'], height=crm_prefs['size'], data=gif_file, subtitle=ply_path, page=page))
    #prt("Finished generating CRM Mesh... Hope it's good.")
    play_snd(Snd.ALERT, page)
    os.chdir(root_dir)

def run_instant_ngp(page):
    global instant_ngp_prefs, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.instant_ngp_output.controls.append(line)
      page.instant_ngp_output.update()
    def clear_last(lines=1):
      clear_line(page.instant_ngp_output, lines=lines)
    if not check_diffusers(page): return
    save_path = os.path.join(root_dir, "my_ngp")
    error = False
    if not os.path.exists(save_path):
      error = True
    elif len(os.listdir(save_path)) == 0:
      error = True
    if len(page.instant_ngp_file_list.controls) == 0:
      error = True
    if error:
      alert_msg(page, "Couldn't find a list of images to train model. Add image files to the list...")
      return
    page.instant_ngp_output.controls.clear()
    page.instant_ngp_output.update()
    prt(Installing("Downloading Instant-NGP Packages..."))
    instant_ngp_dir = os.path.join(root_dir, 'instant-ngp')
    '''if not os.path.exists(diffusers_dir):
      os.chdir(root_dir)
      run_process("git clone https://github.com/Skquark/diffusers.git", realtime=False, cwd=root_dir)
    run_process('pip install git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]', cwd=root_dir, realtime=False)
    os.chdir(diffusers_dir)
    run_sp('pip install -e ".[training]"', cwd=diffusers_dir, realtime=False)
    '''
    try:
        run_sp("apt-get install \
            cmake \
            libgoogle-glog-dev \
            libgflags-dev \
            libatlas-base-dev \
            libeigen3-dev \
            libsuitesparse-dev \
            libboost-program-options-dev \
            libboost-filesystem-dev \
            libboost-graph-dev \
            libboost-system-dev \
            libboost-test-dev \
            libfreeimage-dev \
            libmetis-dev \
            libglew-dev \
            qtbase5-dev \
            libqt5opengl5-dev \
            libcgal-dev")
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR Installing Instant-NGP Packages...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
        return
    if not os.path.exists(root_dir, 'ceres-solver'):
      download_file("https://github.com/camenduru/instant-ngp-colab/releases/download/v1.0/ceres-solver.zip", root_dir)
      #run_sp("wget https://github.com/camenduru/instant-ngp-colab/releases/download/v1.0/ceres-solver.zip", realtime=False)
      run_sp(f"unzip {os.path.join(root_dir, 'ceres-solver.zip')} -d ceres-solver", realtime=False)
      os.remove(os.path.join(root_dir, 'ceres-solver.zip'))
    run_sp(f"yes | cp -r {os.path.join(root_dir, 'ceres-solver', 'lib', '.')} {os.path.join('/usr', 'local', 'lib')}", realtime=False)
    run_sp(f"chmod 755 {os.path.join(root_dir, 'ceres-solver', 'bin', 'colmap')}", realtime=False)
    run_sp(f"yes | cp -r {os.path.join(root_dir, 'ceres-solver', 'bin', '.')} {os.path.join('/usr', 'local', 'bin')}", realtime=False)
    #run_sp(f"cp -r /content/ceres-solver/bin/. /usr/local/bin")
    if not os.path.exists(root_dir, 'instant-ngp'):
      download_file("https://github.com/camenduru/instant-ngp-colab/releases/download/v1.0/instant-ngp.zip", root_dir)
      #run_sp("wget https://github.com/camenduru/instant-ngp-colab/releases/download/v1.0/instant-ngp.zip", realtime=False)
      #run_sp("unzip /content/instant-ngp.zip -d instant-ngp")
      run_sp(f"unzip {os.path.join(root_dir, 'instant-ngp.zip')} -d instant-ngp", realtime=False)
      os.remove(os.path.join(root_dir, 'instant-ngp.zip'))
    #run_sp("pip install commentjson huggingface-hub", realtime=False)
    os.chdir(instant_ngp_dir)
    #from huggingface_hub import create_repo, upload_folder
    #scene_path = "/content/drive/MyDrive/fox"
    train_path = os.path.join(root_dir, 'my_ngp')
    #train_path = "/content/train"
    #if not os.path.isdir(scene_path):
    #    raise NotADirectoryError(scene_path)
    '''
    rm -rf {train_path}
    mkdir {train_path}
    cp -r {scene_path}/. /content/train
    '''
    clear_pipes()
    clear_last()
    prt(Text("Running training on Images... This'll take a while, see console...", weight=FontWeight.BOLD))
    progress = ProgressBar(bar_height=8)
    prt(progress)
    name_of_your_model = instant_ngp_prefs['name_of_your_model']
    transforms_path = os.path.join(train_path, f"transforms.json")
    train_steps = instant_ngp_prefs['train_steps']
    snapshot_path = os.path.join(train_path, f"{train_steps}.msgpack")
    mesh_path = os.path.join(train_path, f"{name_of_your_model or train_steps}.ply")
    os.chdir(instant_ngp_dir)
    run_args = f"--scene {train_path} --mode nerf --n_steps {train_steps} --save_snapshot {snapshot_path} --save_mesh {mesh_path} --screenshot_dir {train_path}"
    if instant_ngp_prefs["vr_mode"]: run_args += " --vr"
    if instant_ngp_prefs["sharpen"] != 0.0: run_args += f" --sharpen {instant_ngp_prefs['sharpen']}"
    if instant_ngp_prefs["exposure"] != 0.0: run_args += f" --exposure {instant_ngp_prefs['exposure']}"
    try:
        run_sp(f"python ./scripts/colmap2nerf.py --colmap_matcher exhaustive --run_colmap --aabb_scale 4 --images {train_path} --out {transforms_path}", cwd=instant_ngp_dir)
        run_sp(f"python ./scripts/run.py {run_args}", cwd=instant_ngp_dir)
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR Running InstantNGP Training...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
        return
    #instant_ngp_dir = os.path.join(diffusers_dir, "examples", "dreambooth")
    #instant_ngp_dir = os.path.join(diffusers_dir, "examples", "text_to_image")
    #save_path = "./my_model"
    #if not os.path.exists(save_path):
    #  os.mkdir(save_path)
    output_dir = prefs['image_output'].rpartition(slash)[0] + slash + '3D_out'
    if bool(name_of_your_model):
      output_dir = os.path.join(output_dir, name_of_your_model)
    if not os.path.exists(output_dir): os.makedirs(output_dir)
    shutil.copytree(train_path, output_dir, dirs_exist_ok=True)
    '''video_camera_path = os.path.join(scene_path, "base_cam.json")
    if not os.path.isfile(video_camera_path):
      raise FileNotFoundError(video_camera_path)
    video_n_seconds = 5
    video_fps = 25
    width = 720
    height = 720
    output_video_path = os.path.join(scene_path, "output_video.mp4")
    python scripts/run.py {snapshot_path} --video_camera_path {video_camera_path} --video_n_seconds 2 --video_fps 25 --width 720 --height 720 --video_output {output_video_path}
    print(f"Generated video saved to:\n{output_video_path}")'''
    clear_last(2)
    prt(Markdown(f"## Your model was saved successfully to _{output_dir}_.\nNow take those files and load then locally on Windows following [instant-ngp gui instructions](https://github.com/NVlabs/instant-ngp#testbed-controls) to export videos and meshes or try MeshLab... (wish we can do that for ya here)", on_tap_link=lambda e: e.page.launch_url(e.data)))
    play_snd(Snd.ALERT, page)

def run_meshy(page):
    global meshy_prefs
    def prt(line):
        if type(line) == str:
            line = Text(line, size=17)
        page.Meshy.controls.append(line)
        page.Meshy.update()
    def clear_last(lines=1):
      clear_line(page.Meshy, lines=lines)
    def clear_list():
      page.Meshy.controls = page.Meshy.controls[:1]
    def autoscroll(scroll=True):
      page.Meshy.auto_scroll = scroll
      page.Meshy.update()
    if not bool(prefs["meshy_api_key"]):
        alert_msg(page, f"ERROR: You must provide your own Meshy.ai API Key to use...")
        return
    mode = meshy_prefs['meshy_mode']
    if not bool(meshy_prefs['init_model']) and mode == "text-to-texture":
        alert_msg(page, f"ERROR: You must provide your 3D Model file to apply a texture onto.")
        return
    if not bool(meshy_prefs['init_image']) and mode == "image-to-3d":
        alert_msg(page, f"ERROR: You must provide your image to create 3D Mesh from.")
        return
    autoscroll(True)
    clear_list()
    if mode == "text-to-texture":
        model_path = meshy_prefs["init_model"]
        if model_path.startswith('http'):
            model_path = download_file(model_path, uploads_dir)
        else:
            if not os.path.isfile(model_path):
                alert_msg(page, f"ERROR: Couldn't find your init_model {model_path}")
                return
    if mode == "image-to-3d":
        image_path = meshy_prefs["init_image"]
        if image_path.startswith('http'):
            image_path = download_file(image_path, uploads_dir)
        else:
            if not os.path.isfile(image_path):
                alert_msg(page, f"ERROR: Couldn't find your init_image {image_path}")
                return
    file_name = format_filename(meshy_prefs['title']) if bool(meshy_prefs['title']) else format_filename(meshy_prefs['prompt']) if bool(meshy_prefs['prompt']) else format_filename(meshy_prefs['batch_folder_name'])
    batch_output = os.path.join(prefs['image_output'], meshy_prefs['batch_folder_name'])
    makedir(batch_output)
    pb = Progress(f"Running Meshy {mode.title()} API Client...")
    prt(pb)
    headers = {"Authorization": f"Bearer {prefs['meshy_api_key']}"}
    random_seed = 0
    if mode == "text-to-texture":
        dest = "v1/text-to-texture"
        model_url = transfersh_file(model_path)
        payload = {
            "model_url": model_url,
            "object_prompt": meshy_prefs["prompt"],
            "style_prompt": meshy_prefs["style_prompt"],
            "art_style": meshy_prefs["art_style_texture"].lower(),
            "enable_original_uv": True,
            "enable_pbr": True,
            "resolution": meshy_prefs["resolution"],
            "negative_prompt": meshy_prefs["negative_prompt"]
        }
    elif mode == "text-to-3d":
        dest = "v2/text-to-3d"
        random_seed = get_seed(meshy_prefs['seed'])
        payload = {
            "mode": "preview",
            "prompt": meshy_prefs["prompt"],
            "art_style": meshy_prefs["art_style_3d"].lower(),
            "seed": random_seed,
            "negative_prompt": meshy_prefs["negative_prompt"]
        }
    elif mode == "image-to-3d":
        dest = "v1/image-to-3d"
        image_url = transfersh_file(image_path)
        time.sleep(1)
        payload = {
            "image_url": image_url,
            "enable_pbr": True,
        }
    try:
        response = requests.post(
            f"https://api.meshy.ai/{dest}",
            headers=headers,
            json=payload,
        )
        results = response.json()
        if response.status_code >= 400:
            alert_msg(page, f"ERROR Code {response.status_code} Running API", content=Text(results['message']))
            return
        response.raise_for_status()
        task_id = results['result']
        #print(response.json())
        #pb.set_message(f"Running Meshy.ai 3D... Credits {credits.remaining}/{credits.total}")
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR: Problem with Meshy API Client...", content=Column([Text(str(e)), Text(str(results)), Text(str(traceback.format_exc(), Text(str(payload))), selectable=True)]))
        return
    pb.status(f"...submitted task: {task_id}")
    try:
        while True:
            response = requests.get(f"https://api.meshy.ai/{dest}/{task_id}", headers=headers)
            results = response.json()
            response.raise_for_status()
            percent = int(results['progress'])
            status_info = results['status']
            pb.progress.value = percent * 0.01 if percent != 0 else None
            pb.progress.update()
            pb.status(f"...status: {status_info}")
            if status_info == "SUCCEEDED" or status_info == "FAILED" or status_info == "EXPIRED":
                break
            time.sleep(5)
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR: Problem checking Task Status...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    clear_last()
    if 'thumbnail_url' in results:
        thumbnail = download_file(results['thumbnail_url'], batch_output, file_name)
        save_metadata(thumbnail, meshy_prefs, f"Meshy.ai {mode.title()}", seed=random_seed)
        prt(Row([ImageButton(src=thumbnail, data=thumbnail, width=256, height=256, page=page)], alignment=MainAxisAlignment.CENTER))
        prt(Row([Text(thumbnail)], alignment=MainAxisAlignment.CENTER))
    if 'model_urls' in results:
        models = []
        for model, url in results['model_urls'].items():
            filename = available_file(batch_output, file_name, ext=model, no_num=True)
            fname = os.path.basename(filename).rpartition('.')[0]
            filename = download_file(url, batch_output, fname, ext=model)
            models.append(f"[{model}]({filepath_to_url(filename)})")
        model_names = and_list(models)
        prt(Row([Markdown(f"Saved Models as {model_names}", on_tap_link=lambda e: e.page.launch_url(e.data))], alignment=MainAxisAlignment.CENTER))
    #print(results)
    if 'texture_urls' in results:
        textures = []
        for result in results['texture_urls']:
            for texture, url in result.items():
                filename = available_file(batch_output, f"{file_name}-{texture}", no_num=True)
                fname = os.path.basename(filename).rpartition('.')[0]
                filename = download_file(url, batch_output, fname)
                textures.append(f"[{texture}]({filepath_to_url(filename)})")
            texture_names = and_list(textures)
            prt(Row([Markdown(f"Saved Textures {texture_names}", on_tap_link=lambda e: e.page.launch_url(e.data))], alignment=MainAxisAlignment.CENTER))
    play_snd(Snd.ALERT, page)
    def refine_model(preview_task):
        clear_last()
        pb.progress.value = None
        prt(pb)
        payload = {
            "mode": "refine",
            "preview_task_id": preview_task,
            "texture_richness": meshy_prefs["texture_richness"].lower()
        }
        try:
            response = requests.post(
                f"https://api.meshy.ai/v2/text-to-3d",
                headers=headers,
                json=payload,
            )
            response.raise_for_status()
            results = response.json()
            refine_task_id = results['result']
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Problem with Meshy Refine API Client...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        pb.status(f"...refine task id: {refine_task_id}")
        try:
            while True:
                response = requests.get(f"https://api.meshy.ai/v2/text-to-3d/{refine_task_id}", headers=headers)
                response.raise_for_status()
                results = response.json()
                percent = int(results['progress'])
                status_info = results['status']
                pb.progress.value = percent * 0.01 if percent != 0 else None
                pb.progress.update()
                pb.status(f"...status: {status_info}")
                if status_info == "SUCCEEDED" or status_info == "FAILED" or status_info == "EXPIRED":
                    break
                time.sleep(5)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Problem checking Task Status...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        clear_last()
        autoscroll(True)
        if 'thumbnail_url' in results:
            thumbnail = download_file(results['thumbnail_url'], batch_output, "refined-"+file_name)
            save_metadata(thumbnail, meshy_prefs, f"Meshy.ai {mode.title()}", seed=random_seed)
            prt(Row([ImageButton(src=thumbnail, data=thumbnail, width=256, height=256, page=page)], alignment=MainAxisAlignment.CENTER))
            prt(Row([Text(thumbnail)], alignment=MainAxisAlignment.CENTER))
        if 'model_urls' in results:
            models = []
            for model, url in results['model_urls'].items():
                filename = available_file(batch_output, file_name+"-refined", ext=model, no_num=True)
                fname = os.path.basename(filename).rpartition('.')[0]
                filename = download_file(url, batch_output, fname, ext=model)
                models.append(f"[{model}]({filepath_to_url(filename)})")
            model_names = and_list(models)
            prt(Row([Markdown(f"Saved Refined Models as {model_names}", on_tap_link=lambda e: e.page.launch_url(e.data))], alignment=MainAxisAlignment.CENTER))
        if 'texture_urls' in results:
            textures = []
            for result in results['texture_urls']:
                for texture, url in result.items():
                    filename = available_file(batch_output, f"{file_name}-refined-{texture}", no_num=True)
                    fname = os.path.basename(filename).rpartition('.')[0]
                    filename = download_file(url, batch_output, fname)
                    textures.append(f"[{texture}]({filepath_to_url(filename)})")
                texture_names = and_list(textures)
                prt(Row([Markdown(f"Saved Refined Textures {texture_names}", on_tap_link=lambda e: e.page.launch_url(e.data))], alignment=MainAxisAlignment.CENTER))
        autoscroll(False)
        play_snd(Snd.ALERT, page)
    if mode == "text-to-3d":
        prt(Row([ft.FilledTonalButton("Refine 3D Model (+20 Credits)", on_click=lambda e:refine_model(task_id))], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)


def run_luma_vid_to_3d(page):
    global luma_vid_to_3d_prefs
    def prt(line):
        if type(line) == str:
            line = Text(line, size=17)
        page.Luma.controls.append(line)
        page.Luma.update()
    def clear_last(lines=1):
      clear_line(page.Luma, lines=lines)
    def clear_list():
      page.Luma.controls = page.Luma.controls[:1]
    def autoscroll(scroll=True):
      page.Luma.auto_scroll = scroll
      page.Luma.update()
    if not bool(luma_vid_to_3d_prefs['init_video']):
        alert_msg(page, f"ERROR: You must provide your mp4 or avi file with a smooth walk-thru of scene or walk-around object.")
        return
    if not bool(prefs["luma_api_key"]):
        alert_msg(page, f"ERROR: You must provide your own LumaLabs.ai API Key to use...")
        return
    installer = Installing("Installing Luma Video-to-3D API Client...")
    autoscroll(True)
    clear_list()
    prt(installer)
    pip_install("lumaapi", installer=installer, upgrade=True)
    from lumaapi import LumaClient, CameraType
    
    video_path = luma_vid_to_3d_prefs["init_video"]
    if video_path.startswith('http'):
        installer.status("...downloading url")
        video_path = download_file(video_path, uploads_dir, ext="mp4")
    else:
        if not os.path.isfile(video_path):
            alert_msg(page, f"ERROR: Couldn't find your init_video {video_path}")
            return
    title = luma_vid_to_3d_prefs["title"]
    file_name = format_filename(title)
    camera_type = CameraType.EQUIRECTANGULAR if luma_vid_to_3d_prefs["camera_type"] == "Equirectangular 360" else CameraType.FISHEYE if luma_vid_to_3d_prefs["camera_type"] == "Fisheye Lens" else CameraType.NORMAL
    batch_output = os.path.join(prefs['image_output'], luma_vid_to_3d_prefs['batch_folder_name'])
    makedir(batch_output)
    clear_last()
    pb = Progress("Running Luma Video-to-3D on your file...")
    prt(pb)
    try:
        luma_client = LumaClient(prefs["luma_api_key"])
        credits = luma_client.credits()
        pb.set_message(f"Running Luma Video-to-3D on your file... Credits {credits.remaining}/{credits.total}")
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR: Problem Authenticating API Client...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    slug = luma_client.submit(video_path, title, cam_model=camera_type)
    pb.status(f"...submitted with slug: {slug}")
    while True:
        status_info = luma_client.status(slug)
        capture_status = str(status_info.status)
        if '.' in capture_status:
            capture_status = capture_status.rpartition('.')[2]
        pb.status(f"...status: {capture_status}")
        if str(capture_status) == "COMPLETE":
            break
        time.sleep(5)
    pb.status(f"...getting {slug}")
    captures = luma_client.get(title)
    clear_last()
    for capture_info in captures:
        for artifact in capture_info.latest_run.artifacts:
            artifact_type = artifact["type"]
            artifact_url = artifact["url"]
            fname = artifact_url.rpartition('/')[2]
            ext = fname.rpartition('.')[2]
            fname = fname.rpartition('.')[0]
            filename = available_file(batch_output, fname, ext=ext, no_num=True)
            fname = os.path.basename(filename).rpartition('.')[0]
            #print(f"type: {artifact_type} url:{artifact_url}")
            filename = download_file(artifact_url, batch_output, fname, ext=ext)
            #if ext == "jpg":
            f = filepath_to_url(filename)
            prt(Markdown(f"Saved {artifact_type} [{fname}]({f})", on_tap_link=lambda e: e.page.launch_url(e.data)))
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_dall_e(page, from_list=False):
    global dall_e_2_prefs, prefs, prompts
    if (not bool(dall_e_2_prefs['prompt']) and not from_list) or (from_list and (len(prompts) == 0)):
      alert_msg(page, "You must provide a text prompt to process your image generation...")
      return
    if not bool(prefs['OpenAI_api_key']):
      alert_msg(page, "You must provide your OpenAI API Key in Settings to process your Dall-e 2 Creation...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.DallE2.controls.append(line)
      page.DallE2.update()
    def clear_last(lines=1):
      clear_line(page.DallE2, lines=lines)
    def autoscroll(scroll=True):
      page.DallE2.auto_scroll = scroll
      page.DallE2.update()
    def clear_list():
      page.DallE2.controls = page.DallE2.controls[:1]
    progress = ProgressBar(bar_height=8)
    try:
        import openai
    except:
        prt(Installing("Installing OpenAI DALL•E 2 API..."))
        run_process("pip install -q openai", realtime=False)
        clear_last()
        import openai
        pass
    try:
        from openai import OpenAI
        openai_client = OpenAI(api_key=prefs['OpenAI_api_key'])
    except Exception as e:
        alert_msg(page, f"Seems like your OpenAI API Key is Invalid. Check it again...", content=Text(str(e)))
        return
    import requests
    from io import BytesIO
    from PIL import ImageOps

    save_dir = os.path.join(root_dir, 'dalle_inputs')
    init_img = None
    dall_e_list = []
    if from_list:
        if len(prompts) > 0:
            for p in prompts:
                dall_e_list.append({'prompt': p.prompt, 'init_image': p.arg['init_image'], 'mask_image': p.arg['mask_image']})
        else:
            alert_msg(page, f"Your Prompts List is empty. Add to your batch list to use feature.")
            return
    else:
        dall_e_list.append({'prompt': dall_e_2_prefs['prompt'], 'init_image': dall_e_2_prefs['init_image'], 'mask_image': dall_e_2_prefs['mask_image']})
    clear_list()
    autoscroll(True)
    for p in dall_e_list:
        init_image = p['init_image']
        mask_image = p['mask_image']
        if bool(init_image):
            fname = init_image.rpartition(slash)[2]
            init_file = os.path.join(save_dir, fname)
            if init_image.startswith('http'):
                init_img = PILImage.open(requests.get(init_image, stream=True).raw)
            else:
                if os.path.isfile(init_image):
                    init_img = PILImage.open(init_image)
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {init_image}")
                    return
            init_img = init_img.resize((dall_e_2_prefs['size'], dall_e_2_prefs['size']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
            init_img.save(init_file)
        mask_img = None
        if bool(mask_image):
            fname = init_image.rpartition(slash)[2]
            mask_file = os.path.join(save_dir, fname)
            if mask_image.startswith('http'):
                mask_img = PILImage.open(requests.get(mask_image, stream=True).raw)
            else:
                if os.path.isfile(mask_image):
                    mask_img = PILImage.open(mask_image)
                else:
                    alert_msg(page, f"ERROR: Couldn't find your mask_image {mask_image}")
                    return
                if dall_e_2_prefs['invert_mask']:
                    mask_img = ImageOps.invert(mask_img.convert('RGB'))
            mask_img = mask_img.resize((dall_e_2_prefs['size'], dall_e_2_prefs['size']), resample=PILImage.NEAREST)
            mask_img = ImageOps.exif_transpose(init_img).convert("RGB")
            mask_img.save(mask_file)
        #print(f'Resize to {width}x{height}')
        #clear_pipes()
        prt("Generating your DALL•E 2 Image...")
        prt(progress)
        autoscroll(False)
        try:
            if bool(init_image) and bool(dall_e_2_prefs['variation']):
                response = openai_client.images.create_variation(image=open(init_file, 'rb'), size=dall_e_2_prefs['size'], n=dall_e_2_prefs['num_images'])
            elif bool(init_image) and not bool(mask_image):
                response = openai_client.images.edit(prompt=p['prompt'], size=dall_e_2_prefs['size'], n=dall_e_2_prefs['num_images'], image=open(init_file, 'rb'))
            elif bool(init_image) and bool(mask_image):
                response = openai_client.images.edit(prompt=p['prompt'], size=dall_e_2_prefs['size'], n=dall_e_2_prefs['num_images'], image=open(init_file, 'rb'), mask=open(mask_file, 'rb'))
            else:
                response = openai_client.images.generate(prompt=p['prompt'], size=dall_e_2_prefs['size'], n=dall_e_2_prefs['num_images'])
        except Exception as e:
            clear_last()
            clear_last()
            alert_msg(page, f"ERROR: Something went wrong generating image form API...", content=Text(str(e)))
            return
        clear_last()
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = os.path.join(prefs['image_output'], dall_e_2_prefs['batch_folder_name'])
        makedir(batch_output)
        #print(str(images))
        if response is None:
            prt(f"ERROR: Problem generating images, check your settings and run above blocks again, or report the error to Skquark if it really seems broken.")
            return
        #print(str(response))
        idx = 0
        for i in response.data:
            image = i.url
            #random_seed += idx
            fname = format_filename(p['prompt'])
            #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{dall_e_2_prefs["file_prefix"]}{fname}'
            txt2img_output = stable_dir
            if bool(dall_e_2_prefs['batch_folder_name']):
                txt2img_output = os.path.join(stable_dir, dall_e_2_prefs['batch_folder_name'])
            makedir(txt2img_output)
            image_path = available_file(txt2img_output, fname, 1)
            #image.save(image_path)
            response = requests.get(image, stream=True)
            with open(image_path, "wb") as f:
                f.write(response.content)
            #img = i['url']
            new_file = image_path.rpartition(slash)[2].rpartition('-')[0]
            size = int(dall_e_2_prefs['size'].rpartition('x')[0])
            out_path = batch_output# if save_to_GDrive else txt2img_output
            new_path = available_file(out_path, new_file, idx)
            if not dall_e_2_prefs['display_upscaled_image'] or not dall_e_2_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, dall_e_2_prefs, "Dall-E 2")
                prt(Row([ImageButton(src=image_path, data=new_path, width=size, height=size, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=image_path, width=size, height=size, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            #if save_to_GDrive:
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': dall_e_2_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder

            if dall_e_2_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscaled_path = new_path
                upscale_image(image_path, upscaled_path, scale=dall_e_2_prefs["enlarge_scale"], face_enhance=dall_e_2_prefs["face_enhance"])
                save_metadata(upscaled_path, dall_e_2_prefs, "Dall-E 2")
                if dall_e_2_prefs['display_upscaled_image']:
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=size * float(dall_e_2_prefs["enlarge_scale"]), height=size * float(dall_e_2_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path,fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            else:
                shutil.copy(image_path, new_path)#os.path.join(out_path, new_file))
            prt(Row([Text(new_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_dall_e_3(page, from_list=False):
    global dall_e_3_prefs, prefs, prompts
    if (not bool(dall_e_3_prefs['prompt']) and not from_list) or (from_list and (len(prompts) == 0)):
      alert_msg(page, "You must provide a text prompt to process your image generation...")
      return
    if not bool(prefs['OpenAI_api_key']):
      alert_msg(page, "You must provide your OpenAI API Key in Settings to process your DALL•E 3 Creation...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.DallE.controls.append(line)
      page.DallE.update()
    def clear_last(lines=1):
      clear_line(page.DallE, lines=lines)
    def autoscroll(scroll=True):
      page.DallE.auto_scroll = scroll
      page.DallE.update()
    def clear_list():
      page.DallE.controls = page.DallE.controls[:1]
    progress = ProgressBar(bar_height=8)
    try:
        import openai
        #print(f"OpenAI {version.parse(openai.__version__).base_version} v{openai.__version__}")
        if version.parse(openai.__version__).base_version < version.parse("1.12.0"):
            run_process("pip uninstall -y openai", realtime=False)
            raise ModuleNotFoundError("Forcing update")
        if force_update("openai"): raise ModuleNotFoundError("Forcing update")
    except Exception:
        prt(Installing("Installing OpenAI DALL•E 3 API..."))
        run_process("pip install -q --upgrade openai", realtime=False)
        clear_last()
        import openai
        pass
    try:
        #openai.api_key = prefs['OpenAI_api_key']
        from openai import OpenAI
        client = OpenAI(api_key=prefs['OpenAI_api_key'])
    except Exception as e:
        alert_msg(page, f"Seems like your OpenAI API Key is Invalid. Check it again...", content=Text(str(e)))
        return
    import requests
    from io import BytesIO
    from PIL import ImageOps
    save_dir = os.path.join(root_dir, 'dalle_inputs')
    init_img = None
    dall_e_list = []
    if from_list:
        if len(prompts) > 0:
            for p in prompts:
                dall_e_list.append({'prompt': p.prompt, 'init_image': p.arg['init_image'], 'mask_image': p.arg['mask_image']})
        else:
            alert_msg(page, f"Your Prompts List is empty. Add to your batch list to use feature.")
            return
    else:
        dall_e_list.append({'prompt': dall_e_3_prefs['prompt'], 'init_image': dall_e_3_prefs['init_image'], 'mask_image': dall_e_3_prefs['mask_image']})
    clear_list()
    autoscroll(True)
    for p in dall_e_list:
        init_image = p['init_image']
        mask_image = p['mask_image']
        size = dall_e_3_prefs['size'].partition('x')
        w, h = int(size[0]), int(size[2])
        if bool(init_image):
            fname = init_image.rpartition(slash)[2]
            init_file = os.path.join(save_dir, fname)
            if init_image.startswith('http'):
                init_img = PILImage.open(requests.get(init_image, stream=True).raw)
            else:
                if os.path.isfile(init_image):
                    init_img = PILImage.open(init_image)
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {init_image}")
                    return
            init_img = init_img.resize((w, h), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
            init_img.save(init_file)
        mask_img = None
        if bool(mask_image):
            fname = init_image.rpartition(slash)[2]
            mask_file = os.path.join(save_dir, fname)
            if mask_image.startswith('http'):
                mask_img = PILImage.open(requests.get(mask_image, stream=True).raw)
            else:
                if os.path.isfile(mask_image):
                    mask_img = PILImage.open(mask_image)
                else:
                    alert_msg(page, f"ERROR: Couldn't find your mask_image {mask_image}")
                    return
                if dall_e_3_prefs['invert_mask']:
                    mask_img = ImageOps.invert(mask_img.convert('RGB'))
            mask_img = mask_img.resize((w, h), resample=PILImage.NEAREST)
            mask_img = ImageOps.exif_transpose(init_img).convert("RGB")
            mask_img.save(mask_file)
        prt("Generating your DALL•E 3 Image...")
        prt(progress)
        autoscroll(False)
        try:
            if bool(init_image) and bool(dall_e_3_prefs['variation']):
                response = client.images.create_variation(image=open(init_file, 'rb'), size=dall_e_3_prefs['size'], n=dall_e_3_prefs['num_images'])
            elif bool(init_image) and not bool(mask_image):
                response = client.images.edit(prompt=p['prompt'], size=dall_e_3_prefs['size'], n=dall_e_3_prefs['num_images'], image=open(init_file, 'rb'))
            elif bool(init_image) and bool(mask_image):
                response = client.images.edit(prompt=p['prompt'], size=dall_e_3_prefs['size'], n=dall_e_3_prefs['num_images'], image=open(init_file, 'rb'), mask=open(mask_file, 'rb'))
            else:
                response = client.images.generate(model="dall-e-3", prompt=p['prompt'], size=dall_e_3_prefs['size'], n=1, quality="hd" if dall_e_3_prefs['hd_quality'] else "standard", style="natural" if dall_e_3_prefs['natural_style'] else "vivid")
        except Exception as e:
            clear_last()
            clear_last()
            alert_msg(page, f"ERROR: Something went wrong generating image form API...", content=Text(str(e)))
            return
        clear_last()
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        #print(str(images))
        if response is None:
            prt(f"ERROR: Problem generating images, check your settings and run above blocks again, or report the error to Skquark if it really seems broken.")
            return
        #print(str(response))
        idx = 0
        for i in response.data:
            image = i.url #i['url']
            fname = format_filename(p['prompt'])
            fname = f'{dall_e_3_prefs["file_prefix"]}{fname}'
            txt2img_output = stable_dir
            if bool(dall_e_3_prefs['batch_folder_name']):
                txt2img_output = os.path.join(stable_dir, dall_e_3_prefs['batch_folder_name'])
            if not os.path.exists(txt2img_output):
                os.makedirs(txt2img_output)
            image_path = available_file(txt2img_output, fname, 1)
            response = requests.get(image, stream=True)
            with open(image_path, "wb") as f:
                f.write(response.content)
            new_file = image_path.rpartition(slash)[2].rpartition('-')[0]
            #size = int(dall_e_3_prefs['size'].rpartition('x')[0])
            batch_output = os.path.join(prefs['image_output'], dall_e_3_prefs['batch_folder_name'])
            makedir(batch_output)
            out_path = batch_output# if save_to_GDrive else txt2img_output
            new_path = available_file(out_path, new_file, idx)
            if not dall_e_3_prefs['display_upscaled_image'] or not dall_e_3_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, dall_e_3_prefs, "Dall-E 3")
                prt(Row([ImageButton(src=image_path, data=new_path, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': dall_e_3_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder

            if dall_e_3_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscaled_path = new_path
                upscale_image(image_path, upscaled_path, scale=dall_e_3_prefs["enlarge_scale"], face_enhance=dall_e_3_prefs["face_enhance"])
                save_metadata(upscaled_path, dall_e_3_prefs, "Dall-E 3")
                if dall_e_3_prefs['display_upscaled_image']:
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=w * float(dall_e_3_prefs["enlarge_scale"]), height=h * float(dall_e_3_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path,fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            else:
                shutil.copy(image_path, new_path)#os.path.join(out_path, new_file))
            prt(Row([Text(new_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

loaded_kandinsky_task = ""
def run_kandinsky3(page, from_list=False, with_params=False):
    global kandinsky_3_prefs, pipe_kandinsky, prefs, loaded_kandinsky_task
    if not check_diffusers(page): return
    if int(status['cpu_memory']) <= 10:
      alert_msg(page, f"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run Kandinsky 3 right now. Either Change runtime type to High-RAM mode and restart or use other Kandinsky 2.1 in Extras.")
      return
    kandinsky_3_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            kandinsky_3_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':kandinsky_3_prefs['init_image'], 'mask_image':kandinsky_3_prefs['mask_image'], 'guidance_scale':kandinsky_3_prefs['guidance_scale'], 'steps':kandinsky_3_prefs['steps'], 'width':kandinsky_3_prefs['width'], 'height':kandinsky_3_prefs['height'], 'strength':kandinsky_3_prefs['strength'], 'num_images':kandinsky_3_prefs['num_images'], 'seed':kandinsky_3_prefs['seed']})
        else:
            kandinsky_3_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':p['init_image'], 'mask_image':p['mask_image'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(kandinsky_3_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      kandinsky_3_prompts.append({'prompt': kandinsky_3_prefs['prompt'], 'negative_prompt':kandinsky_3_prefs['negative_prompt'], 'init_image':kandinsky_3_prefs['init_image'], 'mask_image':kandinsky_3_prefs['mask_image'], 'guidance_scale':kandinsky_3_prefs['guidance_scale'], 'steps':kandinsky_3_prefs['steps'], 'width':kandinsky_3_prefs['width'], 'height':kandinsky_3_prefs['height'], 'strength':kandinsky_3_prefs['strength'], 'num_images':kandinsky_3_prefs['num_images'], 'seed':kandinsky_3_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.Kandinsky.controls.append(line)
        if update:
          page.Kandinsky.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.Kandinsky, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.Kandinsky.auto_scroll = scroll
        page.Kandinsky.update()
      else:
        page.Kandinsky.auto_scroll = scroll
        page.Kandinsky.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.Kandinsky.controls = page.Kandinsky.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = kandinsky_3_prefs['steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    model_id = "kandinsky-community/kandinsky-3" if kandinsky_3_prefs['kandinsky_model'] == "Kandinsky 3" else "kandinsky-community/kandinsky-2-2-decoder"
    installer = Installing(f"Installing {kandinsky_3_prefs['kandinsky_model']} Engine & Models... See console log for progress.")
    clear_pipes("kandinsky")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    cpu_offload = kandinsky_3_prefs['cpu_offload']
    for pr in kandinsky_3_prompts:
        prt(installer)
        init_img = None
        if bool(pr['init_image']):
            fname = pr['init_image'].rpartition(slash)[2]
            #init_file = os.path.join(save_dir, fname)
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        mask_img = None
        if bool(pr['mask_image']):
            fname = pr['mask_image'].rpartition(slash)[2]
            if pr['mask_image'].startswith('http'):
                mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['mask_image']):
                    mask_img = PILImage.open(pr['mask_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your mask_image {pr['mask_image']}")
                    return
            if kandinsky_3_prefs['invert_mask']:
                mask_img = ImageOps.invert(mask_img.convert('RGB'))
            mask_img = mask_img.resize((pr['width'], pr['height']), resample=PILImage.NEAREST)
            mask_img = ImageOps.exif_transpose(mask_img).convert("RGB")
        task_type = "inpainting" if bool(pr['init_image']) and bool(pr['mask_image']) else "img2img" if bool(pr['init_image']) and not bool(pr['mask_image']) else "text2img"
        installer.status(f"...{kandinsky_3_prefs['kandinsky_model']} {task_type} Pipeline")
        if pipe_kandinsky == None:
            clear_pipes('kandinsky')
            try:
                if task_type == "text2img":
                    from diffusers import AutoPipelineForText2Image
                    pipe_kandinsky = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16, variant="fp16", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                elif task_type == "img2img":
                    from diffusers import AutoPipelineForImage2Image
                    pipe_kandinsky = AutoPipelineForImage2Image.from_pretrained(model_id, torch_dtype=torch.float16, variant="fp16", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                elif task_type == "inpainting":
                    from diffusers import AutoPipelineForInpainting#"kandinsky-community/kandinsky-2-2-decoder-inpaint"
                    pipe_kandinsky = AutoPipelineForInpainting.from_pretrained(model_id+"-inpaint", variant="fp16", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                if prefs['enable_torch_compile']:
                    installer.status(f"...Torch compiling unet")
                    pipe_kandinsky.unet.to(memory_format=torch.channels_last)
                    pipe_kandinsky.unet = torch.compile(pipe_kandinsky.unet, mode="reduce-overhead", fullgraph=True)
                    pipe_kandinsky = pipe_kandinsky.to("cuda")
                elif cpu_offload:
                    pipe_kandinsky.enable_model_cpu_offload()
                else:
                    pipe_kandinsky.to("cuda")
                loaded_kandinsky_task = task_type
            except Exception as e:
                clear_last()
                alert_msg(page, f"ERROR Initializing Kandinsky, try running without installing Diffusers first...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
        elif loaded_kandinsky_task != task_type:
            clear_pipes('kandinsky')
            try:
                if task_type == "text2img":
                    from diffusers import AutoPipelineForText2Image
                    pipe_kandinsky = AutoPipelineForText2Image.from_pipe(pipe_kandinsky)
                elif task_type == "img2img":
                    from diffusers import AutoPipelineForImage2Image
                    pipe_kandinsky = AutoPipelineForImage2Image.from_pipe(pipe_kandinsky)
                elif task_type == "inpainting":
                    from diffusers import AutoPipelineForInpainting#"kandinsky-community/kandinsky-2-2-decoder-inpaint"
                    pipe_kandinsky = AutoPipelineForInpainting.from_pipe(pipe_kandinsky)
                loaded_kandinsky_task = task_type
            except Exception as e:
                clear_last()
                alert_msg(page, f"ERROR Converting Kandinsky from pipe...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
                return
        else:
            clear_pipes('kandinsky')
        clear_last()
        prt(f"Generating your {kandinsky_3_prefs['kandinsky_model']} Image...")
        prt(progress)
        autoscroll(False)
        total_steps = pr['steps']
        random_seed = get_seed(pr['seed'])
        generator = torch.Generator(device="cuda").manual_seed(random_seed)
        try:
            if task_type == "text2img":
                images = pipe_kandinsky(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    num_images_per_prompt=pr['num_images'],
                    width=pr['width'],
                    height=pr['height'],
                    num_inference_steps=pr['steps'],
                    guidance_scale=pr['guidance_scale'],
                    generator=generator,
                    callback_on_step_end=callback_fnc,
                ).images
            elif task_type == "img2img":
                images = pipe_kandinsky(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    image=init_img,
                    strength=pr['strength'],
                    num_images_per_prompt=pr['num_images'],
                    width=pr['width'],
                    height=pr['height'],
                    num_inference_steps=pr['steps'],
                    guidance_scale=pr['guidance_scale'],
                    generator=generator,
                    callback_on_step_end=callback_fnc,
                ).images
            elif task_type == "inpainting":
                images = pipe_kandinsky(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    image=init_img,
                    mask_image=mask_img,
                    num_images_per_prompt=pr['num_images'],
                    width=pr['width'],
                    height=pr['height'],
                    num_inference_steps=pr['steps'],
                    guidance_scale=pr['guidance_scale'],
                    generator=generator,
                    callback_on_step_end=callback_fnc,
                ).images
        except Exception as e:
            clear_last(2)
            alert_msg(page, f"ERROR: Something went wrong generating {task_type} images...", content=Text(str(e)))
            return
        clear_last(2)
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(kandinsky_3_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, kandinsky_3_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        #print(str(images))
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            fname = f'{kandinsky_3_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            if not kandinsky_3_prefs['display_upscaled_image'] or not kandinsky_3_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, kandinsky_3_prefs, f"Kandinsky 3 {task_type}", "kandinsky-community/kandinsky-3", random_seed, extra=pr)
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            batch_output = os.path.join(prefs['image_output'], kandinsky_3_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)

            if kandinsky_3_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=kandinsky_3_prefs["enlarge_scale"], face_enhance=kandinsky_3_prefs["face_enhance"])
                image_path = upscaled_path
                save_metadata(upscaled_path, kandinsky_3_prefs, f"Kandinsky 3 {task_type}", "kandinsky-community/kandinsky-3", random_seed, extra=pr)
                if kandinsky_3_prefs['display_upscaled_image']:
                    prt(Row([Img(src=asset_dir(upscaled_path), width=pr['width'] * float(kandinsky_3_prefs["enlarge_scale"]), height=pr['height'] * float(kandinsky_3_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            new_file = available_file(os.path.join(prefs['image_output'], kandinsky_3_prefs['batch_folder_name']), fname, 0)
            out_path = new_file
            shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_kandinsky(page, from_list=False, with_params=False):
    global kandinsky_prefs, pipe_kandinsky, pipe_kandinsky_prior, prefs, loaded_kandinsky_task
    if not check_diffusers(page): return
    if int(status['cpu_memory']) <= 10:
      alert_msg(page, f"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run Kandinsky 2.2 right now. Either Change runtime type to High-RAM mode and restart or use other Kandinsky 2.1 in Extras.")
      return
    kandinsky_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            kandinsky_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':kandinsky_prefs['init_image'], 'mask_image':kandinsky_prefs['mask_image'], 'guidance_scale':kandinsky_prefs['guidance_scale'], 'steps':kandinsky_prefs['steps'], 'width':kandinsky_prefs['width'], 'height':kandinsky_prefs['height'], 'strength':kandinsky_prefs['strength'], 'num_images':kandinsky_prefs['num_images'], 'seed':kandinsky_prefs['seed']})
        else:
            kandinsky_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':p['init_image'], 'mask_image':p['mask_image'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(kandinsky_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      kandinsky_prompts.append({'prompt': kandinsky_prefs['prompt'], 'negative_prompt':kandinsky_prefs['negative_prompt'], 'init_image':kandinsky_prefs['init_image'], 'mask_image':kandinsky_prefs['mask_image'], 'guidance_scale':kandinsky_prefs['guidance_scale'], 'steps':kandinsky_prefs['steps'], 'width':kandinsky_prefs['width'], 'height':kandinsky_prefs['height'], 'strength':kandinsky_prefs['strength'], 'num_images':kandinsky_prefs['num_images'], 'seed':kandinsky_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.Kandinsky.controls.append(line)
        if update:
          page.Kandinsky.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.Kandinsky, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.Kandinsky.auto_scroll = scroll
        page.Kandinsky.update()
      else:
        page.Kandinsky.auto_scroll = scroll
        page.Kandinsky.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.Kandinsky.controls = page.Kandinsky.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = kandinsky_prefs['steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):#(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Kandinsky 2.2 Engine & Models... See console log for progress.")
    clear_pipes("kandinsky")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    #from diffusers import KandinskyV22PriorPipeline
    #installer.status("...kandinsky-2-2-prior Pipeline")
    #if pipe_kandinsky_prior == None:
    #    pipe_kandinsky_prior = KandinskyV22PriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16)
    #    pipe_kandinsky_prior.to("cuda")
    #save_dir = os.path.join(root_dir, 'kandinsky_inputs')
    cpu_offload = False
    for pr in kandinsky_prompts:
        prt(installer)
        init_img = None
        if bool(pr['init_image']):
            fname = pr['init_image'].rpartition(slash)[2]
            #init_file = os.path.join(save_dir, fname)
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
            #init_img.save(init_file)
        mask_img = None
        if bool(pr['mask_image']):
            fname = pr['mask_image'].rpartition(slash)[2]
            #mask_file = os.path.join(save_dir, fname)
            if pr['mask_image'].startswith('http'):
                mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['mask_image']):
                    mask_img = PILImage.open(pr['mask_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your mask_image {pr['mask_image']}")
                    return
            if kandinsky_prefs['invert_mask']:
                mask_img = ImageOps.invert(mask_img.convert('RGB'))
            mask_img = mask_img.resize((pr['width'], pr['height']), resample=PILImage.NEAREST)
            mask_img = ImageOps.exif_transpose(mask_img).convert("RGB")
            #mask_img = np.asarray(mask_img)
            #mask_img.save(mask_file)
        #print(f'Resize to {width}x{height}')
        task_type = "inpainting" if bool(pr['init_image']) and bool(pr['mask_image']) else "img2img" if bool(pr['init_image']) and not bool(pr['mask_image']) else "text2img"
        installer.status(f"...kandinsky-2-2 {task_type} Pipeline")
        if pipe_kandinsky == None or loaded_kandinsky_task != task_type:
            clear_pipes('kandinsky_prior')
            try:
                #if bool(kandinsky_prefs['init_image']) and not bool(kandinsky_prefs['mask_image']):
                #    pipe_kandinsky = get_kandinsky2('cuda', task_type='img2img', model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                #pipe_kandinsky = get_kandinsky2('cuda', task_type=task_type, model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                '''if task_type == "text2img":
                    from diffusers import KandinskyV22Pipeline
                    pipe_kandinsky = KandinskyV22Pipeline.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                elif task_type == "img2img":
                    from diffusers import KandinskyV22Img2ImgPipeline
                    pipe_kandinsky = KandinskyV22Img2ImgPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                elif task_type == "inpainting":
                    from diffusers import KandinskyV22InpaintPipeline
                    pipe_kandinsky = KandinskyV22InpaintPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-decoder-inpaint", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                '''
                if task_type == "text2img":
                    from diffusers import AutoPipelineForText2Image
                    pipe_kandinsky = AutoPipelineForText2Image.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                elif task_type == "img2img":
                    from diffusers import AutoPipelineForImage2Image
                    pipe_kandinsky = AutoPipelineForImage2Image.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                elif task_type == "inpainting":
                    from diffusers import AutoPipelineForInpainting
                    pipe_kandinsky = AutoPipelineForInpainting.from_pretrained("kandinsky-community/kandinsky-2-2-decoder-inpaint", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                if prefs['enable_torch_compile']:
                    installer.status(f"...Torch compiling unet")
                    pipe_kandinsky.unet.to(memory_format=torch.channels_last)
                    pipe_kandinsky.unet = torch.compile(pipe_kandinsky.unet, mode="reduce-overhead", fullgraph=True)
                    pipe_kandinsky = pipe_kandinsky.to("cuda")
                elif cpu_offload:
                    pipe_kandinsky.enable_model_cpu_offload()
                else:
                    pipe_kandinsky.to("cuda")
                loaded_kandinsky_task = task_type
            except Exception as e:
                clear_last()
                alert_msg(page, f"ERROR Initializing Kandinsky, try running without installing Diffusers first...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
                return
        else:
            clear_pipes('kandinsky')
        clear_last()
        prt("Generating your Kandinsky 2.2 Image...")
        prt(progress)
        autoscroll(False)
        total_steps = pr['steps']
        random_seed = get_seed(pr['seed'])
        generator = torch.Generator(device="cuda").manual_seed(random_seed)
        try:
            #image_embeds, negative_image_embeds = pipe_kandinsky_prior(pr['prompt'], negative_prompt=pr['negative_prompt'], num_inference_steps=5, guidance_scale=1.0, generator=generator).to_tuple()
            #negative_image_embeds = pipe_kandinsky_prior(pr['negative_prompt'], guidance_scale=2.0, num_inference_steps=25, generator=generator, negative_prompt=pr['negative_prompt']).images
            #guidance_scale=pr['guidance_scale'], num_inference_steps=pr['steps']
            if task_type == "text2img":
                #images_texts = [kandinsky_prefs['prompt'], init_img]
                #weights = [0.5, kandinsky_prefs['strength']]
                #images = pipe_kandinsky.generate_img2img(kandinsky_prefs['prompt'], init_img, strength=kandinsky_prefs['strength'], batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], denoised_type=kandinsky_prefs['denoised_type'], dynamic_threshold_v=kandinsky_prefs['dynamic_threshold_v'], sampler=kandinsky_prefs['sampler'], ddim_eta=kandinsky_prefs['ddim_eta'], guidance_scale=kandinsky_prefs['guidance_scale'])
                #images = pipe_kandinsky.generate_img2img(kandinsky_prefs['prompt'], init_img, strength=kandinsky_prefs['strength'], batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], prior_cf_scale=kandinsky_prefs['prior_cf_scale'], prior_steps=str(kandinsky_prefs['prior_steps']), sampler=kandinsky_prefs['sampler'], guidance_scale=kandinsky_prefs['guidance_scale'])
                #images = pipe_kandinsky.mix_images(images_texts, weights, batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], prior_cf_scale=kandinsky_prefs['prior_cf_scale'], prior_steps=str(kandinsky_prefs['prior_steps']), sampler=kandinsky_prefs['sampler'], guidance_scale=kandinsky_prefs['guidance_scale'])
                images = pipe_kandinsky(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    prior_guidance_scale=kandinsky_prefs['prior_guidance_scale'], prior_num_inference_steps=kandinsky_prefs['prior_steps'],
                    #pr['prompt'],
                    #image_embeds=image_embeds,
                    #negative_image_embeds=negative_image_embeds,
                    num_images_per_prompt=pr['num_images'],
                    width=pr['width'],
                    height=pr['height'],
                    num_inference_steps=pr['steps'],
                    guidance_scale=pr['guidance_scale'],
                    generator=generator,
                    callback_on_step_end=callback_fnc,
                ).images
            elif task_type == "img2img":
                #images = pipe_kandinsky.generate_inpainting(kandinsky_prefs['prompt'], init_img, mask_img, batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], denoised_type=kandinsky_prefs['denoised_type'], dynamic_threshold_v=kandinsky_prefs['dynamic_threshold_v'], sampler=kandinsky_prefs['sampler'], ddim_eta=kandinsky_prefs['ddim_eta'], guidance_scale=kandinsky_prefs['guidance_scale'])
                #images = pipe_kandinsky.generate_inpainting(kandinsky_prefs['prompt'], init_img, mask_img, batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], prior_cf_scale=kandinsky_prefs['prior_cf_scale'], prior_steps=str(kandinsky_prefs['prior_steps']), sampler=kandinsky_prefs['sampler'], guidance_scale=kandinsky_prefs['guidance_scale'])
                images = pipe_kandinsky(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    prior_guidance_scale=kandinsky_prefs['prior_guidance_scale'], prior_num_inference_steps=kandinsky_prefs['prior_steps'],
                    #pr['prompt'],
                    image=init_img,
                    strength=pr['strength'],
                    #image_embeds=image_embeds,
                    #negative_image_embeds=negative_image_embeds,
                    num_images_per_prompt=pr['num_images'],
                    width=pr['width'],
                    height=pr['height'],
                    num_inference_steps=pr['steps'],
                    guidance_scale=pr['guidance_scale'],
                    generator=generator,
                    callback_on_step_end=callback_fnc,
                ).images
            elif task_type == "inpainting":
                #images = pipe_kandinsky.generate_text2img(kandinsky_prefs['prompt'], batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], denoised_type=kandinsky_prefs['denoised_type'], dynamic_threshold_v=kandinsky_prefs['dynamic_threshold_v'], sampler=kandinsky_prefs['sampler'], ddim_eta=kandinsky_prefs['ddim_eta'], guidance_scale=kandinsky_prefs['guidance_scale'])
                #images = pipe_kandinsky.generate_text2img(kandinsky_prefs['prompt'], batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], prior_cf_scale=kandinsky_prefs['prior_cf_scale'], prior_steps=str(kandinsky_prefs['prior_steps']), sampler=kandinsky_prefs['sampler'], guidance_scale=kandinsky_prefs['guidance_scale'])
                images = pipe_kandinsky(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    prior_guidance_scale=kandinsky_prefs['prior_guidance_scale'], prior_num_inference_steps=kandinsky_prefs['prior_steps'],
                    #pr['prompt'],
                    image=init_img,
                    mask_image=mask_img,
                    #strength=pr['strength'],
                    #image_embeds=image_embeds,
                    #negative_image_embeds=negative_image_embeds,
                    num_images_per_prompt=pr['num_images'],
                    width=pr['width'],
                    height=pr['height'],
                    num_inference_steps=pr['steps'],
                    guidance_scale=pr['guidance_scale'],
                    generator=generator,
                    callback_on_step_end=callback_fnc,
                ).images
        except Exception as e:
            clear_last(2)
            alert_msg(page, f"ERROR: Something went wrong generating {task_type} images...", content=Text(str(e)))
            return
        clear_last(2)
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(kandinsky_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, kandinsky_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        #print(str(images))
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{kandinsky_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            if not kandinsky_prefs['display_upscaled_image'] or not kandinsky_prefs['apply_ESRGAN_upscale']:
                save_metadata(image_path, kandinsky_prefs, f"Kandinsky 2.1 {task_type}", "kandinsky-community/kandinsky-2-2-decoder", random_seed, extra=pr)
                #prt(Row([Img(src=image_path, width=kandinsky_prefs['width'], height=kandinsky_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            #if save_to_GDrive:
            batch_output = os.path.join(prefs['image_output'], kandinsky_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': kandinsky_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder
            #out_path = batch_output# if save_to_GDrive else txt2img_output
            out_path = os.path.dirname(image_path)
            upscaled_path = os.path.join(out_path, output_file)

            if kandinsky_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                upscale_image(image_path, upscaled_path, scale=kandinsky_prefs["enlarge_scale"], face_enhance=kandinsky_prefs["face_enhance"])
                image_path = upscaled_path
                os.chdir(stable_dir)
                save_metadata(upscaled_path, kandinsky_prefs, f"Kandinsky 2.1 {task_type}", "kandinsky-community/kandinsky-2-2-decoder", random_seed, extra=pr)
                if kandinsky_prefs['display_upscaled_image']:
                    prt(Row([Img(src=asset_dir(upscaled_path), width=pr['width'] * float(kandinsky_prefs["enlarge_scale"]), height=pr['height'] * float(kandinsky_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            #else:
            #    time.sleep(1.2)
            #    shutil.copy(image_path, os.path.join(out_path, output_file))
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], kandinsky_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], kandinsky_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)


loaded_kandinsky21_task = ""
def run_kandinsky21(page):
    global kandinsky21_prefs, pipe_kandinsky, prefs, loaded_kandinsky21_task
    #if status['installed_diffusers']:
    #  alert_msg(page, "Sorry, currently incompatible with Diffusers installed...", content=Text("To run Kandinsky, restart runtime fresh and DO NOT install HuggingFace Diffusers library first, but you can install ESRGAN to use. Kandinsky is currently using an older version of Transformers and we haven't figured out how to easily downgrade version yet to run models together.. Sorry, trying to fix."))
    #  return
    if not bool(kandinsky21_prefs['prompt']):
      alert_msg(page, "You must provide a text prompt to process your image generation...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.Kandinsky21.controls.append(line)
      page.Kandinsky21.update()
    def clear_last(lines=1):
      clear_line(page.Kandinsky21, lines=lines)
    def autoscroll(scroll=True):
      page.Kandinsky21.auto_scroll = scroll
      page.Kandinsky21.update()
    def clear_list():
      page.Kandinsky21.controls = page.Kandinsky21.controls[:1]
    progress = ProgressBar(bar_height=8)
    clear_list()
    autoscroll(True)
    prt(Installing("Installing Kandinsky 2.1 Engine & Models... See console log for progress."))
    clear_pipes("kandinsky")
    '''try:
        if transformers.__version__ != "4.23.1": # Kandinsky conflict
          run_sp("pip uninstall -y transformers", realtime=True)
          run_process("pip uninstall -y git+https://github.com/huggingface/transformers", realtime=False)
    except Exception:
        pass
    finally:
        run_sp("pip install --target lib --upgrade transformers==4.23.1 -q", realtime=True)
        #print(f"Installed transformers v{transformers.__version__}")
    run_process("pip install -q sentencepiece", realtime=False)'''
    try:
        import accelerate
    except ModuleNotFoundError:
        run_sp("pip install -q --upgrade git+https://github.com/huggingface/accelerate.git", realtime=True)
        pass
    try:
        import clip
    except ModuleNotFoundError:
        run_sp('pip install git+https://github.com/openai/CLIP.git', realtime=True)
        pass
    try:
        from kandinsky2 import get_kandinsky2
    except ModuleNotFoundError:
        #run_process("pip install transformers==4.23.1 --upgrade --force-reinstall -q", realtime=False)
        #run_process("pip install -q git+https://github.com/ai-forever/Kandinsky-2.0.git", realtime=False)
        #run_sp('pip install -q "git+https://github.com/ai-forever/Kandinsky-2.0.git"', realtime=True)
        run_sp('pip install "git+https://github.com/Skquark/Kandinsky-2.git"', realtime=True)
        from kandinsky2 import get_kandinsky2
        pass
    import requests
    from io import BytesIO
    from PIL import ImageOps
    #save_dir = os.path.join(root_dir, 'kandinsky21_inputs')
    init_img = None
    if bool(kandinsky21_prefs['init_image']):
        fname = kandinsky21_prefs['init_image'].rpartition(slash)[2]
        #init_file = os.path.join(save_dir, fname)
        if kandinsky21_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(kandinsky21_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(kandinsky21_prefs['init_image']):
                init_img = PILImage.open(kandinsky21_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {kandinsky21_prefs['init_image']}")
                return
        init_img = init_img.resize((kandinsky21_prefs['width'], kandinsky21_prefs['height']), resample=PILImage.Resampling.LANCZOS)
        init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        #init_img.save(init_file)
    mask_img = None
    if bool(kandinsky21_prefs['mask_image']):
        fname = kandinsky21_prefs['init_image'].rpartition(slash)[2]
        #mask_file = os.path.join(save_dir, fname)
        if kandinsky21_prefs['mask_image'].startswith('http'):
            mask_img = PILImage.open(requests.get(kandinsky21_prefs['mask_image'], stream=True).raw)
        else:
            if os.path.isfile(kandinsky21_prefs['mask_image']):
                mask_img = PILImage.open(kandinsky21_prefs['mask_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your mask_image {kandinsky21_prefs['mask_image']}")
                return
            if kandinsky21_prefs['invert_mask']:
                mask_img = ImageOps.invert(mask_img.convert('RGB'))
        mask_img = mask_img.resize((kandinsky21_prefs['width'], kandinsky21_prefs['height']), resample=PILImage.NEAREST)
        mask_img = ImageOps.exif_transpose(mask_img).convert("RGB")
        mask_img = np.asarray(mask_img)
        #mask_img.save(mask_file)
    #print(f'Resize to {width}x{height}')
    task_type = "inpainting" if bool(kandinsky21_prefs['init_image']) and bool(kandinsky21_prefs['mask_image']) else "text2img"
    if pipe_kandinsky == None or loaded_kandinsky21_task != task_type:
        clear_pipes()
        try:
            #if bool(kandinsky21_prefs['init_image']) and not bool(kandinsky21_prefs['mask_image']):
            #    pipe_kandinsky = get_kandinsky2('cuda', task_type='img2img', model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            pipe_kandinsky = get_kandinsky2('cuda', task_type=task_type, model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            loaded_kandinsky21_task = task_type
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Kandinsky, try running without installing Diffusers first...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
            return
    else:
        clear_pipes('kandinsky')
    clear_last()
    prt("Generating your Kandinsky 2.1 Image...")
    prt(progress)
    autoscroll(False)

    try:
        if bool(kandinsky21_prefs['init_image']) and not bool(kandinsky21_prefs['mask_image']):
            images_texts = [kandinsky21_prefs['prompt'], init_img]
            weights = [0.5, kandinsky21_prefs['strength']]
            #images = pipe_kandinsky.generate_img2img(kandinsky21_prefs['prompt'], init_img, strength=kandinsky21_prefs['strength'], batch_size=kandinsky21_prefs['num_images'], w=kandinsky21_prefs['width'], h=kandinsky21_prefs['height'], num_steps=kandinsky21_prefs['steps'], denoised_type=kandinsky21_prefs['denoised_type'], dynamic_threshold_v=kandinsky21_prefs['dynamic_threshold_v'], sampler=kandinsky21_prefs['sampler'], ddim_eta=kandinsky21_prefs['ddim_eta'], guidance_scale=kandinsky21_prefs['guidance_scale'])
            #images = pipe_kandinsky.generate_img2img(kandinsky21_prefs['prompt'], init_img, strength=kandinsky21_prefs['strength'], batch_size=kandinsky21_prefs['num_images'], w=kandinsky21_prefs['width'], h=kandinsky21_prefs['height'], num_steps=kandinsky21_prefs['steps'], prior_cf_scale=kandinsky21_prefs['prior_cf_scale'], prior_steps=str(kandinsky21_prefs['prior_steps']), sampler=kandinsky21_prefs['sampler'], guidance_scale=kandinsky21_prefs['guidance_scale'])
            images = pipe_kandinsky.mix_images(images_texts, weights, batch_size=kandinsky21_prefs['num_images'], w=kandinsky21_prefs['width'], h=kandinsky21_prefs['height'], num_steps=kandinsky21_prefs['steps'], prior_cf_scale=kandinsky21_prefs['prior_cf_scale'], prior_steps=str(kandinsky21_prefs['prior_steps']), sampler=kandinsky21_prefs['sampler'], guidance_scale=kandinsky21_prefs['guidance_scale'])
        elif bool(kandinsky21_prefs['init_image']) and bool(kandinsky21_prefs['mask_image']):
            #images = pipe_kandinsky.generate_inpainting(kandinsky21_prefs['prompt'], init_img, mask_img, batch_size=kandinsky21_prefs['num_images'], w=kandinsky21_prefs['width'], h=kandinsky21_prefs['height'], num_steps=kandinsky21_prefs['steps'], denoised_type=kandinsky21_prefs['denoised_type'], dynamic_threshold_v=kandinsky21_prefs['dynamic_threshold_v'], sampler=kandinsky21_prefs['sampler'], ddim_eta=kandinsky21_prefs['ddim_eta'], guidance_scale=kandinsky21_prefs['guidance_scale'])
            images = pipe_kandinsky.generate_inpainting(kandinsky21_prefs['prompt'], init_img, mask_img, batch_size=kandinsky21_prefs['num_images'], w=kandinsky21_prefs['width'], h=kandinsky21_prefs['height'], num_steps=kandinsky21_prefs['steps'], prior_cf_scale=kandinsky21_prefs['prior_cf_scale'], prior_steps=str(kandinsky21_prefs['prior_steps']), sampler=kandinsky21_prefs['sampler'], guidance_scale=kandinsky21_prefs['guidance_scale'])
        else:
            #images = pipe_kandinsky.generate_text2img(kandinsky21_prefs['prompt'], batch_size=kandinsky21_prefs['num_images'], w=kandinsky21_prefs['width'], h=kandinsky21_prefs['height'], num_steps=kandinsky21_prefs['steps'], denoised_type=kandinsky21_prefs['denoised_type'], dynamic_threshold_v=kandinsky21_prefs['dynamic_threshold_v'], sampler=kandinsky21_prefs['sampler'], ddim_eta=kandinsky21_prefs['ddim_eta'], guidance_scale=kandinsky21_prefs['guidance_scale'])
            images = pipe_kandinsky.generate_text2img(kandinsky21_prefs['prompt'], batch_size=kandinsky21_prefs['num_images'], w=kandinsky21_prefs['width'], h=kandinsky21_prefs['height'], num_steps=kandinsky21_prefs['steps'], prior_cf_scale=kandinsky21_prefs['prior_cf_scale'], prior_steps=str(kandinsky21_prefs['prior_steps']), sampler=kandinsky21_prefs['sampler'], guidance_scale=kandinsky21_prefs['guidance_scale'])
    except Exception as e:
        clear_last(2)
        alert_msg(page, f"ERROR: Something went wrong generating images...", content=Text(str(e)))
        return
    clear_last(2)
    autoscroll(True)
    txt2img_output = stable_dir
    batch_output = prefs['image_output']
    #print(str(images))
    if images is None:
        prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
        return
    idx = 0
    for image in images:
        fname = format_filename(kandinsky21_prefs['prompt'])
        #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
        fname = f'{kandinsky21_prefs["file_prefix"]}{fname}'
        txt2img_output = stable_dir
        if bool(kandinsky21_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, kandinsky21_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        image_path = available_file(txt2img_output, fname, 1)
        image.save(image_path)
        new_file = image_path.rpartition(slash)[2]
        if not kandinsky21_prefs['display_upscaled_image'] or not kandinsky21_prefs['apply_ESRGAN_upscale']:
            save_metadata(image_path, kandinsky21_prefs, "Kandinsky 2.1")
            #prt(Row([Img(src=image_path, width=kandinsky21_prefs['width'], height=kandinsky21_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            prt(Row([ImageButton(src=image_path, width=kandinsky21_prefs['width'], height=kandinsky21_prefs['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
        #if save_to_GDrive:
        batch_output = os.path.join(prefs['image_output'], kandinsky21_prefs['batch_folder_name'])
        if not os.path.exists(batch_output):
            os.makedirs(batch_output)
        if storage_type == "PyDrive Google Drive":
            newFolder = gdrive.CreateFile({'title': kandinsky21_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
            newFolder.Upload()
            batch_output = newFolder
        out_path = batch_output# if save_to_GDrive else txt2img_output
        new_path = os.path.join(out_path, new_file)
        if kandinsky21_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            upscaled_path = new_path
            upscale_image(image_path, upscaled_path, scale=kandinsky21_prefs["enlarge_scale"], face_enhance=kandinsky21_prefs["face_enhance"])
            save_metadata(upscaled_path, kandinsky21_prefs, "Kandinsky 2.1")
            if kandinsky21_prefs['display_upscaled_image']:
                prt(Row([Img(src=asset_dir(upscaled_path), width=kandinsky21_prefs['width'] * float(kandinsky21_prefs["enlarge_scale"]), height=kandinsky21_prefs['height'] * float(kandinsky21_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        else:
            shutil.copy(image_path, new_path)
        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_kandinsky_fuse(page):
    global kandinsky_fuse_prefs, pipe_kandinsky, pipe_kandinsky_prior, prefs, loaded_kandinsky_task
    if not check_diffusers(page): return
    if int(status['cpu_memory']) <= 12:
      alert_msg(page, f"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run Kandinsky 2.2 right now. Either Change runtime type to High-RAM mode and restart or use other Kandinsky 2.1 in Extras.")
      return
    if len(kandinsky_fuse_prefs['mixes']) < 1:
      alert_msg(page, "You must provide layers to fuse to process your image generation...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.KandinskyFuse.controls.append(line)
      page.KandinskyFuse.update()
    def clear_last(lines=1):
      clear_line(page.KandinskyFuse, lines=lines)
    def autoscroll(scroll=True):
      page.KandinskyFuse.auto_scroll = scroll
      page.KandinskyFuse.update()
    def clear_list():
      page.KandinskyFuse.controls = page.KandinskyFuse.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = kandinsky_fuse_prefs['steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):#(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    clear_list()
    autoscroll(True)
    prt(Installing("Installing Kandinsky 2.2 Engine & Models... See console log for progress."))
    clear_pipes("kandinsky")
    import requests
    from io import BytesIO
    from PIL import ImageOps
    from diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline
    if pipe_kandinsky_prior == None:
        pipe_kandinsky_prior = KandinskyV22PriorPipeline.from_pretrained(
            "kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16
        )
        pipe_kandinsky_prior.to("cuda")
    #save_dir = os.path.join(root_dir, 'kandinsky_fuse_inputs')
    images_texts = []
    weights = []
    mix_names = []
    for mix in kandinsky_fuse_prefs['mixes']:
        if 'prompt' in mix:
            images_texts.append(mix['prompt'])
            mix_names.append(mix['prompt'])
        else:
            init_img = None
            fname = mix['init_image'].rpartition(slash)[2]
            #init_file = os.path.join(save_dir, fname)
            if mix['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(mix['init_image'], stream=True).raw)
            else:
                if os.path.isfile(mix['init_image']):
                    init_img = PILImage.open(mix['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {mix['init_image']}")
                    return
            init_img = init_img.resize((kandinsky_fuse_prefs['width'], kandinsky_fuse_prefs['height']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
            images_texts.append(init_img)
        weights.append(mix['weight'])
    mix_name = " - ".join(mix_names)
    #print(f'Resize to {width}x{height}')
    task_type = "text2img"
    if pipe_kandinsky == None or loaded_kandinsky_task != task_type:
        clear_pipes('kandinsky_prior')
        try:
            #pipe_kandinsky = get_kandinsky2('cuda', task_type='text2img', model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            pipe_kandinsky = KandinskyV22Pipeline.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            if prefs['enable_torch_compile']:
                pipe_kandinsky.unet.to(memory_format=torch.channels_last)
                pipe_kandinsky.unet = torch.compile(pipe_kandinsky.unet, mode="reduce-overhead", fullgraph=True)
            else:
                pipe_kandinsky.to("cuda")
            loaded_kandinsky_task = "text2img"
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Kandinsky, try running without installing Diffusers first...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
            return
    else:
        clear_pipes("kandinsky")
    clear_last()
    prt("Generating your Kandinsky 2.2 Fused Image...")
    prt(progress)
    autoscroll(False)
    random_seed = get_seed(kandinsky_fuse_prefs['seed'])
    generator = torch.Generator(device="cuda").manual_seed(random_seed)

    try:
        prior_out = pipe_kandinsky_prior.interpolate(images_texts, weights)
        #images = pipe_kandinsky.mix_images(images_texts, weights, batch_size=kandinsky_fuse_prefs['num_images'], w=kandinsky_fuse_prefs['width'], h=kandinsky_fuse_prefs['height'], num_steps=kandinsky_fuse_prefs['steps'], prior_cf_scale=kandinsky_fuse_prefs['prior_cf_scale'], prior_steps=str(kandinsky_fuse_prefs['prior_steps']), sampler=kandinsky_fuse_prefs['sampler'], guidance_scale=kandinsky_fuse_prefs['guidance_scale'])
        images = pipe_kandinsky(
            "",
            **prior_out,
            num_images_per_prompt=kandinsky_fuse_prefs['num_images'],
            height=kandinsky_fuse_prefs['height'],
            width=kandinsky_fuse_prefs['width'],
            num_inference_steps=kandinsky_fuse_prefs['steps'],
            guidance_scale=kandinsky_fuse_prefs['guidance_scale'],
            generator=generator,
            callback_on_step_end=callback_fnc,
        ).images
    except Exception as e:
        clear_last(2)
        alert_msg(page, f"ERROR: Something went wrong generating images...", content=Text(str(e)))
        return
    clear_last(2)
    autoscroll(True)
    txt2img_output = stable_dir
    batch_output = prefs['image_output']
    #print(str(images))
    if images is None:
        prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
        return
    idx = 0
    for image in images:
        fname = format_filename(mix_name)
        #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
        fname = f'{kandinsky_fuse_prefs["file_prefix"]}{fname}'
        txt2img_output = stable_dir
        if bool(kandinsky_fuse_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, kandinsky_fuse_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        image_path = available_file(txt2img_output, fname, 1)
        image.save(image_path)
        new_file = image_path.rpartition(slash)[2]
        if not kandinsky_fuse_prefs['display_upscaled_image'] or not kandinsky_fuse_prefs['apply_ESRGAN_upscale']:
            #prt(Row([Img(src=image_path, width=kandinsky_fuse_prefs['width'], height=kandinsky_fuse_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            save_metadata(image_path, kandinsky_fuse_prefs, "Kandinsky Fuse", seed=random_seed)
            prt(Row([ImageButton(src=image_path, width=kandinsky_fuse_prefs['width'], height=kandinsky_fuse_prefs['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))

        #if save_to_GDrive:
        batch_output = os.path.join(prefs['image_output'], kandinsky_fuse_prefs['batch_folder_name'])
        if not os.path.exists(batch_output):
            os.makedirs(batch_output)
        if storage_type == "PyDrive Google Drive":
            newFolder = gdrive.CreateFile({'title': kandinsky_fuse_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
            newFolder.Upload()
            batch_output = newFolder
        out_path = batch_output# if save_to_GDrive else txt2img_output
        new_path = os.path.join(out_path, new_file)
        if kandinsky_fuse_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            upscaled_path = os.path.join(out_path, new_file)
            upscale_image(image_path, upscaled_path, scale=kandinsky_fuse_prefs["enlarge_scale"], face_enhance=kandinsky_fuse_prefs["face_enhance"])
            save_metadata(upscaled_path, kandinsky_fuse_prefs, "Kandinsky Fuse", seed=random_seed)
            if kandinsky_fuse_prefs['display_upscaled_image']:
                prt(Row([Img(src=asset_dir(upscaled_path), width=kandinsky_fuse_prefs['width'] * float(kandinsky_fuse_prefs["enlarge_scale"]), height=kandinsky_fuse_prefs['height'] * float(kandinsky_fuse_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        else:
            shutil.copy(image_path, new_path)
        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_kandinsky21_fuse(page):
    global kandinsky21_fuse_prefs, pipe_kandinsky, prefs, loaded_kandinsky21_task
    #if status['installed_diffusers']:
    #  alert_msg(page, "Sorry, currently incompatible with Diffusers installed...", content=Text("To run Kandinsky, restart runtime fresh and DO NOT install HuggingFace Diffusers library first, but you can install ESRGAN to use. Kandinsky is currently using an older version of Transformers and we haven't figured out how to easily downgrade version yet to run models together.. Sorry, trying to fix."))
    #  return
    if len(kandinsky21_fuse_prefs['mixes']) < 1:
      alert_msg(page, "You must provide layers to fuse to process your image generation...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.Kandinsky21Fuse.controls.append(line)
      page.Kandinsky21Fuse.update()
    def clear_last(lines=1):
      clear_line(page.Kandinsky21Fuse, lines=lines)
    def autoscroll(scroll=True):
      page.Kandinsky21Fuse.auto_scroll = scroll
      page.Kandinsky21Fuse.update()
    def clear_list():
      page.Kandinsky21Fuse.controls = page.Kandinsky21Fuse.controls[:1]
    progress = ProgressBar(bar_height=8)
    clear_list()
    autoscroll(True)
    prt(Installing("Installing Kandinsky 2.1 Engine & Models... See console log for progress."))
    clear_pipes("kandinsky")
    try:
        import clip
    except ModuleNotFoundError:
        run_sp('pip install git+https://github.com/openai/CLIP.git', realtime=False)
    try:
        from kandinsky2 import get_kandinsky2
    except ModuleNotFoundError:
        run_sp('pip install -q "git+https://github.com/Skquark/Kandinsky-2.git"', realtime=False)
        from kandinsky2 import get_kandinsky2
        pass
    import requests
    from io import BytesIO
    from PIL import ImageOps
    #save_dir = os.path.join(root_dir, 'kandinsky21_fuse_inputs')
    images_texts = []
    weights = []
    mix_names = []
    for mix in kandinsky21_fuse_prefs['mixes']:
        if 'prompt' in mix:
            images_texts.append(mix['prompt'])
            mix_names.append(mix['prompt'])
        else:
            init_img = None
            fname = mix['init_image'].rpartition(slash)[2]
            #init_file = os.path.join(save_dir, fname)
            if mix['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(mix['init_image'], stream=True).raw)
            else:
                if os.path.isfile(mix['init_image']):
                    init_img = PILImage.open(mix['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {mix['init_image']}")
                    return
            init_img = init_img.resize((kandinsky21_fuse_prefs['width'], kandinsky21_fuse_prefs['height']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
            images_texts.append(init_img)
        weights.append(mix['weight'])
    mix_name = " - ".join(mix_names)
    #print(f'Resize to {width}x{height}')
    task_type = "text2img"
    if pipe_kandinsky == None or loaded_kandinsky21_task != task_type:
        clear_pipes()
        try:
            pipe_kandinsky = get_kandinsky2('cuda', task_type='text2img', model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            loaded_kandinsky21_task = "text2img"
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Kandinsky, try running without installing Diffusers first...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
            return
    else:
        clear_pipes("kandinsky")
    clear_last()
    prt("Generating your Kandinsky 2.1 Fused Image...")
    prt(progress)
    autoscroll(False)

    try:
        images = pipe_kandinsky.mix_images(images_texts, weights, batch_size=kandinsky21_fuse_prefs['num_images'], w=kandinsky21_fuse_prefs['width'], h=kandinsky21_fuse_prefs['height'], num_steps=kandinsky21_fuse_prefs['steps'], prior_cf_scale=kandinsky21_fuse_prefs['prior_cf_scale'], prior_steps=str(kandinsky21_fuse_prefs['prior_steps']), sampler=kandinsky21_fuse_prefs['sampler'], guidance_scale=kandinsky21_fuse_prefs['guidance_scale'])
    except Exception as e:
        clear_last(2)
        alert_msg(page, f"ERROR: Something went wrong generating images...", content=Text(str(e)))
        return
    clear_last(2)
    autoscroll(True)
    txt2img_output = stable_dir
    batch_output = prefs['image_output']
    #print(str(images))
    if images is None:
        prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
        return
    idx = 0
    for image in images:
        fname = format_filename(mix_name)
        #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
        fname = f'{kandinsky21_fuse_prefs["file_prefix"]}{fname}'
        txt2img_output = stable_dir
        if bool(kandinsky21_fuse_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, kandinsky21_fuse_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        image_path = available_file(txt2img_output, fname, 1)
        image.save(image_path)
        new_file = image_path.rpartition(slash)[2]
        if not kandinsky21_fuse_prefs['display_upscaled_image'] or not kandinsky21_fuse_prefs['apply_ESRGAN_upscale']:
            save_metadata(image_path, kandinsky21_fuse_prefs, "Kandinsky 2.1 Fuse")
            #prt(Row([Img(src=image_path, width=kandinsky21_fuse_prefs['width'], height=kandinsky21_fuse_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            prt(Row([ImageButton(src=image_path, width=kandinsky21_fuse_prefs['width'], height=kandinsky21_fuse_prefs['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))

        #if save_to_GDrive:
        batch_output = os.path.join(prefs['image_output'], kandinsky21_fuse_prefs['batch_folder_name'])
        if not os.path.exists(batch_output):
            os.makedirs(batch_output)
        if storage_type == "PyDrive Google Drive":
            newFolder = gdrive.CreateFile({'title': kandinsky21_fuse_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
            newFolder.Upload()
            batch_output = newFolder
        out_path = batch_output# if save_to_GDrive else txt2img_output
        new_path = os.path.join(out_path, new_file)
        if kandinsky21_fuse_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            upscaled_path = new_path
            upscale_image(image_path, upscaled_path, scale=kandinsky21_fuse_prefs["enlarge_scale"], face_enhance=kandinsky21_fuse_prefs["face_enhance"])
            save_metadata(upscaled_path, kandinsky21_fuse_prefs, "Kandinsky 2.1 Fuse")
            if kandinsky21_fuse_prefs['display_upscaled_image']:
                prt(Row([Img(src=asset_dir(upscaled_path), width=kandinsky21_fuse_prefs['width'] * float(kandinsky21_fuse_prefs["enlarge_scale"]), height=kandinsky21_fuse_prefs['height'] * float(kandinsky21_fuse_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        else:
            shutil.copy(image_path, new_path)
        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)

def run_kandinsky_controlnet(page, from_list=False, with_params=False):
    global kandinsky_controlnet_prefs, pipe_kandinsky, pipe_kandinsky_controlnet_prior, prefs, loaded_kandinsky_task, depth_estimator
    if not check_diffusers(page): return
    if int(status['cpu_memory']) <= 12:
      alert_msg(page, f"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run Kandinsky 2.2 right now. Either Change runtime type to High-RAM mode and restart or use other Kandinsky 2.1 in Extras.")
      return
    kandinsky_controlnet_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            kandinsky_controlnet_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':kandinsky_controlnet_prefs['init_image'], 'guidance_scale':kandinsky_controlnet_prefs['guidance_scale'], 'steps':kandinsky_controlnet_prefs['steps'], 'width':kandinsky_controlnet_prefs['width'], 'height':kandinsky_controlnet_prefs['height'], 'strength':kandinsky_controlnet_prefs['strength'], 'num_images':kandinsky_controlnet_prefs['num_images'], 'batch_size':kandinsky_controlnet_prefs['batch_size'], 'seed':kandinsky_controlnet_prefs['seed']})
        else:
            kandinsky_controlnet_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':p['init_image'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'strength':p['init_image_strength'], 'num_images':p['n_iterations'], 'batch_size':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(kandinsky_controlnet_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      kandinsky_controlnet_prompts.append({'prompt': kandinsky_controlnet_prefs['prompt'], 'negative_prompt':kandinsky_controlnet_prefs['negative_prompt'], 'init_image':kandinsky_controlnet_prefs['init_image'], 'guidance_scale':kandinsky_controlnet_prefs['guidance_scale'], 'steps':kandinsky_controlnet_prefs['steps'], 'width':kandinsky_controlnet_prefs['width'], 'height':kandinsky_controlnet_prefs['height'], 'strength':kandinsky_controlnet_prefs['strength'], 'num_images':kandinsky_controlnet_prefs['num_images'], 'batch_size':kandinsky_controlnet_prefs['batch_size'], 'seed':kandinsky_controlnet_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.KandinskyControlNet.controls.append(line)
        if update:
          page.KandinskyControlNet.update()
    def clear_last(lines=1):
      if from_list:
        clear_line(page.imageColumn, lines=lines)
      else:
        clear_line(page.KandinskyControlNet, lines=lines)
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.KandinskyControlNet.auto_scroll = scroll
        page.KandinskyControlNet.update()
      else:
        page.KandinskyControlNet.auto_scroll = scroll
        page.KandinskyControlNet.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.KandinskyControlNet.controls = page.KandinskyControlNet.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = kandinsky_controlnet_prefs['steps']
    def callback_fnc(pipe, step, timestep, callback_kwargs):#(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep:.1f}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Kandinsky 2.2 Engine & Models... See console log for progress.")
    prt(installer)
    clear_pipes("kandinsky")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    from diffusers import KandinskyV22PriorEmb2EmbPipeline, KandinskyV22ControlnetImg2ImgPipeline
    from transformers import pipeline
    if pipe_kandinsky_controlnet_prior == None:
        installer.status("...kandinsky-2-2-prior Emb2Emb Pipeline")
        pipe_kandinsky_controlnet_prior = KandinskyV22PriorEmb2EmbPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-prior",torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        pipe_kandinsky_controlnet_prior.to("cuda")
    task_type = "controlnet"
    if pipe_kandinsky == None or loaded_kandinsky_task != task_type:
        installer.status(f"...kandinsky-2-2 {task_type} Pipeline")
        clear_pipes('kandinsky_controlnet_prior')
        try:
            if task_type == "controlnet":
                pipe_kandinsky = KandinskyV22ControlnetImg2ImgPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-controlnet-depth", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            if prefs['enable_torch_compile']:
                installer.status(f"...run torch compile")
                pipe_kandinsky.unet.to(memory_format=torch.channels_last)
                pipe_kandinsky.unet = torch.compile(pipe_kandinsky.unet, mode="reduce-overhead", fullgraph=True)
                pipe_kandinsky = pipe_kandinsky.to("cuda")
            elif int(status['cpu_memory']) <= 12:
                installer.status(f"...enable_model_cpu_offload")
                pipe_kandinsky.enable_model_cpu_offload()
            else:
                pipe_kandinsky.to("cuda")
            loaded_kandinsky_task = task_type
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing KandinskyV22ControlnetImg2ImgPipeline...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
            return
    else:
        clear_pipes('kandinsky')
    #save_dir = os.path.join(root_dir, 'kandinsky_controlnet_inputs')
    def make_hint(image, depth_estimator):
        image = depth_estimator(image)['depth']
        image = np.array(image)
        image = image[:, :, None]
        image = np.concatenate([image, image, image], axis=2)
        detected_map = torch.from_numpy(image).float() / 255.0
        hint = detected_map.permute(2, 0, 1)
        return hint
    # step1: create models and pipelines
    if depth_estimator == None:
        installer.status("...depth-estimation Pipeline")
        depth_estimator = pipeline('depth-estimation')
    clear_last()
    for pr in kandinsky_controlnet_prompts:
        init_img = None
        if bool(pr['init_image']):
            fname = pr['init_image'].rpartition(slash)[2]
            #init_file = os.path.join(save_dir, fname)
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
            #init_img.save(init_file)
        #clear_last()
        prt("Generating your Kandinsky 2.2 ControlNet Image...")
        prt(progress)
        autoscroll(False)
        hint = make_hint(init_img, depth_estimator).unsqueeze(0).half().to('cuda')
        for num in range(pr['num_images']):
            random_seed = get_seed(int(pr['seed'] + num))
            generator = torch.Generator(device="cuda").manual_seed(random_seed)
            try:
                image_embeds = pipe_kandinsky_controlnet_prior(prompt=pr['prompt'], image=init_img, strength=kandinsky_controlnet_prefs['prior_strength'], num_inference_steps = kandinsky_controlnet_prefs['prior_steps'], generator=generator)
                negative_image_embeds = pipe_kandinsky_controlnet_prior(prompt=pr['negative_prompt'], image=init_img, strength=1, num_inference_steps=kandinsky_controlnet_prefs['prior_steps'], generator=generator)
                #image_embeds, negative_image_embeds = pipe_kandinsky_controlnet_prior(pr['prompt'], negative_prompt=pr['negative_prompt'], num_inference_steps=5, guidance_scale=1.0, generator=generator).to_tuple()
                #if task_type == "controlnet":
                images = pipe_kandinsky(
                    #pr['prompt'],
                    image=init_img,
                    image_embeds=image_embeds.image_embeds,
                    negative_image_embeds=negative_image_embeds.image_embeds,
                    hint=hint,
                    strength=pr['strength'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=pr['steps'],
                    guidance_scale=pr['guidance_scale'],
                    num_images_per_prompt=pr['batch_size'],
                    generator=generator,
                    callback_on_step_end=callback_fnc,
                ).images

            except Exception as e:
                clear_last(2)
                alert_msg(page, f"ERROR: Something went wrong generating {task_type} images...", content=Text(str(e)))
                return
            clear_last(2)
            autoscroll(True)
            txt2img_output = stable_dir
            batch_output = prefs['image_output']
            txt2img_output = stable_dir
            if bool(kandinsky_controlnet_prefs['batch_folder_name']):
                txt2img_output = os.path.join(stable_dir, kandinsky_controlnet_prefs['batch_folder_name'])
            if not os.path.exists(txt2img_output):
                os.makedirs(txt2img_output)
            #print(str(images))
            if images is None:
                prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
                return
            idx = 0
            for image in images:
                fname = format_filename(pr['prompt'])
                #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
                fname = f'{kandinsky_controlnet_prefs["file_prefix"]}{fname}'
                image_path = available_file(txt2img_output, fname, 1)
                image.save(image_path)
                output_file = image_path.rpartition(slash)[2]
                if not kandinsky_controlnet_prefs['display_upscaled_image'] or not kandinsky_controlnet_prefs['apply_ESRGAN_upscale']:
                    save_metadata(image_path, kandinsky_controlnet_prefs, f"Kandinsky 2.1 {task_type}", "kandinsky-community/kandinsky-2-2-controlnet-depth", random_seed, extra=pr)
                    #prt(Row([Img(src=image_path, width=kandinsky_controlnet_prefs['width'], height=kandinsky_controlnet_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                    prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
                #if save_to_GDrive:
                batch_output = os.path.join(prefs['image_output'], kandinsky_controlnet_prefs['batch_folder_name'])
                if not os.path.exists(batch_output):
                    os.makedirs(batch_output)
                if storage_type == "PyDrive Google Drive":
                    newFolder = gdrive.CreateFile({'title': kandinsky_controlnet_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                    newFolder.Upload()
                    batch_output = newFolder
                #out_path = batch_output# if save_to_GDrive else txt2img_output
                out_path = os.path.dirname(image_path)
                upscaled_path = os.path.join(out_path, output_file)

                if kandinsky_controlnet_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    upscale_image(image_path, upscaled_path, scale=kandinsky_controlnet_prefs["enlarge_scale"], face_enhance=kandinsky_controlnet_prefs["face_enhance"])
                    image_path = upscaled_path
                    save_metadata(image_path, kandinsky_controlnet_prefs, f"Kandinsky 2.1 {task_type}", "kandinsky-community/kandinsky-2-2-controlnet-depth", random_seed, extra=pr)
                    if kandinsky_controlnet_prefs['display_upscaled_image']:
                        prt(Row([Img(src=asset_dir(upscaled_path), width=pr['width'] * float(kandinsky_controlnet_prefs["enlarge_scale"]), height=pr['height'] * float(kandinsky_controlnet_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                #else:
                #    time.sleep(1.2)
                #    shutil.copy(image_path, os.path.join(out_path, output_file))
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], kandinsky_controlnet_prefs['batch_folder_name']), fname, 0)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], kandinsky_controlnet_prefs['batch_folder_name']), fname, 0)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def run_deep_daze(page):
    global deep_daze_prefs, status
    #https://colab.research.google.com/drive/1_YOHdORb0Fg1Q7vWZ_KlrtFe9Ur3pmVj?usp=sharing#scrollTo=6IPQ_rdA2Sa7
    def add_to_deep_daze_output(o):
      page.DeepDaze.controls.append(o)
      page.DeepDaze.update()
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.DeepDaze.controls.append(line)
      page.DeepDaze.update()
    def clear_last(lines=1):
      clear_line(page.DeepDaze, lines=lines)
    def autoscroll(scroll=True):
      page.DeepDaze.auto_scroll = scroll
      page.DeepDaze.update()
    def clear_list():
      page.DeepDaze.controls = page.DeepDaze.controls[:1]
    clear_list()
    autoscroll(True)
    progress = ProgressBar(bar_height=8, expand=True)
    total_steps = deep_daze_prefs['iterations']
    def callback_fnc(step: int) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}"
      progress.update()
    prt(Installing("Initializing Deep Daze Pipeline..."))
    from PIL.PngImagePlugin import PngInfo
    from tqdm.notebook import trange
    try:
        from deep_daze import Imagine
    except Exception:
        run_process("pip install deep-daze --upgrade", page=page)
        from deep_daze import Imagine
    output_dir = os.path.join(root_dir, "deep_daze")
    if bool(deep_daze_prefs['batch_folder_name']):
        output_dir = os.path.join(output_dir, deep_daze_prefs['batch_folder_name'])
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    os.chdir(output_dir)
    abort_run = False
    def abort_daze(e):
        nonlocal abort_run
        abort_run = True
        autoscroll(False)
        page.snd_error.play()
        page.snd_delete.play()
    pipe_deep_daze = Imagine(
        text = deep_daze_prefs['prompt'],
        num_layers = deep_daze_prefs['num_layers'],
        save_every = deep_daze_prefs['save_every'],
        image_width = deep_daze_prefs['max_size'],
        lr = deep_daze_prefs['learning_rate'],
        iterations = deep_daze_prefs['iterations'],
        save_progress = True,#deep_daze_prefs['save_progress']
    )
    clear_last()
    prt("    Dreaming up your DeepDaze Image...")
    autoscroll(False)
    prt(Row([progress, IconButton(icon=icons.CANCEL, tooltip="Abort Daze & Save Progress", on_click=abort_daze)], alignment=MainAxisAlignment.SPACE_BETWEEN))
    #filename = format_filename(deep_daze_prefs['prompt'])
    filename = deep_daze_prefs['prompt'].replace(' ', '_')
    img = None
    image_files = []
    s = 0
    for epoch in trange(20, desc = 'epochs'):
        for i in trange(deep_daze_prefs['iterations'], desc = 'iteration'):
            pipe_deep_daze.train_step(epoch, i)
            callback_fnc(i)
            if abort_run: break
            if i % pipe_deep_daze.save_every != 0:
                continue
            num_str = '' if s==0 else f'.{str(s).zfill(6)}'
            fname = f"{filename}{num_str}.jpg"
            output_file = os.path.join(output_dir, fname)
            image_files.append(output_file)
            if s == 0:
                img = Img(src=asset_dir(output_file), fit=ImageFit.FIT_WIDTH, gapless_playback=True)
                prt(Row([img], alignment=MainAxisAlignment.CENTER))
            else:
                img.src=output_file
                img.update()
            s += 1
        if abort_run: break
            #image = Image(f'./{filename}.jpg')
            #display(image)
    output_filename = deep_daze_prefs['file_prefix'] + format_filename(deep_daze_prefs['prompt'])
    unscaled_path = fname
    image_path = available_file(output_dir, output_filename, 0)
    unscaled_path = image_path
    output_file = image_path.rpartition(slash)[2]
    out_path = os.path.dirname(image_path)
    upscaled_path = os.path.join(out_path, output_file)
    input = PILImage.open(fname)
    input.save(image_path)
    #shutil.copy(fname, image_path)
    clear_last(3)
    autoscroll(True)
    if not deep_daze_prefs['display_upscaled_image'] or not deep_daze_prefs['apply_ESRGAN_upscale']:
        prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))
        #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
    if deep_daze_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
        upscaled_path = os.path.join(out_path, output_file)
        upscale_image(image_path, upscaled_path, scale=deep_daze_prefs["enlarge_scale"])
        image_path = upscaled_path
    save_metadata(image_path, deep_daze_prefs, f"Deep Daze", "deepdaze")
    if prefs['save_image_metadata']:
        img = PILImage.open(image_path)
        metadata = PngInfo()
        metadata.add_text("artist", prefs['meta_ArtistName'])
        metadata.add_text("copyright", prefs['meta_Copyright'])
        metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {deep_daze_prefs['enlarge_scale']}x with ESRGAN" if deep_daze_prefs['apply_ESRGAN_upscale'] else "")
        metadata.add_text("pipeline", f"Deep Daze")
        if prefs['save_config_in_metadata']:
            metadata.add_text("title", deep_daze_prefs['prompt'])
            config_json = deep_daze_prefs.copy()
            #config_json['model_path'] = model_id
            del config_json['num_images']
            del config_json['display_upscaled_image']
            del config_json['batch_folder_name']
            if not config_json['apply_ESRGAN_upscale']:
                del config_json['enlarge_scale']
            del config_json['apply_ESRGAN_upscale']
            metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
        img.save(image_path, pnginfo=metadata)
    #TODO: PyDrive
    if storage_type == "Colab Google Drive":
        new_file = available_file(os.path.join(prefs['image_output'], deep_daze_prefs['batch_folder_name']), output_filename, 0)
        out_path = new_file
        shutil.copy(image_path, new_file)
    elif bool(prefs['image_output']):
        new_file = available_file(os.path.join(prefs['image_output'], deep_daze_prefs['batch_folder_name']), output_filename, 0)
        out_path = new_file
        shutil.copy(image_path, new_file)
    if deep_daze_prefs['save_progress']:
        for f in image_files:
            if os.path.exists(f):
                shutil.copy(f, os.path.join(prefs['image_output'], deep_daze_prefs['batch_folder_name'], f.rpartition(slash)[2]))
    else:
        for f in image_files[:-1]:
            if os.path.exists(f):
                os.remove(f)
    if deep_daze_prefs['display_upscaled_image']:
        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(deep_daze_prefs["enlarge_scale"]), height=512 * float(deep_daze_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
        #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
    prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))

    del pipe_deep_daze
    flush()
    autoscroll(False)
    play_snd(Snd.ALERT, page)


def main(page: Page):
    global status
    page.title = "Stable Diffusion Deluxe"
    def open_url(e):
        page.launch_url(e.data)
    def exit_disconnect(e):
        save_settings_file(page, change_icon=is_Colab)
        if is_Colab:
          #run_sp("install pyautogui", realtime=False)
          #import pyautogui
          #pyautogui.hotkey('ctrl', 'w')
          #import keyboard
          #keyboard.press_and_release('ctrl+w')
          #time.sleep(1.5)
          #close_tab()
          page.window.close()
          from google.colab import runtime
          runtime.unassign()
        else:
          page.window.close()
    def minimize_window(e):
        page.window.minimized = True
        page.update()
    def maximize_window(e):
        if page.window.maximized:
            page.window.maximized = False
        else:
            page.window.maximized = True
        page.update()
    def window_event(e):
        #print(e.data)
        if str(e.data) == "unmaximize":
            appbar.actions[3].icon=icons.CHECK_BOX_OUTLINE_BLANK
            appbar.actions[3].tooltip = "Maximize Window"
            prefs['window_maximized'] = False
            page.update()
        elif str(e.data) == "maximize":
            appbar.actions[3].icon=icons.FILTER_NONE
            appbar.actions[3].tooltip = "Restore Window"
            prefs['window_maximized'] = True
            page.update()
    if not is_Colab:
        page.window.on_event = window_event
    def get_memory():
        from subprocess import getoutput
        s = getoutput('nvidia-smi -L')
        if 'T4' in s: gpu = 'T4'
        elif 'P100' in s: gpu = 'P100'
        elif 'V100' in s: gpu = 'V100'
        elif 'A100' in s: gpu = 'A100'
        else:
            gpu = re.sub(r"\([^()]*\)", "", s).strip()
        status['cpu_used'] = psutil.virtual_memory().used / (1024 * 1024 * 1024)
        #memory_stats = torch.cuda.memory_stats(device=torch.device("cuda"))
        #available_memory = memory_stats["reserved_bytes.all.allocated"] - memory_stats["allocated_bytes.all.current"]
        try:
            status['gpu_used'] = torch.cuda.max_memory_allocated(device=torch.device("cuda")) / (1024 * 1024 * 1024)
            gpu_mem = f"{status['gpu_used']:.1f}/{status['gpu_memory']:.0f}GB"
        except Exception:
            status['gpu_used'] = 0
            gpu_mem = "N/A"
            pass
        memory = f"GPU VRAM: {gpu_mem} - CPU RAM: {status['cpu_used']:.1f}/{status['cpu_memory']:.0f}GB | Runnning {gpu}"
        return memory
    help_dlg = None
    def open_help_dlg(e):
        nonlocal help_dlg
        def clear_the_pipes(e):
            nonlocal memory_text
            clear_pipes()
            time.sleep(0.8)
            memory_text.value = get_memory()
            memory_text.update()
            time.sleep(3.2)
            memory_text.value = get_memory()
            memory_text.update()
            time.sleep(4.6)
            memory_text.value = get_memory()
            memory_text.update()
        memory_text = Text(get_memory())
        clear_pipes_btn = ElevatedButton(content=Text("Clear Memory", size=16), on_click=clear_the_pipes)
        memory_row = Row([memory_text, Container(content=None, expand=True), clear_pipes_btn])
        help_dlg = AlertDialog(
            title=Text("💁   Help/Information - Stable Diffusion Deluxe " + SDD_version), content=Column([memory_row, Text("If you don't know what Stable Diffusion is, you're in for a pleasant surprise.. If you're already familiar, you're gonna love how easy it is to be an artist with the help of our AI Friends using our pretty interface."),
                  Text("Simply go through the self-explanitory tabs step-by-step and set your preferences to get started. The default values are good for most, but you can have some fun experimenting. Most values are automatically saved as you make changes and change tabs."),
                  Text("Each time you open the app, you should start in the Installers section, turn on all the components you plan on using in you session, then Run the Installers and let them download. You can multitask and work in other tabs while it's installing."),
                  Text("In the Prompts List, add as many text prompts as you can think of, and edit any prompt to override any default Image Parameter.  Once you're ready, Run Diffusion on your Prompts List and watch it fill your Drive with beauty.."),
                  Text("Try out any and all of our Prompt Helpers to use practical text AIs to make unique descriptive prompts fast, with our Prompt Generator, Remixer, Brainstormer and Advanced Writer.  You'll never run out of inspiration again..."),
                  Text("Use the other Stable Diffusers to get unique results with advanced AI tools to refine your art to the next level with powerful surpises.. There's a lot of specialized Diffusers that are pure magic when you challenge it. Have fun, and get creative..."),
            ], scroll=ScrollMode.AUTO),
            actions=[TextButton("👍  Thanks! ", on_click=close_help_dlg)], actions_alignment=MainAxisAlignment.END,
        )
        page.overlay.append(help_dlg)
        help_dlg.open = True
        page.update()
    def close_help_dlg(e):
        nonlocal help_dlg
        help_dlg.open = False
        page.update()
    def open_credits_dlg(e):
        page.overlay.append(credits_dlg)
        credits_dlg.open = True
        page.update()
    def close_credits_dlg(e):
        credits_dlg.open = False
        page.update()
    credits_markdown = '''This toolkit is an Open-Source side project by [Skquark, Inc.](https://Skquark.com), primarily created by Alan Bedian for fun and full-feature functionality.  Official website is [DiffusionDeluxe.com](https://DiffusionDeluxe.com) for more info.

The real credit goes to the team at [Stability.ai](https://Stability.ai) for making Stable Diffusion so great, and [HuggingFace](https://HuggingFace.co) for their work on the [Diffusers Pipelines](https://github.com/huggingface/diffusers). The HuggingFace Diffusers team includes Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Aryan V S, Thomas Wolf & many others.

For the great app UI framework, we thank [Flet](https://Flet.dev) with the amazing Flutter based Python library with a very functional dev platform that made this possible.

For the brains behind our Prompt Helpers, we thank our friends [OpenAI](https://beta.OpenAI.com), [Google Gemini](https://gemini.google.com), [Bloom-AI](https://huggingface.co/bigscience/bloom), [TextSynth](https://TextSynth.com) and others for making an AIs so fun to talk to and use.

Shoutouts to the Discord Communities of [Disco Diffusion](https://discord.gg/d5ZVbAfm), [Stable Diffusion](https://discord.gg/stablediffusion), [HuggingFace](https://discord.gg/hugging-face-879548962464493619), [Banodoco](https://discord.gg/tFSXjJ2C), [Division by Zer0](https://discord.gg/WHp5JBec) and [Flet](https://discord.gg/nFqy742h) for their support and user contributions.'''
    credits_dlg = AlertDialog(
        title=Text("🙌   Credits/Acknowledgments"), content=Column([Markdown(credits_markdown, extension_set="gitHubWeb", on_tap_link=open_url)
        ], scroll=ScrollMode.AUTO),
        actions=[TextButton("👊   Good Stuff... ", on_click=close_credits_dlg)], actions_alignment=MainAxisAlignment.END,
    )
    from_name = TextField(label="From Name", keyboard_type=KeyboardType.NAME)
    from_email = TextField(label="From Email (optional)", keyboard_type=KeyboardType.EMAIL)
    message = TextField(label="Message to Skquark", keyboard_type=KeyboardType.MULTILINE, filled=True, multiline=True, max_lines=10)
    def send_message(e):
        if not bool(message.value.strip()):
            toast_msg(page, f"📭  Provide a message to send first.")
            return
        send_debug_email(message.value, "Contact from DiffusionDeluxe", from_name.value.strip(), from_email.value.strip())
        toast_msg(page, f"📧  Sent Email to Skquark... Thanks, I hope.")
        close_contact_dlg(e)
    submit_btn = ft.OutlinedButton("📩  Send Message ", on_click=send_message)
    contact_form = Column([
      Row([from_name, from_email]),
      message, submit_btn,
    ])
    def open_contact_dlg(e):
        page.overlay.append(contact_dlg)
        contact_dlg.open = True
        page.update()
    def close_contact_dlg(e):
        contact_dlg.open = False
        page.update()
    contact_dlg = AlertDialog(
        title=Text("📨  Contact Us"), content=Column([
          Text("If you want to reach Alan Bedian/Skquark, Inc. for any reason, feel free to send me a message (as long as it's not spam) with whatever method you prefer. Testimonials & Suggestions are welcomed too."),
          Text("If you're a developer and want to help with this project (or one of my other almost done apps) then you can Join my Discord Channel and get involved. It's fairly quiet in there though..."),
          contact_form,
          Row([ft.FilledButton("Email Skquark", on_click=lambda _:page.launch_url("mailto:Alan@Skquark.com")), ft.FilledButton("Discord DM", on_click=lambda _:page.launch_url("https://discord.com/channels/@me/988620354815688744")), ft.FilledButton("Skquark Discord", on_click=lambda _:page.launch_url("https://discord.gg/fTraJ96Z")), ft.FilledButton("Skquark.com", on_click=lambda _:page.launch_url("https://Skquark.com"))], alignment=MainAxisAlignment.CENTER),
        ], scroll=ScrollMode.AUTO),
        actions=[TextButton("👁️‍🗨️  Maybe...", on_click=close_contact_dlg)], actions_alignment=MainAxisAlignment.END,
    )
    def open_donate_dlg(e):
        page.overlay.append(donate_dlg)
        donate_dlg.open = True
        page.update()
    def close_donate_dlg(e):
        donate_dlg.open = False
        page.update()
    donate_dlg = AlertDialog(
        title=Text("🎁  Donate to the Cause"), content=Column([
          Text("This app has been a one-man labour of love with a lot of effort poured into it for over a year. It's powerful enough to be a paid commercial application, however it's all open-source code behind it created by many contributers to make the tools as cool as they are."),
          Text("While we offer this software free of charge, your support would be greatly appreciated to continue the efforts to keep making it better. If you find value in this software and are able to show some love, any amount you can offer is welcomed... I don't even own a GPU good enough to run my own app locally!"),
          Text("If you're technical enough, you can also donate by making contributions to the code, offering suggestions for improvements, adding to the Community Fine-Tuned Models list, or whatever enhancements to the project you can provide..."),
          Row([ft.FilledButton("Donate with PayPal", on_click=lambda _:page.launch_url("https://paypal.me/StarmaTech")), ft.FilledButton("Donate with Venmo", on_click=lambda _:page.launch_url("https://venmo.com/u/Alan-Bedian"))], alignment=MainAxisAlignment.CENTER),
        ], scroll=ScrollMode.AUTO),
        actions=[TextButton("💸  Much appreciated", on_click=close_donate_dlg)], actions_alignment=MainAxisAlignment.END,
    )
    #page.window_full_screen = True
    page.etas = []
    page.upscalers = []
    page.theme_mode = prefs['theme_mode'].lower()
    if prefs['theme_mode'] == 'Dark':
        page.dark_theme = theme.Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))#, use_material3=True)
    else:
        page.theme = theme.Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))
    app_icon_color = colors.AMBER_800
    space = " "  if (page.width if page.web else page.window.width) >= 1024 else ""
    def clear_memory(e):
        play_snd(Snd.DELETE, page)
        clear_pipes()
    page.stats = Column([Text("", size=10), Text("", size=10)], tight=True, spacing=4)
    appbar=AppBar(title=ft.WindowDragArea(Row([Container(Text(f"👨‍🎨️{space}  Stable Diffusion - Deluxe Edition  {space}🧰" if ((page.width or page.window.width) if page.web else page.window.width) >= 768 else "Stable Diffusion Deluxe  🖌️", weight=FontWeight.BOLD, color=colors.ON_SURFACE, overflow=ft.TextOverflow.ELLIPSIS, expand=True))], alignment=MainAxisAlignment.CENTER, expand=True), expand=False), elevation=20,
      center_title=True,
      bgcolor=colors.SURFACE,
      toolbar_height=46,
      leading_width=46,
      leading=IconButton(icon=icons.LOCAL_FIRE_DEPARTMENT_OUTLINED, icon_color=app_icon_color, icon_size=32, tooltip="Save Settings File", on_click=lambda _: app_icon_save()),
      #leading_width=40,
      actions=[ft.GestureDetector(page.stats, on_tap=lambda _:update_stats(page), on_double_tap=lambda _:toggle_stats(page), on_long_press_end=clear_memory), 
        PopupMenuButton(items=[
          PopupMenuItem(text="🤔  Help/Info", on_click=open_help_dlg),
          PopupMenuItem(text="👏  Credits", on_click=open_credits_dlg),
          PopupMenuItem(text="🤧  Issues/Suggestions", on_click=lambda _:page.launch_url("https://github.com/Skquark/AI-Friends/issues")),
          PopupMenuItem(text="😋  Contact Skquark", on_click=open_contact_dlg),
          #PopupMenuItem(text="📨  Email Skquark", on_click=lambda _:page.launch_url("mailto:Alan@Skquark.com")),
          PopupMenuItem(text="🤑  Offer Donation", on_click=open_donate_dlg),
          #PopupMenuItem(text="❎  Exit/Disconnect Runtime", on_click=exit_disconnect) if is_Colab else PopupMenuItem(),
      ])])
    if is_Colab:
        appbar.actions[1].items.append(PopupMenuItem())
        appbar.actions[1].items.append(PopupMenuItem(text="❎  Exit/Disconnect Runtime", on_click=exit_disconnect))
    else:
        appbar.actions.append(IconButton(icon=icons.MINIMIZE, tooltip="Minimize Window", on_click=minimize_window))
        appbar.actions.append(IconButton(icon=icons.CHECK_BOX_OUTLINE_BLANK, tooltip="Maximize Window", on_click=maximize_window))
        appbar.actions.append(IconButton(icon=icons.CLOSE, tooltip="❎  Exit Application", on_click=exit_disconnect))
        page.window.title_bar_hidden = True
    page.appbar = appbar
    def app_icon_save():
        app_icon_color = colors.GREEN_800
        appbar.leading = IconButton(icon=icons.LOCAL_FIRE_DEPARTMENT_OUTLINED, icon_color=app_icon_color, icon_size=32, tooltip="Saving Settings File")
        appbar.update()
        time.sleep(0.6)
        app_icon_color = colors.AMBER_800
        appbar.leading = IconButton(icon=icons.LOCAL_FIRE_DEPARTMENT_OUTLINED, icon_color=app_icon_color, icon_size=32, tooltip="Save Settings File", on_click=lambda _: app_icon_save())
        appbar.update()
    page.app_icon_save = app_icon_save
    page.vertical_alignment = MainAxisAlignment.START
    page.horizontal_alignment = CrossAxisAlignment.START
    t = buildTabs(page)
    t.on_change = tab_on_change
    #(t,page)
    def close_banner(e):
        page.top_banner.open = False
        page.update()
    page.close_banner = close_banner
    #leading=Icon(icons.DOWNLOADING, color=colors.AMBER, size=40),
    page.status_msg = Text("")
    page.top_banner = Banner(bgcolor=colors.SECONDARY_CONTAINER, content=Column([]), actions=[Row([page.status_msg, TextButton("Close", on_click=page.close_banner)])])
    page.overlay.append(page.top_banner)
    def show_banner_click(e):
        page.top_banner.open = True
        page.update()
    def set_status(msg=""):
        page.status_msg.value = msg
        page.status_msg.update()
        page.top_banner.update()
        page.update()
    page.status = set_status
    page.add(t)
    if not is_Colab:
        if 'window_maximized' not in prefs:
            prefs['window_maximized'] = True
        if prefs['window_maximized']:
            maximize_window("e")
    page.update()
    time.sleep(0.6)
    initState(page)
    if not status['initialized']:
        status['initialized'] = True
    #page.add(ElevatedButton("Show Banner", on_click=show_banner_click))
    #page.add (Text ("Enhanced Stable Diffusion Deluxe by Skquark, Inc."))

class Header(Stack):
    def __init__(self, title="", subtitle="", value="", actions=[], divider=True):
        super().__init__()
        self.title = title
        self.subtitle = subtitle
        self.value = value
        self.actions = actions
        self.divider = divider
        self.build()
    def build(self):
        #self.column = Column([Row([Text(self.title, style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Row(self.actions) if bool(self.actions) else Container(content=None)], alignment=MainAxisAlignment.SPACE_BETWEEN, spacing=0, vertical_alignment=CrossAxisAlignment.END)], spacing=4)
        self.column = Column([Row([Column([Text(self.title, theme_style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Text(self.subtitle, theme_style=TextThemeStyle.TITLE_SMALL, color=colors.TERTIARY) if bool(self.subtitle) else Container(content=None)], spacing=4, expand=True), Row(self.actions) if bool(self.actions) else Container(content=None)], alignment=MainAxisAlignment.SPACE_BETWEEN, spacing=1, vertical_alignment=CrossAxisAlignment.END), Divider(thickness=3, height=5, color=colors.SURFACE_VARIANT) if self.divider else Container(content=None), Container(content=None, height=3)], spacing=2)
        return self.column

class FileInput(Stack):
    def __init__(self, label="Initial Image", ftype="image", pref="", key="", expand=False, col=None, filled=False, visible=True, max_size=None, output_dir=None, page=None):
        super().__init__()
        self.label = label
        self.ftype = ftype
        self.pref = pref
        self.key = key
        self.expand = expand
        self.col = col
        self.filled = filled
        self._visible = visible
        self.max_size = max_size
        self.output_dir = output_dir
        self.page = page
        self.build()
    def build(self):
        def save_file(file_name):
            #TODO: Make save dir default to /root/uploads dir
            save_dir = uploads_dir if self.output_dir == None else self.output_dir
            if not os.path.exists(save_dir):
              os.mkdir(save_dir)
            if not slash in file_name:
              f = os.path.join(root_dir, file_name)
              fname = os.path.join(save_dir, file_name)
              if f != fname:
                shutil.move(f, fname)
              #self.pref[self.key] = fpath #e.file_name.rpartition('.')[0]
            else:
              fname = file_name
            #print(f"{e.file_name} to {fname}")
            if self.max_size != None:
              original_img = PILImage.open(fname)
              width, height = original_img.size
              width, height = scale_dimensions(width, height, self.max_size)
              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              original_img.save(fname)
            self.textfield.value = fname
            self.textfield.update()
            self.pref[self.key] = fname
            self.page.update()
        def upload_files(e):
            uf = []
            if self.file_picker.result != None and self.file_picker.result.files != None:
                for f in self.file_picker.result.files:
                  if self.page.web:
                    uf.append(FilePickerUploadFile(f.name, upload_url=self.page.get_upload_url(f.name, 600)))
                  else:
                    save_file(f.path)
                    #on_upload_progress(FilePickerUploadEvent(f.path, 1.0, ""))
                self.file_picker.upload(uf)
        def file_picker_result(e: FilePickerResultEvent):
            if e.files != None:
              upload_files(e)
        def on_upload_progress(e: FilePickerUploadEvent):
            if e.progress == 1:
              save_file(e.file_name)
            elif e.progress is not None and isinstance(e.progress, float):
              percent = int(e.progress * 100)
              self.textfield.value = f"  {percent}%"
              self.textfield.update()
        def pick_file(e):
            img_ext = ["png", "PNG", "jpg", "jpeg"]
            vid_ext = ["mp4", "avi", "MP4", "AVI"]
            gif_ext = ["gif", "GIF"]
            aud_ext = ["mp3", "wav", "MP3", "WAV"]
            midi_ext = ["mid", "midi", "MID", "MIDI"]
            font_ext = ["ttf", "TTF"]
            model_ext = ["fbx", "obj", "stl", "gltf", "glb"]
            ext = img_ext if self.ftype == "image" else vid_ext if self.ftype == "video" else font_ext if self.ftype == "font" else model_ext if self.ftype == "model" else gif_ext if self.ftype == "gif" else aud_ext if self.ftype == "audio" else midi_ext if self.ftype == "midi" else vid_ext+aud_ext if self.ftype == "media" else img_ext+gif_ext+vid_ext if self.ftype == "picture" else img_ext
            name = self.key.replace("_", " ").title()
            if self.ftype == "folder":
                if is_Colab:
                    alert_msg(self.page, "Switch to Colab tab and press Files button on the Left & Find the Path you want to use as Init Folder, Right Click and Copy Path, then Paste here")
                else:
                    self.file_picker.get_directory_path(dialog_title="Select the folder containing Images to process")
            else:
                self.file_picker.pick_files(allow_multiple=False, allowed_extensions=ext, dialog_title=f"Pick {name} File")
        def changed_path(e):
            self.pref[self.key] = e.control.value
            self.textfield.focus()
        def get_directory_result(e):
            path = e.path
            if path:
                self.textfield.value = path
                self.textfield.update()
                self.pref[self.key] = path
        if self.ftype == "folder":
            self.file_picker = FilePicker(on_result=get_directory_result)
        else:
            self.file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
        self.page.overlay.append(self.file_picker)
        icon = icons.VIDEO_CALL if self.ftype == "video" else icons.AUDIO_FILE if self.ftype == "audio" else icons.FONT_DOWNLOAD if self.ftype == "font" else icons.FOLDER_COPY if self.ftype == "folder" else icons.DRIVE_FOLDER_UPLOAD
        self.textfield = TextField(label=self.label, value=self.pref[self.key], expand=self.expand, filled=self.filled, visible=self._visible, autofocus=False, on_change=changed_path, height=60, suffix=IconButton(icon=icon, on_click=pick_file))
        return self.textfield
    @property
    def value(self):
        return self.textfield.value
    @value.setter
    def value(self, value):
        self.pref[self.key] = value
        self.textfield.value = value
        self.textfield.update()
    @property
    def show(self):
        return self._visible
    @show.setter
    def show(self, value):
        self._visible = value
        self.textfield.visible = value
        self.textfield.update()

class ImageButton(Stack):
    def __init__(self, src="", subtitle="", actions=[], center=True, width=None, height=None, data=None, src_base64=None, fit=ImageFit.SCALE_DOWN, show_subtitle=False, zoom=False, page=None):
        super().__init__()
        self.src = src
        self.src_base64 = src_base64
        self.subtitle = subtitle
        self.actions = actions
        self.center = center
        self.width = width
        self.height = height
        self.data = data or src
        self.fit = fit
        self.show_subtitle = show_subtitle
        self.zoom = zoom
        self.page = page
        self.build()
    def build(self):
        #self.column = Column([Row([Text(self.title, style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Row(self.actions) if bool(self.actions) else Container(content=None)], alignment=MainAxisAlignment.SPACE_BETWEEN, spacing=0, vertical_alignment=CrossAxisAlignment.END)], spacing=4)
        #Img(src=self.src, width=self.width, height=self.height, fit=self.fit, gapless_playback=True),
        def download_image(e):
            #print(f"{type(self.data)} {self.data}")
            if is_Colab:
                from urllib.parse import quote
                self.page.launch_url(quote(asset_dir(self.data)))
                '''from google.colab import files
                if os.path.isfile(self.data):
                    files.download(self.data)
                else:
                    time.sleep(4)
                    files.download(self.data)'''
            else:#, initial_directory=
                self.file_saver = FilePicker(on_result=file_saver_result)
                self.page.overlay.append(self.file_saver)
                try:
                    self.file_saver.save_file(dialog_title="Save Image to...", allowed_extensions=["png", "PNG"], file_name=os.path.basename(self.data), file_type=ft.FilePickerFileType.IMAGE)
                except Exception as e:
                    toast_msg(self.page, f"📲  Error Downloading {self.data}... {e}")
                    return
            toast_msg(self.page, f"📲  Downloading {self.data}...{' May have to Stop Script for Downloads to start in Colab.' if is_Colab else ''}")
        def file_saver_result(e: FilePickerResultEvent):
            if e.path != None:
                shutil.copy(os.path.join(self.data), os.path.join(e.path))
        def copy_path(e):
            self.page.set_clipboard(self.data)
            toast_msg(self.page, f"📋  {self.data} copied to clipboard... Paste as Init Image.")
        def image_details(e):
          #TODO: Get size & meta
            img = Img(src=asset_dir(self.data), gapless_playback=True)
            #if self.zoom:
            #img = PanZoom(img, self.width, self.height, width=self.width, height=self.height)
            alert_msg(self.page, "Image Details", content=Column([Text(self.subtitle or self.data, selectable=True), img], horizontal_alignment=CrossAxisAlignment.CENTER), sound=False)
        def delete_image(e):
            #self.image = Container(content=None)
            if os.path.exists(self.src):
              os.remove(self.src)
            if self.data != self.src:
              if os.path.exists(self.data):
                os.remove(self.data)
            self.image.visible = False
            self.image.update()
            self.visible = False
            self.update()
            toast_msg(self.page, f"🗑️  Deleted {self.data}...")
        def scale_width(width, height, max_width):
            if width > max_width: return max_width, int(height / (width / max_width))
            else: return width, height
        #self.image = Row([Img(src=self.src, width=self.width, height=self.height, fit=self.fit, gapless_playback=True)], alignment=MainAxisAlignment.CENTER)
        if self.width == None:
            if self.src_base64 != None:
              self.image = Img(src_base64=self.src_base64, fit=self.fit, gapless_playback=True)
            else:
              self.image = Img(src=asset_dir(self.src), fit=self.fit, gapless_playback=True)
        else:
            self.width, self.height = scale_width(self.width, self.height, (self.page.width if self.page.web else self.page.window.width) - 28)
            if self.src_base64 != None:
              self.image = Img(src_base64=self.src_base64, width=self.width, height=self.height, fit=self.fit, gapless_playback=True)
            else:
              self.image = Img(src=asset_dir(self.src), width=self.width, height=self.height, fit=self.fit, gapless_playback=True)
        if self.zoom:
            self.image = PanZoom(self.image, self.width, self.height,
                      width=self.width,
                      height=self.height)#, on_click=lambda e: print(f'CLICK {e.local_x} {e.local_y}'))
        self.column = Column([Row([PopupMenuButton(
            items = [
                PopupMenuItem(text="View Image", icon=icons.FULLSCREEN, on_click=image_details),
                PopupMenuItem(text="Copy Path", icon=icons.CONTENT_PASTE, on_click=copy_path),
                PopupMenuItem(text="Download Locally" if is_Colab else "Save As", icon=icons.DOWNLOAD, on_click=download_image),
                PopupMenuItem(text="Delete Image", icon=icons.DELETE, on_click=delete_image),
            ], expand=True, tooltip="👁️",
            content=Row([self.image], expand=True),
            #content=Row([Img(src=self.src, width=self.width, height=self.height, fit=self.fit, gapless_playback=True)], alignment=MainAxisAlignment.CENTER if self.center else MainAxisAlignment.START),
            data=self.src if self.data==None else self.data, width=self.width, height=self.height)],
            alignment=MainAxisAlignment.CENTER if self.center else MainAxisAlignment.START)], horizontal_alignment=CrossAxisAlignment.CENTER if self.center else CrossAxisAlignment.START)
        if self.show_subtitle:
            self.column.controls.append(Row([Text(self.subtitle)], alignment=MainAxisAlignment.CENTER if self.center else MainAxisAlignment.START))
        return self.column


class NumberPicker(Row):
    def __init__(self, label="", value=1, min=0, max=20, step=1, height=45, tooltip=None, on_change=None):
        super().__init__()
        self.value = value
        self.min = min
        self.max = max
        self.step = step
        self.label = label
        self.height = height
        self.tooltip = tooltip
        self.on_change = on_change
        self.build()
    def build(self):
        def changed(e):
            self.value = int(e.control.value)
            if self.value < self.min:
              self.value = self.min
              self.txt_number.value = self.value
              self.txt_number.update()
            if self.value > self.max:
              self.value = self.max
              self.txt_number.value = self.value
              self.txt_number.update()
            if self.on_change is not None:
              e.control = self
              self.on_change(e)
        def minus_click(e):
            self.value = int(self.value)
            if self.value > self.min:
              self.value -= self.step
              self.txt_number.value = self.value
              self.txt_number.update()
              e.control = self
              if self.on_change is not None:
                self.on_change(e)
        def plus_click(e):
            self.value = int(self.value)
            if self.value < self.max:
              self.value += self.step
              self.txt_number.value = self.value
              self.txt_number.update()
              e.control = self
              if self.on_change is not None:
                self.on_change(e)
        self.txt_number = TextField(value=str(self.value), text_align=TextAlign.CENTER, width=55, height=self.height, content_padding=padding.only(top=0), keyboard_type=KeyboardType.NUMBER, on_change=changed)
        label_text = Text(self.label) if not self.tooltip else Tooltip(message=self.tooltip, content=Text(self.label))
        return Row([label_text, IconButton(icons.REMOVE, on_click=minus_click), self.txt_number, IconButton(icons.ADD, on_click=plus_click)], spacing=1)

class SliderRow(Stack):
    def __init__(self, label="", value=None, min=0, max=20, divisions=20, step=None, multiple=1, round=0, suffix="", left_text=None, right_text=None, visible=True, tooltip="", pref=None, key=None, expand=None, col=None, stack=False, on_change=None, data=None):
        super().__init__()
        self.value = value or pref[key]
        self.min = min
        self.max = max
        if step:
            self.divisions = (max - min) / step
        else:
            self.divisions = divisions
        self.multiple = multiple
        self.label = label
        self.round = round
        self.suffix = suffix
        self.left_text = left_text
        self.right_text = right_text
        self._visible = visible
        self.slider_row = Container(content=None)
        self.tooltip = tooltip
        self.expand = expand
        self.pref = pref
        self.key = key
        self.col = col
        self.stack = stack
        self.on_change = on_change
        self.data = data
        self.build()
    def build(self):
        def change_slider(e):
            self.value = round(e.control.value, self.round)
            v = int(self.value) if self.round == 0 else float(self.value)
            # TODO: Figure out how to update label text on web
            #slider.label = f"{v}{self.suffix}"
            #slider.update()
            self.slider_value.value = f"{v}{self.suffix}"
            self.slider_value.update()
            self.slider_edit.value = f"{v}"
            self.slider_edit.update()
            if self.on_change is not None:
              e.control = self
              self.on_change(e)
            if self.pref is not None:
              self.pref[self.key] = int(self.value) if self.round == 0 else float(self.value)
        def changed(e):
            try:
              self.value = int(e.control.value) if self.round == 0 else round(float(e.control.value), self.round)
            except ValueError:
              e.control.value = str(self.value)
              e.control.update()
              return
            if self.value < self.min:
              self.value = self.min
              self.slider_edit.value = self.value
              self.slider_value.value = self.value
              self.slider_value.update()
            if self.value > self.max:
              self.value = self.max
              self.slider_edit.value = self.value
              self.slider_value.value = self.value
              self.slider_value.update()
            if self.multiple != 1:
              v = int(self.multiple * round(self.value / self.multiple))
              if v != self.value:
                self.slider_edit.value = v
                self.slider_edit.update()
                self.value = v
            if self.on_change is not None:
              e.control = self
              self.on_change(e)
            self.slider.value = self.value
            self.slider.label = f"{self.value}{self.suffix}"
            self.slider.update()
            self.slider_value.value = f"{self.value}{self.suffix}"
            self.slider_value.update()
            if self.pref is not None:
              self.pref[self.key] = int(self.value) if self.round == 0 else float(self.value)
        def blur(e):
            self.slider_edit.visible = False
            slider_text.visible = True
            self.slider_edit.update()
            slider_text.update()
            slider_label.value = f"{self.label}: "
            slider_label.update()
        def edit(e):
            self.slider_edit.visible = True
            slider_text.visible = False
            slider_text.update()
            self.slider_edit.update()
            slider_label.value = f"{self.label}"
            slider_label.update()
            #self.slider_number = TextField(value=str(self.value), on_blur=blur, text_align=TextAlign.CENTER, width=55, height=50, content_padding=padding.only(top=4), keyboard_type=KeyboardType.NUMBER, on_change=changed)
            #self.update()
            #e.control.update()
            #e.page.update()
        self.slider_edit = TextField(value=str(self.value), on_blur=blur, autofocus=True, visible=False, text_align=TextAlign.CENTER, width=51, height=45, content_padding=padding.only(top=6), keyboard_type=KeyboardType.NUMBER, on_change=changed)
        self.slider = Slider(min=float(self.min), max=float(self.max), divisions=int(self.divisions), round=self.round, label="{value}" + self.suffix, value=float(self.value), tooltip=self.tooltip, expand=True, on_change=change_slider)
        self.slider_value = Text(f" {self.value}{self.suffix}", weight=FontWeight.BOLD)
        slider_text = GestureDetector(self.slider_value, on_tap=edit, mouse_cursor=ft.MouseCursor.PRECISE)
        slider_label = Text(f"{self.label}:")
        left = Text("", visible=False)
        right = Text("", visible=False)
        if bool(self.left_text): left.value = self.left_text
        if bool(self.right_text): right.value = self.right_text
        self.slider_number = slider_text
        if prefs['slider_stack'] or self.stack:
            self.slider_row = Container(Column([Row([Container(content=None, width=12), slider_label, slider_text, self.slider_edit], spacing=0, height=36), Row([left, self.slider, right], spacing=0, height=12)], spacing=0, tight=True), expand=self.expand, visible=self._visible, col=self.col)
        else:
            self.slider_row = Container(Row([slider_label, slider_text, self.slider_edit, left, self.slider, right], expand=True), expand=self.expand, visible=self._visible, col=self.col)
        return self.slider_row
    def set_value(self, value):
        self.value = value
        self.slider.value = value
        self.slider_edit.value = value
        if self.pref is not None:
          self.pref[self.key] = value
        #self.slider_value.value = f" {self.pref[self.key]}{self.suffix}"
        self.slider_value.value = f" {self.value}{self.suffix}"
        self.slider_value.update()
        self.slider.update()
    def set_min(self, value):
        self.min = value
        self.slider.min = value
    def set_max(self, value):
        self.max = value
        self.slider.max = value
    def set_divisions(self, value):
        self.divisions = value
        self.slider.divisions = value
    @property
    def show(self):
        return self._visible
    @show.setter
    def show(self, value):
        self._visible = value
        self.slider_row.visible = value
        try:
            self.slider_row.update()
        except Exception:
            pass
    def update_slider(self):
        self.slider.update()

class Switcher(Row):
    def __init__(self, label="", value=False, tooltip=None, disabled=False, visible=True, on_change=None, active_color=None, active_track_color=None, label_position=ft.LabelPosition.RIGHT):
        super().__init__()
        self.label = label
        self.value = value
        self.tooltip = tooltip
        self.label_position=label_position
        self.disabled = disabled
        self.visible = visible
        self.on_change = on_change
        self.active_color = active_color if active_color != None else colors.PRIMARY_CONTAINER
        self.active_track_color = active_track_color if active_track_color != None else colors.PRIMARY
        self.build()
    def build(self):
        self.switch = Switch(label=f"  {self.label} ", value=self.value, disabled=self.disabled, visible=self.visible, tooltip=self.tooltip, active_color=self.active_color, active_track_color=self.active_track_color, label_position=self.label_position, on_change=self.on_change)
        return self.switch
        if self.tooltip != None:
            return Tooltip(message=self.tooltip, content=self.switch)
        else:
            return self.switch

class Installing(Stack):
    def __init__(self, message=""):
        super().__init__()
        self.message = message
        self.build()
    def build(self):
        self.message_txt = Text(self.message, theme_style=ft.TextThemeStyle.BODY_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD, max_lines=3)
        self.details = Text("")
        self.progress = ProgressRing()
        return Container(content=Row([self.progress, Container(content=None, width=1), self.message_txt, Container(content=None, expand=True), self.details]), padding=padding.only(left=9, bottom=10))
    def set_message(self, msg=""):
        self.message_txt.value = msg
        self.message_txt.update()
    def status(self, msg=""):
        self.details.value = msg
        self.details.update()
    def show_progress(self, show):
        self.progress.visible = show
        self.progress.update()

class Progress(Stack):
    def __init__(self, message="", steps=50, show_preview=False):
        super().__init__()
        self.message = message
        self.steps = steps
        self.start_step = 0
        self.start_callback = 0
        self.show_preview = show_preview
        self.build()
    def build(self):
        self.message_txt = Text(self.message, theme_style=ft.TextThemeStyle.BODY_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD, max_lines=3)
        self.details = Text("")
        self.progress = ProgressBar(bar_height=8)
        self.preview = Container(content=None) if not self.show_preview else ft.Image()
        return Container(content=Column([Row([self.message_txt, Container(content=None, expand=True), self.details]), self.progress]), padding=padding.only(left=9, bottom=10))
    def set_message(self, msg=""):
        self.message_txt.value = msg
        self.message_txt.update()
    def status(self, msg=""):
        self.details.value = msg
        self.details.update()
    def show_progress(self, show):
        self.progress.visible = show
        self.progress.update()
    @property
    def total_steps(self):
        return self.steps
    @total_steps.setter
    def total_steps(self, value):
        self.steps = value
    def callback_step(self, pipe, step, timestep, callback_kwargs):
        now = time.time()
        itsec = ""
        try:
            self.steps = pipe.num_timesteps
        except:
            pass
        if step < 2:
            self.start_callback = now
        else:
            itsec = f" - {its(now - self.start_step)} - Elapsed: {elapsed(self.start_callback, now)}"
        self.start_step = now
        percent = (step +1)/ self.steps
        self.progress.progress.value = percent
        self.progress.progress.tooltip = f"{int(percent * 100)}% [{step +1} / {self.steps}] (Timestep: {timestep:.1f}){itsec}"
        self.progress.progress.update()
        #if abort_run:
        #    pipe._interrupt = True
    def callback_alt(self, step: int, timestep: int, callback_kwargs) -> None:
        now = time.time()
        itsec = ""
        if step < 2:
            self.start_callback = now
        else:
            itsec = f" - {its(now - self.start_step)} - Elapsed: {elapsed(self.start_callback, now)}"
        self.start_step = now
        percent = (step +1)/ self.steps
        self.progress.value = percent
        self.progress.tooltip = f"{int(percent * 100)}% [{step +1} / {self.steps}]{itsec}"
        self.progress.update()
    def callback_fnc(self, step: int, timestep: int, latents: torch.FloatTensor) -> None:
        self.callback_fnc.has_been_called = True
        now = time.time()
        itsec = ""
        if step < 2:
            self.start_callback = now
        else:
            itsec = f" - {its(now - self.start_step)} - Elapsed: {elapsed(self.start_callback, now)}"
        self.start_step = now
        percent = (step +1)/ self.steps
        self.progress.value = percent
        self.progress.tooltip = f"{int(percent * 100)}% [{step +1} / {self.steps}]{itsec}"
        self.progress.update()

class UpscaleBlock(Stack):
    def __init__(self, pref):
        super().__init__()
        self.pref = pref
        self.build()
    def build(self):
        apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=self.pref['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=self.toggle_ESRGAN)
        enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=self.pref, key='enlarge_scale')
        face_enhance = 'face_enhance' in self.pref
        if face_enhance:
            use_face_enhance = Checkbox(label="Use Face Enhance GPFGAN", value=self.pref['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:self.pref.update(face_enhance=e.control.value))
        display_upscaled_image = Checkbox(label="Display Upscaled Image", value=self.pref['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:self.pref.update(display_upscaled_image=float(e.control.value)))
        self.ESRGAN_settings = Container(Column([enlarge_scale_slider, use_face_enhance if face_enhance else Container(content=None), display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if self.pref['apply_ESRGAN_upscale'] else 0)
        self.ESRGAN_block = Container(Column([apply_ESRGAN_upscale, self.ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if status['installed_ESRGAN'] else 0)
        return self.ESRGAN_block
    def show(self, visible=True):
        self.ESRGAN_block.height = None if visible else 0
        try:
            self.ESRGAN_block.update()
        except Exception as e:
            #print(f"Upscaler {self.pref}: {e}")
            pass
    def toggle_ESRGAN(self, e):
        self.ESRGAN_settings.height = None if e.control.value else 0
        self.pref['apply_ESRGAN_upscale'] = e.control.value
        self.ESRGAN_settings.update()

def show_upscalers(page, show=True):
    for u in page.upscalers:
        u.show(show)

def pip_install(packages, installer=None, print=False, prt=None, cwd=None, upgrade=False, q=False):
    arg = ""
    if upgrade: arg += "--upgrade "
    if q: arg += "-q "
    for package in packages.split():
        try:
            pkg = package
            if '|' in pkg: pkg = pkg.rpartition('|')[2]
            if '-' in pkg: pkg = pkg.replace('-', '_')
            if '=' in pkg:
                pkg = re.split('[=~]|>=|<=', pkg)[0]
            exec(f"import {pkg.lower()}")
        except ImportError:
            if '|' in package: package = package.rpartition('|')[0]
            if '[' in package: package = package.rpartition('[')[0]
            if installer != None:
                installer.status(f"...installing {pkg}")
            try:
                if prt == None:
                    run_sp(f"pip install {arg}{package}", realtime=print, cwd=cwd)
                else:
                    console = RunConsole(show_progress=False)
                    prt(console)
                    console.run_process(f"pip install {arg}{package}", cwd=cwd)
            except Exception as e:
                print(f"Error Installing {package}: {e}")
                if installer != None:
                    installer.status(f"...error installing {package}")
                pass
            pass

def get_percentage(line):
    if '%' not in line:
        return None
    match = re.search(r'\d+%', line)
    if match:
        percentage_str = match.group()
        try:
            return int(percentage_str.rstrip('%'))
        except ValueError:
            return None
    else:
        return None

class RunConsole(Stack):
    def __init__(self, title="", height=200, show_progress=True):
        super().__init__()
        self.title = title
        self.height = height
        self.show_progress = show_progress
        self.progress = ProgressBar(bar_height=8)
        self.last_line = ""
        self.build()
    def add_text(self, text):
        try:
            self.column.controls.append(Text(text, theme_style=TextThemeStyle.TITLE_SMALL, color=colors.ON_SURFACE_VARIANT))
            self.column.update()
        except Exception:
            pass
    def clear_last(self):
        del self.column.controls[-1]
        self.column.update()
    def clear(self):
        self.main_column.controls.clear()
        self.main_column.update()
    def run_process(self, cmd_str, cwd=None):
        cmd_list = cmd_str if type(cmd_str) is list else cmd_str.split()
        if cwd is None:
            process = subprocess.Popen(cmd_str, shell = True, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' )
        else:
            process = subprocess.Popen(cmd_str, shell = True, cwd=cwd, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' )
        while True:
            realtime_output = process.stdout.readline()
            if realtime_output == '' and process.poll() is not None:
                break
            percent = get_percentage(realtime_output.strip())
            last_percent = get_percentage(self.last_line)
            if last_percent is not None and last_percent != 100:
                self.clear_last()
            elif "Downloading" in realtime_output:
                self.clear_last()
            if self.show_progress:
                if percent is not None:
                    self.progress.value = percent
                    self.progress.update()
                else:
                    if self.progress.value is not None:
                        self.progress.value = percent
                        if percent != last_percent:
                            self.progress.update()
            self.add_text(realtime_output.strip())
            self.last_line = realtime_output.strip()
            #self.column.controls.append(Text(realtime_output.strip(), style="titleSmall", color=colors.TERTIARY))
            #self.column.update()
            sys.stdout.flush()
        self.clear()
    def build(self):
        #self.column = Column([Row([Text(self.title, style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Row(self.actions) if bool(self.actions) else Container(content=None)], alignment=MainAxisAlignment.SPACE_BETWEEN, spacing=0, vertical_alignment=CrossAxisAlignment.END)], spacing=4)
        self.column = Column([], spacing=1, scroll=ScrollMode.AUTO, auto_scroll=True) #, width=(page.width if page.web else page.window.width) - 20
        height = (self.height + 48) if bool(self.title) else self.height
        self.container = Container(content=self.column, border_radius=ft.border_radius.all(16), height=self.height, bgcolor=colors.SURFACE_VARIANT, padding=ft.padding.symmetric(10, 12))
        self.main_column = Column([])
        if bool(self.title):
            self.main_column.controls.append(Text(self.title, theme_style=ft.TextThemeStyle.BODY_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD, max_lines=2))
        self.main_column.controls.append(self.container)
        height = (self.height + 48) if bool(self.title) else self.height
        if self.show_progress:
            self.main_column.controls.append(self.progress)
            height += 10
        return Container(content=self.main_column, height=height)

def elapsed(start_time, end_time=None):
    if end_time is None:
        end_time = time.time()
    return f"{end_time-start_time:.0f}s"

def its(step_time):
    if step_time < 1.0:
        its = 1.0 / step_time
        output = f"{its:.1f} it/s"
    else:
        sit = step_time
        output = f"{sit:.1f} s/it"
    return output

def clear_line(column, lines=1, update=True, protect_first=False):
    for l in range(lines):
        if len(column.controls) < (1 if not protect_first else 2): return
        del column.controls[-1]
    if update:
        column.update()

def prt_line(line, column, size=17, update=True, from_list=False, page=None):
    if type(line) == str:
      line = Text(line, size=size)
    if from_list:
      page.imageColumn.controls.append(line)
      if update:
        page.imageColumn.update()
    else:
      column.controls.append(line)
      if update:
        column.update()

def nudge(column:Column, page=None):
    ''' Force an autoscroll column to go down. Mainly to show ProgressBar not scrolling to bottom.'''
    scroll = column.auto_scroll
    if not scroll:
        column.auto_scroll = True
    column.controls.append(Container(content=Text(" ")))
    column.update()
    #time.sleep(0.2)
    del column.controls[-1]
    column.update()
    if not scroll:
        column.auto_scroll = False
    if page is not None:
        page.update()

def asset_dir(path):
    """
    A hack because of change to Flet to get the assets directory path relative to '/content'.
    """
    if is_Colab:
        if path.startswith('/content'):
            return os.path.relpath(path, '/content')
        else:
            return path
    else:
        return path
        if path.startswith(os.path.dirname(__file__)):
            print(os.path.relpath(path, os.path.dirname(__file__)))
            return os.path.relpath(path, os.path.dirname(__file__))
        else:
            return path

def makedir(path):
    if not os.path.exists(path):
        os.makedirs(path, exist_ok=True)

def download_github_folder(repo, folder_path, local_dir):
    url = f"https://api.github.com/repos/{repo}/contents/{folder_path}"
    response = requests.get(url)
    if response.status_code != 200:
        raise Exception(f"Error fetching contents of folder {folder_path} from GitHub repo {repo}. Status code: {response.status_code}")
    folder_contents = response.json()
    if not os.path.exists(local_dir):
        os.makedirs(local_dir)
    for item in folder_contents:
        if item['type'] == 'file':
            download_url = item['download_url']
            file_name = os.path.join(local_dir, item['name'])
            file_response = requests.get(download_url)
            if file_response.status_code == 200:
                with open(file_name, 'wb') as file:
                    file.write(file_response.content)
                #print(f"Downloaded {file_name}")
            else:
                print(f"Failed to download {file_name}. Status code: {file_response.status_code}")
        elif item['type'] == 'dir':
            sub_folder_path = item['path']
            sub_local_dir = os.path.join(local_dir, item['name'])
            download_github_folder(repo, sub_folder_path, sub_local_dir)

def chmod_recursive(path, mode):
    for root, dirs, files in os.walk(path):
        os.chmod(root, mode)
        for file in files:
            file_path = os.path.join(root, file)
            os.chmod(file_path, mode)

def vram_free():
    global status
    status['gpu_used'] = torch.cuda.max_memory_allocated(device=torch.device("cuda")) / (1024 * 1024 * 1024)
    return status['gpu_memory'] - status['gpu_used']

stop_thread = False
def background_update(page):
    try:
      global stop_thread
      while not stop_thread:
          update_stats(page)
          time.sleep(prefs['stats_update'])
    except Exception:
      pass

def start_thread(page):
    global stop_thread
    stop_thread = False
    polling_task = threading.Thread(target=background_update, args=(page,))
    polling_task.daemon = True
    polling_task.start()

def get_memory_stats(used=True):
    global status
    try:
        #status['cpu_memory'] = psutil.virtual_memory().total / (1024 * 1024 * 1024)
        #status['gpu_memory'] = torch.cuda.get_device_properties(0).total_memory / (1024 * 1024 * 1024)
        status['cpu_used'] = psutil.virtual_memory().used / (1024 * 1024 * 1024)
        status['gpu_used'] = torch.cuda.max_memory_allocated(device=torch.device("cuda")) / (1024 * 1024 * 1024)
        gpu_mem = f"{status['gpu_used']:.1f}/{status['gpu_memory']:.0f}GB"
    except Exception:
        status['gpu_used'] = 0
        gpu_mem = "N/A"
        pass
    if not used:
        ram = float(status['cpu_memory']) - float(status['cpu_used'])
        if gpu_mem == "N/A":
            return f"Free VRAM: None", f"Free RAM: {ram:.1f}GB"
        vram = float(status['gpu_memory']) - float(status['gpu_used'])
        return f"Free VRAM: {vram:.1f}GB", f"Free RAM: {ram:.1f}GB"
    else:
        return f"VRAM: {gpu_mem}", f"RAM: {status['cpu_used']:.1f}/{status['cpu_memory']:.0f}GB"

def update_stats(page):
    try:
        vram, ram = get_memory_stats(prefs['stats_used'])
        page.stats.controls[0].value = vram
        page.stats.controls[1].value = ram
        page.stats.update()
        page.appbar.update()
    except Exception as e:
        print(e)
        pass

def toggle_stats(page):
    global prefs
    prefs['stats_used'] = not prefs['stats_used']
    page.stats_used.value = prefs['stats_used']
    page.stats_used.update()
    update_stats(page)

def save_metadata(image_path, pref, pipeline="", model="", seed=None, prompt=None, negative=None, scheduler=False, extra={}):
    if prefs['save_image_metadata']:
        img = PILImage.open(image_path)
        from PIL.PngImagePlugin import PngInfo
        metadata = PngInfo()
        if bool(prefs['meta_ArtistName'].strip()):
            metadata.add_text("artist", prefs['meta_ArtistName'])
        if bool(prefs['meta_Copyright'].strip()):
            metadata.add_text("copyright", prefs['meta_Copyright'])
        upscaled = "" if 'apply_ESRGAN_upscale' not in pref else (f", upscaled {pref['enlarge_scale']}x with ESRGAN" if pref['apply_ESRGAN_upscale'] else "")
        metadata.add_text("software", f"Stable Diffusion Deluxe{upscaled}")
        metadata.add_text("website", "https://DiffusionDeluxe.com")
        if bool(pipeline):
            metadata.add_text("pipeline", pipeline)
        if prefs['save_config_in_metadata']:
            config_json = pref.copy()
            if prompt != None:
                metadata.add_text("title", prompt)
                config_json['prompt'] = prompt
            elif 'prompt' in extra:
                metadata.add_text("title", extra['prompt'])
            elif 'prompt' in pref:
                metadata.add_text("title", pref['prompt'])
            if negative != None:
                config_json['negative_prompt'] = negative
            if bool(model): config_json['model_path'] = model
            if bool(seed): config_json['seed'] = seed
            if scheduler: config_json['scheduler_mode'] = prefs['scheduler_mode']
            if 'num_images' in pref: del config_json['num_images']
            if 'num_videos' in pref: del config_json['num_videos']
            if 'batch_folder_name' in pref: del config_json['batch_folder_name']
            if 'file_name' in pref: del config_json['file_name']
            if 'lower_memory' in pref: del config_json['lower_memory']
            if 'max_size' in pref: del config_json['max_size']
            if 'apply_ESRGAN_upscale' in pref:
                if 'display_upscaled_image' in pref: del config_json['display_upscaled_image']
                if not config_json['apply_ESRGAN_upscale']:
                    del config_json['enlarge_scale']
                    del config_json['apply_ESRGAN_upscale']
                else:
                    config_json['upscale_model'] = prefs['upscale_model']
            if extra:
                config_json.update(extra)
            metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
        if image_path.endswith("jpg"):
            metadata_dict = dict(metadata.items())
            img.info['metadata'] = metadata_dict
            img.save(image_path)
        else:
            img.save(image_path, pnginfo=metadata)
        img.close()
        return img
    else:
        return img

def load_horde_text(page, update=True):
    global horde_text_models
    if not horde_text_models:
      model_request = "https://aihorde.net/api/v2/status/models?type=text"
      headers = {'apikey': prefs['AIHorde_api_key']}
      response = requests.get(model_request, headers=headers)
      if response != None:
          if response.status_code == 200:
            horde_models = json.loads(response.content)
            horde_models = sorted(horde_models, key=lambda x: (-x['count'], x['name']), reverse=False)
            horde_text_models = horde_models
      for model in horde_text_models:
        m = model['name'].rpartition('/')[2]
        page.AIHorde_model_generator.options.append(dropdown.Option(m))
        page.AIHorde_model_remixer.options.append(dropdown.Option(m))
        page.AIHorde_model_brainstormer.options.append(dropdown.Option(m))
      if update:
        page.AIHorde_model_generator.update()
        page.AIHorde_model_remixer.update()
        page.AIHorde_model_brainstormer.update()

def models_AIHorde(e):
    model_request = "https://aihorde.net/api/v2/status/models?type=text"
    headers = {'apikey': prefs['AIHorde_api_key']}
    response = requests.get(model_request, headers=headers)
    if response != None:
        if response.status_code == 200:
          horde_models = json.loads(response.content)
          horde_models = sorted(horde_models, key=lambda x: (-x['count'], x['name']), reverse=False)
          horde_text_models = horde_models
          model_info = [f"{model['name']} - Count: {model['count']}{f' Jobs: '+str(int(model['jobs'])) if model['jobs'] != 0.0 else ''}" for model in horde_models]
          alert_msg(e.page, "🏇  AI-Horde Current Text Model Stats", model_info, sound=False)
        else: print(response)

def get_horde_text(page, column, text_prompt, llm_model="aphrodite/Gryphe/Pantheon-RP-1.0-8b-Llama-3", temperature=0.8):
    global horde_text_models
    def prt(line):
      if type(line) == str:
        line = Text(line)
      column.controls.append(line)
      column.update()
    def clear_last(lines=1):
      clear_line(column, lines=lines)
    progress = Progress(f"Getting Prompts with Stable Horde API - Using {llm_model} Model...")
    prt(progress)
    if '/' not in llm_model:
      horde_model = next((model['name'] for model in horde_text_models if llm_model in model['name']), llm_model)
    else:
      horde_model = llm_model
    import requests
    from io import BytesIO
    import base64
    api_host = 'https://aihorde.net/api'
    #api_check_url = f"{api_host}/v2/generate/check/"
    api_check_url = f"{api_host}/v2/generate/text/status/"
    api_get_result_url = f"{api_host}/v2/generate/text/status/"
    url = f"{api_host}/v2/generate/text/async"
    headers = {
        #'Content-Type': 'application/json',
        #'Accept': 'application/json',
        'apikey': prefs['AIHorde_api_key'],
    }
    payload = {
      "prompt": text_prompt,
      "models": [horde_model],
      "trusted_workers": False,
      "slow_workers": True,
      "dry_run": False,
      "disable_batching": False,
      "allow_downgrade": False,
    }
    params = {
      "n": 1,
      "frmtadsnsp": False,
      "frmtrmblln": False,
      "frmtrmspch": False,
      "frmttriminc": True,
      "max_context_length": 1024,
      "max_length": 512,
      "rep_pen": 3,
      "rep_pen_range": 4096,
      "rep_pen_slope": 10,
      "singleline": False,
      "temperature": temperature,
      "tfs": 1,
      "top_a": 1,
      "top_k": 100,
      "top_p": 0.92,
      "typical": 1,
      "sampler_order": [6, 0, 1, 3, 4, 2, 5],
      "use_default_badwordsids": True,
      #"stop_sequence": ["string"],
      "min_p": 0,
      "smoothing_factor": 0,
      "dynatemp_range": 0,
      "dynatemp_exponent": 1,
    }
    payload["params"] = params
    #print(params)
    try:
        response = requests.post(url, headers=headers, json=payload)#json.dumps(payload, indent = 4))
    except Exception as e:
        alert_msg(page, f"ERROR: Problem sending JSON request and getting response.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]), debug_pref=payload)
        print(payload)
        return ""
    if response != None:
      if response.status_code != 202:
        clear_last()
        if response.status_code == 400:
          alert_msg(page, "Stable Horde-API ERROR: Validation Error...", content=Text(str(response.text)))
          return ""
        else:
          prt(Text(f"Stable Horde-API ERROR {response.status_code}: " + str(response.text), selectable=True))
          print(payload)
          return ""
    artifacts = json.loads(response.content)
    q_id = artifacts['id']
    #print(str(artifacts))
    elapsed_seconds = 0
    try:
      while True:
        if abort_run: break
        check_response = requests.get(api_check_url + q_id)
        check = json.loads(check_response.content)
        #print(check)
        if not check['is_possible']:
          clear_last()
          alert_msg(page, f"💢  {artifacts['message']}", content=[str(payload)])
          print(payload)
          return ""
        try:
          div = check['wait_time'] + elapsed_seconds
          percentage = (1 - check['wait_time'] / div)
        except Exception:
          div = 0
          percentage = None
          #continue
          pass
        #if div == 0: continue
        progress.progress.value = percentage
        progress.progress.update()
        status_txt = f"Stable Horde API LLM - Queued Position: {check['queue_position']} - Waiting: {check['waiting']} - Wait Time: {check['wait_time']} - Elapsed: {elapsed_seconds}s"
        progress.set_message(status_txt)
        if bool(check['finished']):
          kudos = check['kudos']
          print(f"AI-Horde Kudos Used: {kudos}")
          break
        time.sleep(2)
        elapsed_seconds += 2
    except Exception as e:
      alert_msg(page, f"EXCEPTION ERROR: Unknown error processing image. Check parameters and try again. Restart app if persists.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
      return ""
    attempts = 0
    success = False
    while not success:
      try:
        get_response = requests.get(api_get_result_url + q_id)
        success = True
      except Exception as e:
        attempts += 1
        if attempts < 3:
          print(f"ERROR: {e}. Trying again {attempts}/3")
          time.sleep(3)
          pass
        else:
          clear_last()
          alert_msg(page, f"💢  ERROR after 3 attempts: {e}", content=[str(payload)])
          return ""
    final_results = json.loads(get_response.content)
    clear_last()
    return final_results['generations'][0]['text']

def get_perplexity_text(page, column, text_prompt, llm_model="llama-3-sonar-small-32k-chat", temperature=0.2, system_prompt=None):
    def prt(line):
      if type(line) == str:
        line = Text(line)
      column.controls.append(line)
      column.update()
    def clear_last(lines=1):
      clear_line(column, lines=lines)
    progress = Progress(f"Getting Prompts with Perplexity PPLX-API using {llm_model} Model...")
    prt(progress)
    import requests
    from io import BytesIO
    import base64
    system = system_prompt if system_prompt != None else "Write imaginative prompts for an AI Image Generation and return a bullet point list with * astrix at the beginning of each line of requested prompt texts. Do not preface the list with any extra text like Here's a list of... Just give a creative list with the amount specified about the subject provided in the style requested."
    api_host = 'https://api.perplexity.ai/chat/completions'
    headers = {
        "accept": "application/json",
        "content-type": "application/json",
        "authorization": f"Bearer {prefs['Perplexity_api_key']}",
    }
    payload = {
      "model": llm_model,
      "messages": [
          {"role": "system", "content": system},
          {"role": "user", "content": text_prompt}
      ],
      "temperature": temperature,
    }
    #print(params)
    try:
        response = requests.post(api_host, headers=headers, json=payload)#json.dumps(payload, indent = 4))
    except Exception as e:
        alert_msg(page, f"ERROR: Problem sending JSON request and getting response.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]), debug_pref=payload)
        print(payload)
        clear_last()
        return ""
    clear_last()
    if response != None:
      if response.status_code != 200:
        clear_last()
        if response.status_code == 422:
          alert_msg(page, "PPLX-API ERROR: Validation Error...", content=Text(str(response.text)))
          return ""
        else:
          prt(Text(f"PPLX-API ERROR {response.status_code}: " + str(response.text), selectable=True))
          print(payload)
          return ""
    results = json.loads(response.content)
    return results['choices'][0]['message']['content']

def pastebin_file(file_path):
    pip_install("pbwrap")
    from pbwrap import Pastebin
    pb = Pastebin("K-W_E7Djts7N2HE0bTCPa5qqo9Tc77fs", False)
    #file_content = open(file_path, "r").read()
    with open(file_path, "rb") as f:
        file_data = f.read()
    encoded_data = base64.b64encode(file_data).decode("utf-8")
    try:
        paste = pb.create_paste_from_file(file_path, api_paste_private=1, api_user_name="Skquark", api_paste_expire_date="10M")
        print(paste)
        return paste#.private_url
    except Exception as e:
        return ""

def transfersh_file(file_path):
    pip_install("pyperclip wget twine transfersh_client")
    from transfersh_client.app import send_to_transfersh#, create_zip, remove_file
    transfer_sh_url = send_to_transfersh(file_path, False)
    parts = transfer_sh_url.split('/')
    parts.insert(-2, 'get')
    url = '/'.join(parts)
    print(url)
    return url
    
def check_diffusers(page:Page):
    global status
    def install_diffusers(e):
        close_alert_dlg(e)
        #if prefs['install_diffusers']:
        def console_msg(msg, clear=True, show_progress=True):
          if not page.top_banner.open:
            page.top_banner.open = True
          if clear:
            page.top_banner.content.controls = []
          if show_progress:
            page.top_banner.content.controls.append(Row([Stack([Icon(icons.DOWNLOADING, color=colors.AMBER, size=48), Container(content=ProgressRing(), padding=padding.only(top=6, left=6), alignment=alignment.center)]), Container(content=Text("  " + msg.strip() , weight=FontWeight.BOLD, color=colors.ON_SECONDARY_CONTAINER, size=18), alignment=alignment.bottom_left, padding=padding.only(top=6)) ]))
          else:
            page.top_banner.content.controls.append(Text(msg.strip(), weight=FontWeight.BOLD, color=colors.GREEN_600))
          page.update()
        page.console_msg = console_msg
        page.top_banner.content = Column([], scroll=ScrollMode.AUTO, auto_scroll=True, tight=True, spacing=0, alignment=MainAxisAlignment.END)
        page.top_banner.open = True
        console_msg("Installing Hugging Face Diffusers Pipeline...")
        get_diffusers(page)
        status['installed_diffusers'] = True
        play_snd(Snd.DONE, page)
        page.top_banner.open = False
        page.top_banner.update()
        page.update()
    if not status['installed_diffusers']:
        diffusers_button = ft.FilledTonalButton(content=Text("🔄  Install Diffusers Now", size=18), on_click=install_diffusers) if prefs['install_diffusers'] else Container(content=None)
        alert_msg(page, "🤗  You need to Install HuggingFace Diffusers before using...", buttons=[diffusers_button])
        return False
    else:
        return True

def force_update(package):
    global status
    if force_updates:
        if package in status['updated']:
            return False
        else:
            status['updated'].append(package)
            return True
    else:
        return False

def create_pattern(filename, glob=False):
    base, ext = os.path.splitext(filename)
    last_digits = ""
    non_digits = ""
    for c in base:
        if c.isdigit():
            last_digits += c
        else:
            non_digits += c
            last_digits = ""
    padding = min(4, len(last_digits))
    base_name = base[:-padding]
    if glob:
        pattern = f"{base_name}*{ext}"
    else:
        pattern = f"{base_name}%0{padding}d{ext}"
    return pattern

def interpolate_video(frames_dir, input_fps=None, output_fps=30, output_video=None, recursive_interpolation_passes=None, installer=None, denoise=False, sharpen=False, deflicker=False, metadata=None):
    frame_interpolation_dir = os.path.join(root_dir, 'frame-interpolation')
    saved_model_dir = os.path.join(frame_interpolation_dir, 'pretrained_models')
    photos_dir = os.path.join(frame_interpolation_dir, 'photos')
    interpolated = os.path.join(photos_dir, "interpolated.mp4")
    def stat(msg):
        if installer is not None: installer.status(f"...{msg}")
    if not os.path.exists(frame_interpolation_dir):
        stat("cloning frame-interpolation")
        run_sp("git clone https://github.com/google-research/frame-interpolation", cwd=root_dir, realtime=False) #pytti-tools
    try:
        import frame_interpolation
    except ModuleNotFoundError:
        stat("installing frame-interpolation requirements")
        #run_sp(f"pip install -r requirements.txt", cwd=frame_interpolation_dir, realtime=True)
        #run_sp(f"pip install .", cwd=frame_interpolation_dir, realtime=False) apache-beam==2.34.0
        pip_install("tensorflow tensorflow-datasets tensorflow-addons absl-py==0.12.0 gin-config==0.5.0 parameterized==0.8.1 mediapy scikit-image|skimage apache-beam|apache_beam google-cloud-bigquery-storage==1.1.0|google.cloud.bigquery_storage natsort==8.1.0 gdown==4.7.3 tqdm", upgrade=True, installer=installer)
        get_ffmpeg(installer)
        #import frame_interpolation
        if frame_interpolation_dir not in sys.path:
            sys.path.append(frame_interpolation_dir)
    if not os.path.exists(saved_model_dir):
        stat("downloading models")
        makedir(saved_model_dir)
        import gdown
        gdown.download_folder(id="1q8110-qp225asX3DQvZnfLfJPkCHmDpy", output=saved_model_dir)
        #run_sp(f"gdown -q --folder 1q8110-qp225asX3DQvZnfLfJPkCHmDpy -O {saved_model_dir}", cwd=frame_interpolation_dir, realtime=False) #1GhVNBPq20X7eaMsesydQ774CgGcDGkc6
        #run_sp(f"gdown 1C1YwOo293_yrgSS8tAyFbbVcMeXxzftE -O pretrained_models-20220214T214839Z-001.zip", cwd=frame_interpolation_dir, realtime=False) #1GhVNBPq20X7eaMsesydQ774CgGcDGkc6
        #run_sp('unzip -o "pretrained_models-20220214T214839Z-001.zip"', cwd=frame_interpolation_dir, realtime=False) #1GhVNBPq20X7eaMsesydQ774CgGcDGkc6
        #run_sp('rm -rf pretrained_models-20220214T214839Z-001.zip', cwd=frame_interpolation_dir, realtime=False) #1GhVNBPq20X7eaMsesydQ774CgGcDGkc6
    import ffmpeg
    saved_model = './pretrained_models/film_net/Style/saved_model' #os.path.join(saved_model_dir, 'film_net','Style','saved_model') #
    stat("copying photo frames")
    shutil.rmtree(photos_dir)
    makedir(photos_dir)
    #for f in os.listdir(photos_dir):
    #    os.remove(os.path.join(photos_dir, f))
    if isinstance(frames_dir, list):
        for n, f in enumerate(frames_dir):
            if isinstance(f, str):
                if f.endswith('png') or f.endswith('jpg'):
                    shutil.copy(f, os.path.join(photos_dir, f))
            else:
                f.save(os.path.join(photos_dir, str(n).zfill(4) + '.png'))
    else:
        for f in os.listdir(frames_dir):
            if f.endswith('png') or f.endswith('jpg'):
                fr = f[-8:] if len(f) > 8 else f
                shutil.copy(os.path.join(frames_dir, f), os.path.join(photos_dir, fr))
    #run_sp(f"mkdir -p frames", cwd=frames_dir, realtime=False)
    if bool(input_fps):
        recursive_interpolation_passes = int((output_fps - input_fps) / input_fps)
    else:
        recursive_interpolation_passes = recursive_interpolation_passes or 1
    stat("running frame-interpolation")
    try:
        run_sp(f"python -m eval.interpolator_cli --model_path '{saved_model}' --pattern '{photos_dir}' --fps {output_fps} --times_to_interpolate {recursive_interpolation_passes} --output_video", cwd=frame_interpolation_dir, realtime=True)
    except Exception as e:
        stat(f"error: {e}")
        print(f"interpolator_cli Error: {e}")
        return ""
    if os.path.exists(interpolated):
        if denoise or sharpen or deflicker:
            video = ffmpeg.input(interpolated)
            if deflicker:
                stat("deflicker")
                video = ffmpeg.filter(video, "deflicker", mode="pm", size=10)
            if sharpen:
                stat("sharpen")
                video = ffmpeg.filter(video, "unsharp")
            if denoise:
                stat("denoise")
                video = ffmpeg.filter(video, "nlmeans")
            stat("saving ffmpeg")
            meta = {'metadata': metadata} if bool(metadata) else {}
            #video.output(interpolated)
            ffmpeg.output(video, interpolated, vcodec='libx264', pix_fmt='yuv420p', **meta).run(overwrite_output=True)
        if output_video != None:
            if not output_video.endswith('mp4'):
                output_video = os.path.join(output_video, "interpolated.mp4")
            shutil.move(interpolated, output_video)
            return output_video
        else:
            return interpolated
    else:
        print(f"Failed to save video {interpolated}")
        return ""

def get_ffmpeg(installer=None):
    try:
        import ffmpeg
        if not hasattr(ffmpeg, 'input'):#'ffmpeg-python' not in ffmpeg.__file__:
          run_sp("pip uninstall --yes ffmpeg", realtime=False)
          run_sp("pip uninstall --yes ffmpeg-python", realtime=False)
          raise ImportError("Not ffmpeg-python")
    except ImportError as e:
        if installer is not None: installer.status("...installing ffmpeg-python")
        #run_sp("pip install -q ffmpeg", realtime=False)
        run_sp("pip install -q ffmpeg-python", realtime=False)
        import ffmpeg
        pass
    import platform
    system = platform.system()
    if system == "Windows":
        ffmpeg_exe = shutil.which('ffmpeg')
        if ffmpeg_exe:
            if installer is not None: installer.status()
            os.environ["FFMPEG_BINARY"] = ffmpeg_exe
            return
        if installer is not None: installer.status("...downloading ffmpeg")
        url = 'https://www.gyan.dev/ffmpeg/builds/ffmpeg-release-essentials.zip'
        zip_file = 'ffmpeg-release-essentials.zip'
        ffmpeg_zip = os.path.join(dist_dir, zip_file)
        download_cmd = f"curl -Lo {ffmpeg_zip} {url}"
        subprocess.run(download_cmd, shell=True, check=True)
        extract_cmd = f"unzip {ffmpeg_zip}"
        subprocess.run(extract_cmd, shell=True, check=True)
        ffmpeg_path = os.path.join(os.path.dirname(ffmpeg_zip), "ffmpeg", "bin")
        os.environ["FFMPEG_BINARY"] = ffmpeg_path
    elif system == "Linux":
        try:
            subprocess.run(["dpkg", "-l", "ffmpeg"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            return
        except subprocess.CalledProcessError:
            pass
        subprocess.run(["sudo", "apt", "install", "ffmpeg"], check=True)
    elif system == "Darwin":
        try:
            subprocess.run(["dpkg", "-l", "ffmpeg"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            return
        except subprocess.CalledProcessError:
            pass
        subprocess.run(["brew", "install", "ffmpeg"], check=True)
    else:
        print(f"FFmpeg installation not supported for {system} platform")
    if installer is not None: installer.status()
get_ffmpeg()
        
def frames_to_video(frames_dir, pattern="%04d.png", input_fps=None, output_fps=30, output_video=None, installer=None, denoise=False, sharpen=False, deflicker=False, metadata=None):
    get_ffmpeg()
    import ffmpeg
    def stat(msg):
        if installer is not None: installer.status(f"...{msg}")
    stat("frames_to_video")
    input_path = os.path.join(frames_dir, pattern)
    video = ffmpeg.input(input_path, framerate=input_fps or output_fps)
    video = video.filter('fps', fps=input_fps or output_fps, round='up')
    #video = video.pix_fmt('yuv420p')
    if input_fps is not None and input_fps != output_fps:
        stat("changing fps")
        #video = ffmpeg.filter(video, "fps", fps=output_fps, round="up")
        #video = ffmpeg.filter(video, "setpts=pts/TIMEBASE*%f" % (output_fps / input_fps))
        #video = video.filter("fps", fps=output_fps, round="up")
        #video = video.framerate(output_fps)
        video = ffmpeg.filter(video, "minterpolate", fps=output_fps, mi_mode="mci", mc_mode="aobmc")
    if deflicker:
        stat("deflicker")
        #video = video.filter("deflicker", mode="pm", size=10)
        video = ffmpeg.filter(video, "deflicker", mode="pm", size=10)
    if sharpen:
        stat("sharpen")
        #video = video.filter("unsharp", luma_msize_x=3, luma_msize_y=3, luma_amount=1.5)
        #video = video.filter("sharpen")
        video = ffmpeg.filter(video, "unsharp")
    if denoise:
        stat("denoise")
        #video = video.filter("hqdn3d")
        video = ffmpeg.filter(video, "nlmeans")
    if output_video != None:
        if not output_video.endswith('mp4'):
            output_video = os.path.join(output_video, "interpolated.mp4")
    else:
        output_video = os.path.join(os.path.dirname(frames_dir), "interpolated.mp4")
    stat("running ffmpeg")
    try:
        #out, err = ffmpeg.output(video, output_video, capture_stdout=True, capture_stderr=True, vcodec='libx264', pix_fmt='yuv420p').run(overwrite_output=True)
        meta = {'metadata': metadata} if bool(metadata) else {}
        ffmpeg.output(video, output_video, pix_fmt='yuv420p', **meta).run(overwrite_output=True)
        #print("ffmpeg output:", out)
    except ffmpeg.Error as e:
        print(f"ffmpeg error:{e.stderr} pattern: {pattern} path: {input_path} output: {output_video}")
        raise e
    #ffmpeg.output(video, output_video).run(overwrite_output=True)
    #video.output(output_video)
    return output_video

def video_to_frame(video_path: str, fps: int):
    import cv2
    vidcap = cv2.VideoCapture(video_path)
    video_fps = vidcap.get(cv2.CAP_PROP_FPS)
    if video_fps == 0:  # Handle case where FPS cannot be read
        raise ValueError("Cannot read the frame rate of the video.")
    interval = int(video_fps // fps)
    if interval == 0:  # Ensure interval is at least 1
        interval = 1
    success = True
    count = 0
    res = []
    while success:
        success, image = vidcap.read()
        if not success:
            break
        if count % interval == 0:
            if image is not None:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                res.append(image)
                if len(res) >= 8:
                    break
        count += 1
    vidcap.release()
    return res

def scale_video(video_path, to, max_size, multiple=16):
    try:
        from moviepy.editor import VideoFileClip
    except Exception:
        pip_install("moviepy")
        from moviepy.editor import VideoFileClip
        pass
    video = VideoFileClip(video_path)
    original_width, original_height = video.size
    width, height = scale_dimensions(original_width, original_height, max_size, multiple=multiple)
    resized_video = video.resize(width=width, height=height)
    resized_video.write_videofile(to)
    return width, height

class VideoPlayer(Stack):
    def __init__(self, video_file="", width=500, height=500):
        super().__init__()
        self.video_file = video_file
        self.width = width
        self.height = height
        self.build()
    def build(self):
        import threading
        import time
        import base64
        cap = cv2.VideoCapture(self.video_file)
        b64_string = None
        blank_img = np.zeros((self.width, self.height, 1), dtype = "uint8")#cv2.imencode('.jpg', frame)
        b64_string = base64.b64encode(blank_img[1]).decode('utf-8')
        image_box = ft.Image(src_base64=b64_string, width=self.width, height=self.height)
        video_container = ft.Container(image_box, alignment=ft.alignment.center, expand=True)
        def update_images():
            while(cap.isOpened()):
                # Capture frame-by-frame
                ret, frame = cap.read()
                if ret == True:
                    # encode the resulting frame
                    jpg_img = cv2.imencode('.jpg', frame)
                    b64_string = base64.b64encode(jpg_img[1]).decode('utf-8')
                    image_box.src_base64 = b64_string
                    self.page.update()
                else:
                    break
                time.sleep(1/115)
            # when video is finished
            #self.video_container.content.clean()
            #self.video_container.content = ft.Text("Video Ended", size=20)
            self.page.update()
        update_image_thread = threading.Thread(target=update_images)
        update_image_thread.daemon = True
        update_image_thread.start()
        return video_container

class AudioPlayer(Stack):
    def __init__(self, src="", display="", data=None, autoplay=False, page=None, show_wav=True):
        super().__init__()
        self.src = src
        self.display = display
        self.data = data if data != None else src
        self.page = page
        self.show_wav = show_wav
        self.state = "stopped"
        self.duration = 0
        self.loaded = False
        self.audio = Audio(src=asset_dir(self.src), autoplay=autoplay, on_state_changed=self.state_changed)
        self.page.overlay.append(self.audio)
        self.build()
    def play_audio(self, e):
        if self.state == "playing":
            self.audio.pause()
        elif self.state == "paused":
            self.audio.resume()
        else:
            self.audio.play()
    def state_changed(self, e):
        self.state = e.data
        #print(self.state)
        if e.data == "completed" or e.data == "paused":
            self.icon = icons.PLAY_CIRCLE_FILLED
        if e.data == "playing":
            self.icon = icons.PAUSE
        self.button.icon = self.icon
        self.button.update()
    def did_mount(self):
        self.page.update()
        try:
            self.duration = self.audio.get_duration()
        except Exception:
            self.duration = 0
            pass
        #print(duration)
        if self.duration > 0:
            dt = datetime.datetime.fromtimestamp(self.duration / 1000)
            dur = dt.strftime('%H:%M:%S')[:-3]
            self.button.tooltip = f"Duration: {dur}"
            try:
                self.button.update()
            except Exception: pass

    def generate_spectrogram_base64(self):
        try:
            import librosa
        except Exception:
            pip_install("librosa")
            import librosa
            pass
        try:
            import matplotlib.pyplot as plt
        except Exception:
            pip_install("matplotlib")
            import matplotlib.pyplot as plt
            pass
        from base64 import b64encode
        import matplotlib
        matplotlib.use('Agg')
        #print(self.loaded)
        if torch_device == "cpu":
            y, sr = librosa.load(asset_dir(self.src), sr=None)
            CQT = librosa.cqt(y, sr=sr, bins_per_octave=12, n_bins=84)
            CQT_db = librosa.amplitude_to_db(np.abs(CQT), ref=np.max)
        else:
            pip_install("nnAudio")
            from nnAudio import features
            #import torchaudio
            #waveform, sr = torchaudio.load(asset_dir(self.src))
            waveform, sr = librosa.load(asset_dir(self.src))
            waveform = torch.tensor(waveform, device=torch_device).float()
            #cqt_transform = torchaudio.transforms.ConstantQ(sample_rate=sr, n_bins=84, bins_per_octave=12, hop_length=512)
            #CQT = cqt_transform(waveform)
            #CQT_db = 20 * np.log10(CQT.numpy() + 1e-6)
            cqt_transform = features.CQT2010v2(sr=sr, hop_length=512, n_bins=84, bins_per_octave=12).to(torch_device)
            cqt_spectrogram = cqt_transform(waveform)
            CQT_db = cqt_spectrogram.squeeze(0).cpu().numpy()
            #CQT_db = cqt_spectrogram.log2()[0,:,:].detach().numpy()
        fig, ax = plt.subplots(figsize=(15, 1))
        librosa.display.specshow(CQT_db, sr=sr, x_axis=None, y_axis=None, ax=ax, cmap='inferno')
        ax.set_axis_off()
        #ax.text(0.01, 0.05, self.display, fontsize=11, fontweight='bold', ha='left', va='bottom', transform=ax.transAxes, color='white' if prefs['theme_mode'] == 'Dark' else 'black', bbox=dict(facecolor='none', edgecolor='none'))
        #ax.set_title(self.display)
        buf = io.BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0, transparent=True)
        plt.close(fig)
        buf.seek(0)
        base64_string = b64encode(buf.read()).decode('utf-8')
        return base64_string
    def build(self):
        if self.loaded: #Hack because it was building twice
            return Row([self.button, Column([self.wav_img, Text(self.display)], spacing=0, tight=True)])#self.row
        self.icon = icons.PLAY_CIRCLE_FILLED
        dur = ""#dt.strftime('%H:%M:%S.%f')[:-3]
        self.button = IconButton(icon=self.icon, icon_size=48, tooltip=f"Duration: {dur}", on_click=self.play_audio, data=self.data)
        #if self.duration == 0:
        #    return Row([self.button, Text("Loading Waveform...")])
        if self.show_wav:
            wav_img_src = self.generate_spectrogram_base64()
            self.wav_img = Img(src_base64=wav_img_src, fit=ImageFit.FIT_WIDTH)
            self.row = Row([self.button]) #, self.wav_img
        else:
            self.row = Row([self.button, Text(self.display)])
        self.loaded = True
        return self.row

port = 8500#8084
if tunnel_type == "ngrok":
    from pyngrok import conf, ngrok
    conf.get_default().ngrok_version = "v3"
    #public_url = ngrok.connect(port = str(port)).public_url
    public_url = ngrok.connect(port).public_url
elif tunnel_type == "localtunnel":
    import re
    localtunnel = subprocess.Popen(['lt', '--port', str(port), 'http'], stdout=subprocess.PIPE)
    url = str(localtunnel.stdout.readline())
    public_url = (re.search(r"(?P<url>https?:\/\/[^\s]+loca.lt)", url).group("url"))
else: public_url=""
from IPython.display import Javascript
if bool(public_url):
    import urllib
    if auto_launch_website:
        IPython.display(Javascript('window.open("{url}");'.format(url=public_url)))
    print("\nOpen URL in browser to launch app in tab: " + str(public_url))
    if tunnel_type == "localtunnel":
        print("Copy/Paste the Password/Enpoint IP for localtunnel:",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip("\n"))
elif tunnel_type == "Local_Colab":
    from google.colab import output
    port = 8084
    print("\nOpen URL in browser to launch app in tab: ")
    output.serve_kernel_port_as_window(port, anchor_text = "Open Colab URL")
def close_tab():
  IPython.display(Javascript("window.close('', '_parent', '');"))
#await google.colab.kernel.proxyPort(%s)
# Still not working to display app in Colab console, but tried.
def show_port(adr, height=500):
  IPython.display(Javascript("""
  (async ()=>{
    fm = document.createElement('iframe')
    fm.src = '%s'
    fm.width = '100%%'
    fm.height = '%d'
    fm.frameBorder = 0
    document.body.append(fm)
  })();
  """ % (adr, height) ))
#pip_install("PyDrive2|pydrive2")
#from pydrive2.auth import GoogleAuth
#gauth = GoogleAuth()
#gauth.LocalWebserverAuth()
#r = requests.get('http://localhost:4040/api/tunnels')
#url = r.json()['tunnels'][0]['public_url']
#print(url)
#await google.colab.kernel.proxyPort(%s)
#get_ipython().system_raw('python3 -m http.server 8888 &')
#get_ipython().system_raw('./ngrok http 8501 &')
#show_port(public_url.public_url)
#run_sp(f'python -m webbrowser -t "{public_url.public_url}"')
#webbrowser.open(public_url.public_url, new=0, autoraise=True)
#webbrowser.open_new_tab(public_url.public_url)
#import logging
#logging.getLogger("flet_core").setLevel(logging.DEBUG)
#logging.basicConfig(level=logging.DEBUG)
#ft.app(target=main, view=ft.WEB_BROWSER, port=8000, assets_dir=root_dir, upload_dir=root_dir, web_renderer="html")
os.environ["FLET_SECRET_KEY"] = "skquark"
if tunnel_type == "desktop":
  ft.app(target=main, assets_dir=root_dir, upload_dir=root_dir)
else:
  os.environ["FLET_FORCE_WEB_SERVER"] = "false"
  #print(f"Assets Dir: {os.path.abspath(os.path.join(root_dir, 'assets'))}") , host="0.0.0.0"
  ft.app(target=main, view=ft.WEB_BROWSER, port=port, assets_dir=os.path.abspath(root_dir), upload_dir=os.path.abspath(root_dir), use_color_emoji=True)
